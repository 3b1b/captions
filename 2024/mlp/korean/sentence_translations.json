[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "대규모 언어 모델에 '마이클 조던은 농구라는 종목을 한다'는 문구를 입력한 후 다음에 무엇이 나올지 예측하게 하면 농구를 정확하게 예측한다면, 이는 수천억 개의 매개변수 안에 특정 인물과 특정 스포츠에 대한 지식이 구워져 있다는 것을 의미할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "그리고 일반적으로 이러한 모델 중 하나를 사용해 본 사람이라면 누구나 수많은 사실을 암기하고 있다는 것을 분명히 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "따라서 합리적인 질문은 정확히 어떻게 작동하는가 하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "그렇다면 이러한 사실은 어디에 존재할까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "지난 12월, 구글 딥마인드의 몇몇 연구원들이 이 질문에 대한 연구 결과를 게시했는데, 운동선수와 종목을 매칭하는 구체적인 예를 사용했습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "사실들이 어떻게 저장되는지에 대한 완전한 기계론적 이해는 아직 해결되지 않았지만, 사실들이 이러한 네트워크의 특정 부분, 즉 다층 퍼셉트론 또는 줄여서 MLP로 알려진 네트워크 내부에 존재한다는 매우 일반적인 상위 수준의 결론을 포함하여 몇 가지 흥미로운 부분적인 결과를 얻었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "지난 몇 장에서 여러분과 저는 대규모 언어 모델의 기반이 되는 아키텍처인 트랜스포머와 다른 많은 최신 AI의 기반이 되는 아키텍처에 대해 자세히 살펴봤습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "가장 최근 챕터에서는 주의력이라는 작품에 집중했습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "그리고 여러분과 저의 다음 단계는 네트워크의 다른 큰 부분을 구성하는 이러한 다층 퍼셉트론 내부에서 어떤 일이 일어나는지 자세히 살펴보는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "여기서 계산은 사실 주의력과 비교하면 비교적 간단합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "본질적으로 한 쌍의 행렬 곱셈과 그 사이에 간단한 무언가가 있는 것으로 요약됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "그러나 이러한 계산을 해석하는 것은 매우 어려운 일입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "여기서 우리의 주요 목표는 계산을 단계별로 살펴보고 기억하기 쉽게 만드는 것이지만, 적어도 원칙적으로 이러한 블록 중 하나가 구체적인 사실을 어떻게 저장할 수 있는지에 대한 구체적인 예를 보여주는 맥락에서 설명하고자 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "특히 마이클 조던이 농구를 한다는 사실을 저장할 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "여기 레이아웃은 딥마인드 연구원 중 한 명인 닐 난다와 나눈 대화에서 영감을 얻었다는 점을 말씀드리고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "대부분의 경우 지난 두 챕터를 시청했거나 트랜스포머가 무엇인지에 대한 기본적인 이해가 있다고 가정하겠지만, 다시 한 번 복습하는 것도 나쁘지 않으니 전체적인 흐름을 간략하게 정리해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "여러분과 저는 텍스트를 받아 다음에 나올 내용을 예측하도록 훈련된 모델을 연구하고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "입력된 텍스트는 먼저 토큰, 즉 일반적으로 단어 또는 작은 단어 조각으로 이루어진 작은 덩어리로 나뉘며 각 토큰은 고차원 벡터, 즉 긴 숫자 목록과 연관되어 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "이 일련의 벡터는 서로 간에 정보를 전달할 수 있는 주의와 오늘 살펴볼 다층 퍼셉트론이라는 두 가지 연산을 반복적으로 거치고, 그 사이에 특정 정규화 단계도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "벡터의 시퀀스가 이 두 블록의 여러 가지 반복을 거친 후, 마지막에는 각 벡터가 문맥과 입력된 다른 모든 단어, 그리고 훈련을 통해 모델 가중치에 구워진 일반적인 지식으로부터 충분한 정보를 흡수하여 다음에 나올 토큰을 예측하는 데 사용할 수 있기를 바랍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "제가 여러분께 드리고 싶은 핵심 아이디어 중 하나는 이 모든 벡터는 매우 고차원적인 공간에 존재하며, 그 공간에 대해 생각할 때 방향에 따라 다른 종류의 의미를 인코딩할 수 있다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "제가 다시 언급하고 싶은 아주 고전적인 예는 여성이라는 내포어에서 남성이라는 내포어를 빼고, 그 작은 단계를 거쳐 삼촌과 같은 다른 남성 명사에 더하면 해당 여성 명사에 매우 가까운 어딘가에 도착한다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "이러한 의미에서 이 특정 방향은 성별 정보를 인코딩합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "이 초고차원 공간에서 다른 많은 뚜렷한 방향이 모델이 표현하고자 하는 다른 특징에 대응할 수 있다는 아이디어입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "하지만 트랜스포머에서 이러한 벡터는 단순히 단일 단어의 의미만 인코딩하는 것이 아닙니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "네트워크를 통해 흐르면서 주변의 모든 맥락과 모델의 지식을 바탕으로 훨씬 더 풍부한 의미를 담게 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "궁극적으로 각 단어는 다음에 일어날 일을 예측하기에 충분해야 하므로 한 단어의 의미를 훨씬 뛰어넘는 무언가를 인코딩해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "주의 블록을 통해 컨텍스트를 통합하는 방법을 이미 살펴봤지만, 대부분의 모델 매개변수는 실제로 MLP 블록 내부에 있으며, 그 기능에 대해 한 가지 생각할 수 있는 것은 사실을 저장할 수 있는 추가 용량을 제공한다는 점입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "앞서 말했듯이, 마이클 조던이 농구를 한다는 사실을 정확히 어떻게 저장할 수 있는지 구체적인 장난감의 예를 중심으로 교훈을 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "이제 이 장난감 예제를 통해 고차원 공간에 대한 몇 가지 가정을 해보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "먼저, 한 방향은 마이클이라는 이름의 아이디어를 나타내고, 거의 수직에 가까운 다른 방향은 조던이라는 성의 아이디어를 나타내고, 세 번째 방향은 농구라는 아이디어를 나타낸다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "즉, 네트워크에서 처리 중인 벡터 중 하나를 골라낸다면, 그 벡터의 도트 곱이 마이클이라는 이름의 방향이 하나라면, 그 벡터가 그 이름을 가진 사람의 아이디어를 인코딩하고 있다는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "그렇지 않으면 점의 곱이 0이거나 음수가 되어 벡터가 실제로 그 방향과 일치하지 않게 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "그리고 단순화를 위해 점 제품이 1보다 크면 어떤 의미가 있을까 하는 지극히 합리적인 질문은 완전히 무시해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "마찬가지로, 이러한 다른 방향의 도트 제품을 사용하면 조던이라는 성을 나타내는지 농구를 나타내는지 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "예를 들어 벡터가 마이클 조던이라는 전체 이름을 표현한다고 가정하면, 이 두 방향의 도트 곱은 하나가 되어야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "마이클 조던이라는 텍스트가 서로 다른 두 개의 토큰에 걸쳐 있기 때문에, 두 이름을 모두 인코딩할 수 있도록 이전 관심 블록이 두 번째 벡터에 정보를 성공적으로 전달했다고 가정해야 한다는 의미이기도 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "이 모든 것을 가정하고 이제 강의의 핵심을 살펴 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "다층 퍼셉트론 내부에서는 어떤 일이 일어나나요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "이 일련의 벡터가 블록으로 흘러들어간다고 생각할 수 있으며, 각 벡터는 원래 입력 텍스트의 토큰 중 하나와 연관되어 있다는 점을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "이 시퀀스의 각 개별 벡터는 짧은 일련의 연산을 거치고 잠시 후에 압축을 풀고 마지막에 동일한 차원의 다른 벡터를 얻게 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "다른 벡터는 원래 유입된 벡터에 더해지고, 그 합이 흘러나오는 결과입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "이 일련의 연산은 입력의 모든 토큰과 연관된 시퀀스의 모든 벡터에 적용되며, 모두 병렬로 이루어집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "특히 이 단계에서는 벡터가 서로 대화하지 않고 모두 각자의 작업을 수행합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "이 블록을 통해 벡터 중 하나에 어떤 일이 일어나는지 이해하면 모든 벡터에 어떤 일이 일어나는지 효과적으로 이해할 수 있기 때문에 여러분과 저에게는 훨씬 더 간단해집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "이 블록이 마이클 조던이 농구를 한다는 사실을 인코딩할 것이라는 말은, 마이클이라는 이름과 조던이라는 성을 인코딩하는 벡터가 들어오면 이 일련의 계산을 통해 해당 방향의 농구를 포함하는 무언가가 생성되고, 이것이 해당 위치의 벡터에 더해질 것이라는 의미입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "이 프로세스의 첫 번째 단계는 벡터에 매우 큰 행렬을 곱하는 것과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "이것이 바로 딥러닝이니 놀랄 일도 아닙니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "이 행렬은 앞서 살펴본 다른 모든 행렬과 마찬가지로 데이터에서 학습된 모델 매개변수로 채워져 있으며, 모델 동작을 결정하기 위해 조정되고 조정되는 여러 개의 노브와 다이얼로 생각할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "이제 행렬 곱셈에 대해 생각하는 좋은 방법 중 하나는 행렬의 각 행이 자체 벡터라고 상상하고, 그 행과 처리되는 벡터 사이에 여러 개의 점 곱을 취하는 것으로, 임베딩을 위해 E라고 라벨을 붙이겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "예를 들어, 첫 번째 행이 우리가 존재한다고 가정하고 있는 이름 Michael 방향과 같다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "즉, 이 출력의 첫 번째 구성 요소인 여기 이 점의 곱은 벡터가 이름 Michael을 인코딩하면 1이 되고, 그렇지 않으면 0 또는 음수가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "첫 번째 줄이 마이클이라는 이름에 조던이라는 성을 더한 방향이라면 어떤 의미가 있을지 잠시 생각해보면 더욱 재미있을 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "간단하게 설명하기 위해 M에 J를 더한 것으로 적어보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "그런 다음 이 E가 포함된 도트 제품을 가져가면 물건이 정말 멋지게 분포하므로 M 도트 E에 J 도트 E를 더한 것처럼 보입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "즉, 벡터가 마이클 조던이라는 전체 이름을 인코딩하면 최종값은 2가 되고, 그렇지 않으면 1 또는 1보다 작은 값이 된다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "이는 이 매트릭스의 한 행에 불과합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "다른 모든 행은 처리 중인 벡터의 다른 종류의 특징을 조사하면서 다른 종류의 질문을 병렬로 묻는다고 생각할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "이 단계에서는 데이터에서 학습한 모델 파라미터로 가득 찬 또 다른 벡터를 출력에 추가하는 경우가 많습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "이 다른 벡터를 바이어스라고 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "이 예제에서는 첫 번째 구성 요소의 바이어스 값이 음수이므로 최종 출력은 관련 도트 곱과 같지만 1을 뺀 값이라고 상상해 보시기 바랍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "왜 모델이 이것을 학습했다고 가정하길 원하는지 의아해하실 수도 있지만, 잠시 후에 벡터가 마이클 조던이라는 이름을 인코딩하는 경우에만 양수이고 그렇지 않은 경우에는 0 또는 음수인 값을 갖는 것이 왜 매우 깔끔하고 멋진지 알게 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "이 행렬의 총 행 수는 질문의 수와 같은 것으로, 우리가 추적해 온 GPT-3의 경우 그 수가 50,000개에 조금 못 미칩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "실제로 이 임베딩 공간의 차원 수는 정확히 4배에 달합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "이는 디자인 선택입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "더 많이 만들 수도 있고 더 적게 만들 수도 있지만, 깔끔한 배수를 사용하는 것이 하드웨어에 유리한 경향이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "가중치로 가득 찬 이 행렬은 우리를 더 높은 차원의 공간으로 매핑하기 때문에 약어로 W를 붙이겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "처리 중인 벡터를 계속 E로 표시하고 이 바이어스 벡터를 B로 표시한 다음 다이어그램에 다시 내려놓겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "이 시점에서 문제는 이 작업은 순전히 선형적이지만 언어는 매우 비선형적인 프로세스라는 점입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "만약 우리가 측정하는 항목이 마이클과 조던을 합쳐서 높은 수치를 기록한다면, 개념적으로는 관련이 없지만 마이클과 펠프스, 알렉시스와 조던을 합쳐서 어느 정도 유발된 것일 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "원하는 것은 전체 이름에 대한 간단한 예 또는 아니오입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "따라서 다음 단계는 이 큰 중간 벡터를 매우 간단한 비선형 함수를 통해 전달하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "일반적인 선택은 모든 음수 값을 0으로 매핑하고 모든 양수 값을 변경하지 않고 그대로 두는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "지나치게 화려한 이름의 딥러닝 전통을 이어받아 이 매우 간단한 함수를 정류 선형 단위, 줄여서 ReLU라고 부르기도 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "그래프는 다음과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "따라서 중간 벡터의 첫 번째 항목이 전체 이름이 마이클 조던인 경우에만 1이고 그렇지 않은 경우에는 0 또는 음수인 가상의 예를 들어보면, ReLU를 통과한 후에는 모든 0과 음수 값이 0으로 잘린 매우 깨끗한 값만 남게 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "따라서 이 출력은 마이클 조던이라는 이름이 있으면 1이 되고 그렇지 않으면 0이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "즉, AND 게이트의 동작을 매우 직접적으로 모방합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "모델에 따라 기본 모양은 같지만 조금 더 부드러워진 JLU라는 약간 수정된 기능을 사용하는 경우가 많습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "하지만 우리의 목적상 ReLU에 대해서만 생각하면 조금 더 깔끔해집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "또한 사람들이 트랜스포머의 뉴런을 언급하는 것은 바로 이 값에 대해 이야기하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "이 시리즈의 앞부분에서 살펴본 점으로 이루어진 레이어와 이전 레이어와 연결된 선으로 이루어진 일반적인 신경망 그림을 보셨다면, 이는 일반적으로 선형 단계와 행렬 곱셈, 그리고 ReLU와 같은 간단한 용어 중심의 비선형 함수의 조합을 전달하기 위한 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "이 값이 양수이면 이 뉴런은 활성 상태이고 값이 0이면 비활성 상태라고 말할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "다음 단계는 첫 번째 단계와 매우 유사합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "매우 큰 행렬을 곱하고 특정 편향 항을 추가합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "이 경우 출력의 차원 수는 해당 임베딩 공간의 크기로 다시 줄어들기 때문에 이것을 하향 투영 행렬이라고 부르겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "이번에는 행 단위로 생각하는 대신 열 단위로 생각하는 것이 더 낫습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "행렬 곱셈을 머릿속에 떠올릴 수 있는 또 다른 방법은 행렬의 각 열에 처리 중인 벡터의 해당 항을 곱한 다음 재조정된 모든 열을 더한다고 상상하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "이렇게 생각하는 것이 더 좋은 이유는 여기서 기둥은 임베딩 공간과 동일한 치수를 가지므로 해당 공간의 방향이라고 생각할 수 있기 때문입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "예를 들어, 모델이 첫 번째 기둥을 우리가 존재한다고 가정하는 이 농구 방향으로 만드는 법을 배웠다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "즉, 첫 번째 위치의 관련 뉴런이 활성화되면 이 열을 최종 결과에 추가한다는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "하지만 해당 뉴런이 비활성 상태라면, 즉 그 숫자가 0이라면 아무런 효과가 없을 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "꼭 농구만이 아니어도 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "이 모델은 이 열과 마이클 조던이라는 이름이 있는 다른 많은 기능에 연관시키고자 하는 다른 많은 기능도 구울 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "동시에 이 행렬의 다른 모든 열은 해당 뉴런이 활성화되면 최종 결과에 무엇이 추가될지 알려줍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "이 경우 편향이 있다면 뉴런 값과 상관없이 매번 추가되는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "이게 무슨 일인지 궁금하실 겁니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "여기의 모든 매개변수로 채워진 객체와 마찬가지로 정확히 말하기는 어렵습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "네트워크에 필요한 장부 정리가 있을 수 있지만 지금은 무시해도 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "표기를 좀 더 간결하게 하기 위해 이 큰 행렬을 W라고 부르고 마찬가지로 바이어스 벡터를 B라고 부르고 이를 다이어그램에 다시 넣겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "앞서 미리 살펴본 것처럼, 이 최종 결과를 해당 위치의 블록에 유입된 벡터에 더하면 이 최종 결과를 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "예를 들어, 유입되는 벡터가 마이클이라는 이름과 조던이라는 성을 모두 인코딩했다면, 이 연산 시퀀스가 AND 게이트를 트리거하기 때문에 농구 방향이 추가되어 나오는 것은 이 모든 것을 함께 인코딩하게 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "그리고 이것은 모든 벡터에서 동시에 일어나는 과정이라는 것을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "특히 GPT-3의 숫자를 고려하면, 이 블록에는 단순히 5만 개의 뉴런이 있는 것이 아니라 입력된 토큰 수의 5만 배에 달하는 토큰이 있다는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "이것이 전체 작업이며, 각각 바이어스가 추가되고 그 사이에 간단한 클리핑 기능이 있는 두 개의 매트릭스 제품입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "시리즈의 이전 동영상을 보신 분이라면 이 구조가 우리가 연구한 가장 기본적인 종류의 신경망이라는 것을 알 수 있을 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "이 예에서는 손으로 쓴 숫자를 인식하도록 학습되었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "여기서는 대규모 언어 모델을 위한 트랜스포머의 맥락에서, 이것은 더 큰 아키텍처의 한 부분이며 정확히 무엇을 하고 있는지 해석하려는 모든 시도는 정보를 고차원 임베딩 공간의 벡터로 인코딩하는 아이디어와 밀접하게 얽혀 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "이것이 핵심 교훈이지만, 저는 한 발 물러서서 두 가지 다른 것에 대해 생각해보고 싶습니다. 첫 번째는 일종의 장부 작성이고 두 번째는 트랜스포머를 파헤치기 전까지는 몰랐던 고차원에 대한 매우 생각하게 만드는 사실과 관련이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "지난 두 장에서 GPT-3의 총 매개변수 개수를 세어보고 정확히 어디에 있는지 살펴봤으니 여기서 빠르게 게임을 마무리해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "이 상향 투영 행렬의 행 수가 50,000개에 조금 못 미치고 각 행이 임베딩 공간의 크기와 일치한다는 점을 이미 언급했는데, GPT-3의 경우 12,288개입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "이를 곱하면 해당 행렬에 대한 매개변수만 6억 4천만 개가 되며, 아래쪽 투영은 모양만 바뀐 채로 동일한 수의 매개변수를 가집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "따라서 이 두 가지를 합치면 약 12억 개의 매개변수를 제공합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "바이어스 벡터는 몇 가지 매개 변수를 더 설명하지만 전체에서 차지하는 비율은 미미하므로 표시하지 않겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "GPT-3에서 이 임베딩 벡터 시퀀스는 하나가 아닌 96개의 서로 다른 MLP를 통해 흐르기 때문에 이 모든 블록에 할당된 파라미터의 총 수는 약 1,160억 개에 달합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "이는 네트워크 전체 파라미터의 약 3분의 2에 해당하는 수치이며, 여기에 관심 블록, 임베딩, 임베딩 해제 등 이전에 우리가 가지고 있던 모든 것을 더하면 실제로 광고된 대로 총 1,750억 개에 달합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "이 설명에서 간과한 정규화 단계와 관련된 또 다른 매개변수 집합이 있지만 편향 벡터와 마찬가지로 전체에서 차지하는 비중은 매우 미미합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "두 번째 성찰의 지점에 관해서는, 우리가 많은 시간을 할애한 이 중앙 장난감 예제가 실제 대규모 언어 모델에서 사실이 실제로 저장되는 방식을 반영하는지 궁금할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "첫 번째 행렬의 행은 이 임베딩 공간의 방향이라고 생각할 수 있으며, 이는 각 뉴런의 활성화가 주어진 벡터가 특정 방향과 얼마나 일치하는지를 알려준다는 의미입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "또한 두 번째 행렬의 열은 해당 뉴런이 활성화되면 결과에 무엇이 추가될지 알려줍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "이 두 가지 모두 수학적 사실일 뿐입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "그러나 증거에 따르면 개별 뉴런이 마이클 조던처럼 하나의 깨끗한 특징을 나타내는 경우는 매우 드물며, 요즘 해석 가능성 연구자들 사이에서 떠오르는 아이디어인 중첩과 관련된 매우 타당한 이유가 있을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "이는 모델이 특히 해석하기 어려운 이유와 놀랍도록 잘 확장되는 이유를 모두 설명하는 데 도움이 될 수 있는 가설입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "기본 개념은 n차원 공간이 있고 그 공간에서 서로 수직인 방향을 사용해 여러 가지 특징을 표현하고자 할 때, 한 방향으로 구성 요소를 추가해도 다른 방향에 영향을 주지 않는 방식으로, 맞출 수 있는 벡터의 최대 수는 차원 수인 n뿐이라는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "사실 수학자에게는 이것이 차원의 정의입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "하지만 이러한 제약을 조금 완화하고 약간의 소음을 용인한다면 흥미로운 일이 벌어질 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "이러한 특징을 정확히 직각이 아닌 89도에서 91도 사이의 거의 직각에 가까운 벡터로 표현할 수 있도록 허용한다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "우리가 2차원이나 3차원에 있었다면 이것은 아무런 차이가 없습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "따라서 더 많은 벡터를 넣을 수 있는 여유 공간이 거의 없기 때문에 차원이 높아질수록 답이 크게 달라진다는 점이 더욱 직관적이지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "100차원 벡터 목록을 생성하고 각 벡터가 무작위로 초기화되며 이 목록에는 10,000개의 고유 벡터가 포함되므로 차원 수의 100배에 달하는 벡터가 포함되는 엉성한 Python을 사용하여 이를 매우 빠르고 간단하게 설명해드릴 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "이 플롯은 이러한 벡터 쌍 사이의 각도 분포를 보여줍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "따라서 무작위로 시작했기 때문에 0도에서 180도까지 모든 각도가 가능하지만, 무작위 벡터의 경우에도 이미 90도에 가까워지는 편향이 심하다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "그런 다음 이러한 모든 벡터가 서로 더 수직이 되도록 반복적으로 조정하는 특정 최적화 프로세스를 실행합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "이 작업을 여러 번 반복한 후 각도의 분포는 다음과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "벡터 쌍 사이의 가능한 모든 각도가 89도에서 91도 사이의 좁은 범위 안에 있기 때문에 여기서 실제로 확대해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "일반적으로 존슨-린덴스트라우스 정리라는 것의 결과는 이렇게 거의 수직에 가까운 공간에 넣을 수 있는 벡터의 수가 차원 수에 따라 기하급수적으로 증가한다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "이는 독립적인 아이디어를 거의 수직에 가까운 방향으로 연결하면 이점을 얻을 수 있는 대규모 언어 모델에 매우 중요합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "즉, 할당된 공간의 크기보다 훨씬 더 많은 아이디어를 저장할 수 있다는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "이는 모델 성능이 규모에 따라 잘 확장되는 것처럼 보이는 이유를 부분적으로 설명할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "10배 더 넓은 공간에 10배 더 많은 독립적인 아이디어를 저장할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "그리고 이것은 모델을 흐르는 벡터가 있는 임베딩 공간뿐만 아니라 방금 연구한 다층 퍼셉트론의 중간에 있는 뉴런으로 가득 찬 벡터와도 관련이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "즉, GPT-3의 크기에서는 50,000개의 특징만 프로빙하는 것이 아니라, 공간의 거의 수직에 가까운 방향을 사용하여 이 엄청난 추가 용량을 활용한다면 처리 중인 벡터의 훨씬 더 많은 특징을 프로빙할 수 있다는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "하지만 만약 그렇게 했다면, 이는 개별 기능이 하나의 뉴런에 불이 켜지는 것처럼 보이지 않는다는 것을 의미합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "대신 뉴런의 특정 조합, 즉 중첩처럼 보여야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "더 자세히 알고 싶은 분들을 위해 관련 검색어로 '스파스 자동 인코더'가 있는데, 이는 모든 뉴런이 매우 겹쳐져 있어도 실제 특징이 무엇인지 추출하기 위해 사용하는 해석 가능성 도구입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "이와 관련된 몇 가지 훌륭한 인류학 관련 포스팅을 링크해 드리겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "이 시점에서 트랜스포머의 모든 세부 사항을 다루지는 않았지만, 여러분과 저는 가장 중요한 요점을 짚어 보았습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "다음 장에서 주로 다루고자 하는 것은 교육 과정입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "한편으로 훈련이 어떻게 작동하는지에 대한 짧은 대답은 모든 것이 역전파라는 것이며, 시리즈의 이전 챕터에서 역전파에 대해 별도의 맥락에서 다루었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "하지만 언어 모델에 사용되는 특정 비용 함수, 사람의 피드백을 통한 강화 학습을 이용한 미세 조정 아이디어, 스케일링 법칙의 개념 등 논의해야 할 것이 더 많습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "팔로워 여러분께 미리 알려드리자면, 다음 챕터를 만들기 전에 머신러닝과 관련 없는 동영상도 많이 준비 중이므로 시간이 좀 걸릴 수 있지만, 때가 되면 공개할 것을 약속드립니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "감사합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]