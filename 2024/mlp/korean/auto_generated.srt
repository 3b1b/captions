1
00:00:00,000 --> 00:00:03,741
대규모 언어 모델에 '마이클 조던은 농구라는 종목을 

2
00:00:03,741 --> 00:00:07,224
한다'는 문구를 입력한 후 다음에 무엇이 나올지 

3
00:00:07,224 --> 00:00:10,321
예측하게 하면 농구를 정확하게 예측한다면, 

4
00:00:10,321 --> 00:00:13,933
이는 수천억 개의 매개변수 안에 특정 인물과 특정 

5
00:00:13,933 --> 00:00:17,416
스포츠에 대한 지식이 구워져 있다는 것을 의미할 

6
00:00:17,416 --> 00:00:18,320
수 있습니다.

7
00:00:18,940 --> 00:00:21,003
그리고 일반적으로 이러한 모델 중 하나를 

8
00:00:21,003 --> 00:00:23,156
사용해 본 사람이라면 누구나 수많은 사실을 

9
00:00:23,156 --> 00:00:25,400
암기하고 있다는 것을 분명히 알 수 있습니다.

10
00:00:25,700 --> 00:00:27,380
따라서 합리적인 질문은 정확히 

11
00:00:27,380 --> 00:00:29,160
어떻게 작동하는가 하는 것입니다.

12
00:00:29,160 --> 00:00:31,040
그렇다면 이러한 사실은 어디에 존재할까요?

13
00:00:35,720 --> 00:00:38,603
지난 12월, 구글 딥마인드의 몇몇 연구원들이 

14
00:00:38,603 --> 00:00:41,153
이 질문에 대한 연구 결과를 게시했는데, 

15
00:00:41,153 --> 00:00:44,480
운동선수와 종목을 매칭하는 구체적인 예를 사용했습니다.

16
00:00:44,900 --> 00:00:48,043
사실들이 어떻게 저장되는지에 대한 완전한 기계론적 

17
00:00:48,043 --> 00:00:50,064
이해는 아직 해결되지 않았지만, 

18
00:00:50,064 --> 00:00:52,534
사실들이 이러한 네트워크의 특정 부분, 

19
00:00:52,534 --> 00:00:55,454
즉 다층 퍼셉트론 또는 줄여서 MLP로 알려진 

20
00:00:55,454 --> 00:00:58,373
네트워크 내부에 존재한다는 매우 일반적인 상위 

21
00:00:58,373 --> 00:01:01,517
수준의 결론을 포함하여 몇 가지 흥미로운 부분적인 

22
00:01:01,517 --> 00:01:02,640
결과를 얻었습니다.

23
00:01:03,120 --> 00:01:06,104
지난 몇 장에서 여러분과 저는 대규모 언어 모델의 

24
00:01:06,104 --> 00:01:09,195
기반이 되는 아키텍처인 트랜스포머와 다른 많은 최신 

25
00:01:09,195 --> 00:01:11,753
AI의 기반이 되는 아키텍처에 대해 자세히 

26
00:01:11,753 --> 00:01:12,500
살펴봤습니다.

27
00:01:13,060 --> 00:01:16,200
가장 최근 챕터에서는 주의력이라는 작품에 집중했습니다.

28
00:01:16,840 --> 00:01:19,640
그리고 여러분과 저의 다음 단계는 네트워크의 다른 

29
00:01:19,640 --> 00:01:22,440
큰 부분을 구성하는 이러한 다층 퍼셉트론 내부에서 

30
00:01:22,440 --> 00:01:25,040
어떤 일이 일어나는지 자세히 살펴보는 것입니다.

31
00:01:25,680 --> 00:01:27,961
여기서 계산은 사실 주의력과 

32
00:01:27,961 --> 00:01:30,100
비교하면 비교적 간단합니다.

33
00:01:30,560 --> 00:01:32,866
본질적으로 한 쌍의 행렬 곱셈과 그 사이에 

34
00:01:32,866 --> 00:01:34,980
간단한 무언가가 있는 것으로 요약됩니다.

35
00:01:35,720 --> 00:01:38,238
그러나 이러한 계산을 해석하는 

36
00:01:38,238 --> 00:01:40,460
것은 매우 어려운 일입니다.

37
00:01:41,560 --> 00:01:43,787
여기서 우리의 주요 목표는 계산을 단계별로 

38
00:01:43,787 --> 00:01:45,921
살펴보고 기억하기 쉽게 만드는 것이지만, 

39
00:01:45,921 --> 00:01:48,056
적어도 원칙적으로 이러한 블록 중 하나가 

40
00:01:48,056 --> 00:01:50,283
구체적인 사실을 어떻게 저장할 수 있는지에 

41
00:01:50,283 --> 00:01:52,788
대한 구체적인 예를 보여주는 맥락에서 설명하고자 

42
00:01:52,788 --> 00:01:53,160
합니다.

43
00:01:53,580 --> 00:01:55,658
특히 마이클 조던이 농구를 한다는 

44
00:01:55,658 --> 00:01:57,080
사실을 저장할 것입니다.

45
00:01:58,080 --> 00:01:59,731
여기 레이아웃은 딥마인드 연구원 중 

46
00:01:59,731 --> 00:02:01,300
한 명인 닐 난다와 나눈 대화에서 

47
00:02:01,300 --> 00:02:03,200
영감을 얻었다는 점을 말씀드리고 싶습니다.

48
00:02:04,060 --> 00:02:06,347
대부분의 경우 지난 두 챕터를 시청했거나 

49
00:02:06,347 --> 00:02:08,833
트랜스포머가 무엇인지에 대한 기본적인 이해가 

50
00:02:08,833 --> 00:02:11,517
있다고 가정하겠지만, 다시 한 번 복습하는 것도 

51
00:02:11,517 --> 00:02:14,103
나쁘지 않으니 전체적인 흐름을 간략하게 정리해 

52
00:02:14,103 --> 00:02:14,700
보겠습니다.

53
00:02:15,340 --> 00:02:18,036
여러분과 저는 텍스트를 받아 다음에 나올 

54
00:02:18,036 --> 00:02:21,320
내용을 예측하도록 훈련된 모델을 연구하고 있습니다.

55
00:02:21,720 --> 00:02:25,219
입력된 텍스트는 먼저 토큰, 즉 일반적으로 

56
00:02:25,219 --> 00:02:28,427
단어 또는 작은 단어 조각으로 이루어진 

57
00:02:28,427 --> 00:02:32,218
작은 덩어리로 나뉘며 각 토큰은 고차원 벡터, 

58
00:02:32,218 --> 00:02:35,280
즉 긴 숫자 목록과 연관되어 있습니다.

59
00:02:35,840 --> 00:02:40,087
이 일련의 벡터는 서로 간에 정보를 전달할 

60
00:02:40,087 --> 00:02:43,450
수 있는 주의와 오늘 살펴볼 다층 

61
00:02:43,450 --> 00:02:48,406
퍼셉트론이라는 두 가지 연산을 반복적으로 거치고, 

62
00:02:48,406 --> 00:02:52,300
그 사이에 특정 정규화 단계도 있습니다.

63
00:02:53,300 --> 00:02:57,560
벡터의 시퀀스가 이 두 블록의 여러 가지 반복을 

64
00:02:57,560 --> 00:03:02,135
거친 후, 마지막에는 각 벡터가 문맥과 입력된 다른 

65
00:03:02,135 --> 00:03:06,237
모든 단어, 그리고 훈련을 통해 모델 가중치에 

66
00:03:06,237 --> 00:03:10,813
구워진 일반적인 지식으로부터 충분한 정보를 흡수하여 

67
00:03:10,813 --> 00:03:15,231
다음에 나올 토큰을 예측하는 데 사용할 수 있기를 

68
00:03:15,231 --> 00:03:16,020
바랍니다.

69
00:03:16,860 --> 00:03:19,676
제가 여러분께 드리고 싶은 핵심 아이디어 중 

70
00:03:19,676 --> 00:03:22,604
하나는 이 모든 벡터는 매우 고차원적인 공간에 

71
00:03:22,604 --> 00:03:25,420
존재하며, 그 공간에 대해 생각할 때 방향에 

72
00:03:25,420 --> 00:03:28,800
따라 다른 종류의 의미를 인코딩할 수 있다는 것입니다.

73
00:03:30,120 --> 00:03:33,535
제가 다시 언급하고 싶은 아주 고전적인 예는 

74
00:03:33,535 --> 00:03:37,223
여성이라는 내포어에서 남성이라는 내포어를 빼고, 

75
00:03:37,223 --> 00:03:41,185
그 작은 단계를 거쳐 삼촌과 같은 다른 남성 명사에 

76
00:03:41,185 --> 00:03:44,737
더하면 해당 여성 명사에 매우 가까운 어딘가에 

77
00:03:44,737 --> 00:03:46,240
도착한다는 것입니다.

78
00:03:46,440 --> 00:03:48,937
이러한 의미에서 이 특정 방향은 

79
00:03:48,937 --> 00:03:50,880
성별 정보를 인코딩합니다.

80
00:03:51,640 --> 00:03:54,390
이 초고차원 공간에서 다른 많은 뚜렷한 

81
00:03:54,390 --> 00:03:56,890
방향이 모델이 표현하고자 하는 다른 

82
00:03:56,890 --> 00:03:59,640
특징에 대응할 수 있다는 아이디어입니다.

83
00:04:01,400 --> 00:04:03,741
하지만 트랜스포머에서 이러한 벡터는 단순히 

84
00:04:03,741 --> 00:04:06,180
단일 단어의 의미만 인코딩하는 것이 아닙니다.

85
00:04:06,680 --> 00:04:10,930
네트워크를 통해 흐르면서 주변의 모든 맥락과 모델의 

86
00:04:10,930 --> 00:04:15,180
지식을 바탕으로 훨씬 더 풍부한 의미를 담게 됩니다.

87
00:04:15,880 --> 00:04:18,432
궁극적으로 각 단어는 다음에 일어날 일을 

88
00:04:18,432 --> 00:04:20,763
예측하기에 충분해야 하므로 한 단어의 

89
00:04:20,763 --> 00:04:23,760
의미를 훨씬 뛰어넘는 무언가를 인코딩해야 합니다.

90
00:04:24,560 --> 00:04:27,254
주의 블록을 통해 컨텍스트를 통합하는 방법을 

91
00:04:27,254 --> 00:04:29,841
이미 살펴봤지만, 대부분의 모델 매개변수는 

92
00:04:29,841 --> 00:04:31,996
실제로 MLP 블록 내부에 있으며, 

93
00:04:31,996 --> 00:04:34,798
그 기능에 대해 한 가지 생각할 수 있는 것은 

94
00:04:34,798 --> 00:04:37,601
사실을 저장할 수 있는 추가 용량을 제공한다는 

95
00:04:37,601 --> 00:04:38,140
점입니다.

96
00:04:38,720 --> 00:04:41,061
앞서 말했듯이, 마이클 조던이 농구를 한다는 

97
00:04:41,061 --> 00:04:43,590
사실을 정확히 어떻게 저장할 수 있는지 구체적인 

98
00:04:43,590 --> 00:04:46,120
장난감의 예를 중심으로 교훈을 얻을 수 있습니다.

99
00:04:47,120 --> 00:04:49,343
이제 이 장난감 예제를 통해 고차원 

100
00:04:49,343 --> 00:04:51,900
공간에 대한 몇 가지 가정을 해보겠습니다.

101
00:04:52,360 --> 00:04:55,875
먼저, 한 방향은 마이클이라는 이름의 아이디어를 

102
00:04:55,875 --> 00:04:58,999
나타내고, 거의 수직에 가까운 다른 방향은 

103
00:04:58,999 --> 00:05:01,733
조던이라는 성의 아이디어를 나타내고, 

104
00:05:01,733 --> 00:05:05,118
세 번째 방향은 농구라는 아이디어를 나타낸다고 

105
00:05:05,118 --> 00:05:06,420
가정해 보겠습니다.

106
00:05:07,400 --> 00:05:10,957
즉, 네트워크에서 처리 중인 벡터 중 하나를 

107
00:05:10,957 --> 00:05:14,656
골라낸다면, 그 벡터의 도트 곱이 마이클이라는 

108
00:05:14,656 --> 00:05:18,356
이름의 방향이 하나라면, 그 벡터가 그 이름을 

109
00:05:18,356 --> 00:05:22,340
가진 사람의 아이디어를 인코딩하고 있다는 뜻입니다.

110
00:05:23,800 --> 00:05:26,250
그렇지 않으면 점의 곱이 0이거나 음수가 되어 

111
00:05:26,250 --> 00:05:28,700
벡터가 실제로 그 방향과 일치하지 않게 됩니다.

112
00:05:29,420 --> 00:05:31,386
그리고 단순화를 위해 점 제품이 1보다 

113
00:05:31,386 --> 00:05:33,263
크면 어떤 의미가 있을까 하는 지극히 

114
00:05:33,263 --> 00:05:35,320
합리적인 질문은 완전히 무시해 보겠습니다.

115
00:05:36,200 --> 00:05:38,680
마찬가지로, 이러한 다른 방향의 도트 

116
00:05:38,680 --> 00:05:40,806
제품을 사용하면 조던이라는 성을 

117
00:05:40,806 --> 00:05:43,760
나타내는지 농구를 나타내는지 알 수 있습니다.

118
00:05:44,740 --> 00:05:47,183
예를 들어 벡터가 마이클 조던이라는 

119
00:05:47,183 --> 00:05:49,504
전체 이름을 표현한다고 가정하면, 

120
00:05:49,504 --> 00:05:52,680
이 두 방향의 도트 곱은 하나가 되어야 합니다.

121
00:05:53,480 --> 00:05:56,527
마이클 조던이라는 텍스트가 서로 다른 두 개의 

122
00:05:56,527 --> 00:05:59,926
토큰에 걸쳐 있기 때문에, 두 이름을 모두 인코딩할 

123
00:05:59,926 --> 00:06:03,326
수 있도록 이전 관심 블록이 두 번째 벡터에 정보를 

124
00:06:03,326 --> 00:06:06,491
성공적으로 전달했다고 가정해야 한다는 의미이기도 

125
00:06:06,491 --> 00:06:06,960
합니다.

126
00:06:07,940 --> 00:06:09,656
이 모든 것을 가정하고 이제 

127
00:06:09,656 --> 00:06:11,480
강의의 핵심을 살펴 보겠습니다.

128
00:06:11,880 --> 00:06:14,980
다층 퍼셉트론 내부에서는 어떤 일이 일어나나요?

129
00:06:17,100 --> 00:06:20,035
이 일련의 벡터가 블록으로 흘러들어간다고 생각할 

130
00:06:20,035 --> 00:06:22,970
수 있으며, 각 벡터는 원래 입력 텍스트의 토큰 

131
00:06:22,970 --> 00:06:25,580
중 하나와 연관되어 있다는 점을 기억하세요.

132
00:06:26,080 --> 00:06:29,410
이 시퀀스의 각 개별 벡터는 짧은 일련의 

133
00:06:29,410 --> 00:06:32,450
연산을 거치고 잠시 후에 압축을 풀고 

134
00:06:32,450 --> 00:06:36,360
마지막에 동일한 차원의 다른 벡터를 얻게 됩니다.

135
00:06:36,880 --> 00:06:40,579
다른 벡터는 원래 유입된 벡터에 더해지고, 

136
00:06:40,579 --> 00:06:43,200
그 합이 흘러나오는 결과입니다.

137
00:06:43,720 --> 00:06:47,877
이 일련의 연산은 입력의 모든 토큰과 연관된 시퀀스의 

138
00:06:47,877 --> 00:06:51,620
모든 벡터에 적용되며, 모두 병렬로 이루어집니다.

139
00:06:52,100 --> 00:06:54,293
특히 이 단계에서는 벡터가 서로 대화하지 

140
00:06:54,293 --> 00:06:56,200
않고 모두 각자의 작업을 수행합니다.

141
00:06:56,720 --> 00:06:59,079
이 블록을 통해 벡터 중 하나에 어떤 일이 

142
00:06:59,079 --> 00:07:01,439
일어나는지 이해하면 모든 벡터에 어떤 일이 

143
00:07:01,439 --> 00:07:03,503
일어나는지 효과적으로 이해할 수 있기 

144
00:07:03,503 --> 00:07:06,060
때문에 여러분과 저에게는 훨씬 더 간단해집니다.

145
00:07:07,100 --> 00:07:10,133
이 블록이 마이클 조던이 농구를 한다는 사실을 

146
00:07:10,133 --> 00:07:13,051
인코딩할 것이라는 말은, 마이클이라는 이름과 

147
00:07:13,051 --> 00:07:16,551
조던이라는 성을 인코딩하는 벡터가 들어오면 이 일련의 

148
00:07:16,551 --> 00:07:19,819
계산을 통해 해당 방향의 농구를 포함하는 무언가가 

149
00:07:19,819 --> 00:07:23,319
생성되고, 이것이 해당 위치의 벡터에 더해질 것이라는 

150
00:07:23,319 --> 00:07:24,020
의미입니다.

151
00:07:25,600 --> 00:07:27,650
이 프로세스의 첫 번째 단계는 벡터에 

152
00:07:27,650 --> 00:07:29,700
매우 큰 행렬을 곱하는 것과 같습니다.

153
00:07:30,040 --> 00:07:31,980
이것이 바로 딥러닝이니 놀랄 일도 아닙니다.

154
00:07:32,680 --> 00:07:35,596
이 행렬은 앞서 살펴본 다른 모든 행렬과 마찬가지로 

155
00:07:35,596 --> 00:07:38,411
데이터에서 학습된 모델 매개변수로 채워져 있으며, 

156
00:07:38,411 --> 00:07:41,227
모델 동작을 결정하기 위해 조정되고 조정되는 여러 

157
00:07:41,227 --> 00:07:43,540
개의 노브와 다이얼로 생각할 수 있습니다.

158
00:07:44,500 --> 00:07:47,348
이제 행렬 곱셈에 대해 생각하는 좋은 방법 중 

159
00:07:47,348 --> 00:07:50,306
하나는 행렬의 각 행이 자체 벡터라고 상상하고, 

160
00:07:50,306 --> 00:07:53,374
그 행과 처리되는 벡터 사이에 여러 개의 점 곱을 

161
00:07:53,374 --> 00:07:56,113
취하는 것으로, 임베딩을 위해 E라고 라벨을 

162
00:07:56,113 --> 00:07:56,880
붙이겠습니다.

163
00:07:57,280 --> 00:07:59,351
예를 들어, 첫 번째 행이 우리가 

164
00:07:59,351 --> 00:08:01,205
존재한다고 가정하고 있는 이름 

165
00:08:01,205 --> 00:08:04,040
Michael 방향과 같다고 가정해 보겠습니다.

166
00:08:04,320 --> 00:08:07,642
즉, 이 출력의 첫 번째 구성 요소인 여기 이 

167
00:08:07,642 --> 00:08:11,221
점의 곱은 벡터가 이름 Michael을 인코딩하면 

168
00:08:11,221 --> 00:08:14,800
1이 되고, 그렇지 않으면 0 또는 음수가 됩니다.

169
00:08:15,880 --> 00:08:18,451
첫 번째 줄이 마이클이라는 이름에 조던이라는 

170
00:08:18,451 --> 00:08:20,817
성을 더한 방향이라면 어떤 의미가 있을지 

171
00:08:20,817 --> 00:08:23,080
잠시 생각해보면 더욱 재미있을 것입니다.

172
00:08:23,700 --> 00:08:25,450
간단하게 설명하기 위해 M에 

173
00:08:25,450 --> 00:08:27,420
J를 더한 것으로 적어보겠습니다.

174
00:08:28,080 --> 00:08:30,188
그런 다음 이 E가 포함된 도트 제품을 

175
00:08:30,188 --> 00:08:32,488
가져가면 물건이 정말 멋지게 분포하므로 M 

176
00:08:32,488 --> 00:08:34,980
도트 E에 J 도트 E를 더한 것처럼 보입니다.

177
00:08:34,980 --> 00:08:37,860
즉, 벡터가 마이클 조던이라는 전체 이름을 

178
00:08:37,860 --> 00:08:40,020
인코딩하면 최종값은 2가 되고, 

179
00:08:40,020 --> 00:08:43,260
그렇지 않으면 1 또는 1보다 작은 값이 된다는 

180
00:08:43,260 --> 00:08:44,700
것을 알 수 있습니다.

181
00:08:45,340 --> 00:08:47,260
이는 이 매트릭스의 한 행에 불과합니다.

182
00:08:47,600 --> 00:08:50,371
다른 모든 행은 처리 중인 벡터의 다른 

183
00:08:50,371 --> 00:08:53,016
종류의 특징을 조사하면서 다른 종류의 

184
00:08:53,016 --> 00:08:56,040
질문을 병렬로 묻는다고 생각할 수 있습니다.

185
00:08:56,700 --> 00:08:59,516
이 단계에서는 데이터에서 학습한 모델 파라미터로 가득 

186
00:08:59,516 --> 00:09:02,240
찬 또 다른 벡터를 출력에 추가하는 경우가 많습니다.

187
00:09:02,240 --> 00:09:04,560
이 다른 벡터를 바이어스라고 합니다.

188
00:09:05,180 --> 00:09:08,550
이 예제에서는 첫 번째 구성 요소의 바이어스 

189
00:09:08,550 --> 00:09:11,920
값이 음수이므로 최종 출력은 관련 도트 곱과 

190
00:09:11,920 --> 00:09:15,560
같지만 1을 뺀 값이라고 상상해 보시기 바랍니다.

191
00:09:16,120 --> 00:09:19,232
왜 모델이 이것을 학습했다고 가정하길 원하는지 

192
00:09:19,232 --> 00:09:22,105
의아해하실 수도 있지만, 잠시 후에 벡터가 

193
00:09:22,105 --> 00:09:25,097
마이클 조던이라는 이름을 인코딩하는 경우에만 

194
00:09:25,097 --> 00:09:28,209
양수이고 그렇지 않은 경우에는 0 또는 음수인 

195
00:09:28,209 --> 00:09:31,322
값을 갖는 것이 왜 매우 깔끔하고 멋진지 알게 

196
00:09:31,322 --> 00:09:32,160
될 것입니다.

197
00:09:33,040 --> 00:09:36,827
이 행렬의 총 행 수는 질문의 수와 같은 것으로, 

198
00:09:36,827 --> 00:09:40,209
우리가 추적해 온 GPT-3의 경우 그 수가 

199
00:09:40,209 --> 00:09:42,780
50,000개에 조금 못 미칩니다.

200
00:09:43,100 --> 00:09:44,923
실제로 이 임베딩 공간의 차원 

201
00:09:44,923 --> 00:09:46,640
수는 정확히 4배에 달합니다.

202
00:09:46,920 --> 00:09:47,900
이는 디자인 선택입니다.

203
00:09:47,940 --> 00:09:49,924
더 많이 만들 수도 있고 더 적게 만들 수도 있지만, 

204
00:09:49,924 --> 00:09:51,380
깔끔한 배수를 사용하는 것이 하드웨어에 

205
00:09:51,380 --> 00:09:52,240
유리한 경향이 있습니다.

206
00:09:52,740 --> 00:09:55,935
가중치로 가득 찬 이 행렬은 우리를 더 높은 차원의 

207
00:09:55,935 --> 00:09:59,020
공간으로 매핑하기 때문에 약어로 W를 붙이겠습니다.

208
00:09:59,020 --> 00:10:02,818
처리 중인 벡터를 계속 E로 표시하고 이 바이어스 

209
00:10:02,818 --> 00:10:06,074
벡터를 B로 표시한 다음 다이어그램에 다시 

210
00:10:06,074 --> 00:10:07,160
내려놓겠습니다.

211
00:10:09,180 --> 00:10:12,444
이 시점에서 문제는 이 작업은 순전히 선형적이지만 

212
00:10:12,444 --> 00:10:15,360
언어는 매우 비선형적인 프로세스라는 점입니다.

213
00:10:15,880 --> 00:10:18,846
만약 우리가 측정하는 항목이 마이클과 조던을 

214
00:10:18,846 --> 00:10:20,981
합쳐서 높은 수치를 기록한다면, 

215
00:10:20,981 --> 00:10:23,947
개념적으로는 관련이 없지만 마이클과 펠프스, 

216
00:10:23,947 --> 00:10:26,794
알렉시스와 조던을 합쳐서 어느 정도 유발된 

217
00:10:26,794 --> 00:10:28,100
것일 수도 있습니다.

218
00:10:28,540 --> 00:10:30,322
원하는 것은 전체 이름에 대한 

219
00:10:30,322 --> 00:10:32,000
간단한 예 또는 아니오입니다.

220
00:10:32,900 --> 00:10:35,370
따라서 다음 단계는 이 큰 중간 벡터를 매우 

221
00:10:35,370 --> 00:10:37,840
간단한 비선형 함수를 통해 전달하는 것입니다.

222
00:10:38,360 --> 00:10:41,706
일반적인 선택은 모든 음수 값을 0으로 매핑하고 

223
00:10:41,706 --> 00:10:45,300
모든 양수 값을 변경하지 않고 그대로 두는 것입니다.

224
00:10:46,440 --> 00:10:49,355
지나치게 화려한 이름의 딥러닝 전통을 

225
00:10:49,355 --> 00:10:53,243
이어받아 이 매우 간단한 함수를 정류 선형 단위, 

226
00:10:53,243 --> 00:10:56,020
줄여서 ReLU라고 부르기도 합니다.

227
00:10:56,020 --> 00:10:57,880
그래프는 다음과 같습니다.

228
00:10:58,300 --> 00:11:01,433
따라서 중간 벡터의 첫 번째 항목이 전체 

229
00:11:01,433 --> 00:11:04,840
이름이 마이클 조던인 경우에만 1이고 그렇지 

230
00:11:04,840 --> 00:11:08,927
않은 경우에는 0 또는 음수인 가상의 예를 들어보면, 

231
00:11:08,927 --> 00:11:12,470
ReLU를 통과한 후에는 모든 0과 음수 값이 

232
00:11:12,470 --> 00:11:15,740
0으로 잘린 매우 깨끗한 값만 남게 됩니다.

233
00:11:16,100 --> 00:11:17,902
따라서 이 출력은 마이클 조던이라는 이름이 

234
00:11:17,902 --> 00:11:19,780
있으면 1이 되고 그렇지 않으면 0이 됩니다.

235
00:11:20,560 --> 00:11:22,397
즉, AND 게이트의 동작을 

236
00:11:22,397 --> 00:11:24,120
매우 직접적으로 모방합니다.

237
00:11:25,660 --> 00:11:28,733
모델에 따라 기본 모양은 같지만 조금 더 부드러워진 

238
00:11:28,733 --> 00:11:31,489
JLU라는 약간 수정된 기능을 사용하는 경우가 

239
00:11:31,489 --> 00:11:32,020
많습니다.

240
00:11:32,500 --> 00:11:33,949
하지만 우리의 목적상 ReLU에 

241
00:11:33,949 --> 00:11:35,720
대해서만 생각하면 조금 더 깔끔해집니다.

242
00:11:36,740 --> 00:11:39,571
또한 사람들이 트랜스포머의 뉴런을 언급하는 

243
00:11:39,571 --> 00:11:42,520
것은 바로 이 값에 대해 이야기하는 것입니다.

244
00:11:42,900 --> 00:11:46,238
이 시리즈의 앞부분에서 살펴본 점으로 이루어진 

245
00:11:46,238 --> 00:11:49,576
레이어와 이전 레이어와 연결된 선으로 이루어진 

246
00:11:49,576 --> 00:11:52,015
일반적인 신경망 그림을 보셨다면, 

247
00:11:52,015 --> 00:11:54,968
이는 일반적으로 선형 단계와 행렬 곱셈, 

248
00:11:54,968 --> 00:11:58,563
그리고 ReLU와 같은 간단한 용어 중심의 비선형 

249
00:11:58,563 --> 00:12:01,260
함수의 조합을 전달하기 위한 것입니다.

250
00:12:02,500 --> 00:12:05,581
이 값이 양수이면 이 뉴런은 활성 상태이고 

251
00:12:05,581 --> 00:12:08,920
값이 0이면 비활성 상태라고 말할 수 있습니다.

252
00:12:10,120 --> 00:12:12,380
다음 단계는 첫 번째 단계와 매우 유사합니다.

253
00:12:12,560 --> 00:12:16,580
매우 큰 행렬을 곱하고 특정 편향 항을 추가합니다.

254
00:12:16,980 --> 00:12:19,870
이 경우 출력의 차원 수는 해당 임베딩 

255
00:12:19,870 --> 00:12:22,498
공간의 크기로 다시 줄어들기 때문에 

256
00:12:22,498 --> 00:12:25,520
이것을 하향 투영 행렬이라고 부르겠습니다.

257
00:12:26,220 --> 00:12:28,661
이번에는 행 단위로 생각하는 대신 

258
00:12:28,661 --> 00:12:31,360
열 단위로 생각하는 것이 더 낫습니다.

259
00:12:31,860 --> 00:12:36,611
행렬 곱셈을 머릿속에 떠올릴 수 있는 또 다른 방법은 

260
00:12:36,611 --> 00:12:41,046
행렬의 각 열에 처리 중인 벡터의 해당 항을 곱한 

261
00:12:41,046 --> 00:12:45,640
다음 재조정된 모든 열을 더한다고 상상하는 것입니다.

262
00:12:46,840 --> 00:12:49,705
이렇게 생각하는 것이 더 좋은 이유는 여기서 

263
00:12:49,705 --> 00:12:52,570
기둥은 임베딩 공간과 동일한 치수를 가지므로 

264
00:12:52,570 --> 00:12:55,780
해당 공간의 방향이라고 생각할 수 있기 때문입니다.

265
00:12:56,140 --> 00:12:58,211
예를 들어, 모델이 첫 번째 기둥을 

266
00:12:58,211 --> 00:13:00,283
우리가 존재한다고 가정하는 이 농구 

267
00:13:00,283 --> 00:13:03,080
방향으로 만드는 법을 배웠다고 가정해 보겠습니다.

268
00:13:04,180 --> 00:13:07,617
즉, 첫 번째 위치의 관련 뉴런이 활성화되면 

269
00:13:07,617 --> 00:13:10,780
이 열을 최종 결과에 추가한다는 뜻입니다.

270
00:13:11,140 --> 00:13:13,088
하지만 해당 뉴런이 비활성 상태라면, 

271
00:13:13,088 --> 00:13:15,780
즉 그 숫자가 0이라면 아무런 효과가 없을 것입니다.

272
00:13:16,500 --> 00:13:18,060
꼭 농구만이 아니어도 됩니다.

273
00:13:18,220 --> 00:13:20,375
이 모델은 이 열과 마이클 조던이라는 

274
00:13:20,375 --> 00:13:22,839
이름이 있는 다른 많은 기능에 연관시키고자 

275
00:13:22,839 --> 00:13:25,200
하는 다른 많은 기능도 구울 수 있습니다.

276
00:13:26,980 --> 00:13:31,640
동시에 이 행렬의 다른 모든 열은 해당 뉴런이 

277
00:13:31,640 --> 00:13:36,660
활성화되면 최종 결과에 무엇이 추가될지 알려줍니다.

278
00:13:37,360 --> 00:13:40,512
이 경우 편향이 있다면 뉴런 값과 

279
00:13:40,512 --> 00:13:43,500
상관없이 매번 추가되는 것입니다.

280
00:13:44,060 --> 00:13:45,280
이게 무슨 일인지 궁금하실 겁니다.

281
00:13:45,540 --> 00:13:47,430
여기의 모든 매개변수로 채워진 객체와 

282
00:13:47,430 --> 00:13:49,320
마찬가지로 정확히 말하기는 어렵습니다.

283
00:13:49,320 --> 00:13:51,914
네트워크에 필요한 장부 정리가 있을 

284
00:13:51,914 --> 00:13:54,380
수 있지만 지금은 무시해도 됩니다.

285
00:13:54,860 --> 00:13:57,667
표기를 좀 더 간결하게 하기 위해 이 큰 

286
00:13:57,667 --> 00:14:00,475
행렬을 W라고 부르고 마찬가지로 바이어스 

287
00:14:00,475 --> 00:14:03,527
벡터를 B라고 부르고 이를 다이어그램에 다시 

288
00:14:03,527 --> 00:14:04,260
넣겠습니다.

289
00:14:04,740 --> 00:14:07,315
앞서 미리 살펴본 것처럼, 이 최종 

290
00:14:07,315 --> 00:14:10,277
결과를 해당 위치의 블록에 유입된 벡터에 

291
00:14:10,277 --> 00:14:13,240
더하면 이 최종 결과를 얻을 수 있습니다.

292
00:14:13,820 --> 00:14:17,409
예를 들어, 유입되는 벡터가 마이클이라는 이름과 

293
00:14:17,409 --> 00:14:20,067
조던이라는 성을 모두 인코딩했다면, 

294
00:14:20,067 --> 00:14:23,922
이 연산 시퀀스가 AND 게이트를 트리거하기 때문에 

295
00:14:23,922 --> 00:14:27,910
농구 방향이 추가되어 나오는 것은 이 모든 것을 함께 

296
00:14:27,910 --> 00:14:29,240
인코딩하게 됩니다.

297
00:14:29,820 --> 00:14:32,010
그리고 이것은 모든 벡터에서 동시에 

298
00:14:32,010 --> 00:14:34,200
일어나는 과정이라는 것을 기억하세요.

299
00:14:34,800 --> 00:14:37,195
특히 GPT-3의 숫자를 고려하면, 

300
00:14:37,195 --> 00:14:40,428
이 블록에는 단순히 5만 개의 뉴런이 있는 것이 

301
00:14:40,428 --> 00:14:43,782
아니라 입력된 토큰 수의 5만 배에 달하는 토큰이 

302
00:14:43,782 --> 00:14:44,860
있다는 뜻입니다.

303
00:14:48,180 --> 00:14:50,586
이것이 전체 작업이며, 각각 바이어스가 

304
00:14:50,586 --> 00:14:52,664
추가되고 그 사이에 간단한 클리핑 

305
00:14:52,664 --> 00:14:55,180
기능이 있는 두 개의 매트릭스 제품입니다.

306
00:14:56,080 --> 00:14:58,260
시리즈의 이전 동영상을 보신 분이라면 이 

307
00:14:58,260 --> 00:15:00,534
구조가 우리가 연구한 가장 기본적인 종류의 

308
00:15:00,534 --> 00:15:02,620
신경망이라는 것을 알 수 있을 것입니다.

309
00:15:03,080 --> 00:15:04,736
이 예에서는 손으로 쓴 숫자를 

310
00:15:04,736 --> 00:15:06,100
인식하도록 학습되었습니다.

311
00:15:06,580 --> 00:15:10,005
여기서는 대규모 언어 모델을 위한 트랜스포머의 

312
00:15:10,005 --> 00:15:13,562
맥락에서, 이것은 더 큰 아키텍처의 한 부분이며 

313
00:15:13,562 --> 00:15:16,724
정확히 무엇을 하고 있는지 해석하려는 모든 

314
00:15:16,724 --> 00:15:19,886
시도는 정보를 고차원 임베딩 공간의 벡터로 

315
00:15:19,886 --> 00:15:23,180
인코딩하는 아이디어와 밀접하게 얽혀 있습니다.

316
00:15:24,260 --> 00:15:27,024
이것이 핵심 교훈이지만, 저는 한 발 물러서서 

317
00:15:27,024 --> 00:15:29,788
두 가지 다른 것에 대해 생각해보고 싶습니다. 

318
00:15:29,788 --> 00:15:32,339
첫 번째는 일종의 장부 작성이고 두 번째는 

319
00:15:32,339 --> 00:15:35,103
트랜스포머를 파헤치기 전까지는 몰랐던 고차원에 

320
00:15:35,103 --> 00:15:38,080
대한 매우 생각하게 만드는 사실과 관련이 있습니다.

321
00:15:41,080 --> 00:15:44,129
지난 두 장에서 GPT-3의 총 매개변수 

322
00:15:44,129 --> 00:15:46,914
개수를 세어보고 정확히 어디에 있는지 

323
00:15:46,914 --> 00:15:50,760
살펴봤으니 여기서 빠르게 게임을 마무리해 보겠습니다.

324
00:15:51,400 --> 00:15:54,446
이 상향 투영 행렬의 행 수가 50,000개에 

325
00:15:54,446 --> 00:15:57,024
조금 못 미치고 각 행이 임베딩 공간의 

326
00:15:57,024 --> 00:15:59,719
크기와 일치한다는 점을 이미 언급했는데, 

327
00:15:59,719 --> 00:16:02,180
GPT-3의 경우 12,288개입니다.

328
00:16:03,240 --> 00:16:06,604
이를 곱하면 해당 행렬에 대한 매개변수만 

329
00:16:06,604 --> 00:16:09,823
6억 4천만 개가 되며, 아래쪽 투영은 

330
00:16:09,823 --> 00:16:13,920
모양만 바뀐 채로 동일한 수의 매개변수를 가집니다.

331
00:16:14,500 --> 00:16:15,910
따라서 이 두 가지를 합치면 약 

332
00:16:15,910 --> 00:16:17,400
12억 개의 매개변수를 제공합니다.

333
00:16:18,280 --> 00:16:21,093
바이어스 벡터는 몇 가지 매개 변수를 더 설명하지만 

334
00:16:21,093 --> 00:16:23,518
전체에서 차지하는 비율은 미미하므로 표시하지 

335
00:16:23,518 --> 00:16:24,100
않겠습니다.

336
00:16:24,660 --> 00:16:27,699
GPT-3에서 이 임베딩 벡터 시퀀스는 

337
00:16:27,699 --> 00:16:30,876
하나가 아닌 96개의 서로 다른 MLP를 

338
00:16:30,876 --> 00:16:34,191
통해 흐르기 때문에 이 모든 블록에 할당된 

339
00:16:34,191 --> 00:16:38,060
파라미터의 총 수는 약 1,160억 개에 달합니다.

340
00:16:38,820 --> 00:16:41,713
이는 네트워크 전체 파라미터의 약 3분의 2에 

341
00:16:41,713 --> 00:16:44,719
해당하는 수치이며, 여기에 관심 블록, 임베딩, 

342
00:16:44,719 --> 00:16:47,724
임베딩 해제 등 이전에 우리가 가지고 있던 모든 

343
00:16:47,724 --> 00:16:50,729
것을 더하면 실제로 광고된 대로 총 1,750억 

344
00:16:50,729 --> 00:16:51,620
개에 달합니다.

345
00:16:53,060 --> 00:16:56,560
이 설명에서 간과한 정규화 단계와 관련된 또 

346
00:16:56,560 --> 00:16:59,780
다른 매개변수 집합이 있지만 편향 벡터와 

347
00:16:59,780 --> 00:17:03,840
마찬가지로 전체에서 차지하는 비중은 매우 미미합니다.

348
00:17:05,900 --> 00:17:07,835
두 번째 성찰의 지점에 관해서는, 

349
00:17:07,835 --> 00:17:10,280
우리가 많은 시간을 할애한 이 중앙 장난감 

350
00:17:10,280 --> 00:17:12,623
예제가 실제 대규모 언어 모델에서 사실이 

351
00:17:12,623 --> 00:17:15,680
실제로 저장되는 방식을 반영하는지 궁금할 수 있습니다.

352
00:17:16,319 --> 00:17:19,032
첫 번째 행렬의 행은 이 임베딩 공간의 

353
00:17:19,032 --> 00:17:21,128
방향이라고 생각할 수 있으며, 

354
00:17:21,128 --> 00:17:23,841
이는 각 뉴런의 활성화가 주어진 벡터가 

355
00:17:23,841 --> 00:17:27,540
특정 방향과 얼마나 일치하는지를 알려준다는 의미입니다.

356
00:17:27,760 --> 00:17:30,840
또한 두 번째 행렬의 열은 해당 뉴런이 

357
00:17:30,840 --> 00:17:34,340
활성화되면 결과에 무엇이 추가될지 알려줍니다.

358
00:17:34,640 --> 00:17:36,800
이 두 가지 모두 수학적 사실일 뿐입니다.

359
00:17:37,740 --> 00:17:41,659
그러나 증거에 따르면 개별 뉴런이 마이클 조던처럼 

360
00:17:41,659 --> 00:17:45,719
하나의 깨끗한 특징을 나타내는 경우는 매우 드물며, 

361
00:17:45,719 --> 00:17:49,219
요즘 해석 가능성 연구자들 사이에서 떠오르는 

362
00:17:49,219 --> 00:17:53,139
아이디어인 중첩과 관련된 매우 타당한 이유가 있을 

363
00:17:53,139 --> 00:17:54,120
수 있습니다.

364
00:17:54,640 --> 00:17:57,351
이는 모델이 특히 해석하기 어려운 이유와 

365
00:17:57,351 --> 00:18:00,180
놀랍도록 잘 확장되는 이유를 모두 설명하는 

366
00:18:00,180 --> 00:18:02,420
데 도움이 될 수 있는 가설입니다.

367
00:18:03,500 --> 00:18:07,651
기본 개념은 n차원 공간이 있고 그 공간에서 서로 

368
00:18:07,651 --> 00:18:11,802
수직인 방향을 사용해 여러 가지 특징을 표현하고자 

369
00:18:11,802 --> 00:18:15,805
할 때, 한 방향으로 구성 요소를 추가해도 다른 

370
00:18:15,805 --> 00:18:18,770
방향에 영향을 주지 않는 방식으로, 

371
00:18:18,770 --> 00:18:22,329
맞출 수 있는 벡터의 최대 수는 차원 수인 

372
00:18:22,329 --> 00:18:23,960
n뿐이라는 것입니다.

373
00:18:24,600 --> 00:18:27,620
사실 수학자에게는 이것이 차원의 정의입니다.

374
00:18:28,220 --> 00:18:30,647
하지만 이러한 제약을 조금 완화하고 약간의 

375
00:18:30,647 --> 00:18:33,580
소음을 용인한다면 흥미로운 일이 벌어질 수 있습니다.

376
00:18:34,180 --> 00:18:37,436
이러한 특징을 정확히 직각이 아닌 89도에서 

377
00:18:37,436 --> 00:18:40,432
91도 사이의 거의 직각에 가까운 벡터로 

378
00:18:40,432 --> 00:18:43,820
표현할 수 있도록 허용한다고 가정해 보겠습니다.

379
00:18:44,820 --> 00:18:46,549
우리가 2차원이나 3차원에 있었다면 

380
00:18:46,549 --> 00:18:48,020
이것은 아무런 차이가 없습니다.

381
00:18:48,260 --> 00:18:50,915
따라서 더 많은 벡터를 넣을 수 있는 여유 

382
00:18:50,915 --> 00:18:53,571
공간이 거의 없기 때문에 차원이 높아질수록 

383
00:18:53,571 --> 00:18:56,780
답이 크게 달라진다는 점이 더욱 직관적이지 않습니다.

384
00:18:57,660 --> 00:19:00,658
100차원 벡터 목록을 생성하고 각 벡터가 

385
00:19:00,658 --> 00:19:04,031
무작위로 초기화되며 이 목록에는 10,000개의 

386
00:19:04,031 --> 00:19:07,154
고유 벡터가 포함되므로 차원 수의 100배에 

387
00:19:07,154 --> 00:19:10,277
달하는 벡터가 포함되는 엉성한 Python을 

388
00:19:10,277 --> 00:19:13,525
사용하여 이를 매우 빠르고 간단하게 설명해드릴 

389
00:19:13,525 --> 00:19:14,400
수 있습니다.

390
00:19:15,320 --> 00:19:18,039
이 플롯은 이러한 벡터 쌍 사이의 

391
00:19:18,039 --> 00:19:19,900
각도 분포를 보여줍니다.

392
00:19:20,680 --> 00:19:23,500
따라서 무작위로 시작했기 때문에 0도에서 

393
00:19:23,500 --> 00:19:26,074
180도까지 모든 각도가 가능하지만, 

394
00:19:26,074 --> 00:19:28,649
무작위 벡터의 경우에도 이미 90도에 

395
00:19:28,649 --> 00:19:31,960
가까워지는 편향이 심하다는 것을 알 수 있습니다.

396
00:19:32,500 --> 00:19:37,010
그런 다음 이러한 모든 벡터가 서로 더 수직이 되도록 

397
00:19:37,010 --> 00:19:41,520
반복적으로 조정하는 특정 최적화 프로세스를 실행합니다.

398
00:19:42,060 --> 00:19:44,360
이 작업을 여러 번 반복한 후 

399
00:19:44,360 --> 00:19:46,660
각도의 분포는 다음과 같습니다.

400
00:19:47,120 --> 00:19:50,039
벡터 쌍 사이의 가능한 모든 각도가 

401
00:19:50,039 --> 00:19:53,396
89도에서 91도 사이의 좁은 범위 안에 

402
00:19:53,396 --> 00:19:56,900
있기 때문에 여기서 실제로 확대해야 합니다.

403
00:19:58,020 --> 00:20:01,119
일반적으로 존슨-린덴스트라우스 정리라는 

404
00:20:01,119 --> 00:20:04,218
것의 결과는 이렇게 거의 수직에 가까운 

405
00:20:04,218 --> 00:20:07,318
공간에 넣을 수 있는 벡터의 수가 차원 

406
00:20:07,318 --> 00:20:10,840
수에 따라 기하급수적으로 증가한다는 것입니다.

407
00:20:11,960 --> 00:20:14,442
이는 독립적인 아이디어를 거의 수직에 

408
00:20:14,442 --> 00:20:16,924
가까운 방향으로 연결하면 이점을 얻을 

409
00:20:16,924 --> 00:20:19,880
수 있는 대규모 언어 모델에 매우 중요합니다.

410
00:20:20,000 --> 00:20:23,005
즉, 할당된 공간의 크기보다 훨씬 더 

411
00:20:23,005 --> 00:20:26,440
많은 아이디어를 저장할 수 있다는 뜻입니다.

412
00:20:27,320 --> 00:20:29,321
이는 모델 성능이 규모에 따라 잘 확장되는 

413
00:20:29,321 --> 00:20:31,740
것처럼 보이는 이유를 부분적으로 설명할 수 있습니다.

414
00:20:32,540 --> 00:20:35,970
10배 더 넓은 공간에 10배 더 많은 

415
00:20:35,970 --> 00:20:39,400
독립적인 아이디어를 저장할 수 있습니다.

416
00:20:40,420 --> 00:20:43,679
그리고 이것은 모델을 흐르는 벡터가 있는 임베딩 

417
00:20:43,679 --> 00:20:46,697
공간뿐만 아니라 방금 연구한 다층 퍼셉트론의 

418
00:20:46,697 --> 00:20:49,836
중간에 있는 뉴런으로 가득 찬 벡터와도 관련이 

419
00:20:49,836 --> 00:20:50,440
있습니다.

420
00:20:50,960 --> 00:20:54,242
즉, GPT-3의 크기에서는 50,000개의 

421
00:20:54,242 --> 00:20:56,605
특징만 프로빙하는 것이 아니라, 

422
00:20:56,605 --> 00:20:59,756
공간의 거의 수직에 가까운 방향을 사용하여 

423
00:20:59,756 --> 00:21:03,038
이 엄청난 추가 용량을 활용한다면 처리 중인 

424
00:21:03,038 --> 00:21:06,058
벡터의 훨씬 더 많은 특징을 프로빙할 수 

425
00:21:06,058 --> 00:21:07,240
있다는 뜻입니다.

426
00:21:07,780 --> 00:21:10,070
하지만 만약 그렇게 했다면, 이는 개별 

427
00:21:10,070 --> 00:21:12,049
기능이 하나의 뉴런에 불이 켜지는 

428
00:21:12,049 --> 00:21:14,340
것처럼 보이지 않는다는 것을 의미합니다.

429
00:21:14,660 --> 00:21:19,380
대신 뉴런의 특정 조합, 즉 중첩처럼 보여야 합니다.

430
00:21:20,400 --> 00:21:23,609
더 자세히 알고 싶은 분들을 위해 관련 검색어로 

431
00:21:23,609 --> 00:21:25,867
'스파스 자동 인코더'가 있는데, 

432
00:21:25,867 --> 00:21:28,720
이는 모든 뉴런이 매우 겹쳐져 있어도 실제 

433
00:21:28,720 --> 00:21:31,691
특징이 무엇인지 추출하기 위해 사용하는 해석 

434
00:21:31,691 --> 00:21:32,880
가능성 도구입니다.

435
00:21:33,540 --> 00:21:35,211
이와 관련된 몇 가지 훌륭한 인류학 

436
00:21:35,211 --> 00:21:36,800
관련 포스팅을 링크해 드리겠습니다.

437
00:21:37,880 --> 00:21:39,600
이 시점에서 트랜스포머의 모든 세부 

438
00:21:39,600 --> 00:21:41,321
사항을 다루지는 않았지만, 여러분과 

439
00:21:41,321 --> 00:21:43,300
저는 가장 중요한 요점을 짚어 보았습니다.

440
00:21:43,520 --> 00:21:47,640
다음 장에서 주로 다루고자 하는 것은 교육 과정입니다.

441
00:21:48,460 --> 00:21:51,072
한편으로 훈련이 어떻게 작동하는지에 대한 짧은 

442
00:21:51,072 --> 00:21:53,182
대답은 모든 것이 역전파라는 것이며, 

443
00:21:53,182 --> 00:21:56,196
시리즈의 이전 챕터에서 역전파에 대해 별도의 맥락에서 

444
00:21:56,196 --> 00:21:56,900
다루었습니다.

445
00:21:57,220 --> 00:22:00,204
하지만 언어 모델에 사용되는 특정 비용 함수, 

446
00:22:00,204 --> 00:22:02,844
사람의 피드백을 통한 강화 학습을 이용한 

447
00:22:02,844 --> 00:22:05,599
미세 조정 아이디어, 스케일링 법칙의 개념 

448
00:22:05,599 --> 00:22:07,780
등 논의해야 할 것이 더 많습니다.

449
00:22:08,960 --> 00:22:11,213
팔로워 여러분께 미리 알려드리자면, 

450
00:22:11,213 --> 00:22:13,804
다음 챕터를 만들기 전에 머신러닝과 관련 

451
00:22:13,804 --> 00:22:16,395
없는 동영상도 많이 준비 중이므로 시간이 

452
00:22:16,395 --> 00:22:19,211
좀 걸릴 수 있지만, 때가 되면 공개할 것을 

453
00:22:19,211 --> 00:22:20,000
약속드립니다.

454
00:22:35,640 --> 00:22:37,920
감사합니다.

