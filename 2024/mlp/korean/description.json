[
 {
  "input": "Unpacking the multilayer perceptrons in a transformer, and how they may store facts",
  "translatedText": "트랜스포머의 다층 퍼셉트론과 이들이 사실을 저장하는 방법 풀기",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "이 강의는 스폰서 광고가 아닌 시청자가 직접 자금을 지원합니다: https://3b1b.co/support",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to share the videos.",
  "translatedText": "마찬가지로 가치 있는 형태의 지원은 동영상을 공유하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "AI Alignment forum post from the Deepmind researchers referenced at the video's start:",
  "translatedText": "동영상 시작 부분에 언급된 딥마인드 연구원의 AI 얼라인먼트 포럼 게시물을 참조하세요:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall",
  "translatedText": "https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Anthropic posts about superposition referenced near the end:",
  "translatedText": "마지막에 언급된 중첩에 대한 인문학적인 게시물입니다:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2022/toy_model/index.html",
  "translatedText": "https://transformer-circuits.pub/2022/toy_model/index.html",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2023/monosemantic-features",
  "translatedText": "https://transformer-circuits.pub/2023/monosemantic-features",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Sections:",
  "translatedText": "섹션:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Where facts in LLMs live",
  "translatedText": "0:00 - LLM의 사실 자료가 있는 곳",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "2:15 - Quick refresher on transformers",
  "translatedText": "2:15 - 트랜스포머에 대한 빠른 복습",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "4:39 - Assumptions for our toy example",
  "translatedText": "4:39 - 장난감 예제에 대한 가정",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "6:07 - Inside a multilayer perceptron",
  "translatedText": "6:07 - 다층 퍼셉트론의 내부",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "15:38 - Counting parameters",
  "translatedText": "15:38 - 매개변수 계산",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "17:04 - Superposition",
  "translatedText": "17:04 - 중첩",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "21:37 - Up next",
  "translatedText": "21:37 - 다음",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 }
]