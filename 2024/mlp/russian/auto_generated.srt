1
00:00:00,000 --> 00:00:04,260
Если ты скажешь большой языковой модели фразу "Майкл Джордан играет в 

2
00:00:04,260 --> 00:00:07,729
баскетбол" и попросишь ее предсказать, что будет дальше, 

3
00:00:07,729 --> 00:00:11,259
и она правильно предскажет баскетбол, это будет означать, 

4
00:00:11,259 --> 00:00:16,007
что где-то внутри ее сотен миллиардов параметров заложены знания о конкретном 

5
00:00:16,007 --> 00:00:18,320
человеке и его конкретном виде спорта.

6
00:00:18,940 --> 00:00:22,267
И я думаю, что в целом у любого, кто играл с одной из этих моделей, 

7
00:00:22,267 --> 00:00:25,400
возникает четкое ощущение, что он запомнил тонны и тонны фактов.

8
00:00:25,700 --> 00:00:29,160
Поэтому разумный вопрос, который ты можешь задать, - как именно это работает?

9
00:00:29,160 --> 00:00:31,040
И где живут эти факты?

10
00:00:35,720 --> 00:00:38,654
В декабре прошлого года несколько исследователей из Google DeepMind 

11
00:00:38,654 --> 00:00:40,898
опубликовали информацию о работе над этим вопросом, 

12
00:00:40,898 --> 00:00:44,480
и они использовали этот конкретный пример с подбором спортсменов к их видам спорта.

13
00:00:44,900 --> 00:00:49,767
И хотя полное механистическое понимание того, как хранятся факты, остается неразрешенным, 

14
00:00:49,767 --> 00:00:52,742
у них есть несколько интересных частичных результатов, 

15
00:00:52,742 --> 00:00:56,420
включая очень общий высокоуровневый вывод о том, что факты, похоже, 

16
00:00:56,420 --> 00:01:00,855
живут внутри определенной части этих сетей, причудливо известных как многослойные 

17
00:01:00,855 --> 00:01:02,640
перцептроны, или сокращенно MLPs.

18
00:01:03,120 --> 00:01:07,618
В последней паре глав мы с тобой копались в деталях трансформаторов, архитектуры, 

19
00:01:07,618 --> 00:01:12,500
лежащей в основе больших языковых моделей, а также в основе многих других современных ИИ.

20
00:01:13,060 --> 00:01:16,200
В последней главе мы сосредоточились на фрагменте под названием "Внимание".

21
00:01:16,840 --> 00:01:20,059
И следующим шагом для нас с тобой будет копание в деталях того, 

22
00:01:20,059 --> 00:01:22,776
что происходит внутри этих многослойных перцептронов, 

23
00:01:22,776 --> 00:01:25,040
которые составляют другую большую часть сети.

24
00:01:25,680 --> 00:01:28,178
На самом деле вычисления здесь относительно просты, 

25
00:01:28,178 --> 00:01:30,100
особенно если сравнивать их с вниманием.

26
00:01:30,560 --> 00:01:34,980
По сути, он сводится к паре матричных умножений с простым чем-то между ними.

27
00:01:35,720 --> 00:01:40,460
Однако интерпретировать то, что делают эти вычисления, очень сложно.

28
00:01:41,560 --> 00:01:45,560
Наша главная цель здесь - пройтись по вычислениям и сделать их запоминающимися, 

29
00:01:45,560 --> 00:01:49,410
но я бы хотел сделать это в контексте демонстрации конкретного примера того, 

30
00:01:49,410 --> 00:01:53,160
как один из этих блоков может, хотя бы в принципе, хранить конкретный факт.

31
00:01:53,580 --> 00:01:57,080
В частности, он будет хранить тот факт, что Майкл Джордан играет в баскетбол.

32
00:01:58,080 --> 00:02:00,307
Должен заметить, что раскладка здесь навеяна разговором, 

33
00:02:00,307 --> 00:02:03,200
который у меня состоялся с одним из исследователей DeepMind, Нилом Нандой.

34
00:02:04,060 --> 00:02:07,559
По большей части я буду считать, что ты либо смотрел последние две главы, 

35
00:02:07,559 --> 00:02:10,585
либо имеешь базовое представление о том, что такое трансформер, 

36
00:02:10,585 --> 00:02:14,700
но освежить знания никогда не помешает, так что вот краткое напоминание об общей схеме.

37
00:02:15,340 --> 00:02:20,333
Мы с тобой изучали модель, которая обучена воспринимать кусок текста и предсказывать, 

38
00:02:20,333 --> 00:02:21,320
что будет дальше.

39
00:02:21,720 --> 00:02:26,357
Входной текст сначала разбивается на кучу токенов, то есть маленьких кусочков, 

40
00:02:26,357 --> 00:02:30,055
которые обычно являются словами или маленькими кусочками слов, 

41
00:02:30,055 --> 00:02:35,280
и каждый токен ассоциируется с высокоразмерным вектором, то есть с длинным списком чисел.

42
00:02:35,840 --> 00:02:40,849
Затем эта последовательность векторов многократно проходит через два вида операций: 

43
00:02:40,849 --> 00:02:45,083
внимание, которое позволяет векторам передавать информацию друг другу, 

44
00:02:45,083 --> 00:02:49,079
и затем многослойные перцептроны, то, во что мы сегодня углубимся, 

45
00:02:49,079 --> 00:02:52,300
а также между ними есть определенный шаг нормализации.

46
00:02:53,300 --> 00:02:57,998
После того как последовательность векторов пройдет через много-много различных 

47
00:02:57,998 --> 00:03:01,150
итераций обоих этих блоков, к концу можно надеяться, 

48
00:03:01,150 --> 00:03:05,076
что каждый вектор впитал достаточно информации, как из контекста, 

49
00:03:05,076 --> 00:03:08,228
всех остальных слов на входе, так и из общих знаний, 

50
00:03:08,228 --> 00:03:11,618
которые были заложены в веса модели в процессе обучения, 

51
00:03:11,618 --> 00:03:16,020
чтобы использовать их для предсказания того, какой токен придет следующим.

52
00:03:16,860 --> 00:03:20,560
Одна из ключевых идей, которую я хочу, чтобы ты уяснил, заключается в том, 

53
00:03:20,560 --> 00:03:24,063
что все эти векторы живут в очень, очень высокоразмерном пространстве, 

54
00:03:24,063 --> 00:03:27,862
и когда ты думаешь об этом пространстве, разные направления могут кодировать 

55
00:03:27,862 --> 00:03:28,800
разные виды смысла.

56
00:03:30,120 --> 00:03:32,848
Классический пример, к которому я люблю возвращаться, 

57
00:03:32,848 --> 00:03:36,941
- если ты посмотришь на вкрапление женщины и вычтешь из него вкрапление мужчины, 

58
00:03:36,941 --> 00:03:40,984
сделаешь маленький шаг и добавишь его к другому существительному мужского рода, 

59
00:03:40,984 --> 00:03:44,673
например дяде, то окажешься где-то очень-очень близко к соответствующему 

60
00:03:44,673 --> 00:03:46,240
существительному женского рода.

61
00:03:46,440 --> 00:03:50,880
В этом смысле именно это направление кодирует гендерную информацию.

62
00:03:51,640 --> 00:03:54,429
Идея заключается в том, что множество других отчетливых направлений 

63
00:03:54,429 --> 00:03:57,752
в этом сверхвысокоразмерном пространстве могут соответствовать другим признакам, 

64
00:03:57,752 --> 00:03:59,640
которые модель, возможно, захочет представить.

65
00:04:01,400 --> 00:04:06,180
Однако в трансформаторе эти векторы не просто кодируют значение одного слова.

66
00:04:06,680 --> 00:04:10,772
Проходя через сеть, они приобретают гораздо более богатый смысл, 

67
00:04:10,772 --> 00:04:15,180
основанный на всем окружающем их контексте, а также на знаниях модели.

68
00:04:15,880 --> 00:04:18,678
В конечном счете, каждый из них должен кодировать нечто гораздо, 

69
00:04:18,678 --> 00:04:22,209
гораздо большее, чем значение одного слова, поскольку его должно быть достаточно, 

70
00:04:22,209 --> 00:04:23,760
чтобы предсказать, что будет дальше.

71
00:04:24,560 --> 00:04:28,018
Мы уже видели, как блоки внимания позволяют тебе включать контекст, 

72
00:04:28,018 --> 00:04:31,680
но большинство параметров модели на самом деле живут внутри блоков MLP, 

73
00:04:31,680 --> 00:04:34,986
и одна из мыслей о том, что они могут делать, заключается в том, 

74
00:04:34,986 --> 00:04:38,140
что они предлагают дополнительную емкость для хранения фактов.

75
00:04:38,720 --> 00:04:42,718
Как я уже сказал, урок здесь будет сосредоточен на конкретном игрушечном примере того, 

76
00:04:42,718 --> 00:04:46,120
как именно может храниться тот факт, что Майкл Джордан играет в баскетбол.

77
00:04:47,120 --> 00:04:49,631
Итак, этот игрушечный пример потребует от нас с тобой сделать 

78
00:04:49,631 --> 00:04:51,900
пару предположений об этом высокоразмерном пространстве.

79
00:04:52,360 --> 00:04:57,340
Для начала предположим, что одно из направлений представляет идею имени Майкл, 

80
00:04:57,340 --> 00:05:02,637
затем другое, почти перпендикулярное направление представляет идею фамилии Джордан, 

81
00:05:02,637 --> 00:05:06,420
а еще третье направление будет представлять идею баскетбола.

82
00:05:07,400 --> 00:05:12,356
Если ты посмотришь в сеть и выберешь один из обрабатываемых векторов, 

83
00:05:12,356 --> 00:05:17,737
то если его точечное произведение с направлением имени Майкл равно единице, 

84
00:05:17,737 --> 00:05:22,340
то это означает, что вектор кодирует идею человека с этим именем.

85
00:05:23,800 --> 00:05:26,283
В противном случае точечное произведение будет нулевым или отрицательным, 

86
00:05:26,283 --> 00:05:28,700
что означает, что вектор на самом деле не совпадает с этим направлением.

87
00:05:29,420 --> 00:05:32,596
И для простоты давайте полностью проигнорируем вполне резонный вопрос о том, 

88
00:05:32,596 --> 00:05:35,320
что может означать, если это точечное произведение больше единицы.

89
00:05:36,200 --> 00:05:40,910
Аналогично, его точечное произведение с этими другими направлениями скажет тебе, 

90
00:05:40,910 --> 00:05:43,760
представляет ли он фамилию Джордан или баскетбол.

91
00:05:44,740 --> 00:05:48,389
Так что, допустим, вектор должен представлять полное имя Майкла Джордана, 

92
00:05:48,389 --> 00:05:52,680
тогда его точечное произведение с обоими этими направлениями должно быть равно единице.

93
00:05:53,480 --> 00:05:57,866
Поскольку текст "Майкл Джордан" охватывает два разных токена, это также означает, 

94
00:05:57,866 --> 00:06:02,466
что мы должны предположить, что более ранний блок внимания успешно передал информацию 

95
00:06:02,466 --> 00:06:06,960
второму из этих двух векторов, чтобы убедиться, что он может закодировать оба имени.

96
00:06:07,940 --> 00:06:11,480
Исходя из всех этих предпосылок, давай теперь погрузимся в мясо урока.

97
00:06:11,880 --> 00:06:14,980
Что происходит внутри многослойного перцептрона?

98
00:06:17,100 --> 00:06:21,090
Ты можешь представить себе эту последовательность векторов, поступающих в блок, 

99
00:06:21,090 --> 00:06:25,580
и вспомнить, что каждый вектор изначально был связан с одной из лексем из входного текста.

100
00:06:26,080 --> 00:06:29,923
Что произойдет, так это то, что каждый отдельный вектор из этой последовательности 

101
00:06:29,923 --> 00:06:33,720
пройдет через короткую серию операций, мы распакуем их буквально через мгновение, 

102
00:06:33,720 --> 00:06:36,360
и в конце мы получим другой вектор с той же размерностью.

103
00:06:36,880 --> 00:06:39,517
Этот другой вектор будет добавлен к первоначальному, 

104
00:06:39,517 --> 00:06:43,200
который влился внутрь, и эта сумма и будет результатом, вытекающим наружу.

105
00:06:43,720 --> 00:06:48,020
Эту последовательность операций ты применяешь к каждому вектору в последовательности, 

106
00:06:48,020 --> 00:06:51,620
связанному с каждой лексемой на входе, и все это происходит параллельно.

107
00:06:52,100 --> 00:06:54,652
В частности, на этом шаге векторы не разговаривают друг с другом, 

108
00:06:54,652 --> 00:06:56,200
они все как бы занимаются своими делами.

109
00:06:56,720 --> 00:06:59,305
И для нас с тобой это на самом деле делает все намного проще, 

110
00:06:59,305 --> 00:07:01,264
потому что это означает, что если мы понимаем, 

111
00:07:01,264 --> 00:07:04,850
что происходит только с одним из векторов через этот блок, то мы эффективно понимаем, 

112
00:07:04,850 --> 00:07:06,060
что происходит со всеми ними.

113
00:07:07,100 --> 00:07:10,104
Когда я говорю, что этот блок будет кодировать тот факт, 

114
00:07:10,104 --> 00:07:14,532
что Майкл Джордан играет в баскетбол, я имею в виду, что если сюда поступит вектор, 

115
00:07:14,532 --> 00:07:18,907
который кодирует имя Майкл и фамилию Джордан, то эта последовательность вычислений 

116
00:07:18,907 --> 00:07:21,542
выдаст нечто, включающее направление "баскетбол", 

117
00:07:21,542 --> 00:07:24,020
что и будет добавлено к вектору в этой позиции.

118
00:07:25,600 --> 00:07:29,700
Первый шаг этого процесса выглядит как умножение этого вектора на очень большую матрицу.

119
00:07:30,040 --> 00:07:31,980
Ничего удивительного, это глубокое обучение.

120
00:07:32,680 --> 00:07:36,526
И эта матрица, как и все остальные, которые мы видели, заполнена параметрами модели, 

121
00:07:36,526 --> 00:07:40,282
полученными из данных, которые ты можешь представить как кучу ручек и циферблатов, 

122
00:07:40,282 --> 00:07:43,540
которые настраиваются и регулируются, чтобы определить поведение модели.

123
00:07:44,500 --> 00:07:48,327
Теперь один из приятных способов думать о матричном умножении - это представить, 

124
00:07:48,327 --> 00:07:51,020
что каждая строка этой матрицы - это собственный вектор, 

125
00:07:51,020 --> 00:07:54,942
и взять кучу точечных произведений между этими строками и обрабатываемым вектором, 

126
00:07:54,942 --> 00:07:56,880
который я обозначу как E для встраивания.

127
00:07:57,280 --> 00:08:00,659
Например, предположим, что самый первый ряд оказался равным этому 

128
00:08:00,659 --> 00:08:04,040
направлению имени Майкл, которое, как мы предполагаем, существует.

129
00:08:04,320 --> 00:08:07,254
Это будет означать, что первый компонент в этом выводе, 

130
00:08:07,254 --> 00:08:10,031
вот этом точечном произведении, будет равен единице, 

131
00:08:10,031 --> 00:08:13,332
если этот вектор кодирует имя Майкл, и нулю или отрицательному 

132
00:08:13,332 --> 00:08:14,800
значению в противном случае.

133
00:08:15,880 --> 00:08:19,230
Еще веселее, удели время тому, чтобы подумать, что бы это значило, 

134
00:08:19,230 --> 00:08:23,080
если бы в первом ряду было такое направление: имя Майкл плюс фамилия Джордан.

135
00:08:23,700 --> 00:08:27,420
И для простоты давай я запишу это как M плюс J.

136
00:08:28,080 --> 00:08:30,860
Затем, взяв точечное произведение с этим вложением E, 

137
00:08:30,860 --> 00:08:34,980
все распределяется очень красиво, так что это выглядит как M dot E плюс J dot E.

138
00:08:34,980 --> 00:08:38,630
И обрати внимание, что это означает, что конечное значение будет равно двум, 

139
00:08:38,630 --> 00:08:40,954
если вектор кодирует полное имя Майкла Джордана, 

140
00:08:40,954 --> 00:08:44,700
а в противном случае оно будет равно единице или чему-то меньшему, чем единица.

141
00:08:45,340 --> 00:08:47,260
И это только одна строка в этой матрице.

142
00:08:47,600 --> 00:08:52,513
Ты можешь думать, что все остальные ряды параллельно задают какие-то другие вопросы, 

143
00:08:52,513 --> 00:08:56,040
исследуя какие-то другие особенности обрабатываемого вектора.

144
00:08:56,700 --> 00:09:00,081
Очень часто этот шаг также включает в себя добавление к выходу еще одного вектора, 

145
00:09:00,081 --> 00:09:02,240
который полон параметров модели, выученных из данных.

146
00:09:02,240 --> 00:09:04,560
Этот другой вектор известен как смещение.

147
00:09:05,180 --> 00:09:07,424
Для нашего примера я хочу, чтобы ты представил, 

148
00:09:07,424 --> 00:09:11,024
что значение этого смещения в первом компоненте равно отрицательной единице, 

149
00:09:11,024 --> 00:09:14,905
то есть наш конечный результат выглядит как соответствующее точечное произведение, 

150
00:09:14,905 --> 00:09:15,560
но минус один.

151
00:09:16,120 --> 00:09:19,186
Ты можешь резонно спросить, почему я хочу, чтобы ты предположил, 

152
00:09:19,186 --> 00:09:21,922
что модель научилась этому, и через мгновение ты увидишь, 

153
00:09:21,922 --> 00:09:24,753
почему это очень чисто и приятно, если у нас есть значение, 

154
00:09:24,753 --> 00:09:27,112
которое будет положительным тогда и только тогда, 

155
00:09:27,112 --> 00:09:29,470
когда вектор кодирует полное имя Майкла Джордана, 

156
00:09:29,470 --> 00:09:32,160
а в противном случае оно будет нулевым или отрицательным.

157
00:09:33,040 --> 00:09:37,827
Общее количество строк в этой матрице, которая чем-то напоминает количество задаваемых 

158
00:09:37,827 --> 00:09:42,780
вопросов, в случае с GPT-3, за цифрами которого мы следили, составляет чуть меньше 50 000.

159
00:09:43,100 --> 00:09:44,780
На самом деле, это ровно в четыре раза больше, 

160
00:09:44,780 --> 00:09:46,640
чем число измерений в этом пространстве встраивания.

161
00:09:46,920 --> 00:09:47,900
Это выбор дизайнера.

162
00:09:47,940 --> 00:09:50,440
Ты можешь сделать его больше, можешь меньше, но наличие чистого множества, 

163
00:09:50,440 --> 00:09:52,240
как правило, благоприятно для аппаратного обеспечения.

164
00:09:52,740 --> 00:09:57,654
Поскольку эта матрица, полная весов, отображает нас в более высокоразмерное пространство, 

165
00:09:57,654 --> 00:09:59,020
я дам ей сокращение W up.

166
00:09:59,020 --> 00:10:02,599
Я продолжу обозначать вектор, который мы обрабатываем, как E, 

167
00:10:02,599 --> 00:10:07,160
а этот вектор смещения обозначим как B и поместим все это обратно на диаграмму.

168
00:10:09,180 --> 00:10:13,428
На данном этапе проблема заключается в том, что эта операция чисто линейная, 

169
00:10:13,428 --> 00:10:15,360
но язык - очень нелинейный процесс.

170
00:10:15,880 --> 00:10:19,387
Если вход, который мы измеряем, высок для Майкла плюс Джордана, 

171
00:10:19,387 --> 00:10:23,661
то он также обязательно будет в какой-то степени вызван Майклом плюс Фелпсом, 

172
00:10:23,661 --> 00:10:28,100
а также Алексис плюс Джорданом, несмотря на то, что концептуально они не связаны.

173
00:10:28,540 --> 00:10:32,000
Что тебе действительно нужно, так это простое "да" или "нет" для полного имени.

174
00:10:32,900 --> 00:10:35,095
Поэтому следующий шаг - пропустить этот большой 

175
00:10:35,095 --> 00:10:37,840
промежуточный вектор через очень простую нелинейную функцию.

176
00:10:38,360 --> 00:10:43,004
Обычно выбирают такой вариант, при котором все отрицательные значения сводятся к нулю, 

177
00:10:43,004 --> 00:10:45,300
а все положительные остаются без изменений.

178
00:10:46,440 --> 00:10:50,685
И, продолжая традицию глубокого обучения с чересчур вычурными названиями, 

179
00:10:50,685 --> 00:10:54,872
эту очень простую функцию часто называют выпрямленной линейной единицей, 

180
00:10:54,872 --> 00:10:56,020
или сокращенно ReLU.

181
00:10:56,020 --> 00:10:57,880
Вот как выглядит график.

182
00:10:58,300 --> 00:11:02,711
Так что если взять наш воображаемый пример, где первая запись промежуточного вектора 

183
00:11:02,711 --> 00:11:05,930
равна единице, если и только если полное имя - Майкл Джордан, 

184
00:11:05,930 --> 00:11:08,784
и нулю или отрицательному значению в противном случае, 

185
00:11:08,784 --> 00:11:12,262
то после прохождения через ReLU ты получишь очень чистое значение, 

186
00:11:12,262 --> 00:11:15,740
где все нулевые и отрицательные значения просто обрезаются до нуля.

187
00:11:16,100 --> 00:11:17,940
Таким образом, этот результат будет равен единице для 

188
00:11:17,940 --> 00:11:19,780
полного имени Майкл Джордан и нулю в противном случае.

189
00:11:20,560 --> 00:11:24,120
Другими словами, он очень прямолинейно имитирует поведение AND-гейта.

190
00:11:25,660 --> 00:11:29,188
Часто модели используют немного измененную функцию, которая называется JLU, 

191
00:11:29,188 --> 00:11:32,020
у нее та же основная форма, просто она немного более гладкая.

192
00:11:32,500 --> 00:11:35,720
Но для наших целей все будет немного чище, если мы будем думать только о ReLU.

193
00:11:36,740 --> 00:11:40,700
Кроме того, когда ты слышишь, как люди говорят о нейронах трансформатора, 

194
00:11:40,700 --> 00:11:42,520
они имеют в виду вот эти значения.

195
00:11:42,900 --> 00:11:47,721
Когда ты видишь обычную картинку нейронной сети со слоем точек и кучей линий, 

196
00:11:47,721 --> 00:11:51,863
соединяющих предыдущий слой, которую мы видели ранее в этой серии, 

197
00:11:51,863 --> 00:11:55,881
это обычно означает, что она передает комбинацию линейного шага, 

198
00:11:55,881 --> 00:12:01,260
матричного умножения, за которым следует какая-то простая нелинейная функция типа ReLU.

199
00:12:02,500 --> 00:12:04,931
Ты бы сказал, что этот нейрон активен всякий раз, 

200
00:12:04,931 --> 00:12:08,920
когда это значение положительно, и что он неактивен, если это значение равно нулю.

201
00:12:10,120 --> 00:12:12,380
Следующий шаг очень похож на первый.

202
00:12:12,560 --> 00:12:16,580
Ты умножаешь на очень большую матрицу и добавляешь к ней определенный член смещения.

203
00:12:16,980 --> 00:12:21,368
В этом случае количество измерений на выходе уменьшается до размера этого 

204
00:12:21,368 --> 00:12:25,520
пространства встраивания, так что я назову это матрицей проекции вниз.

205
00:12:26,220 --> 00:12:29,019
И на этот раз вместо того, чтобы думать о вещах строка за строкой, 

206
00:12:29,019 --> 00:12:31,360
на самом деле приятнее думать о них столбец за столбцом.

207
00:12:31,860 --> 00:12:36,760
Видишь ли, еще один способ закрепить в голове матричное умножение - это представить, 

208
00:12:36,760 --> 00:12:41,776
как ты берешь каждый столбец матрицы, умножаешь его на соответствующий член в векторе, 

209
00:12:41,776 --> 00:12:45,640
который она обрабатывает, и складываешь все эти измененные столбцы.

210
00:12:46,840 --> 00:12:50,530
Так думать приятнее потому, что здесь столбцы имеют ту же размерность, 

211
00:12:50,530 --> 00:12:55,104
что и пространство встраивания, поэтому мы можем думать о них как о направлениях в этом 

212
00:12:55,104 --> 00:12:55,780
пространстве.

213
00:12:56,140 --> 00:12:59,559
Например, представим, что модель научилась делать ту первую колонну 

214
00:12:59,559 --> 00:13:03,080
в баскетбольном направлении, которое, как мы предполагаем, существует.

215
00:13:04,180 --> 00:13:08,515
Это будет означать, что когда соответствующий нейрон в этой первой позиции будет активен, 

216
00:13:08,515 --> 00:13:10,780
мы добавим этот столбец к конечному результату.

217
00:13:11,140 --> 00:13:14,247
Но если бы этот нейрон был неактивен, если бы это число было равно нулю, 

218
00:13:14,247 --> 00:13:15,780
то это не имело бы никакого эффекта.

219
00:13:16,500 --> 00:13:18,060
И это не обязательно должен быть только баскетбол.

220
00:13:18,220 --> 00:13:21,361
Модель также может запечь в эту колонку и многие другие черты, 

221
00:13:21,361 --> 00:13:25,200
которые она хочет ассоциировать с чем-то, что носит полное имя Майкл Джордан.

222
00:13:26,980 --> 00:13:31,357
И в то же время все остальные столбцы этой матрицы говорят тебе о том, 

223
00:13:31,357 --> 00:13:36,660
что будет добавлено к конечному результату, если соответствующий нейрон будет активен.

224
00:13:37,360 --> 00:13:40,055
И если у тебя есть смещение в этом случае, то это то, 

225
00:13:40,055 --> 00:13:43,500
что ты просто добавляешь каждый раз, независимо от значений нейронов.

226
00:13:44,060 --> 00:13:45,280
Ты можешь задаться вопросом, что это делает.

227
00:13:45,540 --> 00:13:49,320
Как и в случае со всеми предметами, заполненными параметрами, здесь сложно сказать точно.

228
00:13:49,320 --> 00:13:51,909
Возможно, в сети есть какая-то бухгалтерия, которую нужно вести, 

229
00:13:51,909 --> 00:13:54,380
но ты можешь не стесняться и пока не обращать на это внимания.

230
00:13:54,860 --> 00:13:57,853
Снова сделав наши обозначения немного более компактными, 

231
00:13:57,853 --> 00:14:02,369
я назову эту большую матрицу W вниз и аналогично назову этот вектор смещения B вниз и 

232
00:14:02,369 --> 00:14:04,260
помещу это обратно в нашу диаграмму.

233
00:14:04,740 --> 00:14:09,138
Как я уже говорил ранее, ты добавляешь этот конечный результат к вектору, 

234
00:14:09,138 --> 00:14:13,240
который влился в блок в этой позиции, и получаешь конечный результат.

235
00:14:13,820 --> 00:14:18,268
Так, например, если в векторе, втекающем внутрь, закодированы и имя Майкл, 

236
00:14:18,268 --> 00:14:23,546
и фамилия Джордан, то, поскольку эта последовательность операций запустит этот AND-гейт, 

237
00:14:23,546 --> 00:14:27,342
он добавит баскетбольное направление, так что то, что выскочит, 

238
00:14:27,342 --> 00:14:29,240
будет кодировать все это вместе.

239
00:14:29,820 --> 00:14:34,200
И помни, что этот процесс происходит с каждым из этих векторов параллельно.

240
00:14:34,800 --> 00:14:38,625
В частности, если взять числа GPT-3, то это означает, 

241
00:14:38,625 --> 00:14:44,860
что в этом блоке не просто 50 000 нейронов, а в 50 000 раз больше, чем токенов на входе.

242
00:14:48,180 --> 00:14:53,057
Вот и вся операция: два матричных продукта, к каждому из которых добавлено смещение, 

243
00:14:53,057 --> 00:14:55,180
и простая функция обрезки между ними.

244
00:14:56,080 --> 00:14:58,816
Любой из тебя, кто смотрел предыдущие видео из этой серии, 

245
00:14:58,816 --> 00:15:02,620
узнает эту структуру как самый базовый вид нейронной сети, который мы там изучали.

246
00:15:03,080 --> 00:15:06,100
В этом примере он был обучен распознавать рукописные цифры.

247
00:15:06,580 --> 00:15:10,453
Здесь, в контексте трансформатора для большой языковой модели, 

248
00:15:10,453 --> 00:15:15,371
это одна из частей более крупной архитектуры, и любая попытка интерпретировать, 

249
00:15:15,371 --> 00:15:20,597
что именно она делает, сильно переплетается с идеей кодирования информации в векторы 

250
00:15:20,597 --> 00:15:23,180
высокоразмерного пространства встраивания.

251
00:15:24,260 --> 00:15:28,284
Это основной урок, но я хочу сделать шаг назад и поразмышлять о двух разных вещах, 

252
00:15:28,284 --> 00:15:30,660
первая из которых - это своего рода бухгалтерия, 

253
00:15:30,660 --> 00:15:34,637
а вторая включает в себя очень наводящий на размышления факт о высших измерениях, 

254
00:15:34,637 --> 00:15:38,080
о котором я на самом деле не знал, пока не покопался в трансформаторах.

255
00:15:41,080 --> 00:15:45,920
В последних двух главах мы с тобой начали подсчитывать общее количество параметров 

256
00:15:45,920 --> 00:15:50,760
в GPT-3 и смотреть, где именно они живут, так что давай быстро закончим игру здесь.

257
00:15:51,400 --> 00:15:54,835
Я уже упоминал, что эта матрица проекции вверх имеет чуть 

258
00:15:54,835 --> 00:16:00,047
меньше 50 000 строк и что каждая строка соответствует размеру пространства встраивания, 

259
00:16:00,047 --> 00:16:02,180
которое для GPT-3 составляет 12 288.

260
00:16:03,240 --> 00:16:08,232
Умножив их вместе, мы получим 604 миллиона параметров только для этой матрицы, 

261
00:16:08,232 --> 00:16:13,920
а в нисходящей проекции такое же количество параметров, только с транспонированной формой.

262
00:16:14,500 --> 00:16:17,400
Так что вместе они дают около 1,2 миллиарда параметров.

263
00:16:18,280 --> 00:16:20,634
Вектор смещения также учитывает еще пару параметров, 

264
00:16:20,634 --> 00:16:24,100
но это тривиальная доля от общего числа, так что я даже не буду ее показывать.

265
00:16:24,660 --> 00:16:29,701
В GPT-3 эта последовательность векторов встраивания проходит не через один, 

266
00:16:29,701 --> 00:16:33,880
а через 96 отдельных MLP, так что общее количество параметров, 

267
00:16:33,880 --> 00:16:38,060
приходящихся на все эти блоки, составляет около 116 миллиардов.

268
00:16:38,820 --> 00:16:42,671
Это примерно 2/3 всех параметров сети, и если добавить их ко всему, 

269
00:16:42,671 --> 00:16:46,635
что мы имели до этого, к блокам внимания, встраиванию и выстраиванию, 

270
00:16:46,635 --> 00:16:51,620
то действительно получится та грандиозная цифра в 175 миллиардов, которая была заявлена.

271
00:16:53,060 --> 00:16:56,038
Наверное, стоит упомянуть, что есть еще один набор параметров, 

272
00:16:56,038 --> 00:16:59,868
связанных с этими шагами нормализации, которые в этом объяснении были пропущены, 

273
00:16:59,868 --> 00:17:03,840
но, как и вектор смещения, они составляют очень незначительную долю от общего числа.

274
00:17:05,900 --> 00:17:08,854
Что касается второго пункта размышлений, то тебе может быть интересно, 

275
00:17:08,854 --> 00:17:10,852
отражает ли этот центральный игрушечный пример, 

276
00:17:10,852 --> 00:17:12,808
на который мы потратили так много времени, то, 

277
00:17:12,808 --> 00:17:15,680
как на самом деле хранятся факты в реальных больших языковых моделях.

278
00:17:16,319 --> 00:17:19,887
Правда, строки этой первой матрицы можно представить как направления в этом 

279
00:17:19,887 --> 00:17:24,112
пространстве встраивания, а это значит, что активация каждого нейрона говорит тебе о том, 

280
00:17:24,112 --> 00:17:27,540
насколько данный вектор выравнивается с каким-то конкретным направлением.

281
00:17:27,760 --> 00:17:31,050
Также верно, что столбцы этой второй матрицы говорят тебе о том, 

282
00:17:31,050 --> 00:17:34,340
что будет добавлено к результату, если этот нейрон будет активен.

283
00:17:34,640 --> 00:17:36,800
И то, и другое - просто математические факты.

284
00:17:37,740 --> 00:17:41,613
Однако факты свидетельствуют о том, что отдельные нейроны очень редко 

285
00:17:41,613 --> 00:17:44,933
представляют одну чистую характеристику, как Майкл Джордан, 

286
00:17:44,933 --> 00:17:49,250
и на самом деле для этого может быть очень веская причина, связанная с идеей, 

287
00:17:49,250 --> 00:17:54,120
витающей среди исследователей интерпретируемости в наши дни, известной как суперпозиция.

288
00:17:54,640 --> 00:17:57,283
Это гипотеза, которая может помочь объяснить как то, 

289
00:17:57,283 --> 00:18:00,175
почему модели особенно трудно интерпретировать, так и то, 

290
00:18:00,175 --> 00:18:02,420
почему они удивительно хорошо масштабируются.

291
00:18:03,500 --> 00:18:07,736
Основная идея заключается в том, что если у тебя есть n-мерное пространство и ты хочешь 

292
00:18:07,736 --> 00:18:10,865
представить кучу различных характеристик, используя направления, 

293
00:18:10,865 --> 00:18:13,754
которые все перпендикулярны друг другу в этом пространстве, 

294
00:18:13,754 --> 00:18:16,449
то есть если ты добавишь компонент в одном направлении, 

295
00:18:16,449 --> 00:18:20,590
он не повлияет ни на одно из других направлений, то максимальное количество векторов, 

296
00:18:20,590 --> 00:18:23,960
которое ты можешь вместить, составляет всего n - количество измерений.

297
00:18:24,600 --> 00:18:27,620
Для математика, собственно, это и есть определение размерности.

298
00:18:28,220 --> 00:18:30,971
Но вот что становится интересным, так это если ты немного 

299
00:18:30,971 --> 00:18:33,580
ослабишь это ограничение и позволишь себе немного шума.

300
00:18:34,180 --> 00:18:37,622
Допустим, ты позволишь представить эти характеристики векторами, 

301
00:18:37,622 --> 00:18:41,171
которые не совсем перпендикулярны, а только почти перпендикулярны, 

302
00:18:41,171 --> 00:18:43,820
может быть, между 89 и 91 градусами друг от друга.

303
00:18:44,820 --> 00:18:48,020
Если бы мы находились в двух или трех измерениях, это не имело бы никакого значения.

304
00:18:48,260 --> 00:18:50,706
Это почти не дает тебе дополнительной свободы действий, 

305
00:18:50,706 --> 00:18:54,202
чтобы вместить больше векторов, что делает тем более контринтуитивным тот факт, 

306
00:18:54,202 --> 00:18:56,780
что для более высоких измерений ответ кардинально меняется.

307
00:18:57,660 --> 00:19:01,452
Я могу дать тебе очень быструю и грязную иллюстрацию этого на примере 

308
00:19:01,452 --> 00:19:05,623
какого-нибудь убогого Python, который создаст список из 100 мерных векторов, 

309
00:19:05,623 --> 00:19:08,820
каждый из которых будет инициализирован случайным образом, 

310
00:19:08,820 --> 00:19:13,641
и этот список будет содержать 10 000 разных векторов, то есть в 100 раз больше векторов, 

311
00:19:13,641 --> 00:19:14,400
чем измерений.

312
00:19:15,320 --> 00:19:19,900
Вот этот график показывает распределение углов между парами этих векторов.

313
00:19:20,680 --> 00:19:25,181
Так как они начались случайным образом, эти углы могут быть любыми от 0 до 180 градусов, 

314
00:19:25,181 --> 00:19:28,267
но ты заметишь, что уже сейчас, даже для случайных векторов, 

315
00:19:28,267 --> 00:19:31,960
есть сильное смещение в сторону того, чтобы все было ближе к 90 градусам.

316
00:19:32,500 --> 00:19:35,677
Затем я собираюсь запустить определенный процесс оптимизации, 

317
00:19:35,677 --> 00:19:38,393
который итеративно подталкивает все эти векторы так, 

318
00:19:38,393 --> 00:19:41,520
чтобы они старались стать более перпендикулярными друг другу.

319
00:19:42,060 --> 00:19:46,660
Повторив это много раз, вот как выглядит распределение углов.

320
00:19:47,120 --> 00:19:51,870
Мы должны увеличить его, потому что все возможные углы между парами 

321
00:19:51,870 --> 00:19:56,900
векторов находятся внутри этого узкого диапазона между 89 и 91 градусом.

322
00:19:58,020 --> 00:20:01,841
В общем, следствием леммы Джонсона-Линденштрауса является то, 

323
00:20:01,841 --> 00:20:06,032
что количество векторов, которые ты можешь впихнуть в пространство, 

324
00:20:06,032 --> 00:20:10,840
почти перпендикулярное этому, растет экспоненциально с ростом числа измерений.

325
00:20:11,960 --> 00:20:15,835
Это очень важно для больших языковых моделей, которые могут выиграть 

326
00:20:15,835 --> 00:20:19,880
от объединения независимых идей с почти перпендикулярными направлениями.

327
00:20:20,000 --> 00:20:23,655
Это значит, что он может хранить гораздо, гораздо больше идей, 

328
00:20:23,655 --> 00:20:26,440
чем есть размеров в отведенном ему пространстве.

329
00:20:27,320 --> 00:20:29,981
Это может частично объяснить, почему производительность 

330
00:20:29,981 --> 00:20:31,740
модели так хорошо зависит от размера.

331
00:20:32,540 --> 00:20:35,436
В пространстве, которое имеет в 10 раз больше измерений, 

332
00:20:35,436 --> 00:20:39,400
может храниться гораздо, гораздо больше, чем в 10 раз больше независимых идей.

333
00:20:40,420 --> 00:20:43,972
И это относится не только к тому пространству встраивания, где живут векторы, 

334
00:20:43,972 --> 00:20:47,206
проходящие через модель, но и к тому вектору, который полон нейронов в 

335
00:20:47,206 --> 00:20:50,440
середине того многослойного перцептрона, который мы только что изучали.

336
00:20:50,960 --> 00:20:55,453
То есть при размерах GPT-3 он мог бы не просто прощупывать 50 000 признаков, 

337
00:20:55,453 --> 00:20:59,129
но если бы он использовал эту огромную дополнительную емкость, 

338
00:20:59,129 --> 00:21:02,571
используя почти перпендикулярные направления пространства, 

339
00:21:02,571 --> 00:21:07,240
он мог бы прощупывать намного, намного больше признаков обрабатываемого вектора.

340
00:21:07,780 --> 00:21:11,090
Но если он это делает, то это означает, что отдельные 

341
00:21:11,090 --> 00:21:14,340
черты не будут видны в виде загорания одного нейрона.

342
00:21:14,660 --> 00:21:19,380
Вместо этого она должна выглядеть как некая особая комбинация нейронов, суперпозиция.

343
00:21:20,400 --> 00:21:24,051
Если тебе интересно узнать больше, то ключевой поисковый запрос здесь - sparse 

344
00:21:24,051 --> 00:21:27,610
autoencoder, то есть инструмент, который используют некоторые специалисты по 

345
00:21:27,610 --> 00:21:30,337
интерпретации, чтобы попытаться извлечь истинные признаки, 

346
00:21:30,337 --> 00:21:32,880
даже если они очень сильно наложены на все эти нейроны.

347
00:21:33,540 --> 00:21:36,800
Я дам ссылку на пару отличных постов об антропологии, посвященных этому.

348
00:21:37,880 --> 00:21:41,020
На данный момент мы не коснулись всех деталей трансформатора, 

349
00:21:41,020 --> 00:21:43,300
но мы с тобой затронули самые важные моменты.

350
00:21:43,520 --> 00:21:47,640
Главное, о чем я хочу рассказать в следующей главе, - это тренировочный процесс.

351
00:21:48,460 --> 00:21:51,148
С одной стороны, короткий ответ на вопрос о том, как работает обучение, 

352
00:21:51,148 --> 00:21:53,277
заключается в том, что это все обратное распространение, 

353
00:21:53,277 --> 00:21:56,227
и мы рассматривали обратное распространение в отдельном контексте в предыдущих 

354
00:21:56,227 --> 00:21:56,900
главах этой серии.

355
00:21:57,220 --> 00:22:00,681
Но есть еще много чего интересного, например, специфическая функция стоимости, 

356
00:22:00,681 --> 00:22:04,099
используемая для языковых моделей, идея тонкой настройки с помощью обучения с 

357
00:22:04,099 --> 00:22:07,780
подкреплением и обратной связи с человеком, а также понятие законов масштабирования.

358
00:22:08,960 --> 00:22:11,782
Небольшое замечание для активных подписчиков: есть несколько видео, 

359
00:22:11,782 --> 00:22:14,853
не связанных с машинным обучением, в которые мне не терпится погрузиться, 

360
00:22:14,853 --> 00:22:18,132
прежде чем я сделаю следующую главу, так что это может занять некоторое время, 

361
00:22:18,132 --> 00:22:20,000
но я обещаю, что это произойдет в свое время.

362
00:22:35,640 --> 00:22:37,920
Спасибо.

