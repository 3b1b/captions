1
00:00:00,000 --> 00:00:04,536
यदि आप एक बड़े भाषा मॉडल को यह वाक्यांश खिलाते हैं, माइकल जॉर्डन खेल खेलता है, 

2
00:00:04,536 --> 00:00:07,465
और आप उससे भविष्यवाणी करवाते हैं कि आगे क्या होगा, 

3
00:00:07,465 --> 00:00:12,232
और वह बास्केटबॉल की सही भविष्यवाणी करता है, तो इससे यह पता चलता है कि कहीं न कहीं, 

4
00:00:12,232 --> 00:00:16,769
इसके सैकड़ों अरबों मापदंडों के अंदर, एक विशिष्ट व्यक्ति और उसके विशिष्ट खेल के 

5
00:00:16,769 --> 00:00:18,320
बारे में ज्ञान छिपा हुआ है।

6
00:00:18,940 --> 00:00:22,355
और मुझे लगता है कि सामान्य तौर पर, जिसने भी इनमें से किसी मॉडल के साथ काम किया है, 

7
00:00:22,355 --> 00:00:25,400
उसे यह स्पष्ट रूप से पता चल गया होगा कि उसने ढेर सारे तथ्य याद कर लिए हैं।

8
00:00:25,700 --> 00:00:29,160
तो आपके मन में यह सवाल उठ सकता है कि यह वास्तव में कैसे काम करता है?

9
00:00:29,160 --> 00:00:31,040
और वे तथ्य कहां रहते हैं?

10
00:00:35,720 --> 00:00:40,075
पिछले दिसंबर में, गूगल डीपमाइंड के कुछ शोधकर्ताओं ने इस प्रश्न पर काम के बारे में पोस्ट 

11
00:00:40,075 --> 00:00:44,480
किया था, और वे एथलीटों को उनके खेल से मिलान करने के इस विशिष्ट उदाहरण का उपयोग कर रहे थे।

12
00:00:44,900 --> 00:00:47,884
और यद्यपि तथ्यों को किस प्रकार संग्रहीत किया जाता है, 

13
00:00:47,884 --> 00:00:52,747
इसकी पूरी यांत्रिक समझ अभी भी अनसुलझी है, फिर भी उन्हें कुछ रोचक आंशिक परिणाम मिले हैं, 

14
00:00:52,747 --> 00:00:57,058
जिनमें यह सामान्य उच्च-स्तरीय निष्कर्ष भी शामिल है कि तथ्य इन नेटवर्कों के एक 

15
00:00:57,058 --> 00:01:01,479
विशिष्ट भाग के अंदर रहते हैं, जिसे बहु-परत परसेप्ट्रॉन या संक्षेप में एमएलपी के 

16
00:01:01,479 --> 00:01:02,640
रूप में जाना जाता है।

17
00:01:03,120 --> 00:01:06,422
पिछले कुछ अध्यायों में, आप और मैं ट्रांसफॉर्मर्स के पीछे के विवरणों, 

18
00:01:06,422 --> 00:01:09,532
बड़े भाषा मॉडल के अंतर्निहित आर्किटेक्चर, तथा अन्य आधुनिक एआई के 

19
00:01:09,532 --> 00:01:12,500
अंतर्निहित आर्किटेक्चर के बारे में गहराई से अध्ययन कर रहे हैं।

20
00:01:13,060 --> 00:01:16,200
सबसे हालिया अध्याय में, हम ध्यान नामक एक भाग पर ध्यान केंद्रित कर रहे थे।

21
00:01:16,840 --> 00:01:21,022
और आपके और मेरे लिए अगला कदम इन बहु-परत परसेप्ट्रॉनों के अंदर क्या होता है, 

22
00:01:21,022 --> 00:01:25,040
इसका विवरण खोजना है, जो नेटवर्क के दूसरे बड़े हिस्से का निर्माण करते हैं।

23
00:01:25,680 --> 00:01:30,100
यहां गणना वास्तव में अपेक्षाकृत सरल है, विशेषकर जब आप इसकी तुलना ध्यान से करते हैं।

24
00:01:30,560 --> 00:01:34,980
यह मूलतः मैट्रिक्स गुणन की एक जोड़ी के बीच में एक साधारण कुछ के साथ उबलता है।

25
00:01:35,720 --> 00:01:40,460
हालाँकि, इन गणनाओं का क्या अर्थ है, इसकी व्याख्या करना अत्यधिक चुनौतीपूर्ण है।

26
00:01:41,560 --> 00:01:45,791
यहां हमारा मुख्य लक्ष्य गणनाओं को चरणबद्ध तरीके से पूरा करना और उन्हें स्मरणीय बनाना है, 

27
00:01:45,791 --> 00:01:49,594
लेकिन मैं ऐसा एक विशिष्ट उदाहरण दिखाने के संदर्भ में करना चाहूंगा कि कैसे इनमें 

28
00:01:49,594 --> 00:01:53,160
से एक ब्लॉक, कम से कम सिद्धांत रूप में, एक ठोस तथ्य को संग्रहीत कर सकता है।

29
00:01:53,580 --> 00:01:57,080
विशेष रूप से, इसमें यह तथ्य संग्रहित किया जाएगा कि माइकल जॉर्डन बास्केटबॉल खेलता है।

30
00:01:58,080 --> 00:02:00,466
मैं यह बताना चाहूंगा कि यह लेआउट डीपमाइंड के एक 

31
00:02:00,466 --> 00:02:03,200
शोधकर्ता नील नंदा के साथ हुई मेरी बातचीत से प्रेरित है।

32
00:02:04,060 --> 00:02:07,412
अधिकांशतः, मैं यह मान कर चलूँगा कि आपने या तो अंतिम दो अध्याय देख लिए हैं, 

33
00:02:07,412 --> 00:02:10,229
या फिर आपको ट्रांसफार्मर क्या होता है, इसका बुनियादी ज्ञान है, 

34
00:02:10,229 --> 00:02:13,716
लेकिन रिफ्रेशर से कभी कोई नुकसान नहीं होता, इसलिए यहाँ समग्र प्रवाह का त्वरित 

35
00:02:13,716 --> 00:02:14,700
अनुस्मारक दिया गया है।

36
00:02:15,340 --> 00:02:18,352
आप और मैं एक ऐसे मॉडल का अध्ययन कर रहे हैं जो पाठ के एक अंश को लेकर 

37
00:02:18,352 --> 00:02:21,320
यह पूर्वानुमान लगाने के लिए प्रशिक्षित है कि आगे क्या होने वाला है।

38
00:02:21,720 --> 00:02:25,339
उस इनपुट टेक्स्ट को सबसे पहले टोकनों के एक समूह में तोड़ा जाता है, 

39
00:02:25,339 --> 00:02:29,985
जिसका अर्थ है कि छोटे-छोटे टुकड़े जो आमतौर पर शब्द या शब्दों के छोटे टुकड़े होते हैं, 

40
00:02:29,985 --> 00:02:33,010
और प्रत्येक टोकन एक उच्च-आयामी वेक्टर से जुड़ा होता है, 

41
00:02:33,010 --> 00:02:35,280
जिसे संख्याओं की एक लंबी सूची कहा जाता है।

42
00:02:35,840 --> 00:02:40,596
इसके बाद सदिशों का यह क्रम बार-बार दो प्रकार के ऑपरेशन से गुजरता है, ध्यान, 

43
00:02:40,596 --> 00:02:44,664
जो सदिशों को एक दूसरे के बीच सूचना पारित करने की अनुमति देता है, 

44
00:02:44,664 --> 00:02:49,233
और फिर बहुपरत परसेप्ट्रॉन, वह चीज जिसके बारे में हम आज गहराई से जानेंगे, 

45
00:02:49,233 --> 00:02:52,300
और इसके बीच में एक निश्चित सामान्यीकरण चरण भी है।

46
00:02:53,300 --> 00:02:57,707
जब सदिशों का क्रम इन दोनों ब्लॉकों के अनेकों विभिन्न पुनरावृत्तियों से 

47
00:02:57,707 --> 00:03:01,990
होकर प्रवाहित होता है, तो अंत में आशा यह होती है कि प्रत्येक सदिश ने 

48
00:03:01,990 --> 00:03:06,708
पर्याप्त जानकारी ग्रहण कर ली होगी, संदर्भ से, इनपुट में अन्य सभी शब्दों से, 

49
00:03:06,708 --> 00:03:11,115
तथा प्रशिक्षण के माध्यम से मॉडल भार में शामिल किए गए सामान्य ज्ञान से, 

50
00:03:11,115 --> 00:03:16,020
ताकि इसका उपयोग यह पूर्वानुमान लगाने के लिए किया जा सके कि अगला टोकन क्या होगा।

51
00:03:16,860 --> 00:03:19,660
एक मुख्य विचार जो मैं आपके दिमाग में रखना चाहता हूँ, 

52
00:03:19,660 --> 00:03:23,146
वह यह है कि ये सभी सदिश एक बहुत ही उच्च-आयामी स्थान में रहते हैं, 

53
00:03:23,146 --> 00:03:27,056
और जब आप उस स्थान के बारे में सोचते हैं, तो विभिन्न दिशाएँ विभिन्न प्रकार 

54
00:03:27,056 --> 00:03:28,800
के अर्थों को कूटबद्ध कर सकती हैं।

55
00:03:30,120 --> 00:03:33,353
तो एक बहुत ही क्लासिक उदाहरण, जिसका मैं पुनः उल्लेख करना चाहता हूँ, 

56
00:03:33,353 --> 00:03:37,252
वह यह है कि यदि आप स्त्री के सन्निहित अर्थ को देखें और उसमें से पुरुष के सन्निहित 

57
00:03:37,252 --> 00:03:41,342
अर्थ को घटा दें, और आप वह छोटा सा कदम उठाकर उसे किसी अन्य पुल्लिंग संज्ञा के साथ जोड़ 

58
00:03:41,342 --> 00:03:45,336
दें, जैसे कि अंकल, तो आप कहीं न कहीं बहुत ही निकट कहीं समतुल्य स्त्रीलिंग संज्ञा के 

59
00:03:45,336 --> 00:03:46,240
पास पहुँच जाते हैं।

60
00:03:46,440 --> 00:03:50,880
इस अर्थ में, यह विशेष दिशा लिंग संबंधी जानकारी को कूटबद्ध करती है।

61
00:03:51,640 --> 00:03:55,751
विचार यह है कि इस अति उच्च-आयामी अंतरिक्ष में कई अन्य विशिष्ट दिशाएं अन्य 

62
00:03:55,751 --> 00:03:59,640
विशेषताओं के अनुरूप हो सकती हैं, जिन्हें मॉडल प्रदर्शित करना चाहता है।

63
00:04:01,400 --> 00:04:06,180
हालांकि, ट्रांसफार्मर में ये वेक्टर केवल एक शब्द का अर्थ ही नहीं बताते हैं।

64
00:04:06,680 --> 00:04:11,033
जैसे-जैसे वे नेटवर्क से गुजरते हैं, वे अपने आस-पास के संदर्भों 

65
00:04:11,033 --> 00:04:15,180
और मॉडल के ज्ञान के आधार पर अधिक समृद्ध अर्थ ग्रहण करते हैं।

66
00:04:15,880 --> 00:04:19,939
अंततः, प्रत्येक को एक शब्द के अर्थ से कहीं अधिक कहीं अधिक कुछ एनकोड करने की आवश्यकता 

67
00:04:19,939 --> 00:04:23,760
होती है, क्योंकि यह भविष्यवाणी करने के लिए पर्याप्त होना चाहिए कि आगे क्या आएगा।

68
00:04:24,560 --> 00:04:28,628
हम पहले ही देख चुके हैं कि कैसे ध्यान ब्लॉक आपको संदर्भ को शामिल करने देते हैं, 

69
00:04:28,628 --> 00:04:32,290
लेकिन अधिकांश मॉडल पैरामीटर वास्तव में एमएलपी ब्लॉकों के अंदर रहते हैं, 

70
00:04:32,290 --> 00:04:36,512
और एक विचार यह है कि वे क्या कर रहे हैं, वह यह है कि वे तथ्यों को संग्रहीत करने की 

71
00:04:36,512 --> 00:04:38,140
अतिरिक्त क्षमता प्रदान करते हैं।

72
00:04:38,720 --> 00:04:42,420
जैसा कि मैंने कहा, यहां सबक ठोस खिलौने के उदाहरण पर केंद्रित होगा कि यह 

73
00:04:42,420 --> 00:04:46,120
कैसे इस तथ्य को संग्रहीत कर सकता है कि माइकल जॉर्डन बास्केटबॉल खेलता है।

74
00:04:47,120 --> 00:04:49,366
अब, इस खिलौने के उदाहरण के लिए आपको और मुझे उस 

75
00:04:49,366 --> 00:04:51,900
उच्च-आयामी स्थान के बारे में कुछ धारणाएं बनानी होंगी।

76
00:04:52,360 --> 00:04:58,153
सबसे पहले, हम मान लेंगे कि दिशाओं में से एक दिशा प्रथम नाम माइकल के विचार को दर्शाती है, 

77
00:04:58,153 --> 00:05:02,839
और फिर एक अन्य लगभग लंबवत दिशा अंतिम नाम जॉर्डन के विचार को दर्शाती है, 

78
00:05:02,839 --> 00:05:06,420
और फिर एक तीसरी दिशा बास्केटबॉल के विचार को दर्शाती है।

79
00:05:07,400 --> 00:05:12,231
तो विशेष रूप से, मेरे कहने का मतलब यह है कि यदि आप नेटवर्क में देखें और संसाधित किए जा 

80
00:05:12,231 --> 00:05:17,174
रहे वेक्टरों में से किसी एक को चुनें, यदि इस प्रथम नाम माइकल दिशा के साथ इसका डॉट उत्पाद 

81
00:05:17,174 --> 00:05:22,173
एक है, तो इसका यही अर्थ होगा कि वेक्टर उस प्रथम नाम वाले व्यक्ति के विचार को एनकोड कर रहा 

82
00:05:22,173 --> 00:05:22,340
है।

83
00:05:23,800 --> 00:05:26,272
अन्यथा, वह डॉट उत्पाद शून्य या ऋणात्मक होगा, जिसका अर्थ 

84
00:05:26,272 --> 00:05:28,700
है कि वेक्टर वास्तव में उस दिशा के साथ संरेखित नहीं है।

85
00:05:29,420 --> 00:05:32,170
और सरलता के लिए, आइए हम इस बहुत ही उचित प्रश्न को पूरी तरह से 

86
00:05:32,170 --> 00:05:35,320
नजरअंदाज कर दें कि यदि डॉट उत्पाद एक से बड़ा हो तो इसका क्या अर्थ होगा।

87
00:05:36,200 --> 00:05:39,879
इसी प्रकार, इन अन्य दिशाओं के साथ इसका डॉट उत्पाद आपको 

88
00:05:39,879 --> 00:05:43,760
बताएगा कि यह अंतिम नाम जॉर्डन या बास्केटबॉल को दर्शाता है।

89
00:05:44,740 --> 00:05:48,343
तो मान लीजिए कि एक वेक्टर का मतलब पूर्ण नाम, माइकल जॉर्डन, 

90
00:05:48,343 --> 00:05:52,680
को दर्शाना है, तो इन दोनों दिशाओं के साथ इसका डॉट उत्पाद एक होना चाहिए।

91
00:05:53,480 --> 00:05:56,632
चूंकि माइकल जॉर्डन पाठ दो अलग-अलग टोकनों में फैला हुआ है, 

92
00:05:56,632 --> 00:06:01,198
इसका अर्थ यह भी होगा कि हमें यह मानना होगा कि पहले के ध्यान ब्लॉक ने इन दो वेक्टरों 

93
00:06:01,198 --> 00:06:05,546
में से दूसरे को सफलतापूर्वक जानकारी दी है, ताकि यह सुनिश्चित हो सके कि वह दोनों 

94
00:06:05,546 --> 00:06:06,960
नामों को एनकोड कर सकता है।

95
00:06:07,940 --> 00:06:11,480
इन सभी मान्यताओं के साथ, आइए अब पाठ के सार पर आते हैं।

96
00:06:11,880 --> 00:06:14,980
मल्टीलेयर परसेप्ट्रॉन के अंदर क्या होता है?

97
00:06:17,100 --> 00:06:21,025
आप ब्लॉक में प्रवाहित होने वाले वैक्टरों के इस अनुक्रम के बारे में सोच सकते हैं, 

98
00:06:21,025 --> 00:06:25,240
और याद रखें, प्रत्येक वेक्टर मूल रूप से इनपुट टेक्स्ट के टोकनों में से एक के साथ जुड़ा 

99
00:06:25,240 --> 00:06:25,580
हुआ था।

100
00:06:26,080 --> 00:06:29,664
जो होने वाला है वह यह है कि उस अनुक्रम से प्रत्येक व्यक्तिगत वेक्टर 

101
00:06:29,664 --> 00:06:33,618
संचालन की एक छोटी श्रृंखला से गुजरता है, हम उन्हें बस एक पल में खोल देंगे, 

102
00:06:33,618 --> 00:06:36,360
और अंत में, हमें समान आयाम वाला एक और वेक्टर मिलेगा।

103
00:06:36,880 --> 00:06:40,593
वह अन्य सदिश, उस मूल सदिश में जुड़ जाएगा जो अंदर आया था, 

104
00:06:40,593 --> 00:06:43,200
और वह योग ही बाहर जाने वाला परिणाम होगा।

105
00:06:43,720 --> 00:06:47,837
परिचालनों का यह क्रम कुछ ऐसा है जिसे आप अनुक्रम में प्रत्येक वेक्टर पर लागू करते हैं, 

106
00:06:47,837 --> 00:06:51,620
जो इनपुट में प्रत्येक टोकन से संबद्ध होता है, और यह सब समानांतर रूप से होता है।

107
00:06:52,100 --> 00:06:54,848
विशेष रूप से, इस चरण में सदिश एक दूसरे से बात नहीं करते हैं, 

108
00:06:54,848 --> 00:06:56,200
वे सभी अपना-अपना काम करते हैं।

109
00:06:56,720 --> 00:06:59,106
और आपके और मेरे लिए, यह वास्तव में इसे बहुत सरल बनाता है, 

110
00:06:59,106 --> 00:07:02,151
क्योंकि इसका मतलब है कि अगर हम समझते हैं कि इस ब्लॉक के माध्यम से केवल एक 

111
00:07:02,151 --> 00:07:05,237
वेक्टर के साथ क्या होता है, तो हम प्रभावी रूप से यह समझ सकते हैं कि उन सभी 

112
00:07:05,237 --> 00:07:06,060
के साथ क्या होता है।

113
00:07:07,100 --> 00:07:11,797
जब मैं कहता हूं कि यह ब्लॉक इस तथ्य को एनकोड करेगा कि माइकल जॉर्डन बास्केटबॉल खेलता है, 

114
00:07:11,797 --> 00:07:16,067
तो मेरा मतलब यह है कि यदि एक वेक्टर प्रवाहित होता है जो पहले नाम माइकल और अंतिम 

115
00:07:16,067 --> 00:07:20,283
नाम जॉर्डन को एनकोड करता है, तो गणनाओं का यह क्रम कुछ ऐसा उत्पन्न करेगा जिसमें 

116
00:07:20,283 --> 00:07:24,020
बास्केटबॉल की दिशा शामिल होगी, जो उस स्थिति में वेक्टर में जुड़ जाएगी।

117
00:07:25,600 --> 00:07:29,700
इस प्रक्रिया का पहला चरण उस वेक्टर को एक बहुत बड़े मैट्रिक्स से गुणा करने जैसा लगता है।

118
00:07:30,040 --> 00:07:31,980
इसमें कोई आश्चर्य की बात नहीं है, यह गहन शिक्षा है।

119
00:07:32,680 --> 00:07:35,511
और यह मैट्रिक्स, अन्य सभी मैट्रिक्सों की तरह जिन्हें हमने देखा है, 

120
00:07:35,511 --> 00:07:39,145
डेटा से सीखे गए मॉडल मापदंडों से भरा हुआ है, जिसे आप घुंडियों और डायलों के एक समूह के 

121
00:07:39,145 --> 00:07:42,737
रूप में सोच सकते हैं, जिन्हें मॉडल व्यवहार क्या है यह निर्धारित करने के लिए ट्वीक और 

122
00:07:42,737 --> 00:07:43,540
ट्यून किया जाता है।

123
00:07:44,500 --> 00:07:47,505
अब, मैट्रिक्स गुणन के बारे में सोचने का एक अच्छा तरीका यह है कि उस 

124
00:07:47,505 --> 00:07:50,959
मैट्रिक्स की प्रत्येक पंक्ति को उसके स्वयं के वेक्टर के रूप में कल्पना करें, 

125
00:07:50,959 --> 00:07:54,592
और उन पंक्तियों और संसाधित किए जा रहे वेक्टर के बीच डॉट उत्पादों का एक समूह लें, 

126
00:07:54,592 --> 00:07:56,880
जिसे मैं एम्बेडिंग के लिए E के रूप में लेबल करूंगा।

127
00:07:57,280 --> 00:08:01,823
उदाहरण के लिए, मान लीजिए कि सबसे पहली पंक्ति इस प्रथम नाम माइकल दिशा के बराबर है, 

128
00:08:01,823 --> 00:08:04,040
जिसके अस्तित्व का हम अनुमान लगा रहे हैं।

129
00:08:04,320 --> 00:08:08,568
इसका अर्थ यह होगा कि इस आउटपुट में पहला घटक, यह डॉट उत्पाद, 

130
00:08:08,568 --> 00:08:14,800
यदि वह वेक्टर प्रथम नाम माइकल को एनकोड करता है तो एक होगा, अन्यथा शून्य या ऋणात्मक होगा।

131
00:08:15,880 --> 00:08:19,555
इससे भी अधिक मजेदार बात यह है कि एक क्षण के लिए सोचें कि यदि पहली पंक्ति 

132
00:08:19,555 --> 00:08:23,080
में पहला नाम माइकल और अंतिम नाम जॉर्डन दिशा हो तो इसका क्या अर्थ होगा।

133
00:08:23,700 --> 00:08:27,420
और सरलता के लिए, मैं इसे M+J के रूप में लिखूंगा।

134
00:08:28,080 --> 00:08:30,495
फिर, इस एम्बेडिंग E के साथ डॉट प्रोडक्ट लेने पर, 

135
00:08:30,495 --> 00:08:32,811
चीजें वास्तव में अच्छी तरह से वितरित होती हैं, 

136
00:08:32,811 --> 00:08:34,980
इसलिए यह M डॉट E प्लस J डॉट E जैसा दिखता है।

137
00:08:34,980 --> 00:08:39,665
और ध्यान दें कि इसका अर्थ यह है कि यदि वेक्टर माइकल जॉर्डन का पूरा 

138
00:08:39,665 --> 00:08:44,700
नाम एनकोड करता है तो अंतिम मान दो होगा, अन्यथा यह एक या एक से छोटा होगा।

139
00:08:45,340 --> 00:08:47,260
और यह इस मैट्रिक्स में सिर्फ एक पंक्ति है।

140
00:08:47,600 --> 00:08:50,486
आप अन्य सभी पंक्तियों के बारे में ऐसा सोच सकते हैं कि वे समानांतर 

141
00:08:50,486 --> 00:08:53,328
रूप से कुछ अन्य प्रकार के प्रश्न पूछ रही हैं, तथा संसाधित किए जा 

142
00:08:53,328 --> 00:08:56,040
रहे वेक्टर की कुछ अन्य प्रकार की विशेषताओं की जांच कर रही हैं।

143
00:08:56,700 --> 00:09:00,005
अक्सर इस चरण में आउटपुट में एक अन्य वेक्टर को जोड़ना भी शामिल होता है, 

144
00:09:00,005 --> 00:09:02,240
जो डेटा से सीखे गए मॉडल मापदंडों से भरा होता है।

145
00:09:02,240 --> 00:09:04,560
इस अन्य वेक्टर को पूर्वाग्रह के नाम से जाना जाता है।

146
00:09:05,180 --> 00:09:08,535
हमारे उदाहरण के लिए, मैं चाहता हूँ कि आप कल्पना करें कि उस पहले 

147
00:09:08,535 --> 00:09:12,047
घटक में इस पूर्वाग्रह का मान ऋणात्मक एक है, जिसका अर्थ है कि हमारा 

148
00:09:12,047 --> 00:09:15,560
अंतिम आउटपुट उस प्रासंगिक डॉट उत्पाद जैसा दिखता है, लेकिन माइनस एक।

149
00:09:16,120 --> 00:09:20,143
आप बहुत ही उचित रूप से पूछ सकते हैं कि मैं क्यों चाहता हूं कि आप मान लें कि 

150
00:09:20,143 --> 00:09:24,113
मॉडल ने यह सीख लिया है, और एक पल में आप देखेंगे कि यह बहुत साफ और अच्छा है 

151
00:09:24,113 --> 00:09:28,136
कि अगर हमारे पास यहां एक मूल्य है जो सकारात्मक है यदि और केवल यदि एक वेक्टर 

152
00:09:28,136 --> 00:09:32,160
पूर्ण नाम माइकल जॉर्डन को एनकोड करता है, और अन्यथा यह शून्य या नकारात्मक है।

153
00:09:33,040 --> 00:09:37,647
इस मैट्रिक्स में पंक्तियों की कुल संख्या, जो GPT-3 के मामले में पूछे जाने वाले 

154
00:09:37,647 --> 00:09:42,780
प्रश्नों की संख्या के समान है, जिनकी संख्या पर हम नज़र रख रहे हैं, लगभग 50,000 से कम है।

155
00:09:43,100 --> 00:09:46,640
वास्तव में, यह इस एम्बेडिंग स्पेस में आयामों की संख्या का ठीक चार गुना है।

156
00:09:46,920 --> 00:09:47,900
यह एक डिज़ाइन विकल्प है।

157
00:09:47,940 --> 00:09:49,959
आप इसे अधिक भी कर सकते हैं, आप इसे कम भी कर सकते हैं, 

158
00:09:49,959 --> 00:09:52,240
लेकिन एक स्पष्ट गुणक का होना हार्डवेयर के लिए अनुकूल होता है।

159
00:09:52,740 --> 00:09:56,846
चूंकि भार से भरा यह मैट्रिक्स हमें उच्च आयामी स्थान में ले जाता है, 

160
00:09:56,846 --> 00:09:59,020
इसलिए मैं इसे संक्षिप्त नाम W दूंगा।

161
00:09:59,020 --> 00:10:02,655
मैं उस वेक्टर को लेबल करना जारी रखूंगा जिसे हम संसाधित कर रहे हैं, 

162
00:10:02,655 --> 00:10:07,160
और आइए इस पूर्वाग्रह वेक्टर को B के रूप में लेबल करें और इसे वापस आरेख में डाल दें।

163
00:10:09,180 --> 00:10:12,711
इस बिंदु पर, समस्या यह है कि यह प्रक्रिया पूर्णतः रैखिक है, 

164
00:10:12,711 --> 00:10:15,360
लेकिन भाषा एक बहुत ही गैर-रैखिक प्रक्रिया है।

165
00:10:15,880 --> 00:10:20,068
यदि हम जिस प्रविष्टि को माप रहे हैं वह माइकल प्लस जॉर्डन के लिए उच्च है, 

166
00:10:20,068 --> 00:10:24,256
तो यह आवश्यक रूप से कुछ हद तक माइकल प्लस फेल्प्स और एलेक्सिस प्लस जॉर्डन 

167
00:10:24,256 --> 00:10:28,100
द्वारा भी ट्रिगर की गई होगी, भले ही वे वैचारिक रूप से असंबंधित हों।

168
00:10:28,540 --> 00:10:32,000
वास्तव में आप चाहते हैं कि आपके पूरे नाम के लिए केवल हां या नहीं कहा जाए।

169
00:10:32,900 --> 00:10:35,419
तो अगला कदम इस बड़े मध्यवर्ती वेक्टर को एक बहुत ही 

170
00:10:35,419 --> 00:10:37,840
सरल गैर-रैखिक फ़ंक्शन के माध्यम से पारित करना है।

171
00:10:38,360 --> 00:10:41,854
एक सामान्य विकल्प वह है जिसमें सभी नकारात्मक मानों को शून्य पर मैप कर 

172
00:10:41,854 --> 00:10:45,300
दिया जाता है तथा सभी सकारात्मक मानों को अपरिवर्तित छोड़ दिया जाता है।

173
00:10:46,440 --> 00:10:50,573
और अत्यधिक आकर्षक नामों की गहन शिक्षण परंपरा को जारी रखते हुए, 

174
00:10:50,573 --> 00:10:56,020
इस बहुत ही सरल फ़ंक्शन को अक्सर संशोधित रैखिक इकाई या संक्षेप में ReLU कहा जाता है।

175
00:10:56,020 --> 00:10:57,880
ग्राफ इस प्रकार दिखता है।

176
00:10:58,300 --> 00:11:03,359
अतः हमारे कल्पित उदाहरण को लेते हुए, जहां मध्यवर्ती सदिश की यह पहली प्रविष्टि एक है, 

177
00:11:03,359 --> 00:11:07,704
यदि और केवल यदि पूरा नाम माइकल जॉर्डन है और शून्य या ऋणात्मक है, अन्यथा, 

178
00:11:07,704 --> 00:11:12,347
जब आप इसे ReLU से गुजारते हैं, तो आपको एक बहुत ही स्पष्ट मान प्राप्त होता है, 

179
00:11:12,347 --> 00:11:15,740
जहां सभी शून्य और ऋणात्मक मान शून्य पर क्लिप हो जाते हैं।

180
00:11:16,100 --> 00:11:19,780
अतः यह आउटपुट पूर्ण नाम माइकल जॉर्डन के लिए एक होगा, अन्यथा शून्य होगा।

181
00:11:20,560 --> 00:11:24,120
दूसरे शब्दों में, यह सीधे तौर पर AND गेट के व्यवहार की नकल करता है।

182
00:11:25,660 --> 00:11:29,377
अक्सर मॉडलों में थोड़ा संशोधित फ़ंक्शन का उपयोग किया जाता है जिसे JLU कहा जाता है, 

183
00:11:29,377 --> 00:11:32,020
जिसका मूल आकार वही होता है, बस यह थोड़ा अधिक चिकना होता है।

184
00:11:32,500 --> 00:11:34,126
लेकिन हमारे उद्देश्यों के लिए, यदि हम केवल ReLU 

185
00:11:34,126 --> 00:11:35,720
के बारे में सोचें तो यह थोड़ा सा साफ़ हो जाएगा।

186
00:11:36,740 --> 00:11:40,241
इसके अलावा, जब आप लोगों को ट्रांसफार्मर के न्यूरॉन्स का उल्लेख करते हुए सुनते हैं, 

187
00:11:40,241 --> 00:11:42,520
तो वे यहीं इन मूल्यों के बारे में बात कर रहे होते हैं।

188
00:11:42,900 --> 00:11:47,387
जब भी आप बिंदुओं की एक परत और पिछली परत से जुड़ने वाली रेखाओं के समूह के साथ 

189
00:11:47,387 --> 00:11:52,400
उस सामान्य न्यूरल नेटवर्क चित्र को देखते हैं, जिसे हमने इस श्रृंखला में पहले देखा था, 

190
00:11:52,400 --> 00:11:57,529
तो इसका मतलब आमतौर पर एक रैखिक चरण, एक मैट्रिक्स गुणन के संयोजन को व्यक्त करना होता है, 

191
00:11:57,529 --> 00:12:01,260
जिसके बाद ReLU जैसे कुछ सरल शब्द-वार गैर-रेखीय फ़ंक्शन होते हैं।

192
00:12:02,500 --> 00:12:05,788
आप कहेंगे कि जब भी यह मान धनात्मक होता है तो यह न्यूरॉन सक्रिय 

193
00:12:05,788 --> 00:12:08,920
होता है और यदि यह मान शून्य होता है तो यह निष्क्रिय होता है।

194
00:12:10,120 --> 00:12:12,380
अगला चरण पहले चरण के समान ही दिखता है।

195
00:12:12,560 --> 00:12:16,580
आप एक बहुत बड़े मैट्रिक्स से गुणा करते हैं और उसमें एक निश्चित पूर्वाग्रह शब्द जोड़ते हैं।

196
00:12:16,980 --> 00:12:22,162
इस मामले में, आउटपुट में आयामों की संख्या उस एम्बेडिंग स्थान के आकार तक वापस आ जाती है, 

197
00:12:22,162 --> 00:12:25,520
इसलिए मैं आगे बढ़कर इसे डाउन प्रोजेक्शन मैट्रिक्स कहूंगा।

198
00:12:26,220 --> 00:12:31,360
और इस बार, चीजों को पंक्ति दर पंक्ति सोचने के बजाय, स्तंभ दर स्तंभ सोचना अधिक अच्छा है।

199
00:12:31,860 --> 00:12:36,680
आप देखिए, मैट्रिक्स गुणन को अपने दिमाग में रखने का एक और तरीका यह है कि आप मैट्रिक्स 

200
00:12:36,680 --> 00:12:41,386
के प्रत्येक कॉलम को लेकर, उसे उस वेक्टर में संबंधित पद से गुणा करने की कल्पना करें 

201
00:12:41,386 --> 00:12:45,640
जिसे वह प्रोसेस कर रहा है, तथा उन सभी पुनः मापित कॉलमों को एक साथ जोड़ दें।

202
00:12:46,840 --> 00:12:52,002
इस तरह से सोचना बेहतर है क्योंकि यहां स्तंभों का आयाम एम्बेडिंग स्पेस के समान है, 

203
00:12:52,002 --> 00:12:55,780
इसलिए हम उन्हें उस स्पेस में दिशाओं के रूप में सोच सकते हैं।

204
00:12:56,140 --> 00:12:59,658
उदाहरण के लिए, हम कल्पना करेंगे कि मॉडल ने उस पहले कॉलम को उस बास्केटबॉल 

205
00:12:59,658 --> 00:13:03,080
दिशा में बनाना सीख लिया है, जिसके बारे में हम मानते हैं कि वह मौजूद है।

206
00:13:04,180 --> 00:13:08,151
इसका अर्थ यह है कि जब प्रथम स्थिति में संबंधित न्यूरॉन सक्रिय होगा, 

207
00:13:08,151 --> 00:13:10,780
तो हम इस कॉलम को अंतिम परिणाम में जोड़ देंगे।

208
00:13:11,140 --> 00:13:15,780
लेकिन यदि वह न्यूरॉन निष्क्रिय हो, यदि वह संख्या शून्य हो, तो इसका कोई प्रभाव नहीं होगा।

209
00:13:16,500 --> 00:13:18,060
और यह सिर्फ बास्केटबॉल तक ही सीमित नहीं है।

210
00:13:18,220 --> 00:13:21,263
मॉडल इस कॉलम और कई अन्य विशेषताओं को भी शामिल कर सकता है, 

211
00:13:21,263 --> 00:13:25,200
जिसे वह किसी ऐसी चीज के साथ जोड़ना चाहता है जिसका पूरा नाम माइकल जॉर्डन हो।

212
00:13:26,980 --> 00:13:31,895
और साथ ही, इस मैट्रिक्स के अन्य सभी कॉलम आपको बता रहे हैं कि यदि 

213
00:13:31,895 --> 00:13:36,660
संबंधित न्यूरॉन सक्रिय है तो अंतिम परिणाम में क्या जोड़ा जाएगा।

214
00:13:37,360 --> 00:13:39,680
और यदि इस मामले में आपके पास कोई पूर्वाग्रह है, 

215
00:13:39,680 --> 00:13:43,500
तो यह कुछ ऐसा है जिसे आप न्यूरॉन मूल्यों की परवाह किए बिना हर बार जोड़ रहे हैं।

216
00:13:44,060 --> 00:13:45,280
आप सोच रहे होंगे कि यह क्या कर रहा है?

217
00:13:45,540 --> 00:13:49,320
यहां सभी पैरामीटर-भरे ऑब्जेक्ट्स के साथ, सटीक रूप से कहना कठिन है।

218
00:13:49,320 --> 00:13:52,482
हो सकता है कि नेटवर्क को कुछ लेखा-जोखा करने की आवश्यकता हो, 

219
00:13:52,482 --> 00:13:54,380
लेकिन आप इसे अभी अनदेखा कर सकते हैं।

220
00:13:54,860 --> 00:13:57,545
अपने अंकन को थोड़ा और अधिक संक्षिप्त बनाते हुए, 

221
00:13:57,545 --> 00:14:02,133
मैं इस बड़े मैट्रिक्स को W डाउन कहूंगा और इसी प्रकार बायस वेक्टर को B डाउन कहूंगा 

222
00:14:02,133 --> 00:14:04,260
तथा उसे पुनः हमारे आरेख में डाल दूंगा।

223
00:14:04,740 --> 00:14:07,602
जैसा कि मैंने पहले पूर्वावलोकन किया था, इस अंतिम परिणाम के साथ आप 

224
00:14:07,602 --> 00:14:10,464
जो करते हैं वह यह है कि इसे उस वेक्टर में जोड़ते हैं जो उस स्थिति 

225
00:14:10,464 --> 00:14:13,240
में ब्लॉक में प्रवाहित होता है और आपको यह अंतिम परिणाम मिलता है।

226
00:14:13,820 --> 00:14:19,003
उदाहरण के लिए, यदि प्रवाहित वेक्टर प्रथम नाम माइकल और अंतिम नाम जॉर्डन दोनों को 

227
00:14:19,003 --> 00:14:23,668
एनकोड करता है, तो चूंकि संचालन का यह क्रम उस AND गेट को सक्रिय कर देगा, 

228
00:14:23,668 --> 00:14:29,240
यह बास्केटबॉल दिशा को जोड़ देगा, इसलिए जो बाहर आएगा वह उन सभी को एक साथ एनकोड कर देगा।

229
00:14:29,820 --> 00:14:34,200
और याद रखें, यह प्रक्रिया प्रत्येक सदिश के साथ समानांतर रूप से घटित हो रही है।

230
00:14:34,800 --> 00:14:39,725
विशेष रूप से, GPT-3 संख्याओं को लेने का अर्थ है कि इस ब्लॉक में न केवल 

231
00:14:39,725 --> 00:14:44,860
50,000 न्यूरॉन हैं, बल्कि इसमें इनपुट में टोकनों की संख्या 50,000 गुना है।

232
00:14:48,180 --> 00:14:50,617
तो यह संपूर्ण प्रचालन है, दो मैट्रिक्स उत्पाद, 

233
00:14:50,617 --> 00:14:55,180
जिनमें से प्रत्येक में एक पूर्वाग्रह जोड़ा गया है और बीच में एक सरल क्लिपिंग फ़ंक्शन है।

234
00:14:56,080 --> 00:14:58,485
आपमें से जिन लोगों ने इस श्रृंखला के पहले के वीडियो देखे होंगे, 

235
00:14:58,485 --> 00:15:01,454
वे इस संरचना को सबसे बुनियादी प्रकार के तंत्रिका नेटवर्क के रूप में पहचानेंगे, 

236
00:15:01,454 --> 00:15:02,620
जिसका हमने वहां अध्ययन किया था।

237
00:15:03,080 --> 00:15:06,100
उस उदाहरण में, इसे हस्तलिखित अंकों को पहचानने के लिए प्रशिक्षित किया गया था।

238
00:15:06,580 --> 00:15:10,261
यहां, एक बड़े भाषा मॉडल के लिए ट्रांसफार्मर के संदर्भ में, 

239
00:15:10,261 --> 00:15:14,755
यह एक बड़े आर्किटेक्चर का एक हिस्सा है और यह वास्तव में क्या कर रहा है, 

240
00:15:14,755 --> 00:15:20,122
इसकी व्याख्या करने का कोई भी प्रयास उच्च-आयामी एम्बेडिंग स्पेस के वैक्टर में सूचना को 

241
00:15:20,122 --> 00:15:23,180
एनकोड करने के विचार के साथ गहराई से जुड़ा हुआ है।

242
00:15:24,260 --> 00:15:27,957
यह मूल सबक है, लेकिन मैं एक कदम पीछे हटकर दो अलग-अलग चीजों पर विचार करना चाहता हूं, 

243
00:15:27,957 --> 00:15:30,025
जिनमें से पहली एक प्रकार की बहीखाता पद्धति है, 

244
00:15:30,025 --> 00:15:33,414
और दूसरी उच्च आयामों के बारे में एक बहुत ही विचारोत्तेजक तथ्य से संबंधित है, 

245
00:15:33,414 --> 00:15:36,759
जिसके बारे में मुझे तब तक पता नहीं था जब तक कि मैंने ट्रांसफार्मरों के बारे 

246
00:15:36,759 --> 00:15:38,080
में गहनता से अध्ययन नहीं किया।

247
00:15:41,080 --> 00:15:45,948
पिछले दो अध्यायों में, आपने और मैंने GPT-3 में कुल मापदंडों की संख्या गिनना शुरू किया 

248
00:15:45,948 --> 00:15:50,760
और देखा कि वे वास्तव में कहाँ रहते हैं, तो चलिए जल्दी से खेल को यहीं समाप्त करते हैं।

249
00:15:51,400 --> 00:15:56,725
मैंने पहले ही बताया है कि इस अप प्रोजेक्शन मैट्रिक्स में लगभग 50,000 पंक्तियाँ हैं 

250
00:15:56,725 --> 00:16:02,180
और प्रत्येक पंक्ति एम्बेडिंग स्पेस के आकार से मेल खाती है, जो GPT-3 के लिए 12,288 है।

251
00:16:03,240 --> 00:16:06,624
इन्हें आपस में गुणा करने पर, हमें उस मैट्रिक्स के लिए 604 

252
00:16:06,624 --> 00:16:10,126
मिलियन पैरामीटर प्राप्त होते हैं, तथा नीचे के प्रक्षेपण में 

253
00:16:10,126 --> 00:16:13,920
भी पैरामीटर की संख्या समान होती है, केवल आकृति परिवर्तित होती है।

254
00:16:14,500 --> 00:16:17,400
इस प्रकार, कुल मिलाकर वे लगभग 1.2 बिलियन पैरामीटर देते हैं।

255
00:16:18,280 --> 00:16:20,881
पूर्वाग्रह वेक्टर कुछ और मापदंडों को भी ध्यान में रखता है, 

256
00:16:20,881 --> 00:16:24,100
लेकिन यह कुल का एक छोटा सा अनुपात है, इसलिए मैं इसे दिखाने वाला नहीं हूं।

257
00:16:24,660 --> 00:16:31,634
GPT-3 में, एम्बेडिंग वैक्टर का यह क्रम एक नहीं, बल्कि 96 अलग-अलग MLPs से होकर गुजरता है, 

258
00:16:31,634 --> 00:16:38,060
इसलिए इन सभी ब्लॉकों को समर्पित मापदंडों की कुल संख्या लगभग 116 बिलियन हो जाती है।

259
00:16:38,820 --> 00:16:41,258
यह नेटवर्क में कुल मापदंडों का लगभग 2 तिहाई है, 

260
00:16:41,258 --> 00:16:45,321
और जब आप इसे उन सभी चीजों के साथ जोड़ते हैं जो हमारे पास पहले थीं, ध्यान ब्लॉक, 

261
00:16:45,321 --> 00:16:49,639
एम्बेडिंग और अनएम्बेडिंग के लिए, तो आपको वास्तव में 175 बिलियन का वह कुल योग प्राप्त 

262
00:16:49,639 --> 00:16:51,620
होता है, जैसा कि विज्ञापित किया गया है।

263
00:16:53,060 --> 00:16:56,793
यह उल्लेख करना शायद उचित होगा कि उन मानकीकरण चरणों से संबंधित मापदंडों 

264
00:16:56,793 --> 00:16:59,843
का एक और समूह है जिसे इस स्पष्टीकरण में छोड़ दिया गया है, 

265
00:16:59,843 --> 00:17:03,840
लेकिन पूर्वाग्रह वेक्टर की तरह, वे कुल का एक बहुत ही तुच्छ अनुपात बनाते हैं।

266
00:17:05,900 --> 00:17:09,839
चिंतन के दूसरे बिंदु के संबंध में, आप सोच रहे होंगे कि क्या यह केंद्रीय खिलौना उदाहरण, 

267
00:17:09,839 --> 00:17:13,099
जिस पर हम इतना समय खर्च कर रहे हैं, यह दर्शाता है कि वास्तविक बड़े भाषा 

268
00:17:13,099 --> 00:17:15,680
मॉडल में तथ्यों को वास्तव में कैसे संग्रहीत किया जाता है।

269
00:17:16,319 --> 00:17:20,043
यह सच है कि पहले मैट्रिक्स की पंक्तियों को इस एम्बेडिंग स्पेस में दिशाओं के 

270
00:17:20,043 --> 00:17:23,865
रूप में सोचा जा सकता है, और इसका मतलब है कि प्रत्येक न्यूरॉन की सक्रियता आपको 

271
00:17:23,865 --> 00:17:27,540
बताती है कि दिया गया वेक्टर किसी विशिष्ट दिशा के साथ कितना संरेखित होता है।

272
00:17:27,760 --> 00:17:31,050
यह भी सत्य है कि दूसरे मैट्रिक्स के कॉलम आपको बताते हैं 

273
00:17:31,050 --> 00:17:34,340
कि यदि न्यूरॉन सक्रिय है तो परिणाम में क्या जोड़ा जाएगा।

274
00:17:34,640 --> 00:17:36,800
ये दोनों ही केवल गणितीय तथ्य हैं।

275
00:17:37,740 --> 00:17:41,915
हालांकि, साक्ष्य यह सुझाते हैं कि व्यक्तिगत न्यूरॉन्स बहुत कम ही माइकल जॉर्डन 

276
00:17:41,915 --> 00:17:44,859
की तरह एक एकल स्वच्छ विशेषता का प्रतिनिधित्व करते हैं, 

277
00:17:44,859 --> 00:17:47,910
और वास्तव में ऐसा होने का एक बहुत अच्छा कारण हो सकता है, 

278
00:17:47,910 --> 00:17:51,978
जो इन दिनों व्याख्यात्मकता शोधकर्ताओं के बीच घूम रहे एक विचार से संबंधित है 

279
00:17:51,978 --> 00:17:54,120
जिसे सुपरपोजिशन के रूप में जाना जाता है।

280
00:17:54,640 --> 00:17:58,677
यह एक परिकल्पना है जो यह समझाने में मदद कर सकती है कि क्यों इन मॉडलों की व्याख्या 

281
00:17:58,677 --> 00:18:02,420
करना विशेष रूप से कठिन है और क्यों वे आश्चर्यजनक रूप से अच्छे पैमाने पर हैं।

282
00:18:03,500 --> 00:18:07,322
मूल विचार यह है कि यदि आपके पास n-आयामी स्थान है और आप उस स्थान में 

283
00:18:07,322 --> 00:18:12,324
एक-दूसरे के लंबवत दिशाओं का उपयोग करके विभिन्न विशेषताओं का प्रतिनिधित्व करना चाहते हैं, 

284
00:18:12,324 --> 00:18:15,809
तो आप जानते हैं, इस तरह यदि आप एक दिशा में एक घटक जोड़ते हैं, 

285
00:18:15,809 --> 00:18:18,620
तो यह किसी भी अन्य दिशा को प्रभावित नहीं करता है, 

286
00:18:18,620 --> 00:18:22,667
फिर आपके द्वारा फिट किए जा सकने वाले वैक्टर की अधिकतम संख्या केवल n है, 

287
00:18:22,667 --> 00:18:23,960
जो आयामों की संख्या है।

288
00:18:24,600 --> 00:18:27,620
एक गणितज्ञ के लिए, वास्तव में, यही आयाम की परिभाषा है।

289
00:18:28,220 --> 00:18:30,788
लेकिन दिलचस्प बात तब होती है जब आप उस बाध्यता 

290
00:18:30,788 --> 00:18:33,580
में थोड़ी ढील देते हैं और कुछ शोर सहन कर लेते हैं।

291
00:18:34,180 --> 00:18:38,937
मान लीजिए कि आप उन विशेषताओं को सदिशों द्वारा प्रदर्शित करने की अनुमति देते 

292
00:18:38,937 --> 00:18:43,820
हैं जो बिल्कुल लंबवत नहीं हैं, वे लगभग लंबवत हैं, शायद 89 और 91 डिग्री के बीच।

293
00:18:44,820 --> 00:18:48,020
यदि हम दो या तीन आयामों में होते तो इससे कोई फर्क नहीं पड़ता।

294
00:18:48,260 --> 00:18:52,057
इससे आपको अधिक सदिशों को फिट करने के लिए शायद ही कोई अतिरिक्त स्थान मिलता है, 

295
00:18:52,057 --> 00:18:55,222
जिससे यह और भी अधिक विरोधाभासी हो जाता है कि उच्च आयामों के लिए, 

296
00:18:55,222 --> 00:18:56,780
उत्तर नाटकीय रूप से बदल जाता है।

297
00:18:57,660 --> 00:19:01,858
मैं आपको कुछ घटिया पायथन का उपयोग करके इसका एक बहुत ही त्वरित और सरल उदाहरण 

298
00:19:01,858 --> 00:19:05,173
दे सकता हूं जो 100-आयामी सदिशों की एक सूची बनाने जा रहा है, 

299
00:19:05,173 --> 00:19:08,267
जिनमें से प्रत्येक को यादृच्छिक रूप से आरंभ किया जाएगा, 

300
00:19:08,267 --> 00:19:12,134
और इस सूची में 10,000 अलग-अलग सदिश शामिल होंगे, इसलिए जितने आयाम हैं, 

301
00:19:12,134 --> 00:19:14,400
सदिशों की संख्या उससे 100 गुना अधिक होगी।

302
00:19:15,320 --> 00:19:19,900
यह आलेख इन सदिशों के युग्मों के बीच कोणों का वितरण दर्शाता है।

303
00:19:20,680 --> 00:19:24,490
चूँकि उन्होंने यादृच्छिक रूप से शुरुआत की थी, इसलिए वे कोण 0 से 180 डिग्री 

304
00:19:24,490 --> 00:19:27,285
तक कुछ भी हो सकते हैं, लेकिन आप देखेंगे कि पहले से ही, 

305
00:19:27,285 --> 00:19:31,045
यहाँ तक कि यादृच्छिक सदिशों के लिए भी, चीजों के 90 डिग्री के करीब होने की 

306
00:19:31,045 --> 00:19:31,960
भारी प्रवृत्ति है।

307
00:19:32,500 --> 00:19:36,697
फिर मैं एक निश्चित अनुकूलन प्रक्रिया चलाने जा रहा हूं जो इन सभी सदिशों को 

308
00:19:36,697 --> 00:19:41,520
पुनरावृत्त रूप से आगे बढ़ाएगी ताकि वे एक दूसरे के लिए अधिक लंबवत बनने का प्रयास करें।

309
00:19:42,060 --> 00:19:46,660
इसे कई बार दोहराने के बाद, कोणों का वितरण इस प्रकार दिखता है।

310
00:19:47,120 --> 00:19:52,077
हमें वास्तव में इस पर ज़ूम करना होगा क्योंकि सदिशों के जोड़ों के बीच सभी 

311
00:19:52,077 --> 00:19:56,900
संभावित कोण 89 और 91 डिग्री के बीच की इस संकीर्ण सीमा के अंदर होते हैं।

312
00:19:58,020 --> 00:20:02,080
सामान्यतः, जॉनसन-लिंडेनस्ट्रॉस प्रमेयिका के रूप में ज्ञात किसी चीज का 

313
00:20:02,080 --> 00:20:06,083
परिणाम यह होता है कि आप ऐसे स्थान में जितने सदिशों को ठूंस सकते हैं, 

314
00:20:06,083 --> 00:20:10,840
जो लगभग लंबवत् हों, उनकी संख्या आयामों की संख्या के साथ चरघातांकी रूप से बढ़ती है।

315
00:20:11,960 --> 00:20:14,891
यह बड़े भाषा मॉडलों के लिए बहुत महत्वपूर्ण है, 

316
00:20:14,891 --> 00:20:19,880
जो स्वतंत्र विचारों को लगभग लंबवत दिशाओं के साथ जोड़ने से लाभान्वित हो सकते हैं।

317
00:20:20,000 --> 00:20:23,467
इसका अर्थ यह है कि इसे आवंटित स्थान में जितने आयाम हैं, 

318
00:20:23,467 --> 00:20:26,440
उससे कहीं अधिक विचारों को संग्रहित करना संभव है।

319
00:20:27,320 --> 00:20:29,465
इससे आंशिक रूप से यह स्पष्ट हो सकता है कि मॉडल का 

320
00:20:29,465 --> 00:20:31,740
प्रदर्शन आकार के साथ इतना बेहतर क्यों प्रतीत होता है।

321
00:20:32,540 --> 00:20:35,935
एक ऐसा स्थान जिसमें 10 गुना अधिक आयाम हों, वह 10 

322
00:20:35,935 --> 00:20:39,400
गुना अधिक स्वतंत्र विचारों को संग्रहित कर सकता है।

323
00:20:40,420 --> 00:20:42,699
और यह न केवल उस एम्बेडिंग स्पेस के लिए प्रासंगिक है, 

324
00:20:42,699 --> 00:20:45,236
जहां मॉडल के माध्यम से प्रवाहित होने वाले वेक्टर रहते हैं, 

325
00:20:45,236 --> 00:20:48,547
बल्कि उस मल्टीलेयर परसेप्ट्रॉन के मध्य में न्यूरॉन्स से भरे वेक्टर के लिए भी 

326
00:20:48,547 --> 00:20:50,440
प्रासंगिक है, जिसका हमने अभी अध्ययन किया है।

327
00:20:50,960 --> 00:20:56,345
कहने का तात्पर्य यह है कि, GPT-3 के आकार पर, यह न केवल 50,000 विशेषताओं की जांच कर सकता 

328
00:20:56,345 --> 00:21:01,792
है, बल्कि यदि यह अंतरिक्ष की लगभग लंबवत दिशाओं का उपयोग करके इस विशाल अतिरिक्त क्षमता का 

329
00:21:01,792 --> 00:21:07,240
लाभ उठाता है, तो यह संसाधित किए जा रहे वेक्टर की कई और अधिक विशेषताओं की जांच कर सकता है।

330
00:21:07,780 --> 00:21:11,033
लेकिन यदि वह ऐसा कर रहा था, तो इसका अर्थ यह है कि एक न्यूरॉन 

331
00:21:11,033 --> 00:21:14,340
के प्रकाशित होने पर उसकी व्यक्तिगत विशेषताएं दिखाई नहीं देंगी।

332
00:21:14,660 --> 00:21:19,380
यह न्यूरॉनों के किसी विशिष्ट संयोजन, या सुपरपोजिशन जैसा प्रतीत होगा।

333
00:21:20,400 --> 00:21:22,548
आप में से जो लोग अधिक जानने के लिए उत्सुक हैं, 

334
00:21:22,548 --> 00:21:25,702
उनके लिए यहां एक महत्वपूर्ण प्रासंगिक खोज शब्द है स्पार्स ऑटोएनकोडर, 

335
00:21:25,702 --> 00:21:28,811
जो एक ऐसा उपकरण है जिसका उपयोग कुछ व्याख्यात्मक लोग यह जानने के लिए 

336
00:21:28,811 --> 00:21:32,880
करते हैं कि वास्तविक विशेषताएं क्या हैं, भले ही वे सभी न्यूरॉन्स पर बहुत अधिक आरोपित हों।

337
00:21:33,540 --> 00:21:36,800
मैं इस विषय पर कुछ बहुत ही बढ़िया मानवशास्त्रीय पोस्टों का लिंक दूंगा।

338
00:21:37,880 --> 00:21:40,636
इस बिंदु पर, हमने ट्रांसफार्मर के हर विवरण को नहीं छुआ है, 

339
00:21:40,636 --> 00:21:43,300
लेकिन आप और मैंने सबसे महत्वपूर्ण बिंदुओं पर चर्चा की है।

340
00:21:43,520 --> 00:21:47,640
मुख्य बात जो मैं अगले अध्याय में कवर करना चाहता हूं वह है प्रशिक्षण प्रक्रिया।

341
00:21:48,460 --> 00:21:52,752
एक ओर, प्रशिक्षण कैसे काम करता है, इसका संक्षिप्त उत्तर यह है कि यह सब बैकप्रोपेगेशन है, 

342
00:21:52,752 --> 00:21:56,900
और हमने श्रृंखला के पिछले अध्यायों में बैकप्रोपेगेशन को एक अलग संदर्भ में कवर किया है।

343
00:21:57,220 --> 00:22:00,485
लेकिन चर्चा के लिए और भी बहुत कुछ है, जैसे भाषा मॉडल के लिए 

344
00:22:00,485 --> 00:22:04,078
प्रयुक्त विशिष्ट लागत फ़ंक्शन, मानव फीडबैक के साथ सुदृढीकरण सीखने 

345
00:22:04,078 --> 00:22:07,780
का उपयोग करके फ़ाइन-ट्यूनिंग का विचार, और स्केलिंग कानूनों की धारणा।

346
00:22:08,960 --> 00:22:11,269
आप में से सक्रिय अनुयायियों के लिए एक त्वरित नोट, 

347
00:22:11,269 --> 00:22:14,872
कई गैर-मशीन लर्निंग से संबंधित वीडियो हैं जिन्हें मैं अगले अध्याय को बनाने से 

348
00:22:14,872 --> 00:22:17,828
पहले देखने के लिए उत्साहित हूं, इसलिए इसमें कुछ समय लग सकता है, 

349
00:22:17,828 --> 00:22:20,000
लेकिन मैं वादा करता हूं कि यह उचित समय पर आएगा।

350
00:22:35,640 --> 00:22:37,920
धन्यवाद।

