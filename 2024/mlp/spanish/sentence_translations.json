[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Si alimentas a un gran modelo lingüístico con la frase Michael Jordan juega al deporte de fogueo y le haces predecir lo que viene a continuación, y predice correctamente el baloncesto, esto sugeriría que en algún lugar, dentro de sus cientos de miles de millones de parámetros, ha horneado conocimientos sobre una persona concreta y su deporte concreto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "Y creo que, en general, cualquiera que haya jugado con uno de estos modelos tiene la clara sensación de que ha memorizado toneladas y toneladas de datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Así que una pregunta razonable que podrías hacerte es, ¿cómo funciona eso exactamente?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "¿Y dónde viven esos hechos?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "El pasado diciembre, unos investigadores de Google DeepMind publicaron un trabajo sobre esta cuestión, y utilizaban este ejemplo concreto de emparejar atletas con sus deportes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "Y aunque sigue sin resolverse una comprensión mecanicista completa de cómo se almacenan los hechos, obtuvieron algunos resultados parciales interesantes, incluida la conclusión muy general de alto nivel de que los hechos parecen vivir dentro de una parte específica de estas redes, conocidas extravagantemente como perceptrones multicapa, o MLP para abreviar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "En los dos últimos capítulos, tú y yo hemos profundizado en los detalles que hay detrás de los transformadores, la arquitectura subyacente a los grandes modelos lingüísticos, y también subyacente a muchas otras IA modernas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "En el capítulo más reciente, nos centramos en una pieza llamada Atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "Y el siguiente paso para ti y para mí es profundizar en los detalles de lo que ocurre dentro de estos perceptrones multicapa, que constituyen la otra gran parte de la red.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "En realidad, el cálculo aquí es relativamente sencillo, sobre todo si lo comparas con la atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Se reduce esencialmente a un par de multiplicaciones de matrices con un simple algo intermedio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Sin embargo, interpretar lo que hacen estos cálculos es sumamente difícil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Nuestro principal objetivo aquí es recorrer los cálculos y hacerlos memorizables, pero me gustaría hacerlo en el contexto de mostrar un ejemplo específico de cómo uno de estos bloques podría, al menos en principio, almacenar un hecho concreto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Concretamente, será almacenar el hecho de que Michael Jordan juega al baloncesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Debo mencionar que el diseño aquí está inspirado en una conversación que tuve con uno de esos investigadores de DeepMind, Neil Nanda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "En su mayor parte, daré por sentado que has visto los dos últimos capítulos, o bien que tienes una noción básica de lo que es un transformador, pero refrescar la memoria nunca viene mal, así que aquí tienes un rápido recordatorio del flujo general.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Tú y yo hemos estado estudiando un modelo entrenado para tomar un fragmento de texto y predecir lo que viene a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Ese texto de entrada se divide primero en un montón de tokens, es decir, pequeños trozos que suelen ser palabras o trocitos de palabras, y cada token se asocia a un vector de alta dimensión, es decir, a una larga lista de números.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "A continuación, esta secuencia de vectores pasa repetidamente por dos tipos de operaciones: la atención, que permite a los vectores pasarse información entre sí, y luego los perceptrones multicapa, en los que vamos a profundizar hoy, y también hay un cierto paso de normalización intermedio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Después de que la secuencia de vectores haya pasado por muchas, muchas iteraciones diferentes de estos dos bloques, al final, la esperanza es que cada vector haya absorbido suficiente información, tanto del contexto, de todas las demás palabras de la entrada, como del conocimiento general que se incorporó a los pesos del modelo a través del entrenamiento, para que pueda utilizarse para hacer una predicción de qué ficha viene a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Una de las ideas clave que quiero que tengas en tu mente es que todos estos vectores viven en un espacio de muy, muy alta dimensión, y cuando piensas en ese espacio, diferentes direcciones pueden codificar diferentes tipos de significado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Así que un ejemplo muy clásico al que me gusta referirme es cómo si observas la incrustación de mujer y restas la incrustación de hombre, y das ese pequeño paso y lo añades a otro sustantivo masculino, algo así como tío, aterrizas en algún lugar muy, muy cercano al sustantivo femenino correspondiente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "En este sentido, esta dirección concreta codifica información de género.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "La idea es que muchas otras direcciones distintas en este espacio superdimensional podrían corresponder a otras características que el modelo podría querer representar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "Sin embargo, en un transformador, estos vectores no sólo codifican el significado de una sola palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "A medida que fluyen por la red, se impregnan de un significado mucho más rico, basado en todo el contexto que las rodea y también en el conocimiento del modelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "En última instancia, cada una tiene que codificar algo mucho, mucho más allá del significado de una sola palabra, ya que tiene que ser suficiente para predecir lo que vendrá después.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Ya hemos visto cómo los bloques de atención te permiten incorporar el contexto, pero la mayoría de los parámetros del modelo viven en realidad dentro de los bloques MLP, y una idea de lo que podrían estar haciendo es que ofrecen una capacidad extra para almacenar hechos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Como he dicho, la lección aquí se va a centrar en el ejemplo concreto de juguete de cómo podría almacenar exactamente el hecho de que Michael Jordan juegue al baloncesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Ahora bien, este ejemplo de juguete va a requerir que tú y yo hagamos un par de suposiciones sobre ese espacio de alta dimensión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "En primer lugar, supondremos que una de las direcciones representa la idea del nombre Michael, y luego otra dirección casi perpendicular representa la idea del apellido Jordan, y luego una tercera dirección representará la idea del baloncesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "En concreto, lo que quiero decir con esto es que si miras en la red y extraes uno de los vectores que se están procesando, si su producto punto con la dirección de este nombre Michael es uno, eso es lo que significaría que el vector está codificando la idea de una persona con ese nombre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "De lo contrario, ese producto punto sería cero o negativo, lo que significa que el vector no se alinea realmente con esa dirección.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "Y para simplificar, ignoremos por completo la muy razonable cuestión de qué significaría que ese producto punto fuera mayor que uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Del mismo modo, su producto punto con estas otras direcciones te diría si representa el apellido Jordan o baloncesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Así que digamos que un vector pretende representar el nombre completo, Michael Jordan, entonces su producto punto con ambas direcciones tendría que ser uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Puesto que el texto Michael Jordan abarca dos tokens diferentes, esto también significaría que tenemos que suponer que un bloque de atención anterior ha pasado con éxito información al segundo de estos dos vectores para asegurarse de que puede codificar ambos nombres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Con todas estas premisas, vamos a sumergirnos en el meollo de la lección.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "¿Qué ocurre dentro de un perceptrón multicapa?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Puedes pensar en esta secuencia de vectores fluyendo hacia el bloque, y recuerda que cada vector estaba asociado originalmente a una de las palabras del texto de entrada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Lo que va a ocurrir es que cada vector individual de esa secuencia pasa por una breve serie de operaciones, que desmenuzaremos en un momento, y al final obtendremos otro vector con la misma dimensión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "Ese otro vector se sumará al original que entró, y esa suma será el resultado que saldrá.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Esta secuencia de operaciones es algo que se aplica a cada vector de la secuencia, asociado a cada token de la entrada, y todo ocurre en paralelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "En concreto, los vectores no se hablan entre sí en este paso, sino que hacen cada uno lo suyo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "Y para ti y para mí, eso en realidad lo simplifica mucho, porque significa que si entendemos lo que le ocurre a uno solo de los vectores a través de este bloque, entendemos efectivamente lo que les ocurre a todos ellos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Cuando digo que este bloque va a codificar el hecho de que Michael Jordan juega al baloncesto, lo que quiero decir es que si entra un vector que codifica el nombre Michael y el apellido Jordan, esta secuencia de cálculos producirá algo que incluya esa dirección baloncesto, que es lo que se sumará al vector en esa posición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "El primer paso de este proceso parece multiplicar ese vector por una matriz muy grande.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "No hay sorpresas, esto es aprendizaje profundo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "Y esta matriz, como todas las demás que hemos visto, está llena de parámetros del modelo que se aprenden de los datos, que podrías considerar como un montón de mandos y diales que se ajustan y afinan para determinar cuál es el comportamiento del modelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Ahora bien, una forma agradable de pensar en la multiplicación de matrices es imaginar que cada fila de esa matriz es su propio vector, y tomar un montón de productos escalares entre esas filas y el vector que se está procesando, que etiquetaré como E de incrustación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Por ejemplo, supón que esa primera fila resulta ser igual a esta dirección del nombre Michael que suponemos que existe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Eso significaría que el primer componente de esta salida, este producto punto de aquí, sería uno si ese vector codifica el nombre Miguel, y cero o negativo en caso contrario.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Aún más divertido, tómate un momento para pensar en lo que significaría si esa primera fila fuera esta dirección de nombre Michael más apellido Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "Y para simplificar, déjame escribirlo como M más J.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Luego, tomando un producto punto con esta incrustación E, las cosas se distribuyen muy bien, de modo que parece M punto E más J punto E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "Y fíjate en que eso significa que el valor final sería dos si el vector codifica el nombre completo Michael Jordan, y en caso contrario sería uno o algo menor que uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "Y eso es sólo una fila de esta matriz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Podrías pensar que todas las demás filas hacen en paralelo otro tipo de preguntas, sondeando otro tipo de características del vector que se está procesando.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Muy a menudo, este paso también implica añadir otro vector a la salida, que está lleno de parámetros del modelo aprendidos a partir de los datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Este otro vector se conoce como sesgo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Para nuestro ejemplo, quiero que imagines que el valor de este sesgo en ese primer componente es uno negativo, lo que significa que nuestro resultado final se parece a ese producto punto relevante, pero menos uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Podrías preguntarte muy razonablemente por qué querría que supusieras que el modelo ha aprendido esto, y en un momento verás por qué es muy limpio y agradable si tenemos aquí un valor que es positivo si y sólo si un vector codifica el nombre completo Michael Jordan, y en caso contrario es cero o negativo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "El número total de filas de esta matriz, que es algo así como el número de preguntas que se hacen, en el caso de la GPT-3, cuyas cifras hemos estado siguiendo, es de algo menos de 50.000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "De hecho, es exactamente cuatro veces el número de dimensiones de este espacio de incrustación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "Es una elección de diseño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Podrías hacerlo más, podrías hacerlo menos, pero tener un múltiplo limpio tiende a ser amigable para el hardware.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Como esta matriz llena de pesos nos mapea en un espacio dimensional superior, voy a darle la abreviatura W arriba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Voy a seguir etiquetando el vector que estamos procesando como E, y vamos a etiquetar este vector de polarización como B hacia arriba y volver a ponerlo todo abajo en el diagrama.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "En este punto, un problema es que esta operación es puramente lineal, pero el lenguaje es un proceso muy poco lineal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Si la entrada que estamos midiendo es alta para Michael más Jordan, también tendría que serlo necesariamente para Michael más Phelps y también para Alexis más Jordan, a pesar de que no estén relacionados conceptualmente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Lo que realmente quieres es un simple sí o no para el nombre completo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "Así que el siguiente paso es pasar este gran vector intermedio por una función no lineal muy sencilla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Una opción común es la que toma todos los valores negativos y los asigna a cero y deja todos los valores positivos sin cambiar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "Y siguiendo con la tradición del aprendizaje profundo de nombres demasiado rimbombantes, esta función tan sencilla se suele llamar unidad lineal rectificada, o ReLU para abreviar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "Este es el aspecto del gráfico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Tomando nuestro ejemplo imaginario en el que la primera entrada del vector intermedio es uno, si y sólo si el nombre completo es Michael Jordan, y cero o negativo en caso contrario, después de pasarlo por el ReLU, acabas con un valor muy limpio en el que todos los valores cero y negativos se recortan a cero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Así que este resultado sería uno para el nombre completo Michael Jordan y cero en caso contrario.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "En otras palabras, imita muy directamente el comportamiento de una puerta AND.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "A menudo los modelos utilizan una función ligeramente modificada que se llama JLU, que tiene la misma forma básica, sólo que es un poco más suave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Pero para nuestros fines, es un poco más limpio si sólo pensamos en la ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Además, cuando oigas a la gente referirse a las neuronas de un transformador, estarán hablando de estos valores de aquí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Siempre que veas esa imagen común de red neuronal con una capa de puntos y un montón de líneas que conectan con la capa anterior, que hemos visto antes en esta serie, eso suele transmitir esta combinación de un paso lineal, una multiplicación matricial, seguida de alguna función no lineal simple en términos, como una ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Dirías que esta neurona está activa siempre que este valor sea positivo y que está inactiva si ese valor es cero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "El siguiente paso es muy similar al primero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Multiplicas por una matriz muy grande y añades un determinado término de sesgo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "En este caso, el número de dimensiones de la salida se reduce al tamaño de ese espacio de incrustación, así que voy a llamarlo matriz de proyección descendente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "Y esta vez, en lugar de pensar en las cosas fila por fila, en realidad es mejor pensar en ellas columna por columna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Verás, otra forma de retener la multiplicación de matrices en tu cabeza es imaginar que tomas cada columna de la matriz y la multiplicas por el término correspondiente del vector que está procesando y sumas todas esas columnas reescaladas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "La razón por la que es más agradable pensar de este modo es porque aquí las columnas tienen la misma dimensión que el espacio de incrustación, así que podemos pensar en ellas como direcciones en ese espacio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Por ejemplo, imaginaremos que el modelo ha aprendido a hacer esa primera columna en esta dirección de baloncesto que suponemos que existe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Lo que eso significaría es que cuando la neurona correspondiente en esa primera posición esté activa, añadiremos esta columna al resultado final.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Pero si esa neurona estuviera inactiva, si ese número fuera cero, entonces esto no tendría ningún efecto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "Y no sólo tiene que ser baloncesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "El modelo también podría hornear en esta columna y muchas otras características que quiera asociar a algo que tenga el nombre completo de Michael Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Y al mismo tiempo, todas las demás columnas de esta matriz te están diciendo lo que se añadirá al resultado final si la neurona correspondiente está activa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "Y si tienes un sesgo en este caso, es algo que estás añadiendo cada vez, independientemente de los valores de las neuronas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Te preguntarás qué hace eso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Como ocurre con todos los objetos llenos de parámetros aquí, es difícil decirlo con exactitud.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Tal vez haya alguna contabilidad que la red tenga que hacer, pero puedes sentirte libre de ignorarla por ahora.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Haciendo nuestra notación un poco más compacta de nuevo, llamaré a esta gran matriz W hacia abajo y de forma similar llamaré a ese vector de sesgo B hacia abajo y lo pondré de nuevo en nuestro diagrama.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Como he adelantado antes, lo que haces con este resultado final es sumarlo al vector que fluyó hacia el bloque en esa posición y eso te da este resultado final.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Así, por ejemplo, si el vector que entraba codificaba tanto el nombre Michael como el apellido Jordan, como esta secuencia de operaciones activará esa puerta AND, sumará en la dirección baloncesto, de modo que lo que salga codificará todo eso junto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "Y recuerda que se trata de un proceso que ocurre en cada uno de esos vectores en paralelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "En concreto, si tomamos los números GPT-3, significa que este bloque no sólo tiene 50.000 neuronas, sino que tiene 50.000 veces el número de fichas de la entrada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Ésa es toda la operación: dos productos matriciales, cada uno con un sesgo añadido y una simple función de recorte intermedio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Cualquiera de vosotros que haya visto los vídeos anteriores de la serie reconocerá esta estructura como el tipo más básico de red neuronal que estudiamos allí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "En ese ejemplo, se entrenó para reconocer dígitos manuscritos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Aquí, en el contexto de un transformador para un gran modelo lingüístico, se trata de una pieza de una arquitectura mayor y cualquier intento de interpretar qué hace exactamente está muy entrelazado con la idea de codificar información en vectores de un espacio de incrustación de alta dimensión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Ésa es la lección principal, pero quiero dar un paso atrás y reflexionar sobre dos cosas diferentes, la primera de las cuales es una especie de contabilidad, y la segunda implica un hecho muy sugerente sobre las dimensiones superiores que en realidad no conocía hasta que indagué en los transformadores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "En los dos últimos capítulos, tú y yo empezamos a contar el número total de parámetros de GPT-3 y a ver exactamente dónde viven, así que vamos a terminar rápidamente el juego aquí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Ya he mencionado que esta matriz de proyección ascendente tiene algo menos de 50.000 filas y que cada fila coincide con el tamaño del espacio de incrustación, que para GPT-3 es de 12.288.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Multiplicándolos, nos da 604 millones de parámetros sólo para esa matriz, y la proyección hacia abajo tiene el mismo número de parámetros sólo que con una forma transpuesta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Así que, en conjunto, dan unos 1.200 millones de parámetros.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "El vector de sesgo también tiene en cuenta un par de parámetros más, pero es una proporción trivial del total, así que ni siquiera voy a mostrarlo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "En GPT-3, esta secuencia de vectores de incrustación fluye a través no de uno, sino de 96 MLP distintos, por lo que el número total de parámetros dedicados a todos estos bloques suma unos 116.000 millones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Esto supone alrededor de 2 tercios del total de parámetros de la red, y cuando lo sumas a todo lo que teníamos antes, para los bloques de atención, la incrustación y la incrustación, obtienes efectivamente ese gran total de 175.000 millones como se anunciaba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Probablemente merezca la pena mencionar que hay otro conjunto de parámetros asociados a esos pasos de normalización que esta explicación se ha saltado, pero que, al igual que el vector de sesgo, representan una proporción muy trivial del total.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "En cuanto al segundo punto de reflexión, quizá te preguntes si este ejemplo central de juguete al que hemos dedicado tanto tiempo refleja cómo se almacenan realmente los hechos en los grandes modelos lingüísticos reales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Es cierto que las filas de esa primera matriz pueden considerarse direcciones en este espacio de incrustación, y eso significa que la activación de cada neurona te dice cuánto se alinea un vector determinado con alguna dirección específica.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "También es cierto que las columnas de esa segunda matriz te dicen lo que se añadirá al resultado si esa neurona está activa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Ambas cosas no son más que hechos matemáticos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Sin embargo, las pruebas sugieren que las neuronas individuales muy raramente representan un único rasgo limpio como Michael Jordan, y en realidad puede haber una muy buena razón para que esto sea así, relacionada con una idea que flota estos días entre los investigadores de la interpretabilidad, conocida como superposición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Se trata de una hipótesis que podría ayudar a explicar tanto por qué los modelos son especialmente difíciles de interpretar como por qué escalan sorprendentemente bien.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "La idea básica es que si tienes un espacio de n dimensiones y quieres representar un montón de características diferentes utilizando direcciones que sean todas perpendiculares entre sí en ese espacio, ya sabes, de forma que si añades un componente en una dirección, no influya en ninguna de las otras direcciones, entonces el número máximo de vectores que puedes encajar es sólo n, el número de dimensiones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Para un matemático, en realidad, ésta es la definición de dimensión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Pero donde se pone interesante es si relajas un poco esa restricción y toleras algo de ruido.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Digamos que permites que esas características se representen mediante vectores que no son exactamente perpendiculares, sino casi perpendiculares, quizá entre 89 y 91 grados de separación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Si estuviéramos en dos o tres dimensiones, esto no supondría ninguna diferencia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Eso apenas te da margen de maniobra adicional para encajar más vectores, lo que hace aún más contraintuitivo que, para dimensiones mayores, la respuesta cambie drásticamente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Puedo darte una ilustración muy rápida y sucia de esto utilizando un poco de Python que va a crear una lista de vectores de 100 dimensiones, cada uno inicializado aleatoriamente, y esta lista va a contener 10.000 vectores distintos, es decir, 100 veces más vectores que dimensiones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Este gráfico de aquí muestra la distribución de ángulos entre pares de estos vectores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Como empezaron de forma aleatoria, esos ángulos pueden ser de 0 a 180 grados, pero te darás cuenta de que, incluso en el caso de los vectores aleatorios, existe un fuerte sesgo a favor de que las cosas estén más cerca de los 90 grados.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Lo que voy a hacer es ejecutar un proceso de optimización que, de forma iterativa, empuje todos estos vectores para que intenten ser más perpendiculares entre sí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Después de repetirlo muchas veces, éste es el aspecto de la distribución de ángulos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "En realidad, tenemos que ampliarlo aquí, porque todos los ángulos posibles entre pares de vectores se sitúan dentro de este estrecho margen entre 89 y 91 grados.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "En general, una consecuencia de algo conocido como el lema de Johnson-Lindenstrauss es que el número de vectores que puedes meter en un espacio que sean casi perpendiculares como éste crece exponencialmente con el número de dimensiones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Esto es muy significativo para los grandes modelos lingüísticos, que podrían beneficiarse de asociar ideas independientes con direcciones casi perpendiculares.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Significa que puede almacenar muchísimas más ideas que las dimensiones del espacio que tiene asignado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Esto podría explicar en parte por qué el rendimiento del modelo parece escalar tan bien con el tamaño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Un espacio que tiene 10 veces más dimensiones puede almacenar mucho, mucho más que 10 veces más ideas independientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "Y esto es relevante no sólo para ese espacio de incrustación donde viven los vectores que fluyen a través del modelo, sino también para ese vector lleno de neuronas en medio de ese perceptrón multicapa que acabamos de estudiar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "Es decir, con los tamaños de GPT-3, no sólo podría sondear 50.000 rasgos, sino que, si aprovechara esta enorme capacidad añadida utilizando direcciones casi perpendiculares del espacio, podría sondear muchísimos más rasgos del vector que se procesa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Pero si estuviera haciendo eso, lo que significa es que los rasgos individuales no van a ser visibles como una sola neurona que se ilumina.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "En lugar de eso, tendría que parecerse a alguna combinación específica de neuronas, a una superposición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Para cualquiera que tenga curiosidad por saber más, un término clave de búsqueda relevante aquí es autoencoder disperso, que es una herramienta que algunas de las personas de interpretabilidad utilizan para intentar extraer cuáles son las características verdaderas, aunque estén muy superpuestas en todas estas neuronas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Voy a enlazar a un par de posts antrópicos muy buenos sobre este tema.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "Llegados a este punto, no hemos tocado todos los detalles de un transformador, pero tú y yo hemos tocado los puntos más importantes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "Lo principal que quiero tratar en un próximo capítulo es el proceso de formación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "Por un lado, la respuesta breve sobre cómo funciona el entrenamiento es que se trata de retropropagación, y hemos tratado la retropropagación en un contexto aparte con capítulos anteriores de la serie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Pero hay más cosas que discutir, como la función de coste específica utilizada para los modelos lingüísticos, la idea del ajuste fino mediante el aprendizaje por refuerzo con retroalimentación humana y la noción de leyes de escalado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Nota rápida para los seguidores activos entre vosotros: hay una serie de vídeos no relacionados con el aprendizaje automático a los que estoy deseando hincar el diente antes de hacer el siguiente capítulo, así que puede que tarde un poco, pero prometo que llegará a su debido tiempo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Gracias.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]