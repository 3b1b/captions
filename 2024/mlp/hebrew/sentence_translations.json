[
 {
  "translatedText": "אם אתה מאכיל מודל שפה גדול בביטוי, מייקל ג&#39;ורדן משחק בספורט הריק, ויש לך אותו לחזות את מה שיבוא אחר כך, והוא חוזה נכון את הכדורסל, זה יצביע על כך שאיפשהו, בתוך מאות מיליארדי הפרמטרים שלו, הוא אפוי ידע על אדם ספציפי ועל הספורט הספציפי שלו.",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "ואני חושב שבאופן כללי, לכל מי ששיחק עם אחד מהדגמים האלה יש תחושה ברורה שהוא שונן המון המון עובדות.",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "אז שאלה סבירה שאתה יכול לשאול היא איך זה בדיוק עובד?",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "ואיפה העובדות האלה חיות?",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "בדצמבר האחרון, כמה חוקרים מ-Google DeepMind פרסמו על עבודה על השאלה הזו, והם השתמשו בדוגמה הספציפית הזו של התאמת ספורטאים לספורט שלהם.",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "ולמרות שהבנה מכניסטית מלאה של האופן שבו עובדות מאוחסנות נותרה בלתי פתורה, היו להן כמה תוצאות חלקיות מעניינות, כולל המסקנה הכללית ברמה גבוהה שנראה שהעובדות חיות בתוך חלק ספציפי של הרשתות האלה, המכונה בדמיון רב-שכבתי perceptrons, או MLPs בקיצור.",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "בשני הפרקים האחרונים, אתה ואני חפרנו בפרטים שמאחורי שנאים, הארכיטקטורה העומדת בבסיס מודלים של שפות גדולות, וגם בבסיס הרבה AI מודרניים אחרים.",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "בפרק האחרון, התמקדנו ביצירה בשם Attention.",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "והשלב הבא עבורך ולי הוא לחפור בפרטים של מה שקורה בתוך התפיסות הרב-שכבתיות הללו, שמרכיבות את החלק הגדול השני של הרשת.",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "החישוב כאן למעשה פשוט יחסית, במיוחד כאשר משווים אותו לתשומת לב.",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "זה מסתכם בעצם בזוג מכפלות מטריצות עם משהו פשוט באמצע.",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "עם זאת, הפרשנות של מה שהחישובים האלה עושים היא מאתגרת ביותר.",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "המטרה העיקרית שלנו כאן היא לעבור בין החישובים ולהפוך אותם לבלתי נשכחים, אבל אני רוצה לעשות זאת בהקשר של הצגת דוגמה ספציפית כיצד אחד מהבלוקים הללו יכול, לפחות באופן עקרוני, לאחסן עובדה קונקרטית.",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "באופן ספציפי, זה יאחסן את העובדה שמייקל ג&#39;ורדן משחק כדורסל.",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "עלי לציין שהפריסה כאן בהשראת שיחה שניהלתי עם אחד מאותם חוקרי DeepMind, ניל ננדה.",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "לרוב, אני מניח שצפית בשני הפרקים האחרונים, או שאחרת יש לך תחושה בסיסית למה זה שנאי, אבל רענון אף פעם לא הזיק, אז הנה התזכורת המהירה לזרימה הכוללת.",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "אתה ואני למדנו מודל שמאומן לקלוט קטע טקסט ולחזות את ההמשך.",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "טקסט הקלט הזה מחולק תחילה לחבורה של אסימונים, כלומר נתחים קטנים שהם בדרך כלל מילים או פיסות קטנות של מילים, וכל אסימון משויך לוקטור בעל מימד גבוה, כלומר רשימה ארוכה של מספרים.",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "רצף הווקטורים הזה עובר שוב ושוב דרך שני סוגים של פעולות, קשב, שמאפשר לוקטורים להעביר מידע אחד בין השני, ואז התפיסטרים הרב-שכבתיים, הדבר שאנחנו הולכים לחפור בו היום, וגם יש שלב נורמליזציה מסוים בין לבין.",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "לאחר שרצף הוקטורים זורם דרך איטרציות רבות ושונות של שני הבלוקים הללו, עד הסוף, התקווה היא שכל וקטור ספג מספיק מידע, הן מההקשר, מכל המילים האחרות בקלט, והן. גם מהידע הכללי שנאפה במשקולות המודל באמצעות אימון, שניתן להשתמש בו כדי לחזות איזה אסימון מגיע לאחר מכן.",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "אחד הרעיונות המרכזיים שאני רוצה שיהיה לכם בראש הוא שכל הוקטורים האלה חיים במרחב מאוד מאוד גבוה, וכשאתם חושבים על המרחב הזה, כיוונים שונים יכולים לקודד סוגים שונים של משמעות.",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "אז דוגמה מאוד קלאסית שאני אוהב לחזור אליה היא איך אם אתה מסתכל על הטבעת האישה ומחסיר את ההטבעה של הגבר, ואתה עושה את הצעד הקטן הזה ומוסיף אותו לעוד שם עצם זכרי, משהו כמו דוד, אתה נוחת במקום מאוד מאוד קרוב לשם העצם הנשי המקביל.",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "במובן זה, הכיוון המסוים הזה צופן מידע מגדרי.",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "הרעיון הוא שכיוונים רבים אחרים במרחב הסופר-ממדי הזה יכולים להתאים לתכונות אחרות שהדגם עשוי לרצות לייצג.",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "בשנאי, הווקטורים האלה לא רק מקודדים את המשמעות של מילה אחת.",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "כשהם זורמים ברשת, הם סופגים משמעות הרבה יותר עשירה המבוססת על כל ההקשר סביבם, וגם על סמך הידע של המודל.",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "בסופו של דבר, כל אחד צריך לקודד משהו הרבה, הרבה מעבר למשמעות של מילה בודדת, מכיוון שהוא צריך להספיק כדי לחזות את מה שיבוא אחר כך.",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "כבר ראינו איך בלוקי קשב מאפשרים לך לשלב הקשר, אבל רוב הפרמטרים של המודל חיים בעצם בתוך בלוקי ה-MLP, ומחשבה אחת למה שהם עשויים לעשות היא שהם מציעים קיבולת נוספת לאחסן עובדות.",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "כמו שאמרתי, השיעור כאן יתמקד בדוגמה של צעצוע הבטון של איך בדיוק זה יכול לאחסן את העובדה שמייקל ג&#39;ורדן משחק כדורסל.",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "עכשיו, דוגמה צעצוע זו תדרוש שאתה ואני נניח כמה הנחות לגבי החלל הגבוה.",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "ראשית, נניח שאחד הכיוונים מייצג את רעיון השם הפרטי מייקל, ואז כיוון אחר כמעט מאונך מייצג את הרעיון של שם המשפחה ג&#39;ורדן, ואז בכל זאת כיוון שלישי יייצג את רעיון הכדורסל.",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "אז ספציפית, מה שאני מתכוון בזה הוא אם אתה מסתכל ברשת ותוציא את אחד הוקטורים המעובדים, אם המוצר הנקודה שלו עם השם הפרטי הזה מייקל כיוון הוא אחד, זה מה שזה אומר שהווקטור יהיה מקודד הרעיון של אדם עם השם הפרטי הזה.",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "אחרת, תוצר הנקודה הזה יהיה אפס או שלילי, כלומר הווקטור לא באמת מתיישר עם הכיוון הזה.",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "ולמען הפשטות, בואו נתעלם לחלוטין מהשאלה ההגיונית של מה זה עשוי להיות אם המוצר הנקודה הזה היה גדול מאחד.",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "באופן דומה, המוצר הנקודתי שלו עם הכיוונים האחרים האלה יגיד לך אם הוא מייצג את שם המשפחה ג&#39;ורדן או כדורסל.",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "אז נניח שווקטור נועד לייצג את השם המלא, מייקל ג&#39;ורדן, אז המוצר הנקודות שלו עם שני הכיוונים האלה צריך להיות אחד.",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "מכיוון שהטקסט מייקל ג&#39;ורדן משתרע על פני שני אסימונים שונים, זה גם אומר שעלינו להניח שבלוק קשב קודם העביר בהצלחה מידע לשני משני הוקטורים הללו כדי להבטיח שהוא יכול לקודד את שני השמות.",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "עם כל אלה כמו ההנחות, בואו עכשיו נצלול לתוך הבשר של השיעור.",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "מה קורה בתוך פרצפטרון רב שכבתי?",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "אולי תחשבו על רצף הווקטורים הזה שזורם לתוך הבלוק, וזכרו שכל וקטור היה משויך במקור לאחד האסימונים מטקסט הקלט.",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "מה שהולך לקרות הוא שכל וקטור בודד מהרצף הזה עובר סדרה קצרה של פעולות, נפרק אותן תוך רגע, ובסוף, נקבל וקטור נוסף עם אותו מימד.",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "הווקטור האחר הזה יתווסף לזה המקורי שזרם פנימה, והסכום הזה הוא התוצאה הזורמת החוצה.",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "רצף הפעולות הזה הוא משהו שאתה מיישם על כל וקטור ברצף, המשויך לכל אסימון בקלט, והכל קורה במקביל.",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "במיוחד, הווקטורים לא מדברים זה עם זה בשלב הזה, כולם סוג של עושים את שלהם.",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "ומבחינתך ולי, זה למעשה הופך את זה להרבה יותר פשוט, כי זה אומר שאם אנחנו מבינים מה קורה רק לאחד מהווקטורים דרך הבלוק הזה, אנחנו למעשה מבינים מה קורה לכולם.",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "כשאני אומר שהגוש הזה הולך לקודד את העובדה שמייקל ג&#39;ורדן משחק כדורסל, מה שאני מתכוון הוא שאם וקטור זורם פנימה שמקודד את השם הפרטי מייקל ואת שם המשפחה ג&#39;ורדן, אז רצף החישובים הזה יפיק משהו שכולל את הכדורסל בכיוון הזה, וזה מה שיוסיף לוקטור במיקום זה.",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "השלב הראשון של תהליך זה נראה כמו הכפלת הווקטור הזה במטריצה גדולה מאוד.",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "אין הפתעות שם, זו למידה עמוקה.",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "והמטריצה הזו, כמו כל האחרות שראינו, מלאה בפרמטרים של מודל שנלמדים מנתונים, שעלולים לחשוב עליהם כעל חבורה של כפתורים וחוגים שמשתנים ומכוונים כדי לקבוע מהי התנהגות המודל .",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "כעת, דרך נחמדה אחת לחשוב על כפל מטריצה היא לדמיין כל שורה של המטריצה הזו כווקטור משלה, ולקחת חבורה של תוצרי נקודות בין השורות הללו לווקטור המעובד, שאותם אני אתן כ-E להטמעה.",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "לדוגמה, נניח שהשורה הראשונה במקרה השתווה לכיוון השם הפרטי הזה של מייקל שאנו מניחים שקיים.",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "זה אומר שהרכיב הראשון בפלט הזה, המוצר הנקודות הזה כאן, יהיה אחד אם הווקטור הזה מקודד את השם הפרטי מייקל, ואפס או שלילי אחרת.",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "אפילו יותר כיף, קחו רגע לחשוב מה זה היה אומר אם השורה הראשונה הייתה השם הפרטי הזה מייקל פלוס שם המשפחה כיוון ג&#39;ורדן.",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "ולמען הפשטות, הרשו לי להמשיך ולכתוב את זה בתור M פלוס J.",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "לאחר מכן, אם לוקחים מוצר נקודה עם הטבעה E זו, הדברים מתפזרים ממש יפה, כך שזה נראה כמו M dot E ועוד J dot E.",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "ושימו לב איך זה אומר שהערך האולטימטיבי יהיה שניים אם הווקטור מקודד את השם המלא מייקל ג&#39;ורדן, אחרת הוא יהיה אחד או משהו קטן מאחד.",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "וזו רק שורה אחת במטריצה הזו.",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "אתה יכול לחשוב על כל השורות האחרות כמו שואלים במקביל כמה סוגים אחרים של שאלות, בדיקה בכמה מיני תכונות אחרות של הווקטור המעובד.",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "לעתים קרובות מאוד שלב זה כולל גם הוספת וקטור נוסף לפלט, שהוא מלא בפרמטרים של מודל הנלמדים מנתונים.",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "וקטור אחר זה ידוע בשם ההטיה.",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "לדוגמה שלנו, אני רוצה שתדמיינו שהערך של ההטיה הזו ברכיב הראשון הזה הוא שלילי, כלומר הפלט הסופי שלנו נראה כמו המוצר הנקודה הרלוונטי הזה, אבל מינוס אחד.",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "אפשר בהחלט לשאול למה אני רוצה שתניח שהמודל למד את זה, ובעוד רגע תראה למה זה מאוד נקי ונחמד אם יש לנו כאן ערך שהוא חיובי אם ורק אם וקטור מקודד את השם המלא מייקל ג&#39;ורדן, וחוץ מזה זה אפס או שלילי.",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "המספר הכולל של שורות במטריצה הזו, שהוא משהו כמו מספר השאלות הנשאלות, במקרה של GPT-3, שמספרים שלו עקבנו, הוא קצת פחות מ-50,000.",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "למעשה, זה בדיוק פי ארבעה ממספר הממדים בחלל ההטמעה הזה.",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "זו בחירה עיצובית.",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "אתה יכול לעשות את זה יותר, אתה יכול לעשות את זה פחות, אבל ריבוי נקי נוטה להיות ידידותי לחומרה.",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "מכיוון שמטריצה מלאה במשקלים ממפה אותנו למרחב ממדי גבוה יותר, אני אתן לה את הקיצור W up.",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "אני אמשיך לתייג את הווקטור שאנו מעבדים כ-E, ובואו נסמן את וקטור ההטיה הזה כ-B למעלה ונחזיר את כל זה בתרשים.",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "בשלב זה, הבעיה היא שהפעולה הזו היא ליניארית בלבד, אבל השפה היא תהליך מאוד לא ליניארי.",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "אם הערך שאנו מודדים הוא גבוה עבור מייקל פלוס ג&#39;ורדן, זה יהיה בהכרח מופעל במידה מסוימת על ידי מייקל פלוס פלפס וגם אלכסיס פלוס ג&#39;ורדן, למרות אלה שאינם קשורים רעיונית.",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "מה שאתה באמת רוצה זה כן או לא פשוט עבור השם המלא.",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "אז השלב הבא הוא להעביר את וקטור הביניים הגדול הזה דרך פונקציה לא ליניארית פשוטה מאוד.",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "בחירה נפוצה היא כזו שלוקחת את כל הערכים השליליים וממפה אותם לאפס ומשאירה את כל הערכים החיוביים ללא שינוי.",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "ובהמשך למסורת הלמידה העמוקה של שמות מפוארים מדי, הפונקציה הפשוטה הזו נקראת לעתים קרובות היחידה הליניארית המתוקנת, או בקיצור ReLU.",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "כך נראה הגרף.",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "אז אם לוקחים את הדוגמה המדומיינת שלנו שבה הכניסה הראשונה של וקטור הביניים היא אחת, אם ורק אם השם המלא הוא מייקל ג&#39;ורדן ואפס או שלילי אחרת, לאחר שתעביר את זה דרך ה-ReLU, אתה בסופו של דבר עם ערך נקי מאוד שבו כל מהערכים האפסים והשליליים פשוט יחתכו לאפס.",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "אז הפלט הזה יהיה אחד עבור השם המלא מייקל ג&#39;ורדן ואפס אחרת.",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "במילים אחרות, זה מחקה בצורה ישירה מאוד את ההתנהגות של שער AND.",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "לעתים קרובות דגמים ישתמשו בפונקציה מעט שונה שנקראת JLU, בעלת אותה צורה בסיסית, היא רק קצת יותר חלקה.",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "אבל למטרותינו, זה קצת יותר נקי אם נחשוב רק על ה-ReLU.",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "כמו כן, כשאתה שומע אנשים מתייחסים לנוירונים של שנאי, הם מדברים על הערכים האלה כאן.",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "בכל פעם שאתה רואה את תמונת הרשת העצבית הנפוצה עם שכבת נקודות וחבורה של קווים המתחברים לשכבה הקודמת, שהייתה לנו קודם בסדרה זו, זה נועד בדרך כלל להעביר את השילוב הזה של צעד ליניארי, כפל מטריצה, ואחריו איזו פונקציה לא ליניארית פשוטה מבחינה מונחית כמו ReLU.",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "הייתם אומרים שהנוירון הזה פעיל בכל פעם שהערך הזה חיובי ושהוא לא פעיל אם הערך הזה הוא אפס.",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "השלב הבא נראה דומה מאוד לשלב הראשון.",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "אתה מכפיל במטריצה גדולה מאוד ומוסיף על מונח הטיה מסוים.",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "במקרה זה, מספר הממדים בפלט ירד בחזרה לגודל החלל ההטמעה הזה, אז אני אמשיך לקרוא לזה מטריצת ההקרנה למטה.",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "והפעם, במקום לחשוב על דברים שורה אחר שורה, דווקא יותר נחמד לחשוב על זה טור אחר טור.",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "אתה מבין, דרך נוספת שבה אתה יכול להחזיק כפל מטריצה בראש שלך היא לדמיין לקחת כל עמודה של המטריצה ולהכפיל אותה במונח המתאים בווקטור שהיא מעבדת ומחברת את כל העמודות המותאמות מחדש.",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "הסיבה שנחמד יותר לחשוב על כך היא כי כאן לעמודים יש את אותו מימד כמו חלל ההטמעה, כך שנוכל לחשוב עליהם כעל כיוונים בחלל הזה.",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "למשל, נדמיין שהמודל למד להפוך את העמוד הראשון הזה לכיוון הכדורסל הזה שאנחנו מניחים שקיים.",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "מה שזה אומר הוא שכאשר הנוירון הרלוונטי במיקום הראשון הזה פעיל, נוסיף את העמודה הזו לתוצאה הסופית.",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "אבל אם הנוירון הזה לא היה פעיל, אם המספר הזה היה אפס, אז זה לא ישפיע.",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "וזה לא חייב להיות רק כדורסל.",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "הדגם יכול גם לאפות בטור הזה ובמאפיינים רבים אחרים שהוא רוצה לשייך למשהו שיש לו את השם המלא מייקל ג&#39;ורדן.",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "ובאותו זמן, כל העמודות האחרות במטריצה הזו מספרות לך מה יתווסף לתוצאה הסופית אם הנוירון המתאים יהיה פעיל.",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "ואם יש לך הטיה במקרה הזה, זה משהו שאתה פשוט מוסיף בכל פעם, ללא קשר לערכי הנוירון.",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "אתה עשוי לתהות מה זה עושה.",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "כמו בכל האובייקטים המלאים בפרמטרים כאן, קצת קשה לומר בדיוק.",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "אולי ישנה הנהלת חשבונות שהרשת צריכה לעשות, אבל אתה יכול להרגיש חופשי להתעלם ממנה לעת עתה.",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "אם הופך את הסימון שלנו לקצת יותר קומפקטי שוב, אקרא למטריצה הגדולה הזו W למטה ובאופן דומה אקרא לוקטור ההטיה B למטה ואחזיר אותו לתרשים שלנו.",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "כמו שראיתי קודם, מה שאתה עושה עם התוצאה הסופית הזו הוא להוסיף אותה לווקטור שזרם לבלוק במיקום הזה וזה משיג לך את התוצאה הסופית הזו.",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "אז למשל, אם הווקטור שזורם קידד גם את השם הפרטי מייקל וגם את שם המשפחה ג&#39;ורדן, אז בגלל שרצף הפעולות הזה יפעיל את שער ה-AND הזה, הוא יוסיף את כיוון הכדורסל, אז מה שיצוץ יקודד את כל אלה ביחד.",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "וזכור, זהו תהליך שקורה לכל אחד מהווקטורים הללו במקביל.",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "בפרט, אם לוקחים את מספרי GPT-3, זה אומר שבלוק הזה לא יש רק 50,000 נוירונים, יש בו פי 50,000 ממספר האסימונים בקלט.",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "אז זה כל הפעולה, שני מוצרי מטריקס, כל אחד עם הטיה ופונקציית חיתוך פשוטה ביניהם.",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "כל אחד מכם שצפה בסרטונים הקודמים של הסדרה יזהה את המבנה הזה כרשת עצבית מהסוג הבסיסי ביותר שלמדנו שם.",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "בדוגמה זו, הוא הוכשר לזהות ספרות בכתב יד.",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "כאן, בהקשר של שנאי למודל שפה גדול, זהו חלק אחד בארכיטקטורה גדולה יותר וכל ניסיון לפרש מה בדיוק הוא עושה שזור מאוד ברעיון של קידוד מידע לוקטורים של מרחב הטבעה במימד גבוה. .",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "זה השיעור המרכזי, אבל אני כן רוצה ללכת אחורה ולהרהר בשני דברים שונים, הראשון שבהם הוא סוג של הנהלת חשבונות, והשני כולל עובדה מאוד מעוררת מחשבה על ממדים גבוהים יותר שלמעשה לא ידעתי. יודע עד שחפרתי בשנאים.",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "בשני הפרקים האחרונים, אתה ואני התחלנו לספור את המספר הכולל של הפרמטרים ב-GPT-3 ולראות בדיוק היכן הם חיים, אז בואו נסיים מהר את המשחק כאן.",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "כבר ציינתי איך למטריצת ההקרנה הזו יש קצת פחות מ-50,000 שורות ושכל שורה מתאימה לגודל של חלל ההטמעה, שעבור GPT-3 הוא 12,288.",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "מכפילים את אלה יחד, זה נותן לנו 604 מיליון פרמטרים רק עבור המטריצה הזו, ולהקרנה למטה יש אותו מספר של פרמטרים רק עם צורה שעברה טרנספוזיציה.",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "אז ביחד, הם נותנים כ-1.2 מיליארד פרמטרים.",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "וקטור ההטיה אחראי גם לעוד כמה פרמטרים, אבל זה חלק טריוויאלי מהסך הכל, אז אני אפילו לא הולך להראות את זה.",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "ב-GPT-3, רצף זה של וקטורים הטבעה זורם דרך לא אחד, אלא 96 MLPs נפרדים, כך שמספר הפרמטרים הכולל המוקדש לכל הבלוקים הללו מסתכם בכ-116 מיליארד.",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "מדובר בסביבות 2 שליש מסך הפרמטרים ברשת, וכשאתה מוסיף את זה לכל מה שהיה לנו קודם, עבור בלוקי הקשב, ההטמעה וההשבתה, אתה אכן מקבל את הסכום הכולל של 175 מיליארד כפי שפורסם.",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "מן הסתם כדאי להזכיר שיש קבוצה נוספת של פרמטרים הקשורים לאותם שלבי נורמליזציה שההסבר הזה דילג עליהם, אבל כמו וקטור ההטיה, הם מהווים חלק מאוד טריוויאלי מהסך הכל.",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "באשר לנקודת השתקפות השנייה הזו, אתם עשויים לתהות האם דוגמא הצעצוע המרכזית הזו שהשקענו בה כל כך הרבה זמן משקפת את האופן שבו עובדות מאוחסנות בפועל במודלים אמיתיים של שפות גדולות.",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "נכון שניתן לחשוב על השורות של אותה מטריצה ראשונה ככיוונים במרחב ההטמעה הזה, וזה אומר שההפעלה של כל נוירון אומרת לך עד כמה וקטור נתון מתיישר עם כיוון ספציפי כלשהו.",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "זה גם נכון שהעמודות של המטריצה השנייה מספרות לך מה יתווסף לתוצאה אם הנוירון הזה יהיה פעיל.",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "שני אלה הם רק עובדות מתמטיות.",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "עם זאת, הראיות מצביעות על כך שנוירונים בודדים מייצגים רק לעתים רחוקות תכונה אחת נקייה כמו מייקל ג&#39;ורדן, וייתכן שלמעשה יש סיבה טובה מאוד שזה המקרה, הקשורה לרעיון שצף סביב חוקרי פרשנות המכונה בימינו סופרפוזיציה.",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "זוהי השערה שעשויה לעזור להסביר גם מדוע המודלים קשים במיוחד לפירוש וגם מדוע הם מתקדמים בצורה מפתיעה.",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "הרעיון הבסיסי הוא שאם יש לך מרחב נ-ממדי ואתה רוצה לייצג חבורה של תכונות שונות באמצעות כיוונים שכולם מאונכים זה לזה במרחב הזה, אתה יודע, ככה אם אתה מוסיף רכיב בכיוון אחד, זה לא משפיע על אף אחד מהכיוונים האחרים, אז המספר המרבי של הוקטורים שאתה יכול להתאים הוא רק n, מספר הממדים.",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "למתמטיקאי, למעשה, זו ההגדרה של מימד.",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "אבל המקום שבו זה נהיה מעניין הוא אם תרגע קצת את האילוץ הזה ותסבול קצת רעש.",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "נניח שאתה מאפשר לתכונות האלה להיות מיוצגות על ידי וקטורים שאינם בדיוק מאונכים, הם פשוט כמעט מאונכים, אולי בין 89 ל-91 מעלות זה מזה.",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "אם היינו בשני מימדים, זה לא משנה.",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "זה כמעט ולא נותן לך מקום להתנועע נוסף כדי להכניס בו יותר וקטורים, מה שהופך את זה למנוגד יותר מאשר לממדים גבוהים יותר, התשובה משתנה באופן דרמטי.",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "אני יכול לתת לך המחשה ממש מהירה ומלוכלכת של זה באמצעות איזה פייתון מחורבן שייצור רשימה של וקטורים בעלי 100 ממדים, כל אחד מאותחל באופן אקראי, והרשימה הזו תכיל 10,000 וקטורים נפרדים, אז פי 100 וקטורים כמו שיש מימדים.",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "העלילה הזו ממש כאן מציגה את התפלגות הזוויות בין זוגות של וקטורים אלה.",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "אז בגלל שהם התחילו באקראי, הזוויות האלה יכולות להיות כל דבר שבין 0 ל-180 מעלות, אבל תשים לב שכבר, אפילו רק עבור וקטורים אקראיים, יש הטיה כבדה לכך שדברים יהיו קרובים יותר ל-90 מעלות.",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "ואז מה שאני הולך לעשות זה להפעיל תהליך אופטימיזציה מסוים שדוחף באופן איטרטיבי את כל הווקטורים האלה כך שהם מנסים להיות מאונכים יותר אחד לשני.",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "לאחר שחזרתי על זה פעמים רבות ושונות, הנה איך נראית התפלגות הזוויות.",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "אנחנו צריכים למעשה להגדיל את זה כאן כי כל הזוויות האפשריות בין זוגות של וקטורים יושבות בטווח הצר הזה שבין 89 ל-91 מעלות.",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "באופן כללי, תוצאה של משהו המכונה הלמה של ג&#39;ונסון-לינדנשטראוס היא שמספר הוקטורים שאתה יכול לדחוס לתוך מרחב שהם כמעט מאונכים כמו זה גדל באופן אקספוננציאלי עם מספר הממדים.",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "זה מאוד משמעותי עבור מודלים של שפה גדולים, שעשויים להפיק תועלת משיוך רעיונות עצמאיים לכיוונים כמעט מאונכים.",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "זה אומר שהוא יכול לאחסן הרבה הרבה יותר רעיונות ממה שיש מידות בחלל שהוא מוקצה.",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "זה עשוי להסביר חלקית מדוע נראה כי ביצועי הדגם משתנים כל כך טוב עם הגודל.",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "חלל שיש לו פי 10 ממדים יכול לאחסן דרך, הרבה יותר מפי 10 רעיונות עצמאיים.",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "וזה רלוונטי לא רק למרחב ההטמעה שבו חיים הווקטורים הזורמים דרך המודל, אלא גם לוקטור המלא בנוירונים באמצע אותו תפיסה רב-שכבתית שזה עתה למדנו.",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "כלומר, בגדלים של GPT-3, ייתכן שהוא לא רק מחפש 50,000 מאפיינים, אלא אם הוא במקום זאת ימנף את הקיבולת הנוספת העצומה הזו על ידי שימוש בכיוונים כמעט מאונכים של החלל, הוא יכול היה לחקור הרבה הרבה יותר תכונות של הווקטור המעובד.",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "אבל אם זה היה עושה את זה, המשמעות היא שמאפיינים בודדים לא יהיו גלויים כנוירון אחד שנדלק.",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "זה יצטרך להיראות כמו שילוב ספציפי של נוירונים במקום זאת, סופרפוזיציה.",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "לכל אחד מכם שמעוניין ללמוד עוד, מונח חיפוש רלוונטי כאן הוא מקודד אוטומטי דליל, שהוא כלי שחלק מהאפשרויות שאנשים משתמשים בו כדי לנסות לחלץ מהן התכונות האמיתיות, גם אם הן מונחות מאוד על כל אלה. נוירונים.",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "אני אקשר לכמה פוסטים אנתרופיים נהדרים על זה.",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "בשלב זה, לא נגענו בכל פרט של שנאי, אבל אתה ואני פגענו בנקודות החשובות ביותר.",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "הדבר העיקרי שאני רוצה לכסות בפרק הבא הוא תהליך האימון.",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "מצד אחד, התשובה הקצרה לאיך עובדת אימון היא שהכל זה התפשטות לאחור, וכיסינו את ההפצה לאחור בהקשר נפרד עם פרקים קודמים בסדרה.",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "אבל יש עוד מה לדון, כמו פונקציית העלות הספציפית המשמשת למודלים של שפה, הרעיון של כוונון עדין באמצעות למידת חיזוק עם משוב אנושי, והרעיון של חוקי קנה מידה.",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "הערה מהירה לעוקבים הפעילים שביניכם, ישנם מספר סרטונים שאינם קשורים ללימוד מכונה שאני מתרגש לנעוץ בהם שיניים לפני שאעשה את הפרק הבא, אז זה עשוי להיות זמן מה, אבל אני מבטיח את זה יבוא בבוא הזמן.",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "תודה לך.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]