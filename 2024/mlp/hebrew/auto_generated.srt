1
00:00:00,000 --> 00:00:05,299
אם אתה מאכיל מודל שפה גדול בביטוי, מייקל ג&#39;ורדן משחק בספורט הריק, 

2
00:00:05,299 --> 00:00:12,036
ויש לך אותו לחזות את מה שיבוא אחר כך, והוא חוזה נכון את הכדורסל, זה יצביע על כך שאיפשהו, 

3
00:00:12,036 --> 00:00:18,320
בתוך מאות מיליארדי הפרמטרים שלו, הוא אפוי ידע על אדם ספציפי ועל הספורט הספציפי שלו.

4
00:00:18,940 --> 00:00:22,301
ואני חושב שבאופן כללי, לכל מי ששיחק עם אחד מהדגמים 

5
00:00:22,301 --> 00:00:25,400
האלה יש תחושה ברורה שהוא שונן המון המון עובדות.

6
00:00:25,700 --> 00:00:29,160
אז שאלה סבירה שאתה יכול לשאול היא איך זה בדיוק עובד?

7
00:00:29,160 --> 00:00:31,040
ואיפה העובדות האלה חיות?

8
00:00:35,720 --> 00:00:40,492
בדצמבר האחרון, כמה חוקרים מ-Google DeepMind פרסמו על עבודה על השאלה הזו, 

9
00:00:40,492 --> 00:00:44,480
והם השתמשו בדוגמה הספציפית הזו של התאמת ספורטאים לספורט שלהם.

10
00:00:44,900 --> 00:00:50,302
ולמרות שהבנה מכניסטית מלאה של האופן שבו עובדות מאוחסנות נותרה בלתי פתורה, 

11
00:00:50,302 --> 00:00:56,215
היו להן כמה תוצאות חלקיות מעניינות, כולל המסקנה הכללית ברמה גבוהה שנראה שהעובדות 

12
00:00:56,215 --> 00:01:02,640
חיות בתוך חלק ספציפי של הרשתות האלה, המכונה בדמיון רב-שכבתי perceptrons, או MLPs בקיצור.

13
00:01:03,120 --> 00:01:07,073
בשני הפרקים האחרונים, אתה ואני חפרנו בפרטים שמאחורי שנאים, 

14
00:01:07,073 --> 00:01:12,500
הארכיטקטורה העומדת בבסיס מודלים של שפות גדולות, וגם בבסיס הרבה AI מודרניים אחרים.

15
00:01:13,060 --> 00:01:16,200
בפרק האחרון, התמקדנו ביצירה בשם Attention.

16
00:01:16,840 --> 00:01:22,495
והשלב הבא עבורך ולי הוא לחפור בפרטים של מה שקורה בתוך התפיסות הרב-שכבתיות הללו, 

17
00:01:22,495 --> 00:01:25,040
שמרכיבות את החלק הגדול השני של הרשת.

18
00:01:25,680 --> 00:01:30,100
החישוב כאן למעשה פשוט יחסית, במיוחד כאשר משווים אותו לתשומת לב.

19
00:01:30,560 --> 00:01:34,980
זה מסתכם בעצם בזוג מכפלות מטריצות עם משהו פשוט באמצע.

20
00:01:35,720 --> 00:01:40,460
עם זאת, הפרשנות של מה שהחישובים האלה עושים היא מאתגרת ביותר.

21
00:01:41,560 --> 00:01:45,887
המטרה העיקרית שלנו כאן היא לעבור בין החישובים ולהפוך אותם לבלתי נשכחים, 

22
00:01:45,887 --> 00:01:50,695
אבל אני רוצה לעשות זאת בהקשר של הצגת דוגמה ספציפית כיצד אחד מהבלוקים הללו יכול, 

23
00:01:50,695 --> 00:01:53,160
לפחות באופן עקרוני, לאחסן עובדה קונקרטית.

24
00:01:53,580 --> 00:01:57,080
באופן ספציפי, זה יאחסן את העובדה שמייקל ג&#39;ורדן משחק כדורסל.

25
00:01:58,080 --> 00:02:03,200
עלי לציין שהפריסה כאן בהשראת שיחה שניהלתי עם אחד מאותם חוקרי DeepMind, ניל ננדה.

26
00:02:04,060 --> 00:02:10,181
לרוב, אני מניח שצפית בשני הפרקים האחרונים, או שאחרת יש לך תחושה בסיסית למה זה שנאי, 

27
00:02:10,181 --> 00:02:14,700
אבל רענון אף פעם לא הזיק, אז הנה התזכורת המהירה לזרימה הכוללת.

28
00:02:15,340 --> 00:02:21,320
אתה ואני למדנו מודל שמאומן לקלוט קטע טקסט ולחזות את ההמשך.

29
00:02:21,720 --> 00:02:28,383
טקסט הקלט הזה מחולק תחילה לחבורה של אסימונים, כלומר נתחים קטנים שהם בדרך כלל מילים או 

30
00:02:28,383 --> 00:02:35,280
פיסות קטנות של מילים, וכל אסימון משויך לוקטור בעל מימד גבוה, כלומר רשימה ארוכה של מספרים.

31
00:02:35,840 --> 00:02:40,892
רצף הווקטורים הזה עובר שוב ושוב דרך שני סוגים של פעולות, קשב, 

32
00:02:40,892 --> 00:02:46,514
שמאפשר לוקטורים להעביר מידע אחד בין השני, ואז התפיסטרים הרב-שכבתיים, 

33
00:02:46,514 --> 00:02:52,300
הדבר שאנחנו הולכים לחפור בו היום, וגם יש שלב נורמליזציה מסוים בין לבין.

34
00:02:53,300 --> 00:03:00,230
לאחר שרצף הוקטורים זורם דרך איטרציות רבות ושונות של שני הבלוקים הללו, עד הסוף, 

35
00:03:00,230 --> 00:03:06,984
התקווה היא שכל וקטור ספג מספיק מידע, הן מההקשר, מכל המילים האחרות בקלט, והן. 

36
00:03:06,984 --> 00:03:11,458
גם מהידע הכללי שנאפה במשקולות המודל באמצעות אימון, 

37
00:03:11,458 --> 00:03:16,020
שניתן להשתמש בו כדי לחזות איזה אסימון מגיע לאחר מכן.

38
00:03:16,860 --> 00:03:22,726
אחד הרעיונות המרכזיים שאני רוצה שיהיה לכם בראש הוא שכל הוקטורים האלה חיים במרחב מאוד 

39
00:03:22,726 --> 00:03:28,800
מאוד גבוה, וכשאתם חושבים על המרחב הזה, כיוונים שונים יכולים לקודד סוגים שונים של משמעות.

40
00:03:30,120 --> 00:03:35,255
אז דוגמה מאוד קלאסית שאני אוהב לחזור אליה היא איך אם אתה מסתכל על הטבעת 

41
00:03:35,255 --> 00:03:41,603
האישה ומחסיר את ההטבעה של הגבר, ואתה עושה את הצעד הקטן הזה ומוסיף אותו לעוד שם עצם זכרי, 

42
00:03:41,603 --> 00:03:46,240
משהו כמו דוד, אתה נוחת במקום מאוד מאוד קרוב לשם העצם הנשי המקביל.

43
00:03:46,440 --> 00:03:50,880
במובן זה, הכיוון המסוים הזה צופן מידע מגדרי.

44
00:03:51,640 --> 00:03:55,678
הרעיון הוא שכיוונים רבים אחרים במרחב הסופר-ממדי הזה 

45
00:03:55,678 --> 00:03:59,640
יכולים להתאים לתכונות אחרות שהדגם עשוי לרצות לייצג.

46
00:04:01,400 --> 00:04:06,180
בשנאי, הווקטורים האלה לא רק מקודדים את המשמעות של מילה אחת.

47
00:04:06,680 --> 00:04:13,116
כשהם זורמים ברשת, הם סופגים משמעות הרבה יותר עשירה המבוססת על כל ההקשר סביבם, 

48
00:04:13,116 --> 00:04:15,180
וגם על סמך הידע של המודל.

49
00:04:15,880 --> 00:04:20,522
בסופו של דבר, כל אחד צריך לקודד משהו הרבה, הרבה מעבר למשמעות של מילה בודדת, 

50
00:04:20,522 --> 00:04:23,760
מכיוון שהוא צריך להספיק כדי לחזות את מה שיבוא אחר כך.

51
00:04:24,560 --> 00:04:31,389
כבר ראינו איך בלוקי קשב מאפשרים לך לשלב הקשר, אבל רוב הפרמטרים של המודל חיים בעצם בתוך 

52
00:04:31,389 --> 00:04:38,140
בלוקי ה-MLP, ומחשבה אחת למה שהם עשויים לעשות היא שהם מציעים קיבולת נוספת לאחסן עובדות.

53
00:04:38,720 --> 00:04:42,357
כמו שאמרתי, השיעור כאן יתמקד בדוגמה של צעצוע הבטון של איך 

54
00:04:42,357 --> 00:04:46,120
בדיוק זה יכול לאחסן את העובדה שמייקל ג&#39;ורדן משחק כדורסל.

55
00:04:47,120 --> 00:04:51,900
עכשיו, דוגמה צעצוע זו תדרוש שאתה ואני נניח כמה הנחות לגבי החלל הגבוה.

56
00:04:52,360 --> 00:04:57,128
ראשית, נניח שאחד הכיוונים מייצג את רעיון השם הפרטי מייקל, 

57
00:04:57,128 --> 00:05:02,555
ואז כיוון אחר כמעט מאונך מייצג את הרעיון של שם המשפחה ג&#39;ורדן, 

58
00:05:02,555 --> 00:05:06,420
ואז בכל זאת כיוון שלישי יייצג את רעיון הכדורסל.

59
00:05:07,400 --> 00:05:13,518
אז ספציפית, מה שאני מתכוון בזה הוא אם אתה מסתכל ברשת ותוציא את אחד הוקטורים המעובדים, 

60
00:05:13,518 --> 00:05:17,644
אם המוצר הנקודה שלו עם השם הפרטי הזה מייקל כיוון הוא אחד, 

61
00:05:17,644 --> 00:05:22,340
זה מה שזה אומר שהווקטור יהיה מקודד הרעיון של אדם עם השם הפרטי הזה.

62
00:05:23,800 --> 00:05:28,700
אחרת, תוצר הנקודה הזה יהיה אפס או שלילי, כלומר הווקטור לא באמת מתיישר עם הכיוון הזה.

63
00:05:29,420 --> 00:05:32,398
ולמען הפשטות, בואו נתעלם לחלוטין מהשאלה ההגיונית של 

64
00:05:32,398 --> 00:05:35,320
מה זה עשוי להיות אם המוצר הנקודה הזה היה גדול מאחד.

65
00:05:36,200 --> 00:05:39,945
באופן דומה, המוצר הנקודתי שלו עם הכיוונים האחרים האלה 

66
00:05:39,945 --> 00:05:43,760
יגיד לך אם הוא מייצג את שם המשפחה ג&#39;ורדן או כדורסל.

67
00:05:44,740 --> 00:05:48,744
אז נניח שווקטור נועד לייצג את השם המלא, מייקל ג&#39;ורדן, 

68
00:05:48,744 --> 00:05:52,680
אז המוצר הנקודות שלו עם שני הכיוונים האלה צריך להיות אחד.

69
00:05:53,480 --> 00:05:58,143
מכיוון שהטקסט מייקל ג&#39;ורדן משתרע על פני שני אסימונים שונים, 

70
00:05:58,143 --> 00:06:02,660
זה גם אומר שעלינו להניח שבלוק קשב קודם העביר בהצלחה מידע לשני 

71
00:06:02,660 --> 00:06:06,960
משני הוקטורים הללו כדי להבטיח שהוא יכול לקודד את שני השמות.

72
00:06:07,940 --> 00:06:11,480
עם כל אלה כמו ההנחות, בואו עכשיו נצלול לתוך הבשר של השיעור.

73
00:06:11,880 --> 00:06:14,980
מה קורה בתוך פרצפטרון רב שכבתי?

74
00:06:17,100 --> 00:06:21,025
אולי תחשבו על רצף הווקטורים הזה שזורם לתוך הבלוק, 

75
00:06:21,025 --> 00:06:25,580
וזכרו שכל וקטור היה משויך במקור לאחד האסימונים מטקסט הקלט.

76
00:06:26,080 --> 00:06:31,836
מה שהולך לקרות הוא שכל וקטור בודד מהרצף הזה עובר סדרה קצרה של פעולות, 

77
00:06:31,836 --> 00:06:36,360
נפרק אותן תוך רגע, ובסוף, נקבל וקטור נוסף עם אותו מימד.

78
00:06:36,880 --> 00:06:43,200
הווקטור האחר הזה יתווסף לזה המקורי שזרם פנימה, והסכום הזה הוא התוצאה הזורמת החוצה.

79
00:06:43,720 --> 00:06:48,210
רצף הפעולות הזה הוא משהו שאתה מיישם על כל וקטור ברצף, 

80
00:06:48,210 --> 00:06:51,620
המשויך לכל אסימון בקלט, והכל קורה במקביל.

81
00:06:52,100 --> 00:06:56,200
במיוחד, הווקטורים לא מדברים זה עם זה בשלב הזה, כולם סוג של עושים את שלהם.

82
00:06:56,720 --> 00:06:59,813
ומבחינתך ולי, זה למעשה הופך את זה להרבה יותר פשוט, 

83
00:06:59,813 --> 00:07:04,058
כי זה אומר שאם אנחנו מבינים מה קורה רק לאחד מהווקטורים דרך הבלוק הזה, 

84
00:07:04,058 --> 00:07:06,060
אנחנו למעשה מבינים מה קורה לכולם.

85
00:07:07,100 --> 00:07:11,943
כשאני אומר שהגוש הזה הולך לקודד את העובדה שמייקל ג&#39;ורדן משחק כדורסל, 

86
00:07:11,943 --> 00:07:17,252
מה שאני מתכוון הוא שאם וקטור זורם פנימה שמקודד את השם הפרטי מייקל ואת שם המשפחה 

87
00:07:17,252 --> 00:07:21,963
ג&#39;ורדן, אז רצף החישובים הזה יפיק משהו שכולל את הכדורסל בכיוון הזה, 

88
00:07:21,963 --> 00:07:24,020
וזה מה שיוסיף לוקטור במיקום זה.

89
00:07:25,600 --> 00:07:29,700
השלב הראשון של תהליך זה נראה כמו הכפלת הווקטור הזה במטריצה גדולה מאוד.

90
00:07:30,040 --> 00:07:31,980
אין הפתעות שם, זו למידה עמוקה.

91
00:07:32,680 --> 00:07:37,499
והמטריצה הזו, כמו כל האחרות שראינו, מלאה בפרמטרים של מודל שנלמדים מנתונים, 

92
00:07:37,499 --> 00:07:43,090
שעלולים לחשוב עליהם כעל חבורה של כפתורים וחוגים שמשתנים ומכוונים כדי לקבוע מהי התנהגות 

93
00:07:43,090 --> 00:07:43,540
המודל .

94
00:07:44,500 --> 00:07:50,761
כעת, דרך נחמדה אחת לחשוב על כפל מטריצה היא לדמיין כל שורה של המטריצה הזו כווקטור משלה, 

95
00:07:50,761 --> 00:07:56,880
ולקחת חבורה של תוצרי נקודות בין השורות הללו לווקטור המעובד, שאותם אני אתן כ-E להטמעה.

96
00:07:57,280 --> 00:08:04,040
לדוגמה, נניח שהשורה הראשונה במקרה השתווה לכיוון השם הפרטי הזה של מייקל שאנו מניחים שקיים.

97
00:08:04,320 --> 00:08:08,968
זה אומר שהרכיב הראשון בפלט הזה, המוצר הנקודות הזה כאן, 

98
00:08:08,968 --> 00:08:14,800
יהיה אחד אם הווקטור הזה מקודד את השם הפרטי מייקל, ואפס או שלילי אחרת.

99
00:08:15,880 --> 00:08:19,599
אפילו יותר כיף, קחו רגע לחשוב מה זה היה אומר אם השורה הראשונה 

100
00:08:19,599 --> 00:08:23,080
הייתה השם הפרטי הזה מייקל פלוס שם המשפחה כיוון ג&#39;ורדן.

101
00:08:23,700 --> 00:08:27,420
ולמען הפשטות, הרשו לי להמשיך ולכתוב את זה בתור M פלוס J.

102
00:08:28,080 --> 00:08:32,594
לאחר מכן, אם לוקחים מוצר נקודה עם הטבעה E זו, הדברים מתפזרים ממש יפה, 

103
00:08:32,594 --> 00:08:34,980
כך שזה נראה כמו M dot E ועוד J dot E.

104
00:08:34,980 --> 00:08:39,913
ושימו לב איך זה אומר שהערך האולטימטיבי יהיה שניים אם הווקטור מקודד 

105
00:08:39,913 --> 00:08:44,700
את השם המלא מייקל ג&#39;ורדן, אחרת הוא יהיה אחד או משהו קטן מאחד.

106
00:08:45,340 --> 00:08:47,260
וזו רק שורה אחת במטריצה הזו.

107
00:08:47,600 --> 00:08:52,891
אתה יכול לחשוב על כל השורות האחרות כמו שואלים במקביל כמה סוגים אחרים של שאלות, 

108
00:08:52,891 --> 00:08:56,040
בדיקה בכמה מיני תכונות אחרות של הווקטור המעובד.

109
00:08:56,700 --> 00:08:59,865
לעתים קרובות מאוד שלב זה כולל גם הוספת וקטור נוסף לפלט, 

110
00:08:59,865 --> 00:09:02,240
שהוא מלא בפרמטרים של מודל הנלמדים מנתונים.

111
00:09:02,240 --> 00:09:04,560
וקטור אחר זה ידוע בשם ההטיה.

112
00:09:05,180 --> 00:09:10,577
לדוגמה שלנו, אני רוצה שתדמיינו שהערך של ההטיה הזו ברכיב הראשון הזה הוא שלילי, 

113
00:09:10,577 --> 00:09:15,560
כלומר הפלט הסופי שלנו נראה כמו המוצר הנקודה הרלוונטי הזה, אבל מינוס אחד.

114
00:09:16,120 --> 00:09:20,516
אפשר בהחלט לשאול למה אני רוצה שתניח שהמודל למד את זה, 

115
00:09:20,516 --> 00:09:25,890
ובעוד רגע תראה למה זה מאוד נקי ונחמד אם יש לנו כאן ערך שהוא חיובי 

116
00:09:25,890 --> 00:09:32,160
אם ורק אם וקטור מקודד את השם המלא מייקל ג&#39;ורדן, וחוץ מזה זה אפס או שלילי.

117
00:09:33,040 --> 00:09:38,373
המספר הכולל של שורות במטריצה הזו, שהוא משהו כמו מספר השאלות הנשאלות, 

118
00:09:38,373 --> 00:09:42,780
במקרה של GPT-3, שמספרים שלו עקבנו, הוא קצת פחות מ-50,000.

119
00:09:43,100 --> 00:09:46,640
למעשה, זה בדיוק פי ארבעה ממספר הממדים בחלל ההטמעה הזה.

120
00:09:46,920 --> 00:09:47,900
זו בחירה עיצובית.

121
00:09:47,940 --> 00:09:50,410
אתה יכול לעשות את זה יותר, אתה יכול לעשות את זה פחות, 

122
00:09:50,410 --> 00:09:52,240
אבל ריבוי נקי נוטה להיות ידידותי לחומרה.

123
00:09:52,740 --> 00:09:59,020
מכיוון שמטריצה מלאה במשקלים ממפה אותנו למרחב ממדי גבוה יותר, אני אתן לה את הקיצור W up.

124
00:09:59,020 --> 00:10:03,204
אני אמשיך לתייג את הווקטור שאנו מעבדים כ-E, ובואו נסמן 

125
00:10:03,204 --> 00:10:07,160
את וקטור ההטיה הזה כ-B למעלה ונחזיר את כל זה בתרשים.

126
00:10:09,180 --> 00:10:15,360
בשלב זה, הבעיה היא שהפעולה הזו היא ליניארית בלבד, אבל השפה היא תהליך מאוד לא ליניארי.

127
00:10:15,880 --> 00:10:19,929
אם הערך שאנו מודדים הוא גבוה עבור מייקל פלוס ג&#39;ורדן, 

128
00:10:19,929 --> 00:10:25,897
זה יהיה בהכרח מופעל במידה מסוימת על ידי מייקל פלוס פלפס וגם אלכסיס פלוס ג&#39;ורדן, 

129
00:10:25,897 --> 00:10:28,100
למרות אלה שאינם קשורים רעיונית.

130
00:10:28,540 --> 00:10:32,000
מה שאתה באמת רוצה זה כן או לא פשוט עבור השם המלא.

131
00:10:32,900 --> 00:10:37,840
אז השלב הבא הוא להעביר את וקטור הביניים הגדול הזה דרך פונקציה לא ליניארית פשוטה מאוד.

132
00:10:38,360 --> 00:10:41,598
בחירה נפוצה היא כזו שלוקחת את כל הערכים השליליים 

133
00:10:41,598 --> 00:10:45,300
וממפה אותם לאפס ומשאירה את כל הערכים החיוביים ללא שינוי.

134
00:10:46,440 --> 00:10:50,078
ובהמשך למסורת הלמידה העמוקה של שמות מפוארים מדי, 

135
00:10:50,078 --> 00:10:56,020
הפונקציה הפשוטה הזו נקראת לעתים קרובות היחידה הליניארית המתוקנת, או בקיצור ReLU.

136
00:10:56,020 --> 00:10:57,880
כך נראה הגרף.

137
00:10:58,300 --> 00:11:03,998
אז אם לוקחים את הדוגמה המדומיינת שלנו שבה הכניסה הראשונה של וקטור הביניים היא אחת, 

138
00:11:03,998 --> 00:11:10,178
אם ורק אם השם המלא הוא מייקל ג&#39;ורדן ואפס או שלילי אחרת, לאחר שתעביר את זה דרך ה-ReLU, 

139
00:11:10,178 --> 00:11:15,740
אתה בסופו של דבר עם ערך נקי מאוד שבו כל מהערכים האפסים והשליליים פשוט יחתכו לאפס.

140
00:11:16,100 --> 00:11:19,780
אז הפלט הזה יהיה אחד עבור השם המלא מייקל ג&#39;ורדן ואפס אחרת.

141
00:11:20,560 --> 00:11:24,120
במילים אחרות, זה מחקה בצורה ישירה מאוד את ההתנהגות של שער AND.

142
00:11:25,660 --> 00:11:29,221
לעתים קרובות דגמים ישתמשו בפונקציה מעט שונה שנקראת JLU, 

143
00:11:29,221 --> 00:11:32,020
בעלת אותה צורה בסיסית, היא רק קצת יותר חלקה.

144
00:11:32,500 --> 00:11:35,720
אבל למטרותינו, זה קצת יותר נקי אם נחשוב רק על ה-ReLU.

145
00:11:36,740 --> 00:11:42,520
כמו כן, כשאתה שומע אנשים מתייחסים לנוירונים של שנאי, הם מדברים על הערכים האלה כאן.

146
00:11:42,900 --> 00:11:49,140
בכל פעם שאתה רואה את תמונת הרשת העצבית הנפוצה עם שכבת נקודות וחבורה של קווים המתחברים 

147
00:11:49,140 --> 00:11:55,309
לשכבה הקודמת, שהייתה לנו קודם בסדרה זו, זה נועד בדרך כלל להעביר את השילוב הזה של צעד 

148
00:11:55,309 --> 00:12:01,260
ליניארי, כפל מטריצה, ואחריו איזו פונקציה לא ליניארית פשוטה מבחינה מונחית כמו ReLU.

149
00:12:02,500 --> 00:12:08,920
הייתם אומרים שהנוירון הזה פעיל בכל פעם שהערך הזה חיובי ושהוא לא פעיל אם הערך הזה הוא אפס.

150
00:12:10,120 --> 00:12:12,380
השלב הבא נראה דומה מאוד לשלב הראשון.

151
00:12:12,560 --> 00:12:16,580
אתה מכפיל במטריצה גדולה מאוד ומוסיף על מונח הטיה מסוים.

152
00:12:16,980 --> 00:12:22,003
במקרה זה, מספר הממדים בפלט ירד בחזרה לגודל החלל ההטמעה הזה, 

153
00:12:22,003 --> 00:12:25,520
אז אני אמשיך לקרוא לזה מטריצת ההקרנה למטה.

154
00:12:26,220 --> 00:12:31,360
והפעם, במקום לחשוב על דברים שורה אחר שורה, דווקא יותר נחמד לחשוב על זה טור אחר טור.

155
00:12:31,860 --> 00:12:38,750
אתה מבין, דרך נוספת שבה אתה יכול להחזיק כפל מטריצה בראש שלך היא לדמיין לקחת כל עמודה של 

156
00:12:38,750 --> 00:12:45,640
המטריצה ולהכפיל אותה במונח המתאים בווקטור שהיא מעבדת ומחברת את כל העמודות המותאמות מחדש.

157
00:12:46,840 --> 00:12:52,702
הסיבה שנחמד יותר לחשוב על כך היא כי כאן לעמודים יש את אותו מימד כמו חלל ההטמעה, 

158
00:12:52,702 --> 00:12:55,780
כך שנוכל לחשוב עליהם כעל כיוונים בחלל הזה.

159
00:12:56,140 --> 00:13:03,080
למשל, נדמיין שהמודל למד להפוך את העמוד הראשון הזה לכיוון הכדורסל הזה שאנחנו מניחים שקיים.

160
00:13:04,180 --> 00:13:08,466
מה שזה אומר הוא שכאשר הנוירון הרלוונטי במיקום הראשון הזה פעיל, 

161
00:13:08,466 --> 00:13:10,780
נוסיף את העמודה הזו לתוצאה הסופית.

162
00:13:11,140 --> 00:13:15,780
אבל אם הנוירון הזה לא היה פעיל, אם המספר הזה היה אפס, אז זה לא ישפיע.

163
00:13:16,500 --> 00:13:18,060
וזה לא חייב להיות רק כדורסל.

164
00:13:18,220 --> 00:13:21,774
הדגם יכול גם לאפות בטור הזה ובמאפיינים רבים אחרים שהוא 

165
00:13:21,774 --> 00:13:25,200
רוצה לשייך למשהו שיש לו את השם המלא מייקל ג&#39;ורדן.

166
00:13:26,980 --> 00:13:31,819
ובאותו זמן, כל העמודות האחרות במטריצה הזו מספרות לך 

167
00:13:31,819 --> 00:13:36,660
מה יתווסף לתוצאה הסופית אם הנוירון המתאים יהיה פעיל.

168
00:13:37,360 --> 00:13:43,500
ואם יש לך הטיה במקרה הזה, זה משהו שאתה פשוט מוסיף בכל פעם, ללא קשר לערכי הנוירון.

169
00:13:44,060 --> 00:13:45,280
אתה עשוי לתהות מה זה עושה.

170
00:13:45,540 --> 00:13:49,320
כמו בכל האובייקטים המלאים בפרמטרים כאן, קצת קשה לומר בדיוק.

171
00:13:49,320 --> 00:13:54,380
אולי ישנה הנהלת חשבונות שהרשת צריכה לעשות, אבל אתה יכול להרגיש חופשי להתעלם ממנה לעת עתה.

172
00:13:54,860 --> 00:13:59,628
אם הופך את הסימון שלנו לקצת יותר קומפקטי שוב, אקרא למטריצה הגדולה הזו 

173
00:13:59,628 --> 00:14:04,260
W למטה ובאופן דומה אקרא לוקטור ההטיה B למטה ואחזיר אותו לתרשים שלנו.

174
00:14:04,740 --> 00:14:09,120
כמו שראיתי קודם, מה שאתה עושה עם התוצאה הסופית הזו הוא להוסיף אותה 

175
00:14:09,120 --> 00:14:13,240
לווקטור שזרם לבלוק במיקום הזה וזה משיג לך את התוצאה הסופית הזו.

176
00:14:13,820 --> 00:14:20,405
אז למשל, אם הווקטור שזורם קידד גם את השם הפרטי מייקל וגם את שם המשפחה ג&#39;ורדן, 

177
00:14:20,405 --> 00:14:26,589
אז בגלל שרצף הפעולות הזה יפעיל את שער ה-AND הזה, הוא יוסיף את כיוון הכדורסל, 

178
00:14:26,589 --> 00:14:29,240
אז מה שיצוץ יקודד את כל אלה ביחד.

179
00:14:29,820 --> 00:14:34,200
וזכור, זהו תהליך שקורה לכל אחד מהווקטורים הללו במקביל.

180
00:14:34,800 --> 00:14:41,566
בפרט, אם לוקחים את מספרי GPT-3, זה אומר שבלוק הזה לא יש רק 50,000 נוירונים, 

181
00:14:41,566 --> 00:14:44,860
יש בו פי 50,000 ממספר האסימונים בקלט.

182
00:14:48,180 --> 00:14:55,180
אז זה כל הפעולה, שני מוצרי מטריקס, כל אחד עם הטיה ופונקציית חיתוך פשוטה ביניהם.

183
00:14:56,080 --> 00:14:59,350
כל אחד מכם שצפה בסרטונים הקודמים של הסדרה יזהה את 

184
00:14:59,350 --> 00:15:02,620
המבנה הזה כרשת עצבית מהסוג הבסיסי ביותר שלמדנו שם.

185
00:15:03,080 --> 00:15:06,100
בדוגמה זו, הוא הוכשר לזהות ספרות בכתב יד.

186
00:15:06,580 --> 00:15:14,928
כאן, בהקשר של שנאי למודל שפה גדול, זהו חלק אחד בארכיטקטורה גדולה יותר וכל ניסיון לפרש 

187
00:15:14,928 --> 00:15:23,180
מה בדיוק הוא עושה שזור מאוד ברעיון של קידוד מידע לוקטורים של מרחב הטבעה במימד גבוה. .

188
00:15:24,260 --> 00:15:29,025
זה השיעור המרכזי, אבל אני כן רוצה ללכת אחורה ולהרהר בשני דברים שונים, 

189
00:15:29,025 --> 00:15:33,586
הראשון שבהם הוא סוג של הנהלת חשבונות, והשני כולל עובדה מאוד מעוררת 

190
00:15:33,586 --> 00:15:38,080
מחשבה על ממדים גבוהים יותר שלמעשה לא ידעתי. יודע עד שחפרתי בשנאים.

191
00:15:41,080 --> 00:15:46,096
בשני הפרקים האחרונים, אתה ואני התחלנו לספור את המספר הכולל של הפרמטרים 

192
00:15:46,096 --> 00:15:50,760
ב-GPT-3 ולראות בדיוק היכן הם חיים, אז בואו נסיים מהר את המשחק כאן.

193
00:15:51,400 --> 00:15:56,790
כבר ציינתי איך למטריצת ההקרנה הזו יש קצת פחות מ-50,000 שורות 

194
00:15:56,790 --> 00:16:02,180
ושכל שורה מתאימה לגודל של חלל ההטמעה, שעבור GPT-3 הוא 12,288.

195
00:16:03,240 --> 00:16:08,812
מכפילים את אלה יחד, זה נותן לנו 604 מיליון פרמטרים רק עבור המטריצה הזו, 

196
00:16:08,812 --> 00:16:13,920
ולהקרנה למטה יש אותו מספר של פרמטרים רק עם צורה שעברה טרנספוזיציה.

197
00:16:14,500 --> 00:16:17,400
אז ביחד, הם נותנים כ-1.2 מיליארד פרמטרים.

198
00:16:18,280 --> 00:16:22,197
וקטור ההטיה אחראי גם לעוד כמה פרמטרים, אבל זה חלק טריוויאלי מהסך הכל, 

199
00:16:22,197 --> 00:16:24,100
אז אני אפילו לא הולך להראות את זה.

200
00:16:24,660 --> 00:16:31,408
ב-GPT-3, רצף זה של וקטורים הטבעה זורם דרך לא אחד, אלא 96 MLPs נפרדים, 

201
00:16:31,408 --> 00:16:38,060
כך שמספר הפרמטרים הכולל המוקדש לכל הבלוקים הללו מסתכם בכ-116 מיליארד.

202
00:16:38,820 --> 00:16:44,991
מדובר בסביבות 2 שליש מסך הפרמטרים ברשת, וכשאתה מוסיף את זה לכל מה שהיה לנו קודם, 

203
00:16:44,991 --> 00:16:51,620
עבור בלוקי הקשב, ההטמעה וההשבתה, אתה אכן מקבל את הסכום הכולל של 175 מיליארד כפי שפורסם.

204
00:16:53,060 --> 00:16:58,280
מן הסתם כדאי להזכיר שיש קבוצה נוספת של פרמטרים הקשורים לאותם שלבי נורמליזציה 

205
00:16:58,280 --> 00:17:03,840
שההסבר הזה דילג עליהם, אבל כמו וקטור ההטיה, הם מהווים חלק מאוד טריוויאלי מהסך הכל.

206
00:17:05,900 --> 00:17:10,845
באשר לנקודת השתקפות השנייה הזו, אתם עשויים לתהות האם דוגמא הצעצוע המרכזית הזו שהשקענו בה 

207
00:17:10,845 --> 00:17:15,680
כל כך הרבה זמן משקפת את האופן שבו עובדות מאוחסנות בפועל במודלים אמיתיים של שפות גדולות.

208
00:17:16,319 --> 00:17:21,583
נכון שניתן לחשוב על השורות של אותה מטריצה ראשונה ככיוונים במרחב ההטמעה הזה, 

209
00:17:21,583 --> 00:17:27,540
וזה אומר שההפעלה של כל נוירון אומרת לך עד כמה וקטור נתון מתיישר עם כיוון ספציפי כלשהו.

210
00:17:27,760 --> 00:17:34,340
זה גם נכון שהעמודות של המטריצה השנייה מספרות לך מה יתווסף לתוצאה אם הנוירון הזה יהיה פעיל.

211
00:17:34,640 --> 00:17:36,800
שני אלה הם רק עובדות מתמטיות.

212
00:17:37,740 --> 00:17:43,123
עם זאת, הראיות מצביעות על כך שנוירונים בודדים מייצגים רק לעתים רחוקות 

213
00:17:43,123 --> 00:17:49,352
תכונה אחת נקייה כמו מייקל ג&#39;ורדן, וייתכן שלמעשה יש סיבה טובה מאוד שזה המקרה, 

214
00:17:49,352 --> 00:17:54,120
הקשורה לרעיון שצף סביב חוקרי פרשנות המכונה בימינו סופרפוזיציה.

215
00:17:54,640 --> 00:17:58,333
זוהי השערה שעשויה לעזור להסביר גם מדוע המודלים 

216
00:17:58,333 --> 00:18:02,420
קשים במיוחד לפירוש וגם מדוע הם מתקדמים בצורה מפתיעה.

217
00:18:03,500 --> 00:18:08,837
הרעיון הבסיסי הוא שאם יש לך מרחב נ-ממדי ואתה רוצה לייצג חבורה של תכונות 

218
00:18:08,837 --> 00:18:13,581
שונות באמצעות כיוונים שכולם מאונכים זה לזה במרחב הזה, אתה יודע, 

219
00:18:13,581 --> 00:18:19,067
ככה אם אתה מוסיף רכיב בכיוון אחד, זה לא משפיע על אף אחד מהכיוונים האחרים, 

220
00:18:19,067 --> 00:18:23,960
אז המספר המרבי של הוקטורים שאתה יכול להתאים הוא רק n, מספר הממדים.

221
00:18:24,600 --> 00:18:27,620
למתמטיקאי, למעשה, זו ההגדרה של מימד.

222
00:18:28,220 --> 00:18:33,580
אבל המקום שבו זה נהיה מעניין הוא אם תרגע קצת את האילוץ הזה ותסבול קצת רעש.

223
00:18:34,180 --> 00:18:39,993
נניח שאתה מאפשר לתכונות האלה להיות מיוצגות על ידי וקטורים שאינם בדיוק מאונכים, 

224
00:18:39,993 --> 00:18:43,820
הם פשוט כמעט מאונכים, אולי בין 89 ל-91 מעלות זה מזה.

225
00:18:44,820 --> 00:18:48,020
אם היינו בשני מימדים, זה לא משנה.

226
00:18:48,260 --> 00:18:52,192
זה כמעט ולא נותן לך מקום להתנועע נוסף כדי להכניס בו יותר וקטורים, 

227
00:18:52,192 --> 00:18:56,780
מה שהופך את זה למנוגד יותר מאשר לממדים גבוהים יותר, התשובה משתנה באופן דרמטי.

228
00:18:57,660 --> 00:19:03,448
אני יכול לתת לך המחשה ממש מהירה ומלוכלכת של זה באמצעות איזה פייתון מחורבן 

229
00:19:03,448 --> 00:19:08,689
שייצור רשימה של וקטורים בעלי 100 ממדים, כל אחד מאותחל באופן אקראי, 

230
00:19:08,689 --> 00:19:14,400
והרשימה הזו תכיל 10,000 וקטורים נפרדים, אז פי 100 וקטורים כמו שיש מימדים.

231
00:19:15,320 --> 00:19:19,900
העלילה הזו ממש כאן מציגה את התפלגות הזוויות בין זוגות של וקטורים אלה.

232
00:19:20,680 --> 00:19:25,665
אז בגלל שהם התחילו באקראי, הזוויות האלה יכולות להיות כל דבר שבין 0 ל-180 מעלות, 

233
00:19:25,665 --> 00:19:28,719
אבל תשים לב שכבר, אפילו רק עבור וקטורים אקראיים, 

234
00:19:28,719 --> 00:19:31,960
יש הטיה כבדה לכך שדברים יהיו קרובים יותר ל-90 מעלות.

235
00:19:32,500 --> 00:19:36,945
ואז מה שאני הולך לעשות זה להפעיל תהליך אופטימיזציה מסוים שדוחף באופן 

236
00:19:36,945 --> 00:19:41,520
איטרטיבי את כל הווקטורים האלה כך שהם מנסים להיות מאונכים יותר אחד לשני.

237
00:19:42,060 --> 00:19:46,660
לאחר שחזרתי על זה פעמים רבות ושונות, הנה איך נראית התפלגות הזוויות.

238
00:19:47,120 --> 00:19:51,928
אנחנו צריכים למעשה להגדיל את זה כאן כי כל הזוויות האפשריות 

239
00:19:51,928 --> 00:19:56,900
בין זוגות של וקטורים יושבות בטווח הצר הזה שבין 89 ל-91 מעלות.

240
00:19:58,020 --> 00:20:04,280
באופן כללי, תוצאה של משהו המכונה הלמה של ג&#39;ונסון-לינדנשטראוס היא שמספר הוקטורים 

241
00:20:04,280 --> 00:20:10,840
שאתה יכול לדחוס לתוך מרחב שהם כמעט מאונכים כמו זה גדל באופן אקספוננציאלי עם מספר הממדים.

242
00:20:11,960 --> 00:20:15,734
זה מאוד משמעותי עבור מודלים של שפה גדולים, שעשויים 

243
00:20:15,734 --> 00:20:19,880
להפיק תועלת משיוך רעיונות עצמאיים לכיוונים כמעט מאונכים.

244
00:20:20,000 --> 00:20:26,440
זה אומר שהוא יכול לאחסן הרבה הרבה יותר רעיונות ממה שיש מידות בחלל שהוא מוקצה.

245
00:20:27,320 --> 00:20:31,740
זה עשוי להסביר חלקית מדוע נראה כי ביצועי הדגם משתנים כל כך טוב עם הגודל.

246
00:20:32,540 --> 00:20:39,400
חלל שיש לו פי 10 ממדים יכול לאחסן דרך, הרבה יותר מפי 10 רעיונות עצמאיים.

247
00:20:40,420 --> 00:20:45,393
וזה רלוונטי לא רק למרחב ההטמעה שבו חיים הווקטורים הזורמים דרך המודל, 

248
00:20:45,393 --> 00:20:50,440
אלא גם לוקטור המלא בנוירונים באמצע אותו תפיסה רב-שכבתית שזה עתה למדנו.

249
00:20:50,960 --> 00:20:55,600
כלומר, בגדלים של GPT-3, ייתכן שהוא לא רק מחפש 50,000 מאפיינים, 

250
00:20:55,600 --> 00:21:01,273
אלא אם הוא במקום זאת ימנף את הקיבולת הנוספת העצומה הזו על ידי שימוש בכיוונים 

251
00:21:01,273 --> 00:21:07,240
כמעט מאונכים של החלל, הוא יכול היה לחקור הרבה הרבה יותר תכונות של הווקטור המעובד.

252
00:21:07,780 --> 00:21:14,340
אבל אם זה היה עושה את זה, המשמעות היא שמאפיינים בודדים לא יהיו גלויים כנוירון אחד שנדלק.

253
00:21:14,660 --> 00:21:19,380
זה יצטרך להיראות כמו שילוב ספציפי של נוירונים במקום זאת, סופרפוזיציה.

254
00:21:20,400 --> 00:21:25,329
לכל אחד מכם שמעוניין ללמוד עוד, מונח חיפוש רלוונטי כאן הוא מקודד אוטומטי דליל, 

255
00:21:25,329 --> 00:21:30,321
שהוא כלי שחלק מהאפשרויות שאנשים משתמשים בו כדי לנסות לחלץ מהן התכונות האמיתיות, 

256
00:21:30,321 --> 00:21:32,880
גם אם הן מונחות מאוד על כל אלה. נוירונים.

257
00:21:33,540 --> 00:21:36,800
אני אקשר לכמה פוסטים אנתרופיים נהדרים על זה.

258
00:21:37,880 --> 00:21:43,300
בשלב זה, לא נגענו בכל פרט של שנאי, אבל אתה ואני פגענו בנקודות החשובות ביותר.

259
00:21:43,520 --> 00:21:47,640
הדבר העיקרי שאני רוצה לכסות בפרק הבא הוא תהליך האימון.

260
00:21:48,460 --> 00:21:53,025
מצד אחד, התשובה הקצרה לאיך עובדת אימון היא שהכל זה התפשטות לאחור, 

261
00:21:53,025 --> 00:21:56,900
וכיסינו את ההפצה לאחור בהקשר נפרד עם פרקים קודמים בסדרה.

262
00:21:57,220 --> 00:22:02,120
אבל יש עוד מה לדון, כמו פונקציית העלות הספציפית המשמשת למודלים של שפה, 

263
00:22:02,120 --> 00:22:07,780
הרעיון של כוונון עדין באמצעות למידת חיזוק עם משוב אנושי, והרעיון של חוקי קנה מידה.

264
00:22:08,960 --> 00:22:12,812
הערה מהירה לעוקבים הפעילים שביניכם, ישנם מספר סרטונים שאינם קשורים 

265
00:22:12,812 --> 00:22:16,550
ללימוד מכונה שאני מתרגש לנעוץ בהם שיניים לפני שאעשה את הפרק הבא, 

266
00:22:16,550 --> 00:22:20,000
אז זה עשוי להיות זמן מה, אבל אני מבטיח את זה יבוא בבוא הזמן.

267
00:22:35,640 --> 00:22:37,920
תודה לך.

