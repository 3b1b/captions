[
 {
  "translatedText": "Se você alimentar um grande modelo de linguagem com a frase &quot;Michael Jordan joga basquete&quot;, e fizer com que ele preveja o que vem a seguir, e ele prever corretamente o basquete, isso sugeriria que em algum lugar, dentro de suas centenas de bilhões de parâmetros, há conhecimento incorporado sobre uma pessoa específica e seu esporte específico.",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "E acho que, em geral, qualquer um que tenha brincado com um desses modelos tem a clara sensação de que ele memorizou toneladas e toneladas de fatos.",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "Então, uma pergunta razoável que você poderia fazer é: como isso funciona exatamente?",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "E onde esses fatos vivem?",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "Em dezembro passado, alguns pesquisadores do Google DeepMind publicaram um artigo sobre esse assunto e usaram esse exemplo específico de correspondência de atletas com seus esportes.",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "E embora uma compreensão mecanicista completa de como os fatos são armazenados permaneça sem solução, eles tiveram alguns resultados parciais interessantes, incluindo a conclusão geral de alto nível de que os fatos parecem viver dentro de uma parte específica dessas redes, conhecidas fantasiosamente como perceptrons multicamadas, ou MLPs, para abreviar.",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "Nos últimos capítulos, você e eu nos aprofundamos nos detalhes por trás dos transformadores, na arquitetura subjacente a grandes modelos de linguagem e também em muitas outras IAs modernas.",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "No capítulo mais recente, nos concentramos em uma parte chamada Atenção.",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "E o próximo passo para você e para mim é investigar os detalhes do que acontece dentro desses perceptrons multicamadas, que compõem a outra grande parte da rede.",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "O cálculo aqui é relativamente simples, especialmente quando comparado à atenção.",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "Tudo se resume essencialmente a um par de multiplicações de matrizes com algo simples no meio.",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "No entanto, interpretar o que esses cálculos estão fazendo é extremamente desafiador.",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "Nosso principal objetivo aqui é percorrer os cálculos e torná-los memoráveis, mas gostaria de fazer isso no contexto de mostrar um exemplo específico de como um desses blocos poderia, pelo menos em princípio, armazenar um fato concreto.",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "Mais especificamente, ele armazenará o fato de que Michael Jordan joga basquete.",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "Devo mencionar que o layout aqui foi inspirado em uma conversa que tive com um desses pesquisadores da DeepMind, Neil Nanda.",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "Na maior parte, presumo que você tenha assistido aos dois últimos capítulos ou tenha uma noção básica do que é um transformador, mas relembrar nunca é demais, então aqui vai um rápido lembrete do fluxo geral.",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "Você e eu temos estudado um modelo treinado para receber um pedaço de texto e prever o que vem a seguir.",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "O texto de entrada é primeiro dividido em vários tokens, o que significa pequenos pedaços que normalmente são palavras ou pequenos pedaços de palavras, e cada token é associado a um vetor de alta dimensão, ou seja, uma longa lista de números.",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "Essa sequência de vetores passa repetidamente por dois tipos de operação: atenção, que permite que os vetores passem informações entre si, e então os perceptrons multicamadas, que é o que vamos abordar hoje, e também há uma certa etapa de normalização no meio.",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "Depois que a sequência de vetores tiver passado por muitas, muitas iterações diferentes de ambos os blocos, a esperança é que, no final, cada vetor tenha absorvido informações suficientes, tanto do contexto, de todas as outras palavras na entrada, quanto do conhecimento geral que foi incorporado aos pesos do modelo por meio do treinamento, para que ele possa ser usado para fazer uma previsão de qual token vem a seguir.",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "Uma das principais ideias que quero que você tenha em mente é que todos esses vetores vivem em um espaço de altíssima dimensão e, quando você pensa sobre esse espaço, diferentes direções podem codificar diferentes tipos de significado.",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "Então, um exemplo muito clássico ao qual gosto de me referir é como se você observar a incorporação de mulher e subtrair a incorporação de homem, e der esse pequeno passo e adicioná-lo a outro substantivo masculino, algo como tio, você chega a um lugar muito, muito próximo do substantivo feminino correspondente.",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "Nesse sentido, essa direção específica codifica informações de gênero.",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "A ideia é que muitas outras direções distintas neste espaço de super alta dimensão poderiam corresponder a outras características que o modelo poderia querer representar.",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "Em um transformador, esses vetores não codificam apenas o significado de uma única palavra.",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "À medida que fluem pela rede, eles absorvem um significado muito mais rico com base em todo o contexto ao seu redor e também com base no conhecimento do modelo.",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "No final das contas, cada um precisa codificar algo muito, muito além do significado de uma única palavra, já que precisa ser suficiente para prever o que virá a seguir.",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "Já vimos como os blocos de atenção permitem que você incorpore contexto, mas a maioria dos parâmetros do modelo, na verdade, reside dentro dos blocos MLP, e uma ideia do que eles podem estar fazendo é que eles oferecem capacidade extra para armazenar fatos.",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "Como eu disse, a lição aqui vai se concentrar no exemplo do brinquedo de concreto de como exatamente ele poderia armazenar o fato de que Michael Jordan joga basquete.",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "Agora, este exemplo de brinquedo vai exigir que você e eu façamos algumas suposições sobre esse espaço de alta dimensão.",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "Primeiro, vamos supor que uma das direções representa a ideia do primeiro nome Michael, e então outra direção quase perpendicular representa a ideia do sobrenome Jordan, e então uma terceira direção representará a ideia de basquete.",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "Então, especificamente, o que quero dizer com isso é que se você olhar na rede e selecionar um dos vetores que estão sendo processados, se o produto escalar com a direção do primeiro nome Michael for um, isso é o que significaria para o vetor codificar a ideia de uma pessoa com esse primeiro nome.",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "Caso contrário, esse produto escalar seria zero ou negativo, o que significa que o vetor não se alinha realmente com essa direção.",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "E para simplificar, vamos ignorar completamente a questão muito razoável do que significaria se esse produto escalar fosse maior que um.",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "Da mesma forma, seu produto escalar com essas outras direções lhe diria se ele representa o sobrenome Jordan ou basquete.",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "Então, digamos que um vetor deve representar o nome completo, Michael Jordan, então seu produto escalar com ambas as direções teria que ser um.",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "Como o texto Michael Jordan abrange dois tokens diferentes, isso também significa que temos que assumir que um bloco de atenção anterior passou informações com sucesso para o segundo desses dois vetores, de modo a garantir que ele possa codificar ambos os nomes.",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "Com todas essas suposições, vamos agora mergulhar no cerne da lição.",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "O que acontece dentro de um perceptron multicamadas?",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "Você pode pensar nessa sequência de vetores fluindo para o bloco e lembrar que cada vetor foi originalmente associado a um dos tokens do texto de entrada.",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "O que vai acontecer é que cada vetor individual dessa sequência passa por uma curta série de operações, vamos descompactá-las em apenas um momento e, no final, obteremos outro vetor com a mesma dimensão.",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "Esse outro vetor será adicionado ao original que fluiu para dentro, e essa soma será o resultado que flui para fora.",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "Essa sequência de operações é algo que você aplica a cada vetor na sequência, associado a cada token na entrada, e tudo acontece em paralelo.",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "Em particular, os vetores não conversam entre si nesta etapa, eles estão todos fazendo suas próprias coisas.",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "E para você e para mim, isso na verdade torna tudo muito mais simples, porque significa que se entendermos o que acontece com apenas um dos vetores através desse bloco, efetivamente entenderemos o que acontece com todos eles.",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "Quando digo que esse bloco vai codificar o fato de que Michael Jordan joga basquete, o que quero dizer é que se um vetor fluir codificando o primeiro nome Michael e o sobrenome Jordan, então essa sequência de cálculos produzirá algo que inclui essa direção basquete, que é o que será adicionado ao vetor nessa posição.",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "O primeiro passo desse processo parece multiplicar esse vetor por uma matriz muito grande.",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "Não há surpresas nisso, isso é aprendizado profundo.",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "E essa matriz, como todas as outras que vimos, é preenchida com parâmetros de modelo que são aprendidos a partir de dados, o que você pode imaginar como um monte de botões e mostradores que são ajustados e ajustados para determinar qual é o comportamento do modelo.",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "Agora, uma boa maneira de pensar sobre multiplicação de matrizes é imaginar cada linha dessa matriz como sendo seu próprio vetor e pegar um monte de produtos escalares entre essas linhas e o vetor que está sendo processado, que rotularei como E para incorporação.",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "Por exemplo, suponha que a primeira linha seja igual a esta direção do primeiro nome Michael que presumimos que exista.",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "Isso significaria que o primeiro componente nesta saída, este produto escalar aqui, seria um se esse vetor codificasse o primeiro nome Michael, e zero ou negativo caso contrário.",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "Ainda mais divertido, reserve um momento para pensar no que significaria se a primeira fileira fosse essa direção: primeiro nome Michael mais sobrenome Jordan.",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "E para simplificar, deixe-me escrever isso como M mais J.",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "Então, pegando um produto escalar com esse E incorporado, as coisas se distribuem muito bem, então parece M ponto E mais J ponto E.",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "E observe como isso significa que o valor final seria dois se o vetor codificasse o nome completo Michael Jordan, caso contrário seria um ou algo menor que um.",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "E essa é apenas uma linha nesta matriz.",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "Você pode pensar em todas as outras linhas fazendo paralelamente outros tipos de perguntas, investigando outros tipos de características do vetor que está sendo processado.",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "Muitas vezes, essa etapa também envolve a adição de outro vetor à saída, que está cheio de parâmetros do modelo aprendidos a partir dos dados.",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "Esse outro vetor é conhecido como viés.",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "No nosso exemplo, quero que você imagine que o valor desse viés no primeiro componente é negativo um, o que significa que nossa saída final se parece com o produto escalar relevante, mas menos um.",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "Você pode perguntar, com razão, por que eu gostaria que você assumisse que o modelo aprendeu isso, e em um momento você verá por que é muito claro e agradável se tivermos um valor aqui que é positivo se, e somente se, um vetor codifica o nome completo Michael Jordan, e, caso contrário, é zero ou negativo.",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "O número total de linhas nesta matriz, que é algo como o número de perguntas feitas, no caso do GPT-3, cujos números estamos acompanhando, é pouco menos de 50.000.",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "Na verdade, é exatamente quatro vezes o número de dimensões neste espaço de incorporação.",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "Essa é uma escolha de design.",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "Você pode aumentar ou diminuir, mas ter um múltiplo limpo tende a ser favorável ao hardware.",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "Como essa matriz cheia de pesos nos mapeia para um espaço dimensional mais alto, vou chamá-la de W para cima.",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "Vou continuar rotulando o vetor que estamos processando como E, e vamos rotular esse vetor de polarização como B para cima e colocar tudo de volta no diagrama.",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "Neste ponto, o problema é que essa operação é puramente linear, mas a linguagem é um processo muito não linear.",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "Se a entrada que estamos medindo for alta para Michael mais Jordan, ela também seria necessariamente desencadeada por Michael mais Phelps e também Alexis mais Jordan, apesar de não estarem relacionados conceitualmente.",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "O que você realmente quer é um simples sim ou não para o nome completo.",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "Então o próximo passo é passar esse grande vetor intermediário por uma função não linear muito simples.",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "Uma escolha comum é aquela que pega todos os valores negativos e os mapeia para zero, deixando todos os valores positivos inalterados.",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "E continuando com a tradição de aprendizado profundo de nomes excessivamente elaborados, essa função muito simples é frequentemente chamada de unidade linear retificada, ou ReLU, para abreviar.",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "Veja como fica o gráfico.",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "Então, tomando nosso exemplo imaginado, onde essa primeira entrada do vetor intermediário é um, se e somente se o nome completo for Michael Jordan e zero ou negativo caso contrário, depois de passá-lo pelo ReLU, você termina com um valor muito limpo, onde todos os valores zero e negativos são simplesmente cortados para zero.",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "Portanto, essa saída seria um para o nome completo Michael Jordan e zero para outros.",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "Em outras palavras, ele imita muito diretamente o comportamento de uma porta AND.",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "Muitas vezes, os modelos usam uma função ligeiramente modificada chamada JLU, que tem o mesmo formato básico, só que um pouco mais suave.",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "Mas para nossos propósitos, fica um pouco mais limpo se pensarmos apenas no ReLU.",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "Além disso, quando você ouve as pessoas se referindo aos neurônios de um transformador, elas estão falando sobre esses valores aqui.",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "Sempre que você vê aquela imagem comum de rede neural com uma camada de pontos e um monte de linhas conectando-se à camada anterior, que vimos anteriormente nesta série, isso normalmente significa transmitir essa combinação de uma etapa linear, uma multiplicação de matriz, seguida por alguma função não linear simples de termos, como um ReLU.",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "Você diria que esse neurônio está ativo sempre que esse valor for positivo e que ele está inativo se esse valor for zero.",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "O próximo passo é muito parecido com o primeiro.",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "Você multiplica por uma matriz muito grande e adiciona um certo termo de viés.",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "Nesse caso, o número de dimensões na saída é reduzido novamente ao tamanho desse espaço de incorporação, então vou chamá-la de matriz de projeção descendente.",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "E desta vez, em vez de pensar nas coisas linha por linha, é melhor pensar coluna por coluna.",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "Veja bem, outra maneira de manter a multiplicação de matrizes na sua cabeça é imaginar que você está pegando cada coluna da matriz e multiplicando-a pelo termo correspondente no vetor que ela está processando e somando todas essas colunas redimensionadas.",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "A razão pela qual é melhor pensar dessa maneira é porque aqui as colunas têm a mesma dimensão que o espaço de incorporação, então podemos pensar nelas como direções naquele espaço.",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "Por exemplo, vamos imaginar que o modelo aprendeu a fazer a primeira coluna seguir essa direção de basquete que supomos que exista.",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "O que isso significaria é que quando o neurônio relevante naquela primeira posição estiver ativo, adicionaremos esta coluna ao resultado final.",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "Mas se esse neurônio estivesse inativo, se esse número fosse zero, então isso não teria efeito.",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "E não precisa ser só basquete.",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "O modelo também pode fazer parte desta coluna e de muitas outras características que ele deseja associar a algo que tenha o nome completo Michael Jordan.",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "E, ao mesmo tempo, todas as outras colunas nesta matriz estão dizendo o que será adicionado ao resultado final se o neurônio correspondente estiver ativo.",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "E se você tem um viés nesse caso, é algo que você está apenas adicionando todas as vezes, independentemente dos valores dos neurônios.",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "Você deve estar se perguntando o que isso está fazendo.",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "Como acontece com todos os objetos preenchidos com parâmetros aqui, é meio difícil dizer exatamente.",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "Talvez haja alguma contabilidade que a rede precise fazer, mas você pode ignorá-la por enquanto.",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "Para tornar nossa notação um pouco mais compacta novamente, chamarei essa grande matriz de W down e, da mesma forma, chamarei esse vetor de polarização de B down e o colocarei de volta em nosso diagrama.",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "Como mencionei anteriormente, o que você faz com esse resultado final é adicioná-lo ao vetor que fluiu para o bloco naquela posição e isso lhe dá esse resultado final.",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "Então, por exemplo, se o vetor que flui codificasse o primeiro nome Michael e o sobrenome Jordan, então, como essa sequência de operações acionará a porta AND, ela adicionará a direção do basquete, então o que aparecer codificará todos eles juntos.",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "E lembre-se, esse é um processo que acontece em cada um desses vetores em paralelo.",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "Em particular, tomando os números do GPT-3, isso significa que esse bloco não tem apenas 50.000 neurônios, mas tem 50.000 vezes o número de tokens na entrada.",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "Então essa é a operação completa, dois produtos de matriz, cada um com um viés adicionado e uma função de recorte simples no meio.",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "Qualquer um de vocês que assistiu aos vídeos anteriores da série reconhecerá essa estrutura como o tipo mais básico de rede neural que estudamos lá.",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "Nesse exemplo, ele foi treinado para reconhecer dígitos escritos à mão.",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "Aqui, no contexto de um transformador para um grande modelo de linguagem, esta é uma parte de uma arquitetura maior e qualquer tentativa de interpretar exatamente o que ela está fazendo está fortemente interligada com a ideia de codificar informações em vetores de um espaço de incorporação de alta dimensão.",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "Essa é a lição principal, mas eu quero dar um passo para trás e refletir sobre duas coisas diferentes: a primeira é uma espécie de contabilidade, e a segunda envolve um fato muito instigante sobre dimensões superiores que eu realmente não sabia até pesquisar sobre transformadores.",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "Nos dois últimos capítulos, você e eu começamos a contar o número total de parâmetros no GPT-3 e a ver exatamente onde eles estão, então vamos terminar o jogo rapidamente aqui.",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "Já mencionei como essa matriz de projeção ascendente tem pouco menos de 50.000 linhas e que cada linha corresponde ao tamanho do espaço de incorporação, que para GPT-3 é 12.288.",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "Multiplicando esses valores, temos 604 milhões de parâmetros somente para essa matriz, e a projeção para baixo tem o mesmo número de parâmetros, apenas com uma forma transposta.",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "Então, juntos, eles fornecem cerca de 1,2 bilhão de parâmetros.",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "O vetor de viés também é responsável por mais alguns parâmetros, mas é uma proporção trivial do total, então nem vou mostrá-lo.",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "No GPT-3, essa sequência de vetores de incorporação flui não por um, mas por 96 MLPs distintos, então o número total de parâmetros dedicados a todos esses blocos soma cerca de 116 bilhões.",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "Isso representa cerca de 2 terços do total de parâmetros na rede e, quando você adiciona isso a tudo o que tínhamos antes, para os blocos de atenção, a incorporação e a desincorporação, você de fato obtém o total geral de 175 bilhões, conforme anunciado.",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "Provavelmente vale a pena mencionar que há outro conjunto de parâmetros associados a essas etapas de normalização que esta explicação pulou, mas, assim como o vetor de viés, eles representam uma proporção muito trivial do total.",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "Quanto ao segundo ponto de reflexão, você pode estar se perguntando se esse exemplo central de brinquedo no qual temos dedicado tanto tempo reflete como os fatos são realmente armazenados em grandes modelos de linguagem reais.",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "É verdade que as linhas dessa primeira matriz podem ser consideradas como direções nesse espaço de incorporação, e isso significa que a ativação de cada neurônio informa o quanto um determinado vetor se alinha com alguma direção específica.",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "Também é verdade que as colunas dessa segunda matriz informam o que será adicionado ao resultado se esse neurônio estiver ativo.",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "Ambos são apenas fatos matemáticos.",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "No entanto, as evidências sugerem que neurônios individuais raramente representam uma única característica limpa, como Michael Jordan, e pode haver uma boa razão para isso, relacionada a uma ideia que circula entre os pesquisadores de interpretabilidade atualmente, conhecida como superposição.",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "Esta é uma hipótese que pode ajudar a explicar por que os modelos são especialmente difíceis de interpretar e também por que eles escalam surpreendentemente bem.",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "A ideia básica é que se você tem um espaço n-dimensional e quer representar um monte de características diferentes usando direções que são todas perpendiculares entre si naquele espaço, dessa forma, se você adicionar um componente em uma direção, ele não influencia nenhuma das outras direções, então o número máximo de vetores que você pode ajustar é apenas n, o número de dimensões.",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "Para um matemático, na verdade, essa é a definição de dimensão.",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "Mas o mais interessante é quando você relaxa um pouco essa restrição e tolera algum ruído.",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "Digamos que você permita que essas características sejam representadas por vetores que não são exatamente perpendiculares, mas apenas quase perpendiculares, talvez entre 89 e 91 graus de distância.",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "Se estivéssemos em duas ou três dimensões, isso não faria diferença.",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "Isso não lhe dá muito espaço de manobra para encaixar mais vetores, o que torna ainda mais contraintuitivo que, para dimensões maiores, a resposta mude drasticamente.",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "Posso dar uma ilustração bem rápida e prática disso usando um Python simples que criará uma lista de vetores de 100 dimensões, cada um inicializado aleatoriamente, e essa lista conterá 10.000 vetores distintos, ou seja, 100 vezes mais vetores do que dimensões.",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "Este gráfico aqui mostra a distribuição de ângulos entre pares desses vetores.",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "Então, como eles começaram aleatoriamente, esses ângulos podem ser de 0 a 180 graus, mas você notará que, mesmo para vetores aleatórios, há uma forte tendência de que as coisas fiquem mais próximas de 90 graus.",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "Então o que farei é executar um certo processo de otimização que, iterativamente, ajusta todos esses vetores para que eles tentem se tornar mais perpendiculares entre si.",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "Depois de repetir isso várias vezes, veja como fica a distribuição dos ângulos.",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "Na verdade, temos que dar um zoom aqui porque todos os ângulos possíveis entre pares de vetores ficam dentro dessa faixa estreita entre 89 e 91 graus.",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "Em geral, uma consequência de algo conhecido como lema de Johnson-Lindenstrauss é que o número de vetores que você pode enfiar em um espaço que são quase perpendiculares como esse cresce exponencialmente com o número de dimensões.",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "Isso é muito significativo para grandes modelos de linguagem, que podem se beneficiar da associação de ideias independentes com direções quase perpendiculares.",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "Isso significa que é possível armazenar muito mais ideias do que as dimensões do espaço que lhe é atribuído.",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "Isso pode explicar parcialmente por que o desempenho do modelo parece se adaptar tão bem ao tamanho.",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "Um espaço que tem 10 vezes mais dimensões pode armazenar muito, muito mais que 10 vezes mais ideias independentes.",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "E isso é relevante não apenas para o espaço de incorporação onde vivem os vetores que fluem através do modelo, mas também para o vetor cheio de neurônios no meio do perceptron multicamadas que acabamos de estudar.",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "Ou seja, com o tamanho do GPT-3, ele não estaria apenas sondando 50.000 características, mas se aproveitasse essa enorme capacidade adicional usando direções quase perpendiculares do espaço, ele poderia estar sondando muito, muito mais características do vetor que está sendo processado.",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "Mas se isso acontecesse, o que significaria é que as características individuais não seriam visíveis quando um único neurônio se iluminasse.",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "Teria que parecer uma combinação específica de neurônios, uma superposição.",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "Para qualquer um de vocês curioso para saber mais, um termo de pesquisa relevante aqui é autocodificador esparso, que é uma ferramenta que algumas pessoas de interpretabilidade usam para tentar extrair quais são as verdadeiras características, mesmo que elas estejam muito sobrepostas em todos esses neurônios.",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "Vou deixar um link para algumas postagens antrópicas muito boas sobre isso.",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "Até aqui, não abordamos todos os detalhes de um transformador, mas você e eu abordamos os pontos mais importantes.",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "O principal assunto que quero abordar no próximo capítulo é o processo de treinamento.",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "Por um lado, a resposta curta sobre como o treinamento funciona é que tudo é retropropagação, e abordamos a retropropagação em um contexto separado em capítulos anteriores da série.",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "Mas há mais a discutir, como a função de custo específica usada para modelos de linguagem, a ideia de ajuste fino usando aprendizado por reforço com feedback humano e a noção de leis de escala.",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "Nota rápida para os seguidores ativos entre vocês: há uma série de vídeos não relacionados a aprendizado de máquina que estou ansioso para assistir antes de fazer o próximo capítulo, então pode demorar um pouco, mas prometo que sairá no devido tempo.",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "Obrigado.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]