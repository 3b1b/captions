[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "如果你给一个大型语言模型输入 \"迈克尔-乔丹从事空白运动 \"这个短语，然后让它预测接下来会发生什么，而它正确地预测了篮球，这就表明在它的数千亿个参数中，某个地方已经包含了关于某个特定的人和他的特定运动的知识。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "我认为，一般来说，玩过这些模型的人都会清楚地感觉到，它已经记住了成吨成吨的事实。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "因此，一个合理的问题是：这到底是怎么做到的？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "这些事实在哪里？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "去年 12 月，谷歌 DeepMind 的几位研究人员发布了有关这一问题的研究成果，他们使用的具体例子是将运动员与他们的运动项目进行匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "虽然对事实是如何存储的完整机制理解仍未解决，但他们已经有了一些有趣的部分结果，包括一个非常普遍的高层次结论，即事实似乎存在于这些网络的一个特定部分中，这个部分被奇特地称为多层感知器，简称 MLP。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "在过去的几章中，你和我一起深入研究了变换器背后的细节、大型语言模型的底层架构，以及许多其他现代人工智能的底层架构。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "在最近的章节中，我们重点讨论了一篇名为《注意力》的文章。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "对你我来说，下一步就是深入研究这些多层感知器内部发生的细节，它们构成了网络的另一大部分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "这里的计算其实相对简单，尤其是与注意力相比。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "它本质上是一对矩阵乘法，中间有一个简单的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "然而，解释这些计算所做的事情却极具挑战性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "在这里，我们的主要目标是逐步完成计算，并让人过目不忘，但我想通过一个具体的例子来说明，至少在原则上，这些模块中的一个是如何存储一个具体事实的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "具体地说，就是在迈克尔-乔丹打篮球这件事上做文章。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "值得一提的是，这里的布局灵感来自我与 DeepMind 研究人员之一尼尔-南达（Neil Nanda）的一次谈话。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "在大多数情况下，我会假设你已经观看了前两章，或者你已经对变压器有了基本的了解，但温故而知新，所以这里要快速提醒你一下整体流程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "你和我一直在研究一个模型，这个模型经过训练，可以接收一段文字并预测接下来的内容。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "输入文本首先会被分解成一堆标记，也就是通常是单词或单词片段的小块，每个标记都与一个高维向量相关联，也就是一长串数字。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "然后，这个向量序列会反复经过两种操作，一种是注意力操作，它允许向量之间相互传递信息，另一种是多层感知器操作，也就是我们今天要深入研究的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "在向量序列经过这两个模块的多次迭代后，我们希望每个向量都能吸收足够多的信息，这些信息既来自上下文、输入中的所有其他单词，也来自通过训练植入模型权重的一般知识，从而可以用来预测下一个标记是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "我希望你们记住的一个关键概念是，所有这些向量都生活在一个非常非常高的维度空间中，而当你思考这个空间时，不同的方向可以编码不同的意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "我喜欢引用的一个非常经典的例子是，如果你看一下 \"女人 \"的内嵌，然后减去 \"男人 \"的内嵌，再把这一小步加到另一个阳性名词上，比如 \"叔叔\"，你就会发现它与相应的阴性名词非常非常接近。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "从这个意义上说，这个特定的方向编码了性别信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "我们的想法是，在这个超高维空间中，许多其他不同的方向可能对应着模型可能想要表示的其他特征。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "不过，在转换器中，这些向量并不仅仅是对单个单词的含义进行编码。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "当这些信息在网络中流动时，它们会根据周围的环境以及模型的知识吸收更丰富的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "归根结底，每一个词都需要编码远远超出一个词的含义的东西，因为它需要足以预测接下来会发生什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "我们已经看到注意力区块是如何将上下文结合在一起的，但实际上大部分模型参数都存在于 MLP 区块中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "就像我说的，这堂课的中心是一个具体的玩具例子，说明它到底是如何存储迈克尔-乔丹打篮球的事实的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "现在，这个玩具示例需要你我对高维空间做出一些假设。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "首先，我们假设其中一个方向代表迈克尔这个名字的概念，然后另一个几乎垂直的方向代表乔丹这个姓氏的概念，第三个方向代表篮球的概念。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "具体来说，我的意思是，如果你在网络中找到一个正在处理的向量，如果它与这个名字迈克尔方向的点乘积是 1，这就意味着这个向量编码了一个名字叫迈克尔的人的想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "否则，点积将为零或负，这意味着矢量并不真正与该方向一致。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "为了简单起见，让我们完全忽略一个非常合理的问题：如果点积大于 1 意味着什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "同样，它与这些其他方向的点乘积就能告诉你，它代表的是姓乔丹还是姓篮球。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "因此，假设一个向量要表示迈克尔-乔丹的全名，那么它与这两个方向的点积都必须是 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "由于文本迈克尔-乔丹（Michael Jordan）跨越了两个不同的标记，这也就意味着我们必须假设先前的注意力区块已经成功地将信息传递给了这两个向量中的第二个向量，从而确保它可以对这两个名字进行编码。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "有了这些假设，现在让我们进入本课的主要内容。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "多层感知器内部发生了什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "你可以把这一连串的向量想象成流入程序块，记住，每个向量最初都与输入文本中的一个标记相关联。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "接下来，序列中的每个单独向量都会经过一系列简短的运算，我们稍后会解开这些运算，最后，我们会得到另一个具有相同维度的向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "另一个矢量将与流入的原始矢量相加，然后得出流出的结果。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "这一连串的操作会应用到序列中的每个向量，并与输入中的每个标记相关联，所有操作都是并行进行的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "特别是，在这一步中，矢量之间并不对话，它们都在做自己的事情。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "对你我来说，这实际上让事情变得简单多了，因为这意味着如果我们了解了通过这个区块的一个矢量发生了什么，我们就能有效地了解所有矢量发生了什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "当我说这个区块将对迈克尔-乔丹打篮球这一事实进行编码时，我的意思是，如果有一个向量输入，其中编码了迈克尔的名字和乔丹的姓氏，那么这一连串的计算就会产生包含篮球这个方向的结果，这就是在该位置对向量进行添加的结果。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "这个过程的第一步看起来就像用一个很大的矩阵乘以该向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "不出意外，这就是深度学习。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "这个矩阵就像我们看到的其他矩阵一样，充满了从数据中学习到的模型参数，你可以把它想象成一堆旋钮和刻度盘，通过调整这些参数来决定模型的行为。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "现在，一种思考矩阵乘法的好方法是将矩阵的每一行都想象成自己的向量，然后在这些行和被处理的向量之间进行一系列点乘，我将用 E 表示嵌入。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "例如，假设第一行恰好等于我们假定存在的 Michael direction 这个名字。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "这就意味着，如果该向量编码的是迈克尔这个名字，那么输出的第一个分量，也就是这里的点积，就是 1，否则就是 0 或负数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "更有趣的是，花点时间想一想，如果第一排是迈克尔加乔丹的方向，那将意味着什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "为了简单起见，让我把它写成 M 加 J。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "然后，与嵌入 E 进行点积，情况就会很好地分布开来，看起来就像 M 点 E 加上 J 点 E。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "请注意，如果向量编码的是迈克尔-乔丹的全名，那么最终值就是 2，否则就是 1 或小于 1 的值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "这只是矩阵中的一行。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "你可以把所有其他行看作是在并行地提出一些其他类型的问题，探究被处理向量的一些其他类型的特征。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "很多时候，这一步还涉及向输出中添加另一个向量，其中包含从数据中学习到的模型参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "这另一个向量被称为偏差。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "在我们的例子中，我想让你想象一下，第一个分量中的偏差值是负 1，这意味着我们的最终输出看起来就像相关的点乘，但减去了 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "你可能会问，为什么我希望你假定模型已经学会了这一点，稍后你就会明白，如果我们在这里设置一个值，当且仅当一个向量编码了迈克尔-乔丹的全名时，这个值就是正值，否则就是零或负值，那就非常干净利落了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "这个矩阵的总行数，也就是我们一直关注的 GPT-3 的问题数量，略低于 50,000 行。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "事实上，它正好是这个嵌入空间维数的四倍。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "这是一种设计选择。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "你可以做得更多，也可以做得更少，但拥有一个干净的多路径往往对硬件很友好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "由于这个充满权重的矩阵将我们映射到了一个更高的维度空间，所以我把它简称为 W up。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "我会继续将我们正在处理的矢量标记为 E，并将这个偏置矢量标记为 B，然后将它们全部放回图中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "在这一点上，一个问题是这种操作是纯线性的，但语言是一个非常非线性的过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "如果我们测量的迈克尔加乔丹的入选率很高，那么迈克尔加菲尔普斯和亚历克西斯加乔丹也必然会在一定程度上引发入选率，尽管这两者在概念上并不相关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "您真正需要的是全名的简单 \"是 \"或 \"否\"。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "因此，下一步就是通过一个非常简单的非线性函数来传递这个大的中间向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "常见的选择是将所有负值映射为零，而所有正值保持不变。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "为了延续深度学习过于花哨的名称传统，这个非常简单的函数通常被称为整流线性单元，简称 ReLU。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "图表如下",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "因此，以我们想象中的例子为例，中间向量的第一个条目是 1（如果且仅当全名是迈克尔-乔丹），否则就是 0 或负值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "因此，迈克尔-乔丹全名的输出值为 1，否则为 0。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "换句话说，它非常直接地模仿了 AND 门的行为。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "通常情况下，模型会使用一种略有改动的功能，称为 JLU，其基本形状相同，只是更平滑一些。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "但就我们的目的而言，如果我们只考虑 ReLU，就会更简洁一些。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "此外，当你听到人们提到变压器的神经元时，他们说的就是这里的这些值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "在本系列文章的前半部分，我们经常会看到这样一幅神经网络图，图中有一层点和连接上一层的几条线，这通常是为了表达线性步骤的组合，即矩阵乘法，然后是一些简单的项向非线性函数，如 ReLU。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "你可以说，只要这个值为正，这个神经元就处于激活状态；如果这个值为零，它就处于非激活状态。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "下一步看起来与第一步非常相似。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "你乘以一个非常大的矩阵，然后加上一定的偏差项。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "在这种情况下，输出中的维数又回到了嵌入空间的大小，所以我把它叫做向下投影矩阵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "这次，与其逐行思考，不如逐列思考。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "你看，让矩阵乘法在脑中形成印象的另一种方法是，想象将矩阵的每一列乘以它所处理的向量中的相应项，然后将所有这些重新缩放的列相加。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "用这种方法来思考会更好，因为这里的列与嵌入空间的维度相同，所以我们可以把它们看作是嵌入空间中的方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "例如，我们可以设想，模型已经学会将第一根柱子插入我们假设存在的篮球方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "这意味着，当第一个位置上的相关神经元处于活动状态时，我们将在最终结果中添加这一列。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "但是，如果该神经元不活动，如果该数字为零，那么就不会有任何影响。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "而且不一定非得是篮球。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "该模型还可以在这一栏和其他许多功能中加入迈克尔-乔丹全名的元素。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "同时，这个矩阵中的所有其他列都在告诉你，如果相应的神经元处于激活状态，最终结果会增加什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "在这种情况下，如果存在偏差，那么无论神经元的值是多少，每次都会产生偏差。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "你可能会问，这是在干什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "就像这里所有充满参数的对象一样，很难说清楚。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "也许网络需要做一些记账工作，但你可以暂时忽略不计。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "为了使我们的符号更加简洁，我把这个大矩阵 W 称为 \"向下\"，同样把偏置向量 B 称为 \"向下\"，然后把它放回我们的图表中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "就像我之前预览的那样，你要做的就是将最终结果与流入该位置块的矢量相加，从而得到最终结果。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "例如，如果输入的向量同时编码了迈克尔和乔丹这两个名字，那么由于这一连串的操作会触发 AND 门，因此会在篮球方向上进行加法运算，这样跳出来的向量就会把这两个名字一起编码。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "请记住，这是一个并行发生在每个矢量上的过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "特别是，从 GPT-3 数字来看，这意味着这个区块中不仅有 50,000 个神经元，还有 50,000 倍于输入的标记数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "这就是整个操作过程，两个矩阵乘积，每个乘积都添加了偏置，中间还有一个简单的削波功能。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "看过本系列早期视频的朋友都知道，这种结构是我们学习过的最基本的神经网络类型。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "在这个例子中，它接受了识别手写数字的训练。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "在这里，在大型语言模型转换器的背景下，这是一个更大架构中的一个部分，任何试图解释它到底在做什么的尝试，都与将信息编码成高维嵌入空间向量的想法紧密相连。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "这是核心课程，但我确实想退后一步，反思两件不同的事情，第一件是一种簿记，第二件是关于更高维度的一个非常发人深省的事实，实际上，在我深入研究变形金刚之前，我并不知道这个事实。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "在上两章中，我们开始计算 GPT-3 中的参数总数，并了解它们的具体位置，现在让我们快速结束游戏。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "我已经提到过这个向上投影矩阵有不到 50,000 行，而且每一行都与嵌入空间的大小相匹配，GPT-3 的嵌入空间是 12,288 行。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "将这些参数相乘，我们可以得到该矩阵的 6.04 亿个参数，而向下投影的参数数量与此相同，只是形状发生了变化。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "因此，它们总共提供了约 12 亿个参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "偏差向量还涉及另外几个参数，但在总参数中所占比例微不足道，所以我就不展示了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "在 GPT-3 中，这一连串的嵌入向量流经的不是一个，而是 96 个不同的 MLP，因此用于所有这些区块的参数总数加起来约为 1160 亿个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "这约占网络总参数的三分之二，再加上我们之前所做的所有工作，如注意力区块、嵌入和解嵌入，就能得到所宣传的 1 750 亿个总参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "值得一提的还有与这些标准化步骤相关的另一组参数，本解释略过了这些参数，但与偏差向量一样，它们在总参数中所占的比例非常小。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "至于第二点思考，你可能想知道，我们花了这么多时间讨论的这个中心玩具示例，是否反映了事实在真正的大型语言模型中的实际存储方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "诚然，第一个矩阵的行可以被视为嵌入空间中的方向，这意味着每个神经元的激活可以告诉你给定向量与某个特定方向的吻合程度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "同样，如果该神经元处于激活状态，第二个矩阵的列也会告诉你结果会增加什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "这两个都只是数学事实。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "不过，有证据表明，单个神经元很少能像迈克尔-乔丹那样代表一个单一而清晰的特征，而这种情况实际上可能有一个很好的原因，这与最近在可解释性研究人员中流传的一种被称为叠加（superposition）的观点有关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "这一假设可能有助于解释为什么模型特别难以解释，也有助于解释为什么模型的扩展性出奇地好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "其基本思想是，如果你有一个 n 维空间，而你想用空间中相互垂直的方向来表示一系列不同的特征，你知道，这样，如果你在一个方向上添加一个分量，它不会影响任何其他方向，那么你能容纳的向量的最大数量只有 n，即维数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "实际上，对于数学家来说，这就是维度的定义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "但有趣的地方在于，如果你稍微放宽限制，容忍一些噪音。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "假设你允许用矢量来表示这些特征，但这些矢量并不完全垂直，只是接近垂直，可能相距 89 到 91 度之间。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "如果我们在二维或三维空间中，这并没有什么区别。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "这几乎没有任何额外的回旋余地来容纳更多的矢量，这就使得更高维的答案发生了巨大的变化，这就更加违反直觉了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "我可以用一些零碎的 Python 来给你一个非常快速和肮脏的说明，它将创建一个 100 维向量的列表，每个向量都是随机初始化的，这个列表将包含 10,000 个不同的向量，所以向量的数量是维数的 100 倍。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "这幅图显示了这些向量对之间的角度分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "因为它们是随机开始的，所以角度可能从 0 度到 180 度不等，但你会注意到，即使只是随机向量，也有很大的偏向于接近 90 度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "然后，我要做的就是运行一个特定的优化过程，对所有这些矢量进行迭代推移，使它们彼此更加垂直。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "重复多次后，角度的分布情况如下。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "我们必须在这里放大它，因为所有可能的矢量对之间的角度都在 89 度到 91 度之间的这个狭窄范围内。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "一般来说，约翰逊-林登斯特劳斯(Johnson-Lindenstrauss) Lemma 的一个结果是，在一个空间中，像这样几乎垂直的向量数量会随着维数的增加而呈指数增长。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "这对于大型语言模型来说意义重大，因为将独立的想法与几乎垂直的方向联系起来，可能会使大型语言模型受益匪浅。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "这意味着，在它所分配的空间里，可以存储的想法要比尺寸多得多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "这或许可以部分解释为什么模型的性能似乎可以随着尺寸的增大而提高。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "一个空间有 10 倍的维度，可以存储远远超过 10 倍的独立想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "这不仅与流经模型的向量所在的嵌入空间有关，也与我们刚刚研究过的多层感知器中间的神经元向量有关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "也就是说，在 GPT-3 的尺寸下，它可能不只是探测 50,000 个特征，但如果它利用空间中几乎垂直的方向来利用这一巨大的新增容量，它就可以探测被处理向量的更多特征。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "但如果它这样做了，那就意味着单个神经元点亮时，单个特征是不可见的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "它必须看起来像神经元的某种特定组合，一种叠加。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "对于任何想了解更多信息的人来说，这里的一个关键相关搜索词是稀疏自动编码器，它是一些可解释性研究人员用来尝试提取真实特征的工具，即使这些特征叠加在所有这些神经元上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "我将链接到几篇非常棒的人类学文章，都是关于这个问题的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "到此为止，我们还没有触及变压器的每一个细节，但我们已经触及了最重要的部分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "我想在下一章介绍的主要内容是培训过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "一方面，关于训练的原理，简而言之就是反向传播。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "但还有更多的问题需要讨论，比如语言模型所使用的特定成本函数、利用强化学习和人类反馈进行微调的想法，以及缩放规律的概念。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "给你们中的积极追随者提个醒，在我制作下一章之前，还有很多与机器学习无关的视频等着我去挖掘，所以可能还需要一段时间，但我保证它会在适当的时候出现。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "谢谢。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]