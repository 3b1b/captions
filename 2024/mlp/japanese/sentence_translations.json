[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "マイケル・ジョーダンは空白のスポーツをする」というフレーズを大規模な言語モデルに与えて、次に何が起こるかを予測させ、それがバスケットボールを正しく予測したとすると、その数千億のパラメーターのどこかに、特定の人物とその特定のスポーツに関する知識が組み込まれていることを示唆している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "そして一般的に、このようなモデルで遊んだことのある人なら誰でも、大量の事実を記憶しているという明確な感覚を持っていると思う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "では、具体的にどのように機能するのか、というのが妥当な質問だろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "その事実はどこにあるのか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "昨年12月、グーグル・ディープマインドの数人の研究者がこの問題についての研究を投稿し、アスリートとスポーツのマッチングという具体的な例を使っていた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "事実がどのように記憶されるのかについての完全なメカニズム解明は未解決のままだが、彼らは、事実は多層パーセプトロン、略してMLPとして空想的に知られているこれらのネットワークの特定の部分に住んでいるようだ、という非常に一般的でハイレベルな結論を含む、いくつかの興味深い部分的結果を得た。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "ここ数章で、あなたと私は、大規模な言語モデルの基礎となるアーキテクチャであるトランスフォーマーの背後にある詳細、そして他の多くの現代AIの基礎について掘り下げてきた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "直近のチャプターでは、『アテンション』という作品に注目していた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "そして、あなたや私にとっての次のステップは、ネットワークのもう一つの大きな部分を構成する多層パーセプトロンの内部で何が起こっているのか、その詳細を掘り下げることだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "ここでの計算は、特に注意力と比較すると、実際には比較的単純である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "これは本質的には、単純な何かを挟んだ2つの行列の掛け算に集約される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "しかし、これらの計算が何を行っているかを解釈するのは非常に難しい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "ここでの主な目的は、計算を段階的に進め、記憶に残るようにすることだが、少なくとも原理的には、これらのブロックのひとつが具体的な事実を記憶することができるという具体例を示すという文脈でそれを行いたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "具体的には、マイケル・ジョーダンがバスケットボールをプレーしているという事実を記憶することだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "このレイアウトは、ディープマインドの研究者の一人であるニール・ナンダとの会話にインスパイアされたものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "ほとんどの場合、前2章を見たか、そうでなければトランスフォーマーとは何かという基本的な感覚を持っていることを前提に話を進めるが、復習に越したことはないので、ここで全体的な流れを簡単に思い出してほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "あなたと私は、テキストを受け取って次に来るものを予測するように訓練されたモデルを研究してきた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "各トークンは高次元ベクトル、つまり数字の長いリストと関連付けられる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "このベクトル列はその後、2種類の演算を繰り返し通過する。アテンションはベクトル同士の情報の受け渡しを可能にし、次に多層パーセプトロン、つまり今日掘り下げることになるものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "ベクトル列がこの両方のブロックを何度も何度も繰り返した後、最後には、各ベクトルが、文脈や入力の他のすべての単語から、またトレーニングを通じてモデルの重みに組み込まれた一般的な知識から、次にどんなトークンが来るかを予測するのに使えるような十分な情報を吸収していることが望まれる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "これらのベクトルはすべて、非常に高次元の空間に存在し、その空間について考えるとき、異なる方向が異なる種類の意味をコード化することができるということを、頭の片隅に置いておいてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "だから、私が参照したい非常に古典的な例は、女性の埋め込みを見て、男性の埋め込みを引き、その小さなステップを別の男性名詞、例えばおじさんに加えると、対応する女性名詞に非常に近いところに着地する、というものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "その意味で、この特殊な方向性はジェンダー情報をコード化している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "この超高次元空間における他の多くの明確な方向は、モデルが表現したい他の特徴に対応しうるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "トランスフォーマーでは、これらのベクトルは単に1つの単語の意味をエンコードするだけではない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "ネットワーク内を流れるにつれて、周囲の文脈やモデルの知識に基づいて、より豊かな意味を帯びてくる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "結局のところ、ひとつひとつの単語は、その単語の意味をはるかに超えた何かをコード化する必要があるのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "アテンション・ブロックがどのようにコンテキストを取り込むかはすでに見てきたが、モデル・パラメータの大部分はMLPブロックの中にある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "私が言ったように、ここでのレッスンは、マイケル・ジョーダンがバスケットボールをプレーしているという事実を、具体的にどのように保存できるかという、具体的なおもちゃの例が中心になるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "さて、このおもちゃの例では、高次元空間についていくつかの仮定をする必要がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "まず、ある方向がマイケルというファーストネームのアイデアを表し、次にほぼ垂直な別の方向がジョーダンというラストネームのアイデアを表し、さらに3つ目の方向がバスケットボールというアイデアを表すとする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "具体的にどういうことかというと、ネットワークを見て、処理されているベクトルの一つを抜き出すと、このファーストネームのマイケル方向とのドット積が1であれば、そのベクトルはそのファーストネームの人物のアイデアを符号化しているということになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "そうでなければ、ベクトルはゼロか負になり、その方向とは一致しないことになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "また、簡単のために、そのドット積が1より大きいとしたら何を意味するのかという非常に合理的な疑問は完全に無視しておこう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "同様に、他の方向とのドット積で、ジョーダンという姓を表しているのか、バスケットボールを表しているのかがわかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "つまり、あるベクトルがマイケル・ジョーダンというフルネームを表すものだとすると、これらの両方向との内積は1でなければならない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "マイケル・ジョーダンというテキストは2つの異なるトークンにまたがっているため、この場合、先のアテンション・ブロックが、2つの名前を確実にエンコードできるように、これら2つのベクトルのうちの2つ目に情報をうまく渡したと仮定しなければならない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "これらを前提に、レッスンの本題に入ろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "多層パーセプトロンの内部では何が起こっているのか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "この一連のベクターがブロックに流れ込むと考えるかもしれないが、各ベクターはもともと入力テキストのトークンのひとつに関連付けられていたことを思い出してほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "そして最後に、同じ次元の別のベクトルが得られる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "その別のベクトルは、流入した元のベクトルに加算され、その合計が流出した結果となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "この一連の操作は、入力に含まれるすべてのトークンに関連する、シーケンス内のすべてのベクトルに適用されるもので、すべてが並行して行われる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "特に、このステップではベクトル同士が会話することはない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "というのも、このブロックを通してベクトルのひとつに何が起こるかを理解すれば、すべてのベクトルに何が起こるかを理解することができるからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "このブロックがマイケル・ジョーダンがバスケットボールをするという事実を符号化すると言った場合、私が言いたいのは、ファーストネームのマイケルとラストネームのジョーダンを符号化するベクトルが流れ込んできた場合、この一連の計算がバスケットボールという方向を含むものを生成し、それがその位置のベクトルに追加されるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "このプロセスの最初のステップは、そのベクトルに非常に大きな行列を掛けるようなものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "驚きはない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "このマトリクスは、これまで見てきた他のマトリクスと同様、データから学習されたモデル・パラメーターで埋め尽くされており、モデルの挙動を決定するために微調整されるノブやダイヤルの束と考えることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "さて、行列の掛け算について考えるいい方法のひとつは、行列の各行をそれ自身のベクトルとして想像し、それらの行と処理されるベクトルとの間でたくさんのドット積を取ることである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "例えば、その最初の行が、たまたまマイケル・ディレクションと同じ名前だったとしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "ということは、この出力の最初の成分、つまりこの点積は、このベクトルがファーストネームのマイケルをエンコードしていれば1になり、そうでなければゼロかマイナスになるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "さらに楽しいことに、その最初の列が、このファーストネームのマイケルとラストネームのジョーダンの方向だったらどうなるか、ちょっと考えてみてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "簡単のため、M＋Jと書いておこう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "そして、この埋め込みEとドット積を取ると、実にうまく分配され、MドットE＋JドットEのようになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "そして、ベクターがマイケル・ジョーダンというフルネームをエンコードしている場合、究極の値は2になり、そうでない場合は1か1より小さい値になるということに注目してほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "そして、これはこのマトリックスの1行に過ぎない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "他の行はすべて、並行して他の種類の質問をし、処理されるベクトルの他の種類の特徴を探っていると考えてもいいかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "非常に多くの場合、このステップでは、データから学習されたモデル・パラメーターでいっぱいの別のベクトルを出力に加えることになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "この別のベクトルはバイアスと呼ばれる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "この例では、最初の成分のバイアスの値がマイナス1であることを想像してほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "ベクトルがマイケル・ジョーダンというフルネームをエンコードしている場合のみ正となり、そうでない場合はゼロか負となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "このマトリックスの行の総数は、GPT-3の場合、50,000弱である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "実際、この埋め込み空間の次元数のちょうど4倍である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "それはデザインの選択だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "もっと増やすこともできるし、減らすこともできる。しかし、クリーンなマルチプルはハードウェアに優しい傾向がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "この重みでいっぱいの行列は、より高次元の空間にマッピングされるので、省略形のWアップと呼ぶことにする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "今処理しているベクトルをEとラベル付けし、このバイアス・ベクトルをBとラベル付けして図に戻そう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "このとき問題になるのは、この操作は純粋に直線的だが、言語は非常に非線形なプロセスだということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "もし、マイケル＋ジョーダンのエントリー率が高ければ、マイケル＋フェルプスやアレクシス＋ジョーダンのエントリー率も必然的に高くなる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "あなたが本当に望んでいるのは、フルネームに対する単純なイエスかノーである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "そこで次のステップは、この大きな中間ベクトルを非常に単純な非線形関数に通すことだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "一般的な選択は、すべての負の値をゼロにマッピングし、すべての正の値を変更しないものである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "そして、ディープラーニングの伝統である派手すぎる名前を引き継いで、この非常にシンプルな関数はしばしば、rectified linear unit、略してReLUと呼ばれる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "グラフはこんな感じだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "つまり、この中間ベクトルの最初のエントリーが、フルネームがマイケル・ジョーダンである場合のみ1であり、そうでない場合はゼロかマイナスである場合を例にとると、ReLUを通した後、ゼロとマイナスの値がすべてゼロに切り取られた、非常にきれいな値になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "つまりこの出力は、マイケル・ジョーダンというフルネームの場合は1となり、そうでない場合は0となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "つまり、ANDゲートの動作を非常に直接的に模倣しているのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "多くのモデルは、JLUと呼ばれる、基本的な形状は同じで、少し滑らかになっただけの、少し変更された機能を使用する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "しかし、我々の目的からすれば、ReLUのことだけを考えれば少しすっきりする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "また、トランスのニューロンを指しているのを耳にすることがあるが、それはこの値のことである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "ドットのレイヤーと、前のレイヤーに接続する線の束がある、よくあるニューラルネットワークの絵を見るときはいつも、このシリーズの前に出てきた、線形ステップ、行列の乗算、それに続くReLUのような単純な項単位の非線形関数の組み合わせを伝えることを意味する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "このニューロンがアクティブになるのは、この値が正であるときであり、この値がゼロの場合は非アクティブである、と言うことになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "次のステップは、最初のステップとよく似ている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "非常に大きな行列を掛け合わせ、あるバイアス項を加える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "この場合、出力の次元数は埋め込み空間のサイズに戻るので、これをダウン・プロジェクション行列と呼ぶことにする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "そして今回は、行ごとに物事を考えるのではなく、列ごとに物事を考えた方が実はすっきりする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "行列の掛け算を頭に叩き込むもう一つの方法は、行列の各列を取り出し、それを処理するベクトルの対応する項と掛け合わせ、その再スケーリングされた列をすべて足し合わせることを想像することだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "このように考えた方がすっきりするのは、ここでは列が埋め込み空間と同じ次元を持つので、その空間における方向と考えることができるからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "例えば、モデルが最初の列を、我々が存在すると仮定するバスケットボールの方向に作ることを学習したと想像する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "つまり、最初の位置にあるニューロンがアクティブになったとき、この列を最終結果に加えるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "しかし、もしそのニューロンが活動を停止していたら、もしその数字がゼロだったら、何の効果もない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "バスケットボールだけでなくてもいい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "このモデルは、マイケル・ジョーダンというフルネームを持つものから連想させたい、このコラムや他の多くの特徴も焼き付けることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "そして同時に、このマトリックスの他の列はすべて、対応するニューロンがアクティブになった場合、最終結果に何が追加されるかを示している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "この場合、バイアスがかかっているとすれば、それはニューロンの値に関係なく、毎回追加しているだけのことだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "何をやっているんだと思うかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "ここにあるすべてのパラメーター付きオブジェクトと同様、正確なことを言うのはちょっと難しい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "もしかしたら、ネットワークに必要な帳簿付けがあるかもしれないが、今は無視して構わない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "表記をもう少しコンパクトにして、この大きな行列をWと呼び、同様にバイアス・ベクトルBをBと呼び、それを図に戻す。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "先ほどプレビューしたように、この最終結果を、その位置でブロックに流れ込んだベクトルに加えることで、この最終結果が得られる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "例えば、流れてきたベクターがファーストネームのマイケルとラストネームのジョーダンの両方をエンコードしていた場合、この一連の演算がANDゲートをトリガーするため、バスケットボールの方向が加算され、飛び出してきたものはそれらすべてを一緒にエンコードすることになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "そして、これはすべてのベクトルに対して並行して起こっているプロセスであることを忘れないでほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "特にGPT-3の数字を例にとると、このブロックには5万個のニューロンがあるだけでなく、入力のトークン数の5万倍もあるということになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "つまり、2つの行列の積にそれぞれバイアスを加え、その間に単純なクリッピング関数を挟むという操作だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "このシリーズの以前のビデオをご覧になった方なら、この構造が、そこで学んだ最も基本的なニューラルネットワークの一種であることにお気づきだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "この例では、手書きの数字を認識するように訓練されている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "こちらでは、大規模な言語モデルのトランスフォーマーという文脈で、これはより大きなアーキテクチャーの中の1つのピースであり、それが何をしているのかを正確に解釈しようとする試みは、情報を高次元埋め込み空間のベクトルにエンコードするというアイデアと大きく絡み合っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "ひとつは簿記のようなもので、もうひとつは変圧器について調べるまで知らなかった、高次元に関する非常に示唆に富んだ事実だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "前の2つの章では、GPT-3のパラメーターの総数を数え上げ、どこにパラメーターがあるか正確に確認した。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "このアップ射影行列が50,000行弱あり、各行が埋め込み空間のサイズと一致していることはすでに述べた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "それを掛け合わせると、そのマトリックスだけで6億400万個のパラメーターが得られることになり、ダウン・プロジェクションも同じ数のパラメーターを持つが、形状が転置されているだけである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "つまり、合わせて約12億のパラメータが与えられる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "バイアス・ベクトルもさらに2、3のパラメーターを占めるが、全体に占める割合は些細なものなので、表示するつもりもない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "GPT-3では、この一連の埋め込みベクトルは1つではなく、96の異なるMLPを流れるので、これらのブロックすべてに費やされるパラメーターの総数は約1,160億にもなる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "これはネットワーク内の全パラメーターの約3分の2に相当し、注意ブロック、エンベッド、アンベッドなど、これまで持っていたすべてのパラメーターと足すと、確かに宣伝通り1750億という壮大な数字になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "この説明では省略したが、正規化ステップに関連するもう1組のパラメーターがあることを述べておく価値があるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "2つ目の反省点については、私たちが多くの時間を費やしてきたこの中心的なおもちゃの例が、実際の大規模言語モデルでファクトが実際にどのように格納されるかを反映しているのか疑問に思うかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "最初の行列の行は、この埋め込み空間における方向と考えることができるのは事実で、各ニューロンの活性化は、与えられたベクトルがある特定の方向にどれだけ一致するかを示すことになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "また、2番目の行列の列が、そのニューロンがアクティブになった場合に、結果に何が加えられるかを示していることも事実である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "どちらも数学的事実にすぎない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "しかし、個々のニューロンがマイケル・ジョーダンのような単一のきれいな特徴を表現することは非常に稀であることを示す証拠はある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "この仮説は、モデルの解釈が特に難しい理由と、モデルが驚くほどうまくスケールする理由の両方を説明するのに役立つかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "基本的な考え方は、n次元の空間があり、その空間内で互いに垂直な方向を使ってさまざまな特徴を表現したい場合、つまり、ある方向に成分を追加しても他の方向に影響を与えないようにする場合、適合できるベクトルの最大数は、次元数のnだけである、というものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "数学者にとっては、実はこれが次元の定義なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "しかし、興味深いのは、その制約を少し緩めて、多少のノイズを許容する場合だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "例えば、これらの特徴をベクトルで表現することを許可するとしよう。ベクトルは正確に垂直ではなく、89度から91度の間でほぼ垂直である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "二次元でも三次元でも、この違いはない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "これでは、より多くのベクトルを入れるための余分な余地はほとんどない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Pythonで100次元のベクトル・リストを作成し、各ベクトルはランダムに初期化される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "このプロットは、これらのベクトルのペア間の角度の分布を示している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "ランダムなベクトルからスタートしたため、角度は0度から180度まで何でもあり得るが、ランダムなベクトルであっても、90度に近くなるように重く偏っていることにお気づきだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "そして、これらのベクトルが互いにもっと垂直になるように、繰り返し最適化処理を実行する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "これを何度も繰り返した結果、角度の分布はこうなった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "ここで実際に拡大してみなければならないのは、ベクトルのペアの間の可能な角度はすべて、89度から91度の間の狭い範囲に収まっているからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "一般に、ジョンソン・リンデンストラウスのレンマとして知られるものの結果として、このようにほぼ垂直なベクトルを空間に詰め込むことができるベクトルの数は、次元の数とともに指数関数的に増えていく。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "これは大規模な言語モデルにとって非常に重要であり、独立したアイデアをほぼ直角の方向に関連付けることで恩恵を受ける可能性がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "つまり、割り当てられたスペースの寸法よりも、もっともっと多くのアイデアを保存することが可能なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "これは、モデルの性能が大きさによって大きく変化する理由の一部かもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "10倍の次元を持つ空間は、10倍以上の独立したアイデアを保存することができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "そしてこのことは、モデルを流れるベクトルが存在する埋め込み空間だけでなく、先ほど研究した多層パーセプトロンの真ん中にあるニューロンでいっぱいのベクトルにも関係している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "つまり、GPT-3のサイズでは、5万個の特徴量をプローブするだけでなく、空間のほぼ垂直な方向を使用することで、この膨大な追加能力を活用すれば、処理されるベクトルのさらに多くの特徴量をプローブできる可能性がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "しかし、もしそうだとしたら、個々の特徴が1つのニューロンの点灯として見えるわけではないということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "ニューロンの特定の組み合わせ、つまり重ね合わせのように見えるはずだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "もっと詳しく知りたいと思う人のために、ここで重要な関連検索語は、スパースオートエンコーダーである。これは、インタープリタビリティの研究者たちが、たとえすべてのニューロンが非常に重なっていても、真の特徴を抽出しようとするために使うツールである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "この件に関しては、人間学に関する素晴らしい記事がいくつかあるのでリンクしておこう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "この時点では、トランスの細部まで触れたわけではないが、あなたと私は最も重要なポイントを押さえた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "次の章で取り上げたいのは、トレーニングのプロセスだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "一方では、トレーニングがどのように機能するかということについての簡単な答えは、すべてバックプロパゲーションだということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "しかし、言語モデルに使われる特定のコスト関数や、人間のフィードバックによる強化学習を使った微調整の考え方、スケーリング法則の概念など、議論すべきことはまだまだある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "アクティブなフォロワーのために簡単に書いておくと、次の章を作る前に、機械学習関連以外のビデオに没頭したいと思っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "ありがとう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]