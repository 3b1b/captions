1
00:00:00,000 --> 00:00:03,664
マイケル・ジョーダンは空白のスポーツをする」というフレー

2
00:00:03,664 --> 00:00:07,328
ズを大規模な言語モデルに与えて、次に何が起こるかを予測さ

3
00:00:07,328 --> 00:00:10,992
せ、それがバスケットボールを正しく予測したとすると、その

4
00:00:10,992 --> 00:00:14,656
数千億のパラメーターのどこかに、特定の人物とその特定のス

5
00:00:14,656 --> 00:00:18,320
ポーツに関する知識が組み込まれていることを示唆している。

6
00:00:18,940 --> 00:00:21,023
そして一般的に、このようなモデルで遊んだ

7
00:00:21,023 --> 00:00:23,107
ことのある人なら誰でも、大量の事実を記憶

8
00:00:23,107 --> 00:00:25,400
しているという明確な感覚を持っていると思う。

9
00:00:25,700 --> 00:00:27,377
では、具体的にどのように機能する

10
00:00:27,377 --> 00:00:29,160
のか、というのが妥当な質問だろう。

11
00:00:29,160 --> 00:00:31,040
その事実はどこにあるのか？

12
00:00:35,720 --> 00:00:38,561
昨年12月、グーグル・ディープマインドの数人の研

13
00:00:38,561 --> 00:00:41,402
究者がこの問題についての研究を投稿し、アスリート

14
00:00:41,402 --> 00:00:44,480
とスポーツのマッチングという具体的な例を使っていた。

15
00:00:44,900 --> 00:00:48,398
事実がどのように記憶されるのかについての完全なメカニズム

16
00:00:48,398 --> 00:00:51,896
解明は未解決のままだが、彼らは、事実は多層パーセプトロン

17
00:00:51,896 --> 00:00:55,394
、略してMLPとして空想的に知られているこれらのネットワ

18
00:00:55,394 --> 00:00:58,892
ークの特定の部分に住んでいるようだ、という非常に一般的で

19
00:00:58,892 --> 00:01:02,640
ハイレベルな結論を含む、いくつかの興味深い部分的結果を得た。

20
00:01:03,120 --> 00:01:06,171
ここ数章で、あなたと私は、大規模な言語モデルの基礎とな

21
00:01:06,171 --> 00:01:09,222
るアーキテクチャであるトランスフォーマーの背後にある詳

22
00:01:09,222 --> 00:01:12,500
細、そして他の多くの現代AIの基礎について掘り下げてきた。

23
00:01:13,060 --> 00:01:14,630
直近のチャプターでは、『アテンシ

24
00:01:14,630 --> 00:01:16,200
ョン』という作品に注目していた。

25
00:01:16,840 --> 00:01:19,540
そして、あなたや私にとっての次のステップは、ネットワー

26
00:01:19,540 --> 00:01:22,240
クのもう一つの大きな部分を構成する多層パーセプトロンの

27
00:01:22,240 --> 00:01:25,040
内部で何が起こっているのか、その詳細を掘り下げることだ。

28
00:01:25,680 --> 00:01:27,823
ここでの計算は、特に注意力と比較

29
00:01:27,823 --> 00:01:30,100
すると、実際には比較的単純である。

30
00:01:30,560 --> 00:01:32,770
これは本質的には、単純な何かを挟ん

31
00:01:32,770 --> 00:01:34,980
だ2つの行列の掛け算に集約される。

32
00:01:35,720 --> 00:01:38,018
しかし、これらの計算が何を行って

33
00:01:38,018 --> 00:01:40,460
いるかを解釈するのは非常に難しい。

34
00:01:41,560 --> 00:01:44,372
ここでの主な目的は、計算を段階的に進め、記憶に残

35
00:01:44,372 --> 00:01:47,184
るようにすることだが、少なくとも原理的には、これ

36
00:01:47,184 --> 00:01:49,996
らのブロックのひとつが具体的な事実を記憶すること

37
00:01:49,996 --> 00:01:53,160
ができるという具体例を示すという文脈でそれを行いたい。

38
00:01:53,580 --> 00:01:55,294
具体的には、マイケル・ジョーダンがバスケットボー

39
00:01:55,294 --> 00:01:57,080
ルをプレーしているという事実を記憶することだろう。

40
00:01:58,080 --> 00:02:00,640
このレイアウトは、ディープマインドの研究者の一人であ

41
00:02:00,640 --> 00:02:03,200
るニール・ナンダとの会話にインスパイアされたものだ。

42
00:02:04,060 --> 00:02:06,720
ほとんどの場合、前2章を見たか、そうでなければトラ

43
00:02:06,720 --> 00:02:09,380
ンスフォーマーとは何かという基本的な感覚を持ってい

44
00:02:09,380 --> 00:02:12,040
ることを前提に話を進めるが、復習に越したことはない

45
00:02:12,040 --> 00:02:14,700
ので、ここで全体的な流れを簡単に思い出してほしい。

46
00:02:15,340 --> 00:02:18,266
あなたと私は、テキストを受け取って次に来るもの

47
00:02:18,266 --> 00:02:21,320
を予測するように訓練されたモデルを研究してきた。

48
00:02:21,720 --> 00:02:28,500
各トークンは高次元ベクトル、つまり

49
00:02:28,500 --> 00:02:35,280
数字の長いリストと関連付けられる。

50
00:02:35,840 --> 00:02:41,199
このベクトル列はその後、2種類の演算を繰り返し通過する。

51
00:02:41,199 --> 00:02:46,558
アテンションはベクトル同士の情報の受け渡しを可能にし、次

52
00:02:46,558 --> 00:02:52,300
に多層パーセプトロン、つまり今日掘り下げることになるものだ。

53
00:02:53,300 --> 00:02:57,745
ベクトル列がこの両方のブロックを何度も何度も繰り返した

54
00:02:57,745 --> 00:03:02,190
後、最後には、各ベクトルが、文脈や入力の他のすべての単

55
00:03:02,190 --> 00:03:06,635
語から、またトレーニングを通じてモデルの重みに組み込ま

56
00:03:06,635 --> 00:03:11,080
れた一般的な知識から、次にどんなトークンが来るかを予測

57
00:03:11,080 --> 00:03:16,020
するのに使えるような十分な情報を吸収していることが望まれる。

58
00:03:16,860 --> 00:03:20,840
これらのベクトルはすべて、非常に高次元の空間に存在し、その空

59
00:03:20,840 --> 00:03:24,820
間について考えるとき、異なる方向が異なる種類の意味をコード化

60
00:03:24,820 --> 00:03:28,800
することができるということを、頭の片隅に置いておいてほしい。

61
00:03:30,120 --> 00:03:34,032
だから、私が参照したい非常に古典的な例は、女性の埋

62
00:03:34,032 --> 00:03:37,945
め込みを見て、男性の埋め込みを引き、その小さなステ

63
00:03:37,945 --> 00:03:41,857
ップを別の男性名詞、例えばおじさんに加えると、対応

64
00:03:41,857 --> 00:03:46,240
する女性名詞に非常に近いところに着地する、というものだ。

65
00:03:46,440 --> 00:03:48,660
その意味で、この特殊な方向性はジ

66
00:03:48,660 --> 00:03:50,880
ェンダー情報をコード化している。

67
00:03:51,640 --> 00:03:55,640
この超高次元空間における他の多くの明確な方向は、モ

68
00:03:55,640 --> 00:03:59,640
デルが表現したい他の特徴に対応しうるということだ。

69
00:04:01,400 --> 00:04:03,790
トランスフォーマーでは、これらのベクトルは単に

70
00:04:03,790 --> 00:04:06,180
1つの単語の意味をエンコードするだけではない。

71
00:04:06,680 --> 00:04:10,930
ネットワーク内を流れるにつれて、周囲の文脈やモデ

72
00:04:10,930 --> 00:04:15,180
ルの知識に基づいて、より豊かな意味を帯びてくる。

73
00:04:15,880 --> 00:04:19,820
結局のところ、ひとつひとつの単語は、その単語の意味

74
00:04:19,820 --> 00:04:23,760
をはるかに超えた何かをコード化する必要があるのだ。

75
00:04:24,560 --> 00:04:29,086
アテンション・ブロックがどのようにコンテキス

76
00:04:29,086 --> 00:04:33,613
トを取り込むかはすでに見てきたが、モデル・パ

77
00:04:33,613 --> 00:04:38,140
ラメータの大部分はMLPブロックの中にある。

78
00:04:38,720 --> 00:04:40,570
私が言ったように、ここでのレッスンは、マイケル

79
00:04:40,570 --> 00:04:42,420
・ジョーダンがバスケットボールをプレーしている

80
00:04:42,420 --> 00:04:44,270
という事実を、具体的にどのように保存できるかと

81
00:04:44,270 --> 00:04:46,120
いう、具体的なおもちゃの例が中心になるだろう。

82
00:04:47,120 --> 00:04:49,448
さて、このおもちゃの例では、高次元空間

83
00:04:49,448 --> 00:04:51,900
についていくつかの仮定をする必要がある。

84
00:04:52,360 --> 00:04:55,768
まず、ある方向がマイケルというファーストネームの

85
00:04:55,768 --> 00:04:59,176
アイデアを表し、次にほぼ垂直な別の方向がジョーダ

86
00:04:59,176 --> 00:05:02,585
ンというラストネームのアイデアを表し、さらに3つ

87
00:05:02,585 --> 00:05:06,420
目の方向がバスケットボールというアイデアを表すとする。

88
00:05:07,400 --> 00:05:11,040
具体的にどういうことかというと、ネットワークを見て、処理さ

89
00:05:11,040 --> 00:05:14,681
れているベクトルの一つを抜き出すと、このファーストネームの

90
00:05:14,681 --> 00:05:18,322
マイケル方向とのドット積が1であれば、そのベクトルはそのフ

91
00:05:18,322 --> 00:05:21,963
ァーストネームの人物のアイデアを符号化しているということに

92
00:05:21,963 --> 00:05:22,340
なる。

93
00:05:23,800 --> 00:05:26,250
そうでなければ、ベクトルはゼロか負にな

94
00:05:26,250 --> 00:05:28,700
り、その方向とは一致しないことになる。

95
00:05:29,420 --> 00:05:32,320
また、簡単のために、そのドット積が1より大きいとしたら何を

96
00:05:32,320 --> 00:05:35,320
意味するのかという非常に合理的な疑問は完全に無視しておこう。

97
00:05:36,200 --> 00:05:39,911
同様に、他の方向とのドット積で、ジョーダンという姓を表

98
00:05:39,911 --> 00:05:43,760
しているのか、バスケットボールを表しているのかがわかる。

99
00:05:44,740 --> 00:05:47,386
つまり、あるベクトルがマイケル・ジョーダン

100
00:05:47,386 --> 00:05:50,033
というフルネームを表すものだとすると、これ

101
00:05:50,033 --> 00:05:52,680
らの両方向との内積は1でなければならない。

102
00:05:53,480 --> 00:05:56,850
マイケル・ジョーダンというテキストは2つの異なるトークンにま

103
00:05:56,850 --> 00:06:00,220
たがっているため、この場合、先のアテンション・ブロックが、2

104
00:06:00,220 --> 00:06:03,590
つの名前を確実にエンコードできるように、これら2つのベクトル

105
00:06:03,590 --> 00:06:06,960
のうちの2つ目に情報をうまく渡したと仮定しなければならない。

106
00:06:07,940 --> 00:06:11,480
これらを前提に、レッスンの本題に入ろう。

107
00:06:11,880 --> 00:06:14,980
多層パーセプトロンの内部では何が起こっているのか？

108
00:06:17,100 --> 00:06:19,926
この一連のベクターがブロックに流れ込むと考えるかもし

109
00:06:19,926 --> 00:06:22,753
れないが、各ベクターはもともと入力テキストのトークン

110
00:06:22,753 --> 00:06:25,580
のひとつに関連付けられていたことを思い出してほしい。

111
00:06:26,080 --> 00:06:36,360
そして最後に、同じ次元の別のベクトルが得られる。

112
00:06:36,880 --> 00:06:39,962
その別のベクトルは、流入した元のベクトル

113
00:06:39,962 --> 00:06:43,200
に加算され、その合計が流出した結果となる。

114
00:06:43,720 --> 00:06:46,275
この一連の操作は、入力に含まれるすべてのトー

115
00:06:46,275 --> 00:06:48,831
クンに関連する、シーケンス内のすべてのベクト

116
00:06:48,831 --> 00:06:51,620
ルに適用されるもので、すべてが並行して行われる。

117
00:06:52,100 --> 00:06:56,200
特に、このステップではベクトル同士が会話することはない。

118
00:06:56,720 --> 00:06:59,741
というのも、このブロックを通してベクトルのひ

119
00:06:59,741 --> 00:07:02,763
とつに何が起こるかを理解すれば、すべてのベク

120
00:07:02,763 --> 00:07:06,060
トルに何が起こるかを理解することができるからだ。

121
00:07:07,100 --> 00:07:09,866
このブロックがマイケル・ジョーダンがバスケットボール

122
00:07:09,866 --> 00:07:12,633
をするという事実を符号化すると言った場合、私が言いた

123
00:07:12,633 --> 00:07:15,400
いのは、ファーストネームのマイケルとラストネームのジ

124
00:07:15,400 --> 00:07:18,167
ョーダンを符号化するベクトルが流れ込んできた場合、こ

125
00:07:18,167 --> 00:07:20,933
の一連の計算がバスケットボールという方向を含むものを

126
00:07:20,933 --> 00:07:24,020
生成し、それがその位置のベクトルに追加されるということだ。

127
00:07:25,600 --> 00:07:27,650
このプロセスの最初のステップは、そのベクト

128
00:07:27,650 --> 00:07:29,700
ルに非常に大きな行列を掛けるようなものだ。

129
00:07:30,040 --> 00:07:31,980
驚きはない。

130
00:07:32,680 --> 00:07:35,312
このマトリクスは、これまで見てきた他のマトリクス

131
00:07:35,312 --> 00:07:37,945
と同様、データから学習されたモデル・パラメーター

132
00:07:37,945 --> 00:07:40,578
で埋め尽くされており、モデルの挙動を決定するため

133
00:07:40,578 --> 00:07:43,540
に微調整されるノブやダイヤルの束と考えることができる。

134
00:07:44,500 --> 00:07:48,578
さて、行列の掛け算について考えるいい方法のひとつは、行列

135
00:07:48,578 --> 00:07:52,656
の各行をそれ自身のベクトルとして想像し、それらの行と処理

136
00:07:52,656 --> 00:07:56,880
されるベクトルとの間でたくさんのドット積を取ることである。

137
00:07:57,280 --> 00:08:00,577
例えば、その最初の行が、たまたまマイケル

138
00:08:00,577 --> 00:08:04,040
・ディレクションと同じ名前だったとしよう。

139
00:08:04,320 --> 00:08:07,813
ということは、この出力の最初の成分、つまりこの点積は、この

140
00:08:07,813 --> 00:08:11,306
ベクトルがファーストネームのマイケルをエンコードしていれば

141
00:08:11,306 --> 00:08:14,800
1になり、そうでなければゼロかマイナスになるということだ。

142
00:08:15,880 --> 00:08:18,280
さらに楽しいことに、その最初の列が、このファース

143
00:08:18,280 --> 00:08:20,680
トネームのマイケルとラストネームのジョーダンの方

144
00:08:20,680 --> 00:08:23,080
向だったらどうなるか、ちょっと考えてみてほしい。

145
00:08:23,700 --> 00:08:27,420
簡単のため、M＋Jと書いておこう。

146
00:08:28,080 --> 00:08:31,459
そして、この埋め込みEとドット積を取ると、実にう

147
00:08:31,459 --> 00:08:34,980
まく分配され、MドットE＋JドットEのようになる。

148
00:08:34,980 --> 00:08:38,220
そして、ベクターがマイケル・ジョーダンというフルネームを

149
00:08:38,220 --> 00:08:41,460
エンコードしている場合、究極の値は2になり、そうでない場

150
00:08:41,460 --> 00:08:44,700
合は1か1より小さい値になるということに注目してほしい。

151
00:08:45,340 --> 00:08:47,260
そして、これはこのマトリックスの1行に過ぎない。

152
00:08:47,600 --> 00:08:51,748
他の行はすべて、並行して他の種類の質問をし、処理されるベク

153
00:08:51,748 --> 00:08:56,040
トルの他の種類の特徴を探っていると考えてもいいかもしれない。

154
00:08:56,700 --> 00:08:58,546
非常に多くの場合、このステップでは、データ

155
00:08:58,546 --> 00:09:00,393
から学習されたモデル・パラメーターでいっぱ

156
00:09:00,393 --> 00:09:02,240
いの別のベクトルを出力に加えることになる。

157
00:09:02,240 --> 00:09:04,560
この別のベクトルはバイアスと呼ばれる。

158
00:09:05,180 --> 00:09:10,369
この例では、最初の成分のバイアスの値が

159
00:09:10,369 --> 00:09:15,560
マイナス1であることを想像してほしい。

160
00:09:16,120 --> 00:09:24,140
ベクトルがマイケル・ジョーダンというフルネームをエンコード

161
00:09:24,140 --> 00:09:32,160
している場合のみ正となり、そうでない場合はゼロか負となる。

162
00:09:33,040 --> 00:09:37,770
このマトリックスの行の総数は、GP

163
00:09:37,770 --> 00:09:42,780
T-3の場合、50,000弱である。

164
00:09:43,100 --> 00:09:46,640
実際、この埋め込み空間の次元数のちょうど4倍である。

165
00:09:46,920 --> 00:09:47,900
それはデザインの選択だ。

166
00:09:47,940 --> 00:09:49,816
もっと増やすこともできるし、減らすこともできる。

167
00:09:49,816 --> 00:09:51,927
しかし、クリーンなマルチプルはハードウェアに優しい傾向

168
00:09:51,927 --> 00:09:52,240
がある。

169
00:09:52,740 --> 00:09:55,818
この重みでいっぱいの行列は、より高次元の空間にマッ

170
00:09:55,818 --> 00:09:59,020
ピングされるので、省略形のWアップと呼ぶことにする。

171
00:09:59,020 --> 00:10:03,089
今処理しているベクトルをEとラベル付けし、このバ

172
00:10:03,089 --> 00:10:07,160
イアス・ベクトルをBとラベル付けして図に戻そう。

173
00:10:09,180 --> 00:10:12,270
このとき問題になるのは、この操作は純粋に直線的だ

174
00:10:12,270 --> 00:10:15,360
が、言語は非常に非線形なプロセスだということだ。

175
00:10:15,880 --> 00:10:19,828
もし、マイケル＋ジョーダンのエントリー率が

176
00:10:19,828 --> 00:10:23,776
高ければ、マイケル＋フェルプスやアレクシス

177
00:10:23,776 --> 00:10:28,100
＋ジョーダンのエントリー率も必然的に高くなる。

178
00:10:28,540 --> 00:10:30,223
あなたが本当に望んでいるのは、フルネ

179
00:10:30,223 --> 00:10:32,000
ームに対する単純なイエスかノーである。

180
00:10:32,900 --> 00:10:35,309
そこで次のステップは、この大きな中間ベク

181
00:10:35,309 --> 00:10:37,840
トルを非常に単純な非線形関数に通すことだ。

182
00:10:38,360 --> 00:10:41,752
一般的な選択は、すべての負の値をゼロにマッピ

183
00:10:41,752 --> 00:10:45,300
ングし、すべての正の値を変更しないものである。

184
00:10:46,440 --> 00:10:49,559
そして、ディープラーニングの伝統である派手すぎる名前を引

185
00:10:49,559 --> 00:10:52,678
き継いで、この非常にシンプルな関数はしばしば、recti

186
00:10:52,678 --> 00:10:56,020
fied linear unit、略してReLUと呼ばれる。

187
00:10:56,020 --> 00:10:57,880
グラフはこんな感じだ。

188
00:10:58,300 --> 00:11:01,702
つまり、この中間ベクトルの最初のエントリーが、フ

189
00:11:01,702 --> 00:11:05,105
ルネームがマイケル・ジョーダンである場合のみ1で

190
00:11:05,105 --> 00:11:08,508
あり、そうでない場合はゼロかマイナスである場合を

191
00:11:08,508 --> 00:11:11,911
例にとると、ReLUを通した後、ゼロとマイナスの

192
00:11:11,911 --> 00:11:15,740
値がすべてゼロに切り取られた、非常にきれいな値になる。

193
00:11:16,100 --> 00:11:17,902
つまりこの出力は、マイケル・ジョーダンというフル

194
00:11:17,902 --> 00:11:19,780
ネームの場合は1となり、そうでない場合は0となる。

195
00:11:20,560 --> 00:11:24,120
つまり、ANDゲートの動作を非常に直接的に模倣しているのだ。

196
00:11:25,660 --> 00:11:28,839
多くのモデルは、JLUと呼ばれる、基本的な形状は同じで、

197
00:11:28,839 --> 00:11:32,020
少し滑らかになっただけの、少し変更された機能を使用する。

198
00:11:32,500 --> 00:11:34,110
しかし、我々の目的からすれば、ReLU

199
00:11:34,110 --> 00:11:35,720
のことだけを考えれば少しすっきりする。

200
00:11:36,740 --> 00:11:39,565
また、トランスのニューロンを指しているのを耳

201
00:11:39,565 --> 00:11:42,520
にすることがあるが、それはこの値のことである。

202
00:11:42,900 --> 00:11:46,572
ドットのレイヤーと、前のレイヤーに接続する線の束が

203
00:11:46,572 --> 00:11:50,243
ある、よくあるニューラルネットワークの絵を見るとき

204
00:11:50,243 --> 00:11:53,915
はいつも、このシリーズの前に出てきた、線形ステップ

205
00:11:53,915 --> 00:11:57,588
、行列の乗算、それに続くReLUのような単純な項単

206
00:11:57,588 --> 00:12:01,260
位の非線形関数の組み合わせを伝えることを意味する。

207
00:12:02,500 --> 00:12:04,570
このニューロンがアクティブになるのは、こ

208
00:12:04,570 --> 00:12:06,641
の値が正であるときであり、この値がゼロの

209
00:12:06,641 --> 00:12:08,920
場合は非アクティブである、と言うことになる。

210
00:12:10,120 --> 00:12:12,380
次のステップは、最初のステップとよく似ている。

211
00:12:12,560 --> 00:12:16,580
非常に大きな行列を掛け合わせ、あるバイアス項を加える。

212
00:12:16,980 --> 00:12:21,250
この場合、出力の次元数は埋め込み空間のサイズに戻るので

213
00:12:21,250 --> 00:12:25,520
、これをダウン・プロジェクション行列と呼ぶことにする。

214
00:12:26,220 --> 00:12:28,790
そして今回は、行ごとに物事を考えるのではなく

215
00:12:28,790 --> 00:12:31,360
、列ごとに物事を考えた方が実はすっきりする。

216
00:12:31,860 --> 00:12:35,191
行列の掛け算を頭に叩き込むもう一つの方法は、

217
00:12:35,191 --> 00:12:38,522
行列の各列を取り出し、それを処理するベクトル

218
00:12:38,522 --> 00:12:41,854
の対応する項と掛け合わせ、その再スケーリング

219
00:12:41,854 --> 00:12:45,640
された列をすべて足し合わせることを想像することだ。

220
00:12:46,840 --> 00:12:49,820
このように考えた方がすっきりするのは、ここで

221
00:12:49,820 --> 00:12:52,800
は列が埋め込み空間と同じ次元を持つので、その

222
00:12:52,800 --> 00:12:55,780
空間における方向と考えることができるからだ。

223
00:12:56,140 --> 00:12:59,544
例えば、モデルが最初の列を、我々が存在すると仮定する

224
00:12:59,544 --> 00:13:03,080
バスケットボールの方向に作ることを学習したと想像する。

225
00:13:04,180 --> 00:13:07,412
つまり、最初の位置にあるニューロンがアクティブに

226
00:13:07,412 --> 00:13:10,780
なったとき、この列を最終結果に加えるということだ。

227
00:13:11,140 --> 00:13:13,410
しかし、もしそのニューロンが活動を停止していた

228
00:13:13,410 --> 00:13:15,780
ら、もしその数字がゼロだったら、何の効果もない。

229
00:13:16,500 --> 00:13:18,060
バスケットボールだけでなくてもいい。

230
00:13:18,220 --> 00:13:20,475
このモデルは、マイケル・ジョーダンというフ

231
00:13:20,475 --> 00:13:22,730
ルネームを持つものから連想させたい、このコ

232
00:13:22,730 --> 00:13:25,200
ラムや他の多くの特徴も焼き付けることができる。

233
00:13:26,980 --> 00:13:30,206
そして同時に、このマトリックスの他の列はすべ

234
00:13:30,206 --> 00:13:33,433
て、対応するニューロンがアクティブになった場

235
00:13:33,433 --> 00:13:36,660
合、最終結果に何が追加されるかを示している。

236
00:13:37,360 --> 00:13:40,430
この場合、バイアスがかかっているとすれば、それはニュ

237
00:13:40,430 --> 00:13:43,500
ーロンの値に関係なく、毎回追加しているだけのことだ。

238
00:13:44,060 --> 00:13:45,280
何をやっているんだと思うかもしれない。

239
00:13:45,540 --> 00:13:47,387
ここにあるすべてのパラメーター付きオブジェク

240
00:13:47,387 --> 00:13:49,320
トと同様、正確なことを言うのはちょっと難しい。

241
00:13:49,320 --> 00:13:51,791
もしかしたら、ネットワークに必要な帳簿付け

242
00:13:51,791 --> 00:13:54,380
があるかもしれないが、今は無視して構わない。

243
00:13:54,860 --> 00:13:59,560
表記をもう少しコンパクトにして、この大きな行列をWと呼び

244
00:13:59,560 --> 00:14:04,260
、同様にバイアス・ベクトルBをBと呼び、それを図に戻す。

245
00:14:04,740 --> 00:14:07,526
先ほどプレビューしたように、この最終結果

246
00:14:07,526 --> 00:14:10,313
を、その位置でブロックに流れ込んだベクト

247
00:14:10,313 --> 00:14:13,240
ルに加えることで、この最終結果が得られる。

248
00:14:13,820 --> 00:14:16,904
例えば、流れてきたベクターがファーストネームのマイケ

249
00:14:16,904 --> 00:14:19,988
ルとラストネームのジョーダンの両方をエンコードしてい

250
00:14:19,988 --> 00:14:23,072
た場合、この一連の演算がANDゲートをトリガーするた

251
00:14:23,072 --> 00:14:26,156
め、バスケットボールの方向が加算され、飛び出してきた

252
00:14:26,156 --> 00:14:29,240
ものはそれらすべてを一緒にエンコードすることになる。

253
00:14:29,820 --> 00:14:32,010
そして、これはすべてのベクトルに対して並行して起

254
00:14:32,010 --> 00:14:34,200
こっているプロセスであることを忘れないでほしい。

255
00:14:34,800 --> 00:14:38,103
特にGPT-3の数字を例にとると、このブロッ

256
00:14:38,103 --> 00:14:41,406
クには5万個のニューロンがあるだけでなく、入

257
00:14:41,406 --> 00:14:44,860
力のトークン数の5万倍もあるということになる。

258
00:14:48,180 --> 00:14:51,608
つまり、2つの行列の積にそれぞれバイアスを加え、

259
00:14:51,608 --> 00:14:55,180
その間に単純なクリッピング関数を挟むという操作だ。

260
00:14:56,080 --> 00:14:58,228
このシリーズの以前のビデオをご覧になった方なら

261
00:14:58,228 --> 00:15:00,377
、この構造が、そこで学んだ最も基本的なニューラ

262
00:15:00,377 --> 00:15:02,620
ルネットワークの一種であることにお気づきだろう。

263
00:15:03,080 --> 00:15:06,100
この例では、手書きの数字を認識するように訓練されている。

264
00:15:06,580 --> 00:15:09,849
こちらでは、大規模な言語モデルのトランスフォーマーと

265
00:15:09,849 --> 00:15:13,119
いう文脈で、これはより大きなアーキテクチャーの中の1

266
00:15:13,119 --> 00:15:16,389
つのピースであり、それが何をしているのかを正確に解釈

267
00:15:16,389 --> 00:15:19,658
しようとする試みは、情報を高次元埋め込み空間のベクト

268
00:15:19,658 --> 00:15:23,180
ルにエンコードするというアイデアと大きく絡み合っている。

269
00:15:24,260 --> 00:15:31,052
ひとつは簿記のようなもので、もうひとつは変圧器について調べ

270
00:15:31,052 --> 00:15:38,080
るまで知らなかった、高次元に関する非常に示唆に富んだ事実だ。

271
00:15:41,080 --> 00:15:45,825
前の2つの章では、GPT-3のパラメーターの総数を

272
00:15:45,825 --> 00:15:50,760
数え上げ、どこにパラメーターがあるか正確に確認した。

273
00:15:51,400 --> 00:15:56,684
このアップ射影行列が50,000行弱あり、各行が埋

274
00:15:56,684 --> 00:16:02,180
め込み空間のサイズと一致していることはすでに述べた。

275
00:16:03,240 --> 00:16:05,821
それを掛け合わせると、そのマトリックスだけで

276
00:16:05,821 --> 00:16:08,403
6億400万個のパラメーターが得られることに

277
00:16:08,403 --> 00:16:10,985
なり、ダウン・プロジェクションも同じ数のパラ

278
00:16:10,985 --> 00:16:13,920
メーターを持つが、形状が転置されているだけである。

279
00:16:14,500 --> 00:16:17,400
つまり、合わせて約12億のパラメータが与えられる。

280
00:16:18,280 --> 00:16:21,190
バイアス・ベクトルもさらに2、3のパラメーターを占めるが、

281
00:16:21,190 --> 00:16:24,100
全体に占める割合は些細なものなので、表示するつもりもない。

282
00:16:24,660 --> 00:16:29,126
GPT-3では、この一連の埋め込みベクトルは1つではなく

283
00:16:29,126 --> 00:16:33,593
、96の異なるMLPを流れるので、これらのブロックすべて

284
00:16:33,593 --> 00:16:38,060
に費やされるパラメーターの総数は約1,160億にもなる。

285
00:16:38,820 --> 00:16:41,954
これはネットワーク内の全パラメーターの約3分の2

286
00:16:41,954 --> 00:16:45,089
に相当し、注意ブロック、エンベッド、アンベッドな

287
00:16:45,089 --> 00:16:48,224
ど、これまで持っていたすべてのパラメーターと足す

288
00:16:48,224 --> 00:16:51,620
と、確かに宣伝通り1750億という壮大な数字になる。

289
00:16:53,060 --> 00:16:58,351
この説明では省略したが、正規化ステップに関連するもう1

290
00:16:58,351 --> 00:17:03,840
組のパラメーターがあることを述べておく価値があるだろう。

291
00:17:05,900 --> 00:17:08,267
2つ目の反省点については、私たちが多くの時間を

292
00:17:08,267 --> 00:17:10,635
費やしてきたこの中心的なおもちゃの例が、実際の

293
00:17:10,635 --> 00:17:13,003
大規模言語モデルでファクトが実際にどのように格

294
00:17:13,003 --> 00:17:15,680
納されるかを反映しているのか疑問に思うかもしれない。

295
00:17:16,319 --> 00:17:20,060
最初の行列の行は、この埋め込み空間における方向と考えること

296
00:17:20,060 --> 00:17:23,800
ができるのは事実で、各ニューロンの活性化は、与えられたベク

297
00:17:23,800 --> 00:17:27,540
トルがある特定の方向にどれだけ一致するかを示すことになる。

298
00:17:27,760 --> 00:17:31,050
また、2番目の行列の列が、そのニューロンがアクティブになった

299
00:17:31,050 --> 00:17:34,340
場合に、結果に何が加えられるかを示していることも事実である。

300
00:17:34,640 --> 00:17:36,800
どちらも数学的事実にすぎない。

301
00:17:37,740 --> 00:17:43,023
しかし、個々のニューロンがマイケル・ジョ

302
00:17:43,023 --> 00:17:48,307
ーダンのような単一のきれいな特徴を表現す

303
00:17:48,307 --> 00:17:54,120
ることは非常に稀であることを示す証拠はある。

304
00:17:54,640 --> 00:17:57,190
この仮説は、モデルの解釈が特に難しい理由

305
00:17:57,190 --> 00:17:59,741
と、モデルが驚くほどうまくスケールする理

306
00:17:59,741 --> 00:18:02,420
由の両方を説明するのに役立つかもしれない。

307
00:18:03,500 --> 00:18:07,492
基本的な考え方は、n次元の空間があり、その空間内

308
00:18:07,492 --> 00:18:11,484
で互いに垂直な方向を使ってさまざまな特徴を表現し

309
00:18:11,484 --> 00:18:15,476
たい場合、つまり、ある方向に成分を追加しても他の

310
00:18:15,476 --> 00:18:19,468
方向に影響を与えないようにする場合、適合できるベ

311
00:18:19,468 --> 00:18:23,960
クトルの最大数は、次元数のnだけである、というものだ。

312
00:18:24,600 --> 00:18:27,620
数学者にとっては、実はこれが次元の定義なのだ。

313
00:18:28,220 --> 00:18:30,827
しかし、興味深いのは、その制約を少し

314
00:18:30,827 --> 00:18:33,580
緩めて、多少のノイズを許容する場合だ。

315
00:18:34,180 --> 00:18:37,393
例えば、これらの特徴をベクトルで表現すること

316
00:18:37,393 --> 00:18:40,606
を許可するとしよう。ベクトルは正確に垂直では

317
00:18:40,606 --> 00:18:43,820
なく、89度から91度の間でほぼ垂直である。

318
00:18:44,820 --> 00:18:48,020
二次元でも三次元でも、この違いはない。

319
00:18:48,260 --> 00:18:52,520
これでは、より多くのベクトルを入れ

320
00:18:52,520 --> 00:18:56,780
るための余分な余地はほとんどない。

321
00:18:57,660 --> 00:19:06,030
Pythonで100次元のベクトル・リストを

322
00:19:06,030 --> 00:19:14,400
作成し、各ベクトルはランダムに初期化される。

323
00:19:15,320 --> 00:19:17,540
このプロットは、これらのベクトル

324
00:19:17,540 --> 00:19:19,900
のペア間の角度の分布を示している。

325
00:19:20,680 --> 00:19:24,352
ランダムなベクトルからスタートしたため、角度は0度から1

326
00:19:24,352 --> 00:19:28,025
80度まで何でもあり得るが、ランダムなベクトルであっても

327
00:19:28,025 --> 00:19:31,960
、90度に近くなるように重く偏っていることにお気づきだろう。

328
00:19:32,500 --> 00:19:36,905
そして、これらのベクトルが互いにもっと垂直

329
00:19:36,905 --> 00:19:41,520
になるように、繰り返し最適化処理を実行する。

330
00:19:42,060 --> 00:19:46,660
これを何度も繰り返した結果、角度の分布はこうなった。

331
00:19:47,120 --> 00:19:50,284
ここで実際に拡大してみなければならないのは、

332
00:19:50,284 --> 00:19:53,448
ベクトルのペアの間の可能な角度はすべて、89

333
00:19:53,448 --> 00:19:56,900
度から91度の間の狭い範囲に収まっているからだ。

334
00:19:58,020 --> 00:20:01,123
一般に、ジョンソン・リンデンストラウスのレンマ

335
00:20:01,123 --> 00:20:04,227
として知られるものの結果として、このようにほぼ

336
00:20:04,227 --> 00:20:07,331
垂直なベクトルを空間に詰め込むことができるベク

337
00:20:07,331 --> 00:20:10,840
トルの数は、次元の数とともに指数関数的に増えていく。

338
00:20:11,960 --> 00:20:14,600
これは大規模な言語モデルにとって非常に重要

339
00:20:14,600 --> 00:20:17,240
であり、独立したアイデアをほぼ直角の方向に

340
00:20:17,240 --> 00:20:19,880
関連付けることで恩恵を受ける可能性がある。

341
00:20:20,000 --> 00:20:23,154
つまり、割り当てられたスペースの寸法よりも、もっ

342
00:20:23,154 --> 00:20:26,440
ともっと多くのアイデアを保存することが可能なのだ。

343
00:20:27,320 --> 00:20:29,470
これは、モデルの性能が大きさによって

344
00:20:29,470 --> 00:20:31,740
大きく変化する理由の一部かもしれない。

345
00:20:32,540 --> 00:20:35,882
10倍の次元を持つ空間は、10倍以上の

346
00:20:35,882 --> 00:20:39,400
独立したアイデアを保存することができる。

347
00:20:40,420 --> 00:20:43,760
そしてこのことは、モデルを流れるベクトルが存在する埋め込

348
00:20:43,760 --> 00:20:47,100
み空間だけでなく、先ほど研究した多層パーセプトロンの真ん

349
00:20:47,100 --> 00:20:50,440
中にあるニューロンでいっぱいのベクトルにも関係している。

350
00:20:50,960 --> 00:20:54,991
つまり、GPT-3のサイズでは、5万個の特徴量をプロ

351
00:20:54,991 --> 00:20:59,022
ーブするだけでなく、空間のほぼ垂直な方向を使用するこ

352
00:20:59,022 --> 00:21:03,053
とで、この膨大な追加能力を活用すれば、処理されるベク

353
00:21:03,053 --> 00:21:07,240
トルのさらに多くの特徴量をプローブできる可能性がある。

354
00:21:07,780 --> 00:21:11,060
しかし、もしそうだとしたら、個々の特徴が1つのニュ

355
00:21:11,060 --> 00:21:14,340
ーロンの点灯として見えるわけではないということだ。

356
00:21:14,660 --> 00:21:17,020
ニューロンの特定の組み合わせ、つま

357
00:21:17,020 --> 00:21:19,380
り重ね合わせのように見えるはずだ。

358
00:21:20,400 --> 00:21:22,875
もっと詳しく知りたいと思う人のために、ここで重要

359
00:21:22,875 --> 00:21:25,453
な関連検索語は、スパースオートエンコーダーである。

360
00:21:25,453 --> 00:21:27,929
これは、インタープリタビリティの研究者たちが、た

361
00:21:27,929 --> 00:21:30,404
とえすべてのニューロンが非常に重なっていても、真

362
00:21:30,404 --> 00:21:32,880
の特徴を抽出しようとするために使うツールである。

363
00:21:33,540 --> 00:21:35,130
この件に関しては、人間学に関する素晴らし

364
00:21:35,130 --> 00:21:36,800
い記事がいくつかあるのでリンクしておこう。

365
00:21:37,880 --> 00:21:40,590
この時点では、トランスの細部まで触れたわけではな

366
00:21:40,590 --> 00:21:43,300
いが、あなたと私は最も重要なポイントを押さえた。

367
00:21:43,520 --> 00:21:47,640
次の章で取り上げたいのは、トレーニングのプロセスだ。

368
00:21:48,460 --> 00:21:51,227
一方では、トレーニングがどのように機能す

369
00:21:51,227 --> 00:21:53,994
るかということについての簡単な答えは、す

370
00:21:53,994 --> 00:21:56,900
べてバックプロパゲーションだということだ。

371
00:21:57,220 --> 00:22:00,697
しかし、言語モデルに使われる特定のコスト関数や、人間の

372
00:22:00,697 --> 00:22:04,174
フィードバックによる強化学習を使った微調整の考え方、ス

373
00:22:04,174 --> 00:22:07,780
ケーリング法則の概念など、議論すべきことはまだまだある。

374
00:22:08,960 --> 00:22:14,386
アクティブなフォロワーのために簡単に書いておくと、次の章を

375
00:22:14,386 --> 00:22:20,000
作る前に、機械学習関連以外のビデオに没頭したいと思っている。

376
00:22:35,640 --> 00:22:37,920
ありがとう。

