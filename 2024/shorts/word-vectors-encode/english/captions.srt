1
00:00:00,000 --> 00:00:02,840
What is Hitler plus Italy minus Germany?

2
00:00:03,500 --> 00:00:05,682
This came up in a full video that I did dissecting 

3
00:00:05,682 --> 00:00:07,480
what happens inside large language models.

4
00:00:07,920 --> 00:00:10,166
You see, when tools like Chachipt process text, 

5
00:00:10,166 --> 00:00:12,975
the first thing they do is subdivide it into little pieces, 

6
00:00:12,975 --> 00:00:16,580
and they associate each piece with a large vector, some long list of numbers.

7
00:00:16,840 --> 00:00:19,281
This is called an embedding of that piece of text, 

8
00:00:19,281 --> 00:00:23,062
and it's helpful to imagine these embedding vectors as directions in some very 

9
00:00:23,062 --> 00:00:26,747
high dimensional space, even if we struggle to concretely visualize anything 

10
00:00:26,747 --> 00:00:28,040
more than three dimensions.

11
00:00:28,540 --> 00:00:31,616
Models that learn to embed words as vectors like this often 

12
00:00:31,616 --> 00:00:35,000
encode meaning into the directions of this high dimensional space.

13
00:00:35,240 --> 00:00:39,101
If you take the difference between the embeddings of man and woman and you add that 

14
00:00:39,101 --> 00:00:42,780
to the embedding of uncle, you get a vector very close to the embedding of aunt.

15
00:00:43,320 --> 00:00:46,912
If you take the embedding of Italy minus Germany and you add it to the 

16
00:00:46,912 --> 00:00:50,960
embedding of Hitler, you get something very close to the embedding of Mussolini.

17
00:00:50,960 --> 00:00:54,850
It's as if the model learned to associate some directions in this high 

18
00:00:54,850 --> 00:00:59,180
dimensional space with Italian-ness, and others with World War II axis leaders.

