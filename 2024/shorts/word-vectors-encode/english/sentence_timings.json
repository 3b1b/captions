[
 [
  "What is Hitler plus Italy minus Germany?",
  0.0,
  2.84
 ],
 [
  "This came up in a full video that I did dissecting what happens inside large language models.",
  3.5,
  7.48
 ],
 [
  "You see, when tools like Chachipt process text, the first thing they do is subdivide it into little pieces, and they associate each piece with a large vector, some long list of numbers.",
  7.92,
  16.58
 ],
 [
  "This is called an embedding of that piece of text, and it's helpful to imagine these embedding vectors as directions in some very high dimensional space, even if we struggle to concretely visualize anything more than three dimensions.",
  16.84,
  28.04
 ],
 [
  "Models that learn to embed words as vectors like this often encode meaning into the directions of this high dimensional space.",
  28.54,
  35.0
 ],
 [
  "If you take the difference between the embeddings of man and woman and you add that to the embedding of uncle, you get a vector very close to the embedding of aunt.",
  35.24,
  42.78
 ],
 [
  "If you take the embedding of Italy minus Germany and you add it to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  43.32,
  50.96
 ],
 [
  "It's as if the model learned to associate some directions in this high dimensional space with Italian-ness, and others with World War II axis leaders.",
  50.96,
  59.18
 ]
]