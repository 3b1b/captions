1
00:00:00,000 --> 00:00:03,086
Earlier this year, I was contacted by the Computer History Museum, 

2
00:00:03,086 --> 00:00:07,186
asking if I could help make a short explainer video for this exhibit they're doing about 

3
00:00:07,186 --> 00:00:08,200
large language models.

4
00:00:08,600 --> 00:00:11,136
If you're a regular viewer, you'll know that I was also 

5
00:00:11,136 --> 00:00:13,900
making a fair bit of material to visualize this topic anyway.

6
00:00:14,220 --> 00:00:18,080
More importantly, this is a museum that I really love, so this was a very easy yes.

7
00:00:18,480 --> 00:00:21,736
At first, I thought this was just going to be an abridged version of the 

8
00:00:21,736 --> 00:00:24,055
more detailed explainers that I was already making, 

9
00:00:24,055 --> 00:00:27,356
but ultimately it proved to be a really satisfying outlet for emphasizing 

10
00:00:27,356 --> 00:00:30,478
some of the more important ideas that those more technical explainers 

11
00:00:30,478 --> 00:00:31,460
may have glossed over.

12
00:00:31,900 --> 00:00:35,040
I'm very curious in the comments if you think this is useful as 

13
00:00:35,040 --> 00:00:38,083
a lightweight intro to share with others in your life curious 

14
00:00:38,083 --> 00:00:41,420
about large language models, but without further ado, let's dive in.

15
00:00:43,560 --> 00:00:46,396
Imagine you happen across a short movie script that 

16
00:00:46,396 --> 00:00:49,560
describes a scene between a person and their AI assistant.

17
00:00:49,900 --> 00:00:55,480
The script has what the person asks the AI, but the AI's response has been torn off.

18
00:00:55,480 --> 00:00:59,400
Suppose you also have this powerful magical machine that can take 

19
00:00:59,400 --> 00:01:03,380
any text and provide a sensible prediction of what word comes next.

20
00:01:03,920 --> 00:01:07,926
You could then finish the script by feeding in what you have to the machine, 

21
00:01:07,926 --> 00:01:10,788
seeing what it would predict to start the AI's answer, 

22
00:01:10,788 --> 00:01:15,160
and then repeating this over and over with a growing script completing the dialogue.

23
00:01:15,800 --> 00:01:18,900
When you interact with a chatbot, this is exactly what's happening.

24
00:01:19,440 --> 00:01:23,121
A large language model is a sophisticated mathematical function 

25
00:01:23,121 --> 00:01:26,400
that predicts what word comes next for any piece of text.

26
00:01:26,800 --> 00:01:29,822
Instead of predicting one word with certainty, though, 

27
00:01:29,822 --> 00:01:33,340
what it does is assign a probability to all possible next words.

28
00:01:34,040 --> 00:01:39,220
To build a chatbot, you lay out some text that describes an interaction between a user 

29
00:01:39,220 --> 00:01:44,460
and a hypothetical AI assistant, add on whatever the user types in as the first part of 

30
00:01:44,460 --> 00:01:49,580
the interaction, and then have the model repeatedly predict the next word that such a 

31
00:01:49,580 --> 00:01:54,880
hypothetical AI assistant would say in response, and that's what's presented to the user.

32
00:01:55,500 --> 00:01:58,634
In doing this, the output tends to look a lot more natural if 

33
00:01:58,634 --> 00:02:01,920
you allow it to select less likely words along the way at random.

34
00:02:02,560 --> 00:02:06,040
So what this means is even though the model itself is deterministic, 

35
00:02:06,040 --> 00:02:09,520
a given prompt typically gives a different answer each time it's run.

36
00:02:10,460 --> 00:02:14,752
Models learn how to make these predictions by processing an enormous amount of text, 

37
00:02:14,752 --> 00:02:16,520
typically pulled from the internet.

38
00:02:16,520 --> 00:02:21,891
For a standard human to read the amount of text that was used to train GPT-3, 

39
00:02:21,891 --> 00:02:26,780
for example, if they read non-stop 24-7, it would take over 2600 years.

40
00:02:27,140 --> 00:02:29,760
Larger models since then train on much, much more.

41
00:02:30,620 --> 00:02:34,200
You can think of training a little bit like tuning the dials on a big machine.

42
00:02:34,700 --> 00:02:38,721
The way that a language model behaves is entirely determined by these 

43
00:02:38,721 --> 00:02:42,800
many different continuous values, usually called parameters or weights.

44
00:02:43,440 --> 00:02:46,519
Changing those parameters will change the probabilities 

45
00:02:46,519 --> 00:02:49,600
that the model gives for the next word on a given input.

46
00:02:50,280 --> 00:02:53,147
What puts the large in large language model is how 

47
00:02:53,147 --> 00:02:56,240
they can have hundreds of billions of these parameters.

48
00:02:57,620 --> 00:03:00,460
No human ever deliberately sets those parameters.

49
00:03:00,860 --> 00:03:05,063
Instead, they begin at random, meaning the model just outputs gibberish, 

50
00:03:05,063 --> 00:03:08,980
but they're repeatedly refined based on many example pieces of text.

51
00:03:09,560 --> 00:03:13,076
One of these training examples could be just a handful of words, 

52
00:03:13,076 --> 00:03:16,916
or it could be thousands, but in either case, the way this works is to 

53
00:03:16,916 --> 00:03:20,540
pass in all but the last word from that example into the model and 

54
00:03:20,540 --> 00:03:24,760
compare the prediction that it makes with the true last word from the example.

55
00:03:25,680 --> 00:03:29,813
An algorithm called backpropagation is used to tweak all of the parameters 

56
00:03:29,813 --> 00:03:33,616
in such a way that it makes the model a little more likely to choose 

57
00:03:33,616 --> 00:03:37,420
the true last word and a little less likely to choose all the others.

58
00:03:38,160 --> 00:03:41,170
When you do this for many, many trillions of examples, 

59
00:03:41,170 --> 00:03:45,878
not only does the model start to give more accurate predictions on the training data, 

60
00:03:45,878 --> 00:03:50,203
but it also starts to make more reasonable predictions on text that it's never 

61
00:03:50,203 --> 00:03:50,860
seen before.

62
00:03:51,840 --> 00:03:56,339
Given the huge number of parameters and the enormous amount of training data, 

63
00:03:56,339 --> 00:04:01,300
the scale of computation involved in training a large language model is mind-boggling.

64
00:04:02,020 --> 00:04:04,705
To illustrate, imagine that you could perform one 

65
00:04:04,705 --> 00:04:07,820
billion additions and multiplications every single second.

66
00:04:08,480 --> 00:04:11,746
How long do you think it would take for you to do all of the 

67
00:04:11,746 --> 00:04:14,960
operations involved in training the largest language models?

68
00:04:15,880 --> 00:04:17,459
Do you think it would take a year?

69
00:04:18,459 --> 00:04:20,380
Maybe something like 10,000 years?

70
00:04:21,440 --> 00:04:23,220
The answer is actually much more than that.

71
00:04:23,540 --> 00:04:26,320
It's well over 100 million years.

72
00:04:27,940 --> 00:04:29,780
This is only part of the story, though.

73
00:04:29,960 --> 00:04:31,640
This whole process is called pre-training.

74
00:04:31,920 --> 00:04:35,066
The goal of auto-completing a random passage of text from the 

75
00:04:35,066 --> 00:04:38,620
internet is very different from the goal of being a good AI assistant.

76
00:04:39,300 --> 00:04:42,500
To address this, chatbots undergo another type of training, 

77
00:04:42,500 --> 00:04:46,180
just as important, called reinforcement learning with human feedback.

78
00:04:46,900 --> 00:04:49,918
Workers flag unhelpful or problematic predictions, 

79
00:04:49,918 --> 00:04:53,529
and their corrections further change the model's parameters, 

80
00:04:53,529 --> 00:04:57,200
making them more likely to give predictions that users prefer.

81
00:04:57,200 --> 00:05:01,280
Looking back at the pre-training, though, this staggering amount of 

82
00:05:01,280 --> 00:05:05,540
computation is only made possible by using special computer chips that 

83
00:05:05,540 --> 00:05:09,680
are optimized for running many operations in parallel, known as GPUs.

84
00:05:10,540 --> 00:05:14,040
However, not all language models can be easily parallelized.

85
00:05:14,500 --> 00:05:19,237
Prior to 2017, most language models would process text one word at a time, 

86
00:05:19,237 --> 00:05:24,860
but then a team of researchers at Google introduced a new model known as the transformer.

87
00:05:25,720 --> 00:05:29,165
Transformers don't read text from the start to the finish, 

88
00:05:29,165 --> 00:05:31,560
they soak it all in at once, in parallel.

89
00:05:32,320 --> 00:05:37,020
The very first step inside a transformer, and most other language models for that matter, 

90
00:05:37,020 --> 00:05:39,840
is to associate each word with a long list of numbers.

91
00:05:40,280 --> 00:05:44,816
The reason for this is that the training process only works with continuous values, 

92
00:05:44,816 --> 00:05:47,732
so you have to somehow encode language using numbers, 

93
00:05:47,732 --> 00:05:51,674
and each of these lists of numbers may somehow encode the meaning of the 

94
00:05:51,674 --> 00:05:52,700
corresponding word.

95
00:05:52,700 --> 00:05:55,780
What makes transformers unique is their reliance 

96
00:05:55,780 --> 00:05:58,420
on a special operation known as attention.

97
00:05:59,400 --> 00:06:04,104
This operation gives all of these lists of numbers a chance to talk to one another 

98
00:06:04,104 --> 00:06:08,980
and refine the meanings they encode based on the context around, all done in parallel.

99
00:06:09,820 --> 00:06:14,127
For example, the numbers encoding the word bank might be changed based on the 

100
00:06:14,127 --> 00:06:18,600
context surrounding it to somehow encode the more specific notion of a riverbank.

101
00:06:19,700 --> 00:06:23,449
Transformers typically also include a second type of operation known 

102
00:06:23,449 --> 00:06:26,981
as a feed-forward neural network, and this gives the model extra 

103
00:06:26,981 --> 00:06:30,840
capacity to store more patterns about language learned during training.

104
00:06:31,700 --> 00:06:35,821
All of this data repeatedly flows through many different iterations of 

105
00:06:35,821 --> 00:06:38,898
these two fundamental operations, and as it does so, 

106
00:06:38,898 --> 00:06:42,904
the hope is that each list of numbers is enriched to encode whatever 

107
00:06:42,904 --> 00:06:47,084
information might be needed to make an accurate prediction of what word 

108
00:06:47,084 --> 00:06:48,420
follows in the passage.

109
00:06:49,420 --> 00:06:53,954
At the end, one final function is performed on the last vector in this sequence, 

110
00:06:53,954 --> 00:06:58,993
which now has had a chance to be influenced by all the other context from the input text, 

111
00:06:58,993 --> 00:07:02,184
as well as everything the model learned during training, 

112
00:07:02,184 --> 00:07:04,480
to produce a prediction of the next word.

113
00:07:04,900 --> 00:07:09,780
Again, the model's prediction looks like a probability for every possible next word.

114
00:07:10,980 --> 00:07:15,214
Although researchers design the framework for how each of these steps work, 

115
00:07:15,214 --> 00:07:19,782
it's important to understand that the specific behavior is an emergent phenomenon 

116
00:07:19,782 --> 00:07:24,240
based on how those hundreds of billions of parameters are tuned during training.

117
00:07:24,900 --> 00:07:27,490
This makes it incredibly challenging to determine 

118
00:07:27,490 --> 00:07:30,340
why the model makes the exact predictions that it does.

119
00:07:30,860 --> 00:07:36,198
What you can see is that when you use large language model predictions to autocomplete 

120
00:07:36,198 --> 00:07:41,660
a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.

121
00:07:48,580 --> 00:07:51,907
If you happen to be in the Bay Area, I think you would enjoy stopping 

122
00:07:51,907 --> 00:07:55,140
by the Computer History Museum to see the exhibit this was made for.

123
00:07:55,440 --> 00:07:58,547
If you're a new viewer and you're curious about more details on how 

124
00:07:58,547 --> 00:08:01,700
transformers and attention work, boy do I have some material for you.

125
00:08:02,120 --> 00:08:05,801
One option is to jump into a series I made about deep learning, 

126
00:08:05,801 --> 00:08:10,461
where we visualize and motivate the details of attention and all the other steps 

127
00:08:10,461 --> 00:08:11,440
in a transformer.

128
00:08:11,820 --> 00:08:15,250
Also, on my second channel I just posted a talk I gave a couple 

129
00:08:15,250 --> 00:08:18,360
months ago about this topic for the company TNG in Munich.

130
00:08:18,800 --> 00:08:22,822
Sometimes I actually prefer the content I make as a casual talk rather than a produced 

131
00:08:22,822 --> 00:08:26,660
video, but I leave it up to you which one of these feels like the better follow-on.

132
00:08:41,440 --> 00:08:45,920
Thank you.

