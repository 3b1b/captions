[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Le iniziali GPT stanno per Generative Pretrained Transformer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "La prima parola è abbastanza semplice: si tratta di bot che generano nuovo testo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Il termine \"preaddestrato\" si riferisce al fatto che il modello è stato sottoposto a un processo di apprendimento da un'enorme quantità di dati, mentre il prefisso indica che c'è più spazio per perfezionarlo su compiti specifici con un addestramento aggiuntivo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Ma l'ultima parola è il vero pezzo chiave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Un trasformatore è un tipo specifico di rete neurale, un modello di apprendimento automatico, ed è l'invenzione principale alla base dell'attuale boom dell'IA.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "L'obiettivo di questo video e dei capitoli successivi è quello di spiegare visivamente cosa succede all'interno di un trasformatore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Seguiremo i dati che lo attraversano e procederemo passo dopo passo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Esistono diversi tipi di modelli che puoi costruire utilizzando i trasformatori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Alcuni modelli ricevono l'audio e producono una trascrizione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Questa frase proviene da un modello che fa il percorso inverso, producendo un discorso sintetico solo a partire dal testo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Tutti quegli strumenti che hanno conquistato il mondo nel 2022, come Dolly e Midjourney, che accettano una descrizione testuale e producono un'immagine, sono basati sui trasformatori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Anche se non riesco a fargli capire cosa dovrebbe essere una creatura a forma di torta, sono comunque stupito che questo genere di cose sia anche solo lontanamente possibile.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "E il trasformatore originale introdotto nel 2017 da Google è stato inventato per lo specifico caso d'uso della traduzione di testi da una lingua all'altra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Ma la variante su cui ci concentreremo io e te, che è quella che sta alla base di strumenti come ChatGPT, sarà un modello addestrato a recepire un brano di testo, magari con alcune immagini o suoni circostanti che lo accompagnano, e a produrre una previsione su ciò che viene dopo nel brano.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Questa previsione assume la forma di una distribuzione di probabilità su diversi pezzi di testo che potrebbero seguire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "A prima vista, potresti pensare che prevedere la parola successiva sia un obiettivo molto diverso dalla generazione di nuovo testo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Ma una volta che hai un modello di predizione come questo, una cosa semplice per generare un testo più lungo è dargli un frammento iniziale con cui lavorare, fargli prendere un campione casuale dalla distribuzione che ha appena generato, aggiungere quel campione al testo e poi eseguire di nuovo l'intero processo per fare una nuova predizione basata su tutto il nuovo testo, compreso quello appena aggiunto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Non so tu, ma a me sembra proprio che non debba funzionare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "In questa animazione, ad esempio, sto eseguendo GPT-2 sul mio laptop e gli sto facendo prevedere e campionare ripetutamente il pezzo di testo successivo per generare una storia basata sul testo di partenza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "La storia non ha molto senso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Ma se lo sostituisco con chiamate API a GPT-3, che è lo stesso modello di base, solo molto più grande, improvvisamente e quasi magicamente otteniamo una storia sensata, che sembra persino dedurre che una creatura pi greco vivrebbe in una terra di matematica e calcolo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Questo processo di predizione e campionamento ripetuto è essenzialmente ciò che accade quando interagisci con ChatGPT o con altri modelli linguistici di grandi dimensioni e li vedi produrre una parola alla volta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Infatti, una funzione che mi piacerebbe molto è la possibilità di vedere la distribuzione sottostante per ogni nuova parola scelta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Iniziamo con un'anteprima di alto livello di come i dati passano attraverso un trasformatore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Dedicheremo molto più tempo a motivare, interpretare e approfondire i dettagli di ogni fase, ma a grandi linee, quando uno di questi chatbot genera una determinata parola, ecco cosa succede sotto il cofano.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Innanzitutto, l'input viene suddiviso in tanti piccoli pezzi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Questi pezzi sono chiamati token e nel caso del testo tendono a essere parole o piccoli pezzi di parole o altre combinazioni di caratteri comuni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Se si tratta di immagini o di suoni, i token potrebbero essere piccoli frammenti di quell'immagine o di quel suono.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Ognuno di questi token è poi associato a un vettore, cioè a un elenco di numeri, che ha lo scopo di codificare in qualche modo il significato di quel pezzo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Se pensi a questi vettori come a delle coordinate in uno spazio dimensionale molto alto, le parole con significati simili tendono a posizionarsi su vettori vicini tra loro in quello spazio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Questa sequenza di vettori passa poi attraverso un'operazione nota come blocco di attenzione, che permette ai vettori di parlare tra loro e di passare informazioni avanti e indietro per aggiornare i loro valori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Ad esempio, il significato della parola modello nella frase un modello di apprendimento automatico è diverso dal suo significato nella frase un modello di moda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Il blocco dell'attenzione è responsabile di capire quali parole del contesto sono rilevanti per aggiornare i significati di quali altre parole e come esattamente questi significati dovrebbero essere aggiornati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "E ancora, ogni volta che uso la parola significato, questo è in qualche modo interamente codificato nelle voci di questi vettori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "In seguito, questi vettori passano attraverso un altro tipo di operazione e, a seconda della fonte che stai leggendo, saranno indicati come un perceptron multistrato o forse un livello feed-forward.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "E qui i vettori non parlano tra loro, ma eseguono tutti la stessa operazione in parallelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Sebbene questo blocco sia un po' più difficile da interpretare, più avanti parleremo di come il passaggio sia un po' come fare una lunga lista di domande su ogni vettore e poi aggiornarlo in base alle risposte a queste domande.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Tutte le operazioni di questi due blocchi sembrano un enorme mucchio di moltiplicazioni di matrici e il nostro compito principale sarà quello di capire come leggere le matrici sottostanti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Sto tralasciando alcuni dettagli su alcune fasi di normalizzazione che avvengono nel mezzo, ma questa è in fin dei conti un'anteprima di alto livello.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Dopodiché, il processo si ripete essenzialmente, andando avanti e indietro tra blocchi di attenzione e blocchi di perceptron multistrato, fino a quando, alla fine, la speranza è che tutto il significato essenziale del passaggio sia stato in qualche modo incorporato nell'ultimo vettore della sequenza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Poi eseguiamo una certa operazione su quest'ultimo vettore che produce una distribuzione di probabilità su tutti i possibili token, tutti i possibili pezzetti di testo che potrebbero venire dopo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "E come ho detto, una volta che hai uno strumento in grado di prevedere cosa viene dopo, dato un frammento di testo, puoi dargli in pasto un po' di testo di partenza e fargli fare ripetutamente questo gioco di previsione di cosa viene dopo, campionando dalla distribuzione, aggiungendo e ripetendo in continuazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Alcuni di voi conoscitori ricorderanno che, molto tempo prima dell'arrivo di ChatGPT, le prime demo di GPT-3 si presentavano in questo modo: potevi far completare automaticamente storie e saggi sulla base di un frammento iniziale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Per trasformare uno strumento come questo in un chatbot, il punto di partenza più semplice è avere un po' di testo che stabilisca l'ambientazione di un utente che interagisce con un assistente AI utile, quello che chiameremmo il prompt del sistema, e poi usare la domanda o il prompt iniziale dell'utente come primo pezzo di dialogo, per poi iniziare a prevedere cosa direbbe un assistente AI utile in risposta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "C'è molto altro da dire sulla fase di formazione necessaria per far funzionare bene questo sistema, ma a grandi linee l'idea è questa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "In questo capitolo approfondiremo i dettagli di ciò che accade all'inizio della rete e alla fine della rete. Inoltre, vorrei dedicare un po' di tempo a rivedere alcune importanti conoscenze di base, cose che sarebbero state una seconda natura per qualsiasi ingegnere dell'apprendimento automatico all'epoca dei trasformatori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Se ti senti a tuo agio con queste conoscenze di base e sei un po' impaziente, puoi passare al prossimo capitolo, che si concentrerà sui blocchi di attenzione, generalmente considerati il cuore del trasformatore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "In seguito voglio parlare di questi blocchi di perceptron multistrato, di come funziona l'addestramento e di una serie di altri dettagli che sono stati saltati fino a questo punto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Per un contesto più ampio, questi video si aggiungono a una mini-serie sull'apprendimento profondo e non c'è problema se non hai guardato i precedenti, credo che tu possa farlo anche senza ordine, ma prima di immergerti nello specifico dei trasformatori, credo che valga la pena di assicurarsi che siamo sulla stessa lunghezza d'onda riguardo alle premesse e alla struttura di base dell'apprendimento profondo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "A rischio di dire cose ovvie, questo è un approccio all'apprendimento automatico, che descrive qualsiasi modello in cui si utilizzano i dati per determinare in qualche modo il comportamento di un modello.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Con questo intendo dire che, ad esempio, vuoi una funzione che prenda un'immagine e produca un'etichetta che la descriva, oppure il nostro esempio di previsione della parola successiva in un brano di testo, o qualsiasi altro compito che richieda un certo elemento di intuizione e di riconoscimento dei modelli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Al giorno d'oggi lo diamo quasi per scontato, ma l'idea dell'apprendimento automatico è che piuttosto che cercare di definire esplicitamente una procedura per svolgere quel compito nel codice, come si faceva agli albori dell'intelligenza artificiale, si crea una struttura molto flessibile con parametri regolabili, come una serie di manopole e quadranti, e poi in qualche modo si usano molti esempi di come dovrebbe essere l'output per un dato input per modificare e mettere a punto i valori di quei parametri per imitare questo comportamento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Ad esempio, la forma più semplice di apprendimento automatico è la regressione lineare, in cui gli input e gli output sono singoli numeri, come la metratura di una casa e il suo prezzo, e si vuole trovare una linea di migliore adattamento attraverso questi dati, per prevedere i prezzi futuri delle case.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "La linea è descritta da due parametri continui, la pendenza e l'intercetta y, e l'obiettivo della regressione lineare è quello di determinare questi parametri in modo che corrispondano perfettamente ai dati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Inutile dire che i modelli di deep learning diventano molto più complicati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "Il GPT-3, ad esempio, non ha due, ma 175 miliardi di parametri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Ma il punto è che non è scontato che si possa creare un modello gigantesco con un numero enorme di parametri senza che si adatti in modo eccessivo ai dati di addestramento o che sia completamente intrattabile da addestrare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "L'apprendimento profondo descrive una classe di modelli che negli ultimi due decenni ha dimostrato di poter essere scalata in modo eccellente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Ciò che li accomuna è lo stesso algoritmo di addestramento, chiamato backpropagation, e il contesto che voglio che tu abbia mentre andiamo avanti è che per far sì che questo algoritmo di addestramento funzioni bene in scala, questi modelli devono seguire un certo formato specifico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Se conosci questo formato, ti aiuterà a spiegare molte delle scelte di come un trasformatore elabora il linguaggio, che altrimenti rischiano di sembrare arbitrarie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Innanzitutto, qualunque sia il modello che stai realizzando, l'input deve essere formattato come un array di numeri reali.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Può trattarsi di un elenco di numeri, di un array bidimensionale o, molto spesso, di array di dimensioni più elevate, dove il termine generale utilizzato è tensore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Spesso si pensa che i dati in ingresso vengano progressivamente trasformati in molti strati distinti, dove ogni strato è sempre strutturato come una sorta di array di numeri reali, fino ad arrivare a uno strato finale che viene considerato l'output.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Ad esempio, il livello finale del nostro modello di elaborazione del testo è un elenco di numeri che rappresentano la distribuzione di probabilità per tutti i possibili token successivi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "Nel deep learning, questi parametri del modello sono quasi sempre indicati come pesi, perché una caratteristica fondamentale di questi modelli è che l'unico modo in cui questi parametri interagiscono con i dati elaborati è attraverso somme ponderate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Inoltre, si possono aggiungere alcune funzioni non lineari, ma non dipendono da parametri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "In genere, però, invece di vedere le somme ponderate tutte nude e scritte esplicitamente in questo modo, le troverai raggruppate come vari componenti di un prodotto vettoriale matriciale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Se pensi a come funziona la moltiplicazione vettoriale a matrice, ogni componente dell'output appare come una somma ponderata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Spesso è concettualmente più semplice per te e per me pensare a matrici riempite con parametri regolabili che trasformano i vettori ricavati dai dati da elaborare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Ad esempio, i 175 miliardi di pesi del GPT-3 sono organizzati in poco meno di 28.000 matrici distinte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Queste matrici si suddividono a loro volta in otto categorie diverse, e quello che faremo è passare in rassegna ognuna di queste categorie per capire cosa fa quel tipo di matrice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Mentre andiamo avanti, credo sia divertente fare riferimento ai numeri specifici del GPT-3 per contare esattamente da dove provengono quei 175 miliardi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Anche se al giorno d'oggi ci sono modelli più grandi e migliori, questo ha un certo fascino in quanto è stato il modello di grande lingua a catturare davvero l'attenzione del mondo al di fuori delle comunità ML.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Inoltre, dal punto di vista pratico, le aziende tendono a tenere le labbra ben strette sui numeri specifici delle reti più moderne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Voglio solo mettere in chiaro che quando si sbircia sotto il cofano per vedere cosa succede all'interno di uno strumento come ChatGPT, quasi tutti i calcoli effettivi assomigliano alla moltiplicazione vettoriale di matrici.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "C'è il rischio di perdersi in un mare di miliardi di numeri, ma dovresti tracciare una distinzione molto netta nella tua mente tra i pesi del modello, che colorerò sempre di blu o rosso, e i dati elaborati, che colorerò sempre di grigio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "I pesi sono i cervelli veri e propri, sono le cose apprese durante l'allenamento e determinano il comportamento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "I dati elaborati codificano semplicemente qualsiasi input specifico inserito nel modello per una determinata esecuzione, come ad esempio un frammento di testo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Con tutte queste premesse, passiamo alla prima fase di questo esempio di elaborazione del testo, che consiste nel suddividere l'input in piccoli pezzi e trasformarli in vettori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Ho accennato al fatto che questi pezzi sono chiamati token, che possono essere pezzi di parole o di punteggiatura, ma di tanto in tanto in questo capitolo e soprattutto nel prossimo, vorrei far finta che sia suddiviso in modo più pulito in parole.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Poiché noi esseri umani pensiamo a parole, questo renderà molto più facile fare riferimento a piccoli esempi e chiarire ogni passaggio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Il modello ha un vocabolario predefinito, un elenco di tutte le parole possibili, ad esempio 50.000, e la prima matrice che incontreremo, nota come matrice di incorporazione, ha una singola colonna per ciascuna di queste parole.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Queste colonne determinano il vettore in cui ogni parola si trasforma nella prima fase.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "La etichettiamo We e, come tutte le matrici che vediamo, i suoi valori iniziano in modo casuale, ma verranno appresi in base ai dati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "La trasformazione delle parole in vettori era una pratica comune nell'apprendimento automatico molto prima dei trasformatori, ma è un po' strana se non l'hai mai vista prima e pone le basi per tutto ciò che segue, quindi prendiamoci un momento per familiarizzare con essa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Spesso chiamiamo questo incorporamento parola, il che invita a pensare a questi vettori in modo molto geometrico come a dei punti in uno spazio ad alta dimensionalità.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Visualizzare un elenco di tre numeri come coordinate di punti nello spazio 3D non sarebbe un problema, ma le incorporazioni di parole tendono ad essere molto più dimensionali.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "In GPT-3 ci sono 12.288 dimensioni e, come vedrai, è importante lavorare in uno spazio con molte direzioni distinte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Nello stesso modo in cui si può prendere una fetta bidimensionale di uno spazio 3D e proiettare tutti i punti su quella fetta, per animare le incorporazioni di parole che un semplice modello mi fornisce, farò una cosa analoga scegliendo una fetta tridimensionale di questo spazio ad alta dimensionalità, proiettando i vettori di parole su di essa e visualizzando i risultati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "L'idea principale è che, man mano che un modello modifica e regola i suoi pesi per determinare il modo in cui le parole vengono incorporate come vettori durante l'addestramento, tende a stabilizzarsi su un insieme di incorporazioni in cui le direzioni nello spazio hanno una sorta di significato semantico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Per il semplice modello da parola a vettore che sto eseguendo qui, se eseguo una ricerca di tutte le parole le cui incorporazioni si avvicinano di più a quella di torre, noterai che tutte sembrano dare vibrazioni simili a quelle di torre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "E se vuoi usare Python e giocare a casa tua, questo è il modello specifico che sto usando per realizzare le animazioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Non si tratta di un trasformatore, ma è sufficiente per illustrare l'idea che le direzioni nello spazio possono avere un significato semantico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Un esempio molto classico di questo è che se prendi la differenza tra i vettori di donna e uomo, qualcosa che potresti visualizzare come un piccolo vettore che collega la punta di uno alla punta dell'altro, è molto simile alla differenza tra re e regina.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Quindi, supponiamo che tu non conosca la parola che indica un monarca donna, potresti trovarla prendendo re, aggiungendo la direzione donna-uomo e cercando le incorporazioni più vicine a quel punto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "O almeno, più o meno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Nonostante questo sia un esempio classico per il modello con cui sto giocando, la vera incorporazione di regina è in realtà un po' più lontana di quanto si possa pensare, presumibilmente perché il modo in cui regina viene utilizzata nei dati di formazione non è semplicemente una versione femminile di re.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Quando ho giocato, le relazioni familiari sembravano illustrare meglio l'idea.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Il punto è che sembra che durante l'addestramento il modello abbia trovato vantaggioso scegliere le incorporazioni in modo che una direzione di questo spazio codifichi le informazioni sul genere.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Un altro esempio è che se si prende l'embedding dell'Italia, si sottrae l'embedding della Germania e lo si aggiunge all'embedding di Hitler, si ottiene qualcosa di molto simile all'embedding di Mussolini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "È come se il modello avesse imparato ad associare alcune direzioni all'italianità e altre ai leader dell'asse della Seconda Guerra Mondiale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Forse il mio esempio preferito in questo senso è che in alcuni modelli, se si prende la differenza tra Germania e Giappone e la si aggiunge al sushi, si finisce per avvicinarsi al bratwurst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Inoltre, giocando a questo gioco di ricerca dei vicini più vicini, mi ha fatto piacere vedere quanto Kat fosse vicina sia alla bestia che al mostro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Un'intuizione matematica che è utile tenere a mente, soprattutto per il prossimo capitolo, è che il prodotto del punto di due vettori può essere considerato come un modo per misurare il loro allineamento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Dal punto di vista computazionale, i prodotti di punti comportano la moltiplicazione di tutti i componenti corrispondenti e la successiva somma dei risultati, il che è positivo, dato che molti dei nostri calcoli devono assomigliare a somme ponderate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geometricamente, il prodotto del punto è positivo quando i vettori puntano in direzioni simili, è zero se sono perpendicolari ed è negativo quando puntano in direzioni opposte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Ad esempio, supponiamo che tu stia giocando con questo modello e ipotizzi che l'inclusione di gatti meno gatto possa rappresentare una sorta di direzione di pluralità in questo spazio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Per verificarlo, prenderò questo vettore e calcolerò il suo prodotto di punti rispetto agli incorporamenti di alcuni nomi singolari e lo confronterò con i prodotti di punti con i corrispondenti nomi plurali.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Se ci giochi un po', noterai che quelli plurali sembrano dare costantemente valori più alti di quelli singolari, indicando che si allineano maggiormente a questa direzione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "È anche divertente il fatto che se si fa il prodotto dei punti con gli embeddings delle parole 1, 2, 3 e così via, si ottengono valori crescenti, quindi è come se potessimo misurare quantitativamente quanto il modello trovi plurale una determinata parola.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Anche in questo caso, le specifiche del modo in cui le parole vengono incorporate vengono apprese utilizzando i dati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Questa matrice di incorporazione, le cui colonne ci dicono cosa succede a ogni parola, è la prima pila di pesi del nostro modello.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Utilizzando i numeri del GPT-3, la dimensione del vocabolario è di 50.257, e anche in questo caso, tecnicamente non si tratta di parole in sé, ma di token.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "La dimensione di incorporazione è di 12.288, e moltiplicando questi dati si ottiene che si tratta di circa 617 milioni di pesi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Aggiungiamo questi dati a un conteggio continuo, ricordando che alla fine dovremmo arrivare a 175 miliardi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "Nel caso dei trasformatori, è bene pensare che i vettori in questo spazio di incorporazione non rappresentino solo singole parole.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Innanzitutto, codificano anche le informazioni sulla posizione della parola, di cui parleremo più avanti, ma soprattutto è necessario pensare che abbiano la capacità di assorbire il contesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Un vettore che ha iniziato la sua vita come incorporazione della parola re, ad esempio, potrebbe essere progressivamente strattonato e tirato da vari blocchi di questa rete, in modo che alla fine punti in una direzione molto più specifica e sfumata che in qualche modo codifica che si trattava di un re che viveva in Scozia, che aveva ottenuto la sua carica dopo aver assassinato il re precedente e che viene descritto in linguaggio shakespeariano.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Pensa alla tua comprensione di una determinata parola.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Il significato di quella parola è chiaramente influenzato dall'ambiente circostante, che a volte include un contesto a grande distanza. Per questo motivo, nel creare un modello che sia in grado di prevedere quale parola verrà dopo, l'obiettivo è quello di permettergli di incorporare in qualche modo il contesto in modo efficiente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Per essere chiari, in questa prima fase, quando si crea l'array di vettori basati sul testo in ingresso, ognuno di essi viene semplicemente estratto dalla matrice di incorporamento, quindi inizialmente ognuno di essi può codificare solo il significato di una singola parola senza alcun input dall'ambiente circostante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Ma dovresti pensare che l'obiettivo principale di questa rete in cui scorre sia quello di permettere a ognuno di questi vettori di assorbire un significato molto più ricco e specifico di quello che potrebbero rappresentare le singole parole.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "La rete può elaborare solo un numero fisso di vettori alla volta, noto come dimensione del contesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Per GPT-3 è stato addestrato con una dimensione del contesto di 2048, quindi i dati che passano attraverso la rete hanno sempre l'aspetto di questo array di 2048 colonne, ognuna delle quali ha 12.000 dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Questa dimensione del contesto limita la quantità di testo che il trasformatore può incorporare quando fa una previsione della parola successiva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Per questo motivo le conversazioni lunghe con alcuni chatbot, come le prime versioni di ChatGPT, spesso davano la sensazione che il bot perdesse il filo della conversazione quando si continuava troppo a lungo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Entreremo nei dettagli dell'attenzione a tempo debito, ma saltando il discorso voglio parlare per un minuto di ciò che accade alla fine.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Ricorda che l'output desiderato è una distribuzione di probabilità su tutti i token che potrebbero venire dopo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Ad esempio, se l'ultima parola è Professore e il contesto include parole come Harry Potter, e subito prima vediamo l'insegnante meno preferito, e se mi lasciate un po' di margine facendo finta che i token assomiglino semplicemente a parole intere, allora una rete ben addestrata che ha acquisito una conoscenza di Harry Potter presumibilmente assegnerà un numero elevato alla parola Piton.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Questo comporta due diverse fasi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "La prima consiste nell'utilizzare un'altra matrice che mappa l'ultimo vettore di quel contesto in un elenco di 50.000 valori, uno per ogni token del vocabolario.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "C'è poi una funzione che normalizza il tutto in una distribuzione di probabilità, si chiama Softmax e ne parleremo meglio tra un secondo, ma prima potrebbe sembrare un po' strano utilizzare solo quest'ultimo embedding per fare una previsione, quando dopo tutto nell'ultimo passaggio ci sono migliaia di altri vettori nello strato che se ne stanno lì con i loro significati ricchi di contesto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Questo ha a che fare con il fatto che nel processo di addestramento risulta molto più efficiente utilizzare ognuno di questi vettori nello strato finale per fare contemporaneamente una previsione su ciò che avverrà subito dopo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Ci sarà molto altro da dire sull'allenamento in seguito, ma voglio solo ricordarlo adesso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Questa matrice è chiamata matrice di disincarnazione e le diamo l'etichetta WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Anche in questo caso, come tutte le matrici di peso che vediamo, le sue voci iniziano in modo casuale, ma vengono apprese durante il processo di formazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Per restare in tema di parametri totali, questa matrice di disincarnazione ha una riga per ogni parola del vocabolario e ogni riga ha lo stesso numero di elementi della dimensione di incorporazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "È molto simile alla matrice di incorporamento, solo con l'ordine invertito, quindi aggiunge altri 617 milioni di parametri alla rete, il che significa che il nostro conteggio finora è di poco più di un miliardo, una frazione piccola ma non del tutto insignificante dei 175 miliardi che finiremo per avere in totale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "Come ultima mini-lezione di questo capitolo, voglio parlare ancora di questa funzione softmax, dato che farà di nuovo la sua comparsa quando ci immergeremo nei blocchi di attenzione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "L'idea è che se vuoi che una sequenza di numeri agisca come una distribuzione di probabilità, ad esempio una distribuzione su tutte le possibili parole successive, allora ogni valore deve essere compreso tra 0 e 1, e devi anche che tutti i valori si sommino a 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Tuttavia, se stai giocando al gioco dell'apprendimento in cui tutto ciò che fai sembra una moltiplicazione matrice-vettore, i risultati che otterrai per impostazione predefinita non lo rispettano affatto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "I valori sono spesso negativi, o molto più grandi di 1, e quasi sicuramente non sommano a 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax è il modo standard per trasformare un elenco arbitrario di numeri in una distribuzione valida in modo tale che i valori più grandi si avvicinino a 1 e quelli più piccoli a 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Questo è tutto ciò che devi sapere.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Ma se sei curioso, il modo in cui funziona è quello di elevare e alla potenza di ciascuno dei numeri, il che significa che ora hai un elenco di valori positivi, e poi puoi prendere la somma di tutti questi valori positivi e dividere ogni termine per quella somma, che normalizza il tutto in un elenco che somma a 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Noterai che se uno dei numeri in ingresso è significativamente più grande degli altri, nell'output il termine corrispondente domina la distribuzione, quindi se dovessi campionare da esso, quasi certamente sceglieresti solo l'ingresso che massimizza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Ma è più morbido rispetto alla semplice scelta del massimo, nel senso che quando altri valori sono altrettanto grandi, ottengono un peso significativo nella distribuzione, e tutto cambia continuamente al variare degli input.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "In alcune situazioni, come quando ChatGPT utilizza questa distribuzione per creare una parola successiva, c'è spazio per un po' di divertimento in più aggiungendo un po' di pepe a questa funzione, con una costante t inserita nel denominatore degli esponenti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "La chiamiamo temperatura, poiché ricorda vagamente il ruolo della temperatura in alcune equazioni della termodinamica, e l'effetto è che quando t è più grande, si dà più peso ai valori più bassi, il che significa che la distribuzione è un po' più uniforme; se t è più piccolo, allora i valori più grandi domineranno in modo più aggressivo, mentre all'estremo, impostando t uguale a zero, tutto il peso va al valore massimo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Ad esempio, farò in modo che GPT-3 generi una storia con il testo di partenza, C'era una volta A, ma utilizzerò temperature diverse in ogni caso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "La temperatura zero significa che sceglie sempre la parola più prevedibile e ciò che si ottiene finisce per essere un banale derivato di Riccioli d'oro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Una temperatura più alta dà la possibilità di scegliere parole meno probabili, ma comporta un rischio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "In questo caso, la storia inizia in modo più originale, parlando di un giovane web artist della Corea del Sud, ma degenera rapidamente nel nonsense.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Tecnicamente parlando, l'API non ti permette di scegliere una temperatura superiore a 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Non c'è una ragione matematica per questo, è solo un vincolo arbitrario imposto per evitare che il loro strumento sia visto generare cose troppo insensate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Quindi, se sei curioso, il modo in cui funziona questa animazione è che prendo i 20 token successivi più probabili generati da GPT-3, che sembra essere il massimo che mi darà, e poi modifico le probabilità basandomi su un esponente di 1/5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Come ulteriore elemento di gergo, allo stesso modo in cui si possono chiamare probabilità i componenti dell'uscita di questa funzione, spesso ci si riferisce agli ingressi come logiti, o alcuni dicono logiti, altri dicono logiti, io dirò logiti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Quindi, ad esempio, quando inserisci un testo, fai fluire tutte queste incorporazioni di parole attraverso la rete e fai questa moltiplicazione finale con la matrice di non incorporamento, le persone che si occupano di apprendimento automatico si riferiscono ai componenti di questo output grezzo e non normalizzato come ai logit per la previsione della parola successiva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "L'obiettivo di questo capitolo è stato quello di gettare le basi per la comprensione del meccanismo di attenzione, in stile Karate Kid.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Vedi, se hai una forte intuizione per le incorporazioni di parole, per il softmax, per il modo in cui i prodotti di punti misurano la somiglianza, e anche la premessa di fondo che la maggior parte dei calcoli deve assomigliare a una moltiplicazione matriciale con matrici piene di parametri sintonizzabili, allora la comprensione del meccanismo dell'attenzione, questa pietra miliare dell'intero boom moderno dell'IA, dovrebbe essere relativamente semplice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Per questo, unisciti a me nel prossimo capitolo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Mentre sto pubblicando questo articolo, una bozza del prossimo capitolo è disponibile per la revisione da parte dei sostenitori di Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "La versione finale dovrebbe essere resa pubblica tra una o due settimane, di solito dipende da quante cose cambierò in base alla revisione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Nel frattempo, se vuoi immergerti nell'attenzione e se vuoi aiutare un po' il canale, è lì che ti aspetta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]