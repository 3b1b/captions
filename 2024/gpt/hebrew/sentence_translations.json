[
 {
  "translatedText": "ראשי התיבות GPT מייצגים Generative Pretrained Transformer.",
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "translatedText": "אז המילה הראשונה היא פשוטה מספיק, אלו הם בוטים שמייצרים טקסט חדש.",
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "translatedText": "Pretrained מתייחס לאופן שבו המודל עבר תהליך של למידה מכמות עצומה של נתונים, והקידומת מרמזת שיש יותר מקום לכוונן אותו במשימות ספציפיות עם הכשרה נוספת.",
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "translatedText": "אבל המילה האחרונה, זו חתיכת המפתח האמיתית.",
  "input": "But the last word, that's the real key piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "translatedText": "שנאי הוא סוג מסוים של רשת עצבית, מודל למידת מכונה, וזו ההמצאה המרכזית העומדת בבסיס הבום הנוכחי ב-AI.",
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "translatedText": "מה שאני רוצה לעשות עם הסרטון הזה והפרקים הבאים הוא לעבור על הסבר מונע ויזואלי למה שקורה בפועל בתוך שנאי.",
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "translatedText": "אנחנו הולכים לעקוב אחר הנתונים שזורמים בו וללכת צעד אחר צעד.",
  "input": "We're going to follow the data that flows through it and go step by step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "translatedText": "ישנם סוגים רבים ושונים של דגמים שניתן לבנות באמצעות שנאים.",
  "input": "There are many different kinds of models that you can build using transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "translatedText": "דגמים מסוימים קולטים אודיו ומפיקים תמליל.",
  "input": "Some models take in audio and produce a transcript.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "translatedText": "המשפט הזה מגיע ממודל שהולך הפוך, ומייצר דיבור סינתטי רק מטקסט.",
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "translatedText": "כל הכלים האלה שכבשו את העולם בסערה בשנת 2022 כמו דולי ומידג&#39;ורני שמקבלים תיאור טקסט ומייצרים תמונה מבוססים על שנאים.",
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "translatedText": "גם אם אני לא ממש מצליח להבין מה אמור להיות יצור עוגה, אני עדיין המום מכך שדבר כזה אפשרי אפילו במעט.",
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "translatedText": "והשנאי המקורי שהוצג ב-2017 על ידי גוגל הומצא למקרה השימוש הספציפי של תרגום טקסט משפה אחת לשפה אחרת.",
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "translatedText": "אבל הגרסה שבה אתה ואני נתמקד, שהוא הסוג שעומד בבסיס כלים כמו ChatGPT, תהיה מודל שמאומן לקלוט פיסת טקסט, אולי אפילו עם כמה תמונות מסביב או סאונד שמלווים אותה, ולייצר חיזוי למה שיבוא אחר כך בקטע.",
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "translatedText": "התחזית הזו לובשת צורה של התפלגות הסתברות על פני חלקי טקסט רבים ושונים שעשויים להופיע.",
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "translatedText": "במבט ראשון, אולי תחשוב שחיזוי המילה הבאה מרגיש כמו מטרה שונה מאוד מיצירת טקסט חדש.",
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "translatedText": "אבל ברגע שיש לך מודל חיזוי כזה, דבר פשוט שאתה מייצר קטע טקסט ארוך יותר הוא לתת לו קטע טקסט ראשוני לעבוד איתו, לבקש ממנו לקחת דגימה אקראית מההפצה שזה עתה יצר, לצרף את המדגם הזה לטקסט , ולאחר מכן הפעל שוב את כל התהליך כדי לבצע חיזוי חדש המבוסס על כל הטקסט החדש, כולל מה שזה הרגע הוסיף.",
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "translatedText": "אני לא יודע מה איתך, אבל זה ממש לא מרגיש שזה אמור לעבוד.",
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "translatedText": "באנימציה הזו, למשל, אני מריץ את GPT-2 במחשב הנייד שלי ושהוא מנבא שוב ושוב את קטע הטקסט הבא כדי ליצור סיפור המבוסס על טקסט המקור.",
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "translatedText": "הסיפור פשוט לא ממש הגיוני.",
  "input": "The story just doesn't really make that much sense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "translatedText": "אבל אם אני מחליף את זה בקריאות API ל-GPT-3 במקום זאת, שהוא אותו דגם בסיסי, רק הרבה יותר גדול, פתאום כמעט באופן קסום אנחנו מקבלים סיפור הגיוני, כזה שאפילו נראה להסיק שיצור פי יחיה ב ארץ המתמטיקה והחישוב.",
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "translatedText": "התהליך הזה כאן של חיזוי ודגימה חוזרים ונשנים הוא בעצם מה שקורה כשאתה מקיים אינטראקציה עם ChatGPT או עם כל אחד מדגמי השפה הגדולים האלה ואתה רואה אותם מפיקים מילה אחת בכל פעם.",
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "translatedText": "למעשה, תכונה אחת שהייתי מאוד נהנה ממנה היא היכולת לראות את ההתפלגות הבסיסית של כל מילה חדשה שהיא בוחרת.",
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "translatedText": "בואו נתחיל עם תצוגה מקדימה ברמה גבוהה מאוד של האופן שבו הנתונים זורמים דרך שנאי.",
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "translatedText": "נקדיש הרבה יותר זמן למוטיבציה ולפרש ולהרחיב את הפרטים של כל שלב, אבל במילים רחבות, כאשר אחד מהצ&#39;אטבוטים הללו מייצר מילה נתונה, הנה מה שקורה מתחת למכסה המנוע.",
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "translatedText": "ראשית, הקלט מפורק לחבורה של חתיכות קטנות.",
  "input": "First, the input is broken up into a bunch of little pieces.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "translatedText": "חלקים אלה נקראים אסימונים, ובמקרה של טקסט אלה נוטים להיות מילים או חתיכות קטנות של מילים או צירופי תווים נפוצים אחרים.",
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "translatedText": "אם מדובר בתמונות או צליל, אז אסימונים יכולים להיות כתמים קטנים של התמונה הזו או נתחים קטנים של הצליל הזה.",
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "translatedText": "כל אחד מהאסימונים הללו משויך אז לווקטור, כלומר רשימה כלשהי של מספרים, שנועדה איכשהו לקודד את המשמעות של אותו חלק.",
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "translatedText": "אם אתה חושב על הוקטורים האלה כאל נותנים קואורדינטות במרחב ממדי מאוד גבוה, מילים בעלות משמעויות דומות נוטות לנחות על וקטורים שקרובים זה לזה במרחב הזה.",
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "translatedText": "רצף זה של וקטורים עובר לאחר מכן דרך פעולה המכונה בלוק קשב, וזה מאפשר לוקטורים לדבר זה עם זה ולהעביר מידע הלוך ושוב כדי לעדכן את הערכים שלהם.",
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "translatedText": "לדוגמה, המשמעות של המילה מודל בביטוי מודל למידת מכונה שונה ממשמעותה בביטוי מודל אופנה.",
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "translatedText": "חסימת הקשב היא מה שאחראי לגלות אילו מילים בהקשר רלוונטיות לעדכון המשמעויות של אילו מילים אחרות, וכיצד בדיוק יש לעדכן את המשמעויות הללו.",
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "translatedText": "ושוב, בכל פעם שאני משתמש במילה משמעות, זה איכשהו מקודד לחלוטין בערכים של אותם וקטורים.",
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "translatedText": "לאחר מכן, הוקטורים הללו עוברים סוג אחר של פעולה, ובהתאם למקור שאתה קורא זה יקרא תפיסת רב שכבתית או אולי שכבת הזנה קדימה.",
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "translatedText": "וכאן הוקטורים לא מדברים זה עם זה, כולם עוברים את אותה פעולה במקביל.",
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "translatedText": "ולמרות שהגוש הזה קצת יותר קשה לפרש, בהמשך נדבר על איך השלב הוא קצת כמו לשאול רשימה ארוכה של שאלות על כל וקטור, ואז לעדכן אותן על סמך התשובות לשאלות האלה.",
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "translatedText": "כל הפעולות בשני הבלוקים הללו נראות כמו ערימה ענקית של כפל מטריצות, והתפקיד העיקרי שלנו הולך להיות להבין איך לקרוא את המטריצות הבסיסיות.",
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "translatedText": "אני מעלה כמה פרטים על כמה שלבי נורמליזציה שקורים ביניהם, אבל אחרי הכל זו תצוגה מקדימה ברמה גבוהה.",
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "translatedText": "לאחר מכן, התהליך בעצם חוזר על עצמו, אתה עובר הלוך ושוב בין בלוקי קשב ובין בלוקים פרצפטונים רב-שכבתיים, עד שבסופו של דבר התקווה היא שכל המשמעות המהותית של הקטע נאפתה איכשהו לתוך הווקטור האחרון. הרצף.",
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "translatedText": "לאחר מכן אנו מבצעים פעולה מסויימת על אותו וקטור אחרון שמייצרת התפלגות הסתברות על כל האסימונים האפשריים, כל נתחי טקסט קטנים אפשריים שעשויים לבוא אחר כך.",
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "translatedText": "וכמו שאמרתי, ברגע שיש לך כלי שמנבא את מה שבא לאחר מכן בהינתן קטע טקסט, אתה יכול להזין אותו במעט טקסט ראשוני ולגרום לו לשחק שוב ושוב את המשחק הזה של חיזוי מה יבוא אחר כך, דגימה מההפצה, הוספה זה, ואז חוזר שוב ושוב.",
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "translatedText": "כמה מכם היודעים אולי זוכרים כמה זמן לפני ש-ChatGPT נכנס לסצנה, כך נראו הדגמות מוקדמות של GPT-3, הייתם רוצים שהוא ישלים סיפורים ומאמרים על סמך קטע ראשוני.",
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "translatedText": "כדי להפוך כלי כזה לצ&#39;אט בוט, נקודת ההתחלה הקלה ביותר היא לקבל מעט טקסט שיקבע את ההגדרה של משתמש המקיים אינטראקציה עם עוזר בינה מלאכותית מועיל, מה שתקרא להנחיית המערכת, ואז תשתמש ב- השאלה או ההנחיה הראשונית של המשתמש בתור החלק הראשון בדיאלוג, ואז אתה צריך להתחיל לחזות מה עוזר AI כל כך מועיל יגיד בתגובה.",
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "translatedText": "יש עוד מה לומר על שלב של הכשרה שנדרש כדי לגרום לזה לעבוד היטב, אבל ברמה גבוהה זה הרעיון.",
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "translatedText": "בפרק זה, אתה ואני הולכים להרחיב את הפרטים של מה שקורה ממש בתחילת הרשת, ממש בסוף הרשת, ואני גם רוצה להקדיש זמן רב לסקור כמה פיסות חשובות של ידע רקע , דברים שהיו טבע שני לכל מהנדס למידת מכונה עד שהגיעו שנאים.",
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "translatedText": "אם אתה מרגיש בנוח עם ידע הרקע הזה וקצת חסר סבלנות, אתה יכול להרגיש חופשי לדלג לפרק הבא, שעומד להתמקד בחסימות הקשב, הנחשבות בדרך כלל ללב של השנאי.",
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "translatedText": "לאחר מכן אני רוצה לדבר יותר על בלוקי הפרצפטרון הרב-שכבתיים הללו, כיצד האימון עובד, ועוד מספר פרטים שדילג עליהם עד לנקודה זו.",
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "translatedText": "להקשר רחב יותר, הסרטונים האלה הם תוספות למיני-סדרה על למידה עמוקה, וזה בסדר אם לא צפית בקודמים, אני חושב שאתה יכול לעשות את זה בצורה לא מסודרת, אבל לפני שצולל לתוך שנאים ספציפית, אני כן חושב כדאי לוודא שאנחנו באותו עמוד לגבי הנחת היסוד והמבנה של למידה עמוקה.",
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "translatedText": "בסיכון של הצהרה ברורה, זוהי גישה אחת ללמידת מכונה, שמתארת כל מודל שבו אתה משתמש בנתונים כדי לקבוע איכשהו כיצד מודל מתנהג.",
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "translatedText": "מה שאני מתכוון בזה הוא, נניח שאתה רוצה פונקציה שמקבלת תמונה והיא מייצרת תווית המתארת אותה, או הדוגמה שלנו לניבוי המילה הבאה בהינתן קטע טקסט, או כל משימה אחרת שנראית דורשת אלמנט כלשהו של אינטואיציה וזיהוי דפוסים.",
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "translatedText": "אנחנו כמעט לוקחים את זה כמובן מאליו בימינו, אבל הרעיון עם למידת מכונה הוא שבמקום לנסות להגדיר במפורש נוהל כיצד לבצע את המשימה הזו בקוד, וזה מה שאנשים היו עושים בימים הראשונים של AI, במקום זאת הגדר מבנה מאוד גמיש עם פרמטרים ניתנים לשינוי, כמו חבורה של כפתורים וחוגים, ואז איכשהו אתה משתמש בדוגמאות רבות של איך הפלט צריך להיראות עבור קלט נתון כדי לכוונן ולכוון את הערכים של הפרמטרים האלה כדי לחקות את ההתנהגות הזו.",
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "translatedText": "לדוגמה, אולי הצורה הפשוטה ביותר של למידת מכונה היא רגרסיה ליניארית, כאשר התשומות והפלטים שלך הם כל אחד מספרים בודדים, משהו כמו השטח הריבועי של בית והמחיר שלו, ומה שאתה רוצה זה למצוא קו המתאים ביותר באמצעות זה נתונים, אתה יודע, כדי לחזות את מחירי הדירות העתידיים.",
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "translatedText": "הקו הזה מתואר על ידי שני פרמטרים רציפים, נניח השיפוע וחתך ה-y, והמטרה של רגרסיה ליניארית היא לקבוע את הפרמטרים האלה כך שיתאימו היטב לנתונים.",
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "translatedText": "מיותר לציין שמודלים של למידה עמוקה הופכים הרבה יותר מסובכים.",
  "input": "Needless to say, deep learning models get much more complicated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "translatedText": "ל-GPT-3, למשל, אין שניים אלא 175 מיליארד פרמטרים.",
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "translatedText": "אבל זה העניין, זה לא מובן מאליו שאתה יכול ליצור איזה מודל ענק עם מספר עצום של פרמטרים מבלי שהוא יתאים יותר מדי את נתוני האימון או יהיה בלתי נסבל לחלוטין לאימון.",
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "translatedText": "למידה עמוקה מתארת כיתה של מודלים שבעשורים האחרונים הוכיחו כי הם מתקדמים בצורה יוצאת דופן.",
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "translatedText": "מה שמאחד אותם הוא אותו אלגוריתם אימון, הנקרא &#39;התפשטות לאחור&#39;, וההקשר שאני רוצה שיהיה לכם כשנכנסים אליו הוא שכדי שאלגוריתם האימון הזה יעבוד טוב בקנה מידה, המודלים האלה צריכים לפעול לפי פורמט מסוים מסוים.",
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "translatedText": "אם אתה יודע שהפורמט הזה נכנס, זה עוזר להסביר רבות מהאפשרויות לאופן שבו שנאי מעבד שפה, שאחרת מסתכנים בתחושה שרירותית.",
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "translatedText": "ראשית, כל דגם שאתה עושה, הקלט צריך להיות מעוצב כמערך של מספרים אמיתיים.",
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "translatedText": "זה יכול להיות רשימה של מספרים, זה יכול להיות מערך דו מימדי, או לעתים קרובות מאוד אתה עוסק במערכים גבוהים יותר, כאשר המונח הכללי המשמש הוא טנזור.",
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "translatedText": "לעתים קרובות אתה חושב על נתוני הקלט האלה ככאלה שהופכים בהדרגה לשכבות שונות, כאשר שוב, כל שכבה בנויה תמיד כאיזשהו מערך של מספרים ממשיים, עד שאתה מגיע לשכבה הסופית שאתה מחשיב את הפלט.",
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "translatedText": "לדוגמה, השכבה האחרונה במודל עיבוד הטקסט שלנו היא רשימה של מספרים המייצגים את התפלגות ההסתברות עבור כל האסימונים הבאים האפשריים.",
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "translatedText": "בלמידה עמוקה, כמעט תמיד מתייחסים לפרמטרי מודל אלה כמשקלות, וזאת משום שמאפיין מרכזי של מודלים אלה הוא שהדרך היחידה שבה פרמטרים אלה מתקשרים עם הנתונים המעובדים היא באמצעות סכומים משוקללים.",
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "translatedText": "אתה גם מפזר כמה פונקציות לא ליניאריות לאורך, אבל הן לא יהיו תלויות בפרמטרים.",
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "translatedText": "עם זאת, בדרך כלל, במקום לראות את הסכומים המשוקללים כולם חשופים וכתובים בצורה מפורשת כך, במקום זאת תמצאו אותם ארוזים יחד כרכיבים שונים במוצר וקטור מטריצה.",
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "translatedText": "זה מסתכם באמירת אותו הדבר, אם אתה חושב לאחור כיצד פועל כפל וקטור מטריצה, כל רכיב בפלט נראה כמו סכום משוקלל.",
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "translatedText": "זה פשוט לעתים קרובות יותר נקי מבחינה רעיונית עבורך ולי לחשוב על מטריצות שמלאות בפרמטרים ניתנים לשינוי הממיר וקטורים שנמשכים מהנתונים המעובדים.",
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "translatedText": "לדוגמה, אותם 175 מיליארד משקלים ב-GPT-3 מאורגנים בקצת פחות מ-28,000 מטריצות נפרדות.",
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "translatedText": "המטריצות האלה מתחלקות בתורן לשמונה קטגוריות שונות, ומה שאתה ואני הולכים לעשות זה לעבור על כל אחת מהקטגוריות האלה כדי להבין מה הסוג הזה עושה.",
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "translatedText": "בזמן שאנחנו עוברים, אני חושב שזה די כיף להתייחס למספרים הספציפיים מ-GPT-3 כדי לספור בדיוק מאיפה מגיעים ה-175 מיליארד האלה.",
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "translatedText": "גם אם בימינו יש דגמים גדולים וטובים יותר, יש לזה קסם מסוים בתור המודל בשפה הגדולה שבאמת ללכוד את תשומת הלב של העולם מחוץ לקהילות ML.",
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "translatedText": "כמו כן, באופן מעשי, חברות נוטות לשמור על שפתיים הדוקות הרבה יותר סביב המספרים הספציפיים עבור רשתות מודרניות יותר.",
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "translatedText": "אני רק רוצה להגדיר את הסצנה, שכאשר אתה מציץ מתחת למכסה המנוע כדי לראות מה קורה בתוך כלי כמו ChatGPT, כמעט כל החישוב בפועל נראה כמו כפל וקטור מטריצה.",
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "translatedText": "יש סיכון קטן ללכת לאיבוד בים של מיליארדי המספרים, אבל כדאי שתעשה הבחנה חדה מאוד בנפשך בין משקלי הדגם, שאותו תמיד אצבע בכחול או אדום, לבין הנתונים מעובד, שאותו תמיד אצבע באפור.",
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "translatedText": "המשקולות הן המוח האמיתי, הן הדברים שנלמדו במהלך האימון, והן קובעות כיצד הוא מתנהג.",
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "translatedText": "הנתונים המעובדים פשוט מקודדים כל קלט ספציפי שמוזן למודל עבור הפעלה נתונה, כמו קטע טקסט לדוגמה.",
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "translatedText": "עם כל זה כבסיס, בואו נחפור בשלב הראשון של דוגמה זו של עיבוד טקסט, שהוא לפרק את הקלט לנתחים קטנים ולהפוך את הנתחים הללו לוקטורים.",
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "translatedText": "הזכרתי איך הנתחים האלה נקראים אסימונים, שיכולים להיות פיסות מילים או סימני פיסוק, אבל מדי פעם בפרק הזה ובמיוחד בפרק הבא, הייתי רוצה פשוט להעמיד פנים שהוא מפורק בצורה נקייה יותר למילים.",
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "translatedText": "מכיוון שאנו בני האדם חושבים במילים, זה פשוט יקל הרבה יותר להתייחס לדוגמאות קטנות ולהבהיר כל שלב.",
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "translatedText": "למודל יש אוצר מילים מוגדר מראש, רשימה כלשהי של כל המילים האפשריות, נניח 50,000 מהן, והמטריצה הראשונה שנתקל בה, המכונה מטריצת הטבעה, כוללת עמודה אחת לכל אחת מהמילים הללו.",
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "translatedText": "העמודות הללו הן שקובעות לאיזה וקטור הופכת כל מילה בשלב הראשון.",
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "translatedText": "אנחנו מתייגים את זה אנחנו, וכמו כל המטריצות שאנחנו רואים, הערכים שלה מתחילים באקראי, אבל הם הולכים להילמד על סמך נתונים.",
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "translatedText": "הפיכת מילים לוקטורים הייתה נוהג נפוץ בלמידת מכונה הרבה לפני השנאים, אבל זה קצת מוזר אם מעולם לא ראית את זה קודם, וזה קובע את הבסיס לכל מה שאחרי, אז בואו ניקח רגע להכיר את זה.",
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "translatedText": "לעתים קרובות אנו קוראים להטמעה זו מילה, אשר מזמינה אתכם לחשוב על הווקטורים הללו בצורה מאוד גיאומטרית כנקודות במרחב בעל ממדים גבוהים.",
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "translatedText": "הצגת רשימה של שלושה מספרים כקואורדינטות עבור נקודות במרחב תלת-ממד לא תהיה בעיה, אבל הטמעות מילים נוטות להיות ממדיות הרבה יותר גבוהות.",
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "translatedText": "ב-GPT-3 יש להם 12,288 ממדים, וכפי שתראו, חשוב לעבוד בחלל שיש לו הרבה כיוונים ברורים.",
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "translatedText": "באותו אופן שאתה יכול לקחת פרוסה דו מימדית דרך חלל תלת מימד ולהקרין את כל הנקודות על הפרוסה הזו, למען הטבעת מילים הנפשת שמודל פשוט נותן לי, אני הולך לעשות דבר מקביל על ידי בחירת פרוסה תלת מימדית דרך המרחב הגבוה מאוד הזה, והשלכת וקטורי המילה על זה והצגת התוצאות.",
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "translatedText": "הרעיון הגדול כאן הוא שכשמודל מכוונן ומכוון את המשקולות שלו כדי לקבוע איך בדיוק מילים מוטבעות כווקטורים במהלך האימון, הוא נוטה להסתפק בסט של הטבעות שבהן לכיוונים במרחב יש סוג של משמעות סמנטית.",
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "translatedText": "עבור המודל הפשוט של מילה לוקטור שאני מריץ כאן, אם אני אפעיל חיפוש אחר כל המילים שההטבעות שלהן הכי קרובות לזו של מגדל, תבחין איך כולן נותנות אווירה דומות מאוד של מגדל.",
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "translatedText": "ואם אתה רוצה להרים קצת פייתון ולשחק יחד בבית, זה הדגם הספציפי שבו אני משתמש כדי ליצור את האנימציות.",
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "translatedText": "זה לא שנאי, אבל זה מספיק כדי להמחיש את הרעיון שכיוונים במרחב יכולים לשאת משמעות סמנטית.",
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "translatedText": "דוגמה מאוד קלאסית לכך היא איך אם אתה לוקח את ההבדל בין הווקטורים של אישה וגבר, משהו שהיית מדמיין כווקטור קטן המחבר את קצה האחד לקצהו של השני, זה דומה מאוד להבדל בין מלך ל מַלכָּה.",
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "translatedText": "אז נניח שלא ידעת את המילה למלכה נשית, תוכל למצוא אותה על ידי נטילת מלך, הוספת הכיוון הזה של אישה-גבר וחיפוש אחר ההטבעות הקרובות ביותר לאותה נקודה.",
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "translatedText": "לפחות, סוג של.",
  "input": "At least, kind of.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "translatedText": "למרות שזו דוגמה קלאסית למודל שאיתו אני משחק, ההטבעה האמיתית של המלכה היא למעשה קצת יותר רחוקה ממה שזה מרמז, ככל הנראה משום שהדרך שבה משתמשים במלכה בנתוני אימון אינה רק גרסה נשית של מלך.",
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "translatedText": "כששיחקתי, נראה היה שיחסי המשפחה המחישו את הרעיון הרבה יותר טוב.",
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "translatedText": "הנקודה היא, שנראה שבמהלך האימון המודל מצא יתרון לבחור בהטמעות כך שכיוון אחד במרחב הזה מקודד מידע מגדר.",
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "translatedText": "דוגמה נוספת היא שאם לוקחים את ההטבעה של איטליה, ומפחיתים את ההטבעה של גרמניה, ומוסיפים את זה להטבעה של היטלר, מקבלים משהו מאוד קרוב להטבעה של מוסוליני.",
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "translatedText": "זה כאילו המודל למד לקשר כיוונים מסוימים עם סגנון איטלקי ואחרים למנהיגי ציר מלחמת העולם השנייה.",
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "translatedText": "אולי הדוגמה האהובה עליי ברוח זו היא איך בדגמים מסוימים, אם לוקחים את ההבדל בין גרמניה ליפן ומוסיפים אותו לסושי, בסופו של דבר מתקרבים מאוד לברוטוורסט.",
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "translatedText": "גם במשחק הזה של מציאת השכנים הקרובים ביותר, שמחתי לראות עד כמה קאט קרובה גם לחיה וגם למפלצת.",
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "translatedText": "טיפה אחת של אינטואיציה מתמטית שמועיל לזכור, במיוחד עבור הפרק הבא, היא כיצד ניתן לחשוב על מכפלת הנקודה של שני וקטורים כדרך למדוד את מידת התאמתם.",
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "translatedText": "מבחינה חישובית, מוצרי נקודה כוללים הכפלה של כל הרכיבים התואמים ואז הוספת התוצאות, וזה טוב, מכיוון שכל כך הרבה מהחישוב שלנו צריך להיראות כמו סכומים משוקללים.",
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "translatedText": "מבחינה גיאומטרית, מכפלת הנקודה חיובית כאשר וקטורים מצביעים לכיוונים דומים, הוא אפס אם הם מאונכים, והוא שלילי בכל פעם שהם מצביעים בכיוונים מנוגדים.",
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "translatedText": "לדוגמה, נניח ששיחקת עם המודל הזה, ואתה משער שהטבעת חתולים מינוס חתול עשויה לייצג סוג של כיוון ריבוי במרחב הזה.",
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "translatedText": "כדי לבדוק זאת, אני הולך לקחת את הווקטור הזה ולחשב את תוצר הנקודות שלו מול ההטמעות של שמות עצם מסוימים, ולהשוות אותו למוצרי הנקודה עם שמות העצם המקבילים.",
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "translatedText": "אם תשחקו עם זה, תשימו לב שהריבים אכן נותנים באופן עקבי ערכים גבוהים יותר מאלה ביחיד, מה שמצביע על כך שהם מתיישבים יותר עם הכיוון הזה.",
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "translatedText": "זה גם כיף איך אם לוקחים את המוצר הנקודות הזה עם ההטמעות של המילים 1, 2, 3 וכן הלאה, הם נותנים ערכים הולכים וגדלים, אז זה כאילו נוכל למדוד כמותית עד כמה המודל מוצא מילה נתונה.",
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "translatedText": "שוב, הספציפיות לאופן שבו מילים מוטבעות נלמדות באמצעות נתונים.",
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "translatedText": "מטריצת הטבעה הזו, שהעמודות שלה מספרות לנו מה קורה לכל מילה, היא ערימת המשקולות הראשונה במודל שלנו.",
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "translatedText": "באמצעות מספרי GPT-3, גודל אוצר המילים הוא 50,257, ושוב, טכנית זה לא מורכב ממילים כשלעצמן, אלא מאסימונים.",
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "translatedText": "מימד ההטמעה הוא 12,288, וכפל זה אומר לנו שזה מורכב מכ-617 מיליון משקלים.",
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "translatedText": "בוא נמשיך ונוסיף את זה למספר שוטף, ונזכור שעד הסוף אנחנו צריכים לספור עד 175 מיליארד.",
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "translatedText": "במקרה של שנאים, אתה באמת רוצה לחשוב שהווקטורים בחלל ההטמעה הזה אינם מייצגים רק מילים בודדות.",
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "translatedText": "דבר אחד, הם גם מקודדים מידע על המיקום של המילה הזו, שעליו נדבר בהמשך, אבל חשוב מכך, כדאי לחשוב עליהם כבעלי יכולת להשרות בהקשר.",
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "translatedText": "וקטור שהתחיל את חייו כהטמעת המילה מלך, למשל, עלול להימשך ולהימשך בהדרגה על ידי בלוקים שונים ברשת הזו, כך שבסופו הוא מצביע לכיוון הרבה יותר ספציפי וניואנסי שמקודד איכשהו את זה היה מלך שחי בסקוטלנד, ושהשיג את תפקידו לאחר שרצח את המלך הקודם, ושמתואר בשפה שייקספירית.",
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "translatedText": "חשבו על ההבנה שלכם של מילה נתונה.",
  "input": "Think about your own understanding of a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "translatedText": "המשמעות של המילה הזו ניתנת בבירור על ידי הסביבה, ולפעמים זה כולל הקשר ממרחק רב, אז בהרכבת מודל שיש לו את היכולת לחזות איזו מילה מגיעה לאחר מכן, המטרה היא איכשהו להעצים אותו לשלב הקשר ביעילות.",
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "translatedText": "שיהיה ברור, בשלב הראשון הזה, כשאתה יוצר את מערך הוקטורים על סמך טקסט הקלט, כל אחד מהם פשוט נלקח מהמטריצת ההטמעה, כך שבהתחלה כל אחד יכול רק לקודד את המשמעות של מילה אחת בלי כל קלט מסביבתו.",
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "translatedText": "אבל כדאי לחשוב על המטרה העיקרית של הרשת הזו שהיא זורמת דרכה לאפשר לכל אחד מאותם וקטורים לספוג משמעות הרבה יותר עשירה וספציפית ממה שמילים בודדות יכולות לייצג.",
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "translatedText": "הרשת יכולה לעבד רק מספר קבוע של וקטורים בכל פעם, המכונה גודל ההקשר שלה.",
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "translatedText": "עבור GPT-3 הוא הוכשר עם גודל הקשר של 2048, כך שהנתונים הזורמים ברשת נראים תמיד כמו מערך זה של 2048 עמודות, שלכל אחת מהן יש 12,000 ממדים.",
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "translatedText": "גודל ההקשר הזה מגביל את כמות הטקסט שהשנאי יכול לשלב כשהוא מבצע חיזוי של המילה הבאה.",
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "translatedText": "זו הסיבה ששיחות ארוכות עם צ&#39;אטבוטים מסוימים, כמו הגרסאות המוקדמות של ChatGPT, נתנו לעתים קרובות את התחושה שהבוט מאבד את חוט השיחה ככל שהמשכת זמן רב מדי.",
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "translatedText": "אנחנו ניכנס לפרטי תשומת הלב בבוא העת, אבל בדלג קדימה אני רוצה לדבר דקה על מה שקורה ממש בסוף.",
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "translatedText": "זכור, הפלט הרצוי הוא התפלגות הסתברות על כל האסימונים שעשויים להגיע בהמשך.",
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "translatedText": "לדוגמה, אם המילה האחרונה היא פרופסור, וההקשר כולל מילים כמו הארי פוטר, ומיד לפני כן אנו רואים את המורה הפחות אהוב, וגם אם אתה נותן לי קצת מרחב פעולה בכך שאתה נותן לי להעמיד פנים שאסימונים פשוט נראים כמו מילים מלאות, אז יש להניח שרשת מאומנת היטב שבנתה ידע על הארי פוטר תקצה מספר גבוה למילה סנייפ.",
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "translatedText": "זה כרוך בשני שלבים שונים.",
  "input": "This involves two different steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "translatedText": "הראשון הוא להשתמש במטריצה אחרת שממפה את הווקטור האחרון בהקשר זה לרשימה של 50,000 ערכים, אחד לכל אסימון באוצר המילים.",
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "translatedText": "אז יש פונקציה שמנרמלת את זה להתפלגות הסתברות, היא נקראת Softmax ואנחנו נדבר עליה רק בעוד שנייה, אבל לפני זה זה אולי נראה קצת מוזר להשתמש רק בהטבעה האחרונה הזו כדי לבצע חיזוי, כאשר אחרי הכל בשלב האחרון יש אלפי וקטורים אחרים בשכבה שפשוט יושבים שם עם משמעויות עשירות ההקשר שלהם.",
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "translatedText": "זה קשור לעובדה שבתהליך האימון מתברר שזה הרבה יותר יעיל אם תשתמש בכל אחד מאותם וקטורים בשכבה הסופית כדי לבצע בו זמנית חיזוי למה שיבוא מיד אחריו.",
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "translatedText": "יש עוד הרבה מה לומר על אימונים בהמשך, אבל אני רק רוצה לומר זאת עכשיו.",
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "translatedText": "מטריצה זו נקראת מטריצת Unembedding ואנחנו נותנים לה את התווית WU.",
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "translatedText": "שוב, כמו כל מטריצות המשקל שאנו רואים, הערכים שלו מתחילים באקראי, אבל הם נלמדים במהלך תהליך האימון.",
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "translatedText": "תוך שמירה על ציון על ספירת הפרמטרים הכוללת שלנו, למטריצת Unembedding זו יש שורה אחת לכל מילה באוצר המילים, ולכל שורה יש אותו מספר אלמנטים כמו ממד ההטבעה.",
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "translatedText": "זה מאוד דומה למטריצת ההטמעה, רק כשההזמנה הוחלפה, אז זה מוסיף עוד 617 מיליון פרמטרים לרשת, כלומר הספירה שלנו עד כה היא קצת יותר ממיליארד, חלק קטן אך לא זניח לגמרי מ-175 מיליארד שאנחנו. יגמור עם בסך הכל.",
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "translatedText": "בתור המיני-שיעור האחרון לפרק זה, אני רוצה לדבר יותר על פונקציית ה-softmax הזו, מכיוון שהיא מופיעה עבורנו שוב ברגע שאנו צוללים לתוך בלוקי הקשב.",
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "translatedText": "הרעיון הוא שאם אתה רוצה שרצף של מספרים יפעל כהתפלגות הסתברות, נניח התפלגות על כל המילים הבאות האפשריות, אז כל ערך צריך להיות בין 0 ל-1, ואתה גם צריך שכולם יצטרפו ל-1 .",
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "translatedText": "עם זאת, אם אתה משחק במשחק הלמידה שבו כל מה שאתה עושה נראה כמו כפל מטריצה-וקטור, הפלטים שאתה מקבל כברירת מחדל לא עומדים בזה בכלל.",
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "translatedText": "הערכים הם לרוב שליליים, או הרבה יותר גדולים מ-1, והם כמעט בוודאות אינם מסתכמים ב-1.",
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "translatedText": "Softmax היא הדרך הסטנדרטית להפוך רשימה שרירותית של מספרים להתפלגות חוקית באופן שהערכים הגדולים בסופו של דבר מגיעים הקרובים ביותר ל-1, והערכים הקטנים בסופו של דבר קרובים מאוד ל-0.",
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "translatedText": "זה כל מה שאתה באמת צריך לדעת.",
  "input": "That's all you really need to know.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "translatedText": "אבל אם אתה סקרן, הדרך שבה זה עובד היא קודם כל להעלות את e לחזק של כל אחד מהמספרים, מה שאומר שיש לך עכשיו רשימה של ערכים חיוביים, ואז אתה יכול לקחת את הסכום של כל הערכים החיוביים האלה ולחלק כל מונח לפי הסכום הזה, מה שמנרמל אותו לרשימה שמצטברת ל-1.",
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "translatedText": "תבחין שאם אחד המספרים בקלט גדול משמעותית מהשאר, אז בפלט המונח המתאים שולט בהתפלגות, כך שאם היית דוגמת ממנו כמעט בטוח היית בוחר רק את הקלט המקסימלי.",
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "translatedText": "אבל זה רך יותר מסתם לבחור את המקסימום במובן זה שכאשר ערכים אחרים גדולים באופן דומה, הם גם מקבלים משקל משמעותי בהתפלגות, והכל משתנה ללא הרף כאשר אתה משנה ברציפות את התשומות.",
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "translatedText": "במצבים מסוימים, כמו כאשר ChatGPT משתמש בהפצה הזו כדי ליצור מילה הבאה, יש מקום למעט כיף נוסף על ידי הוספת מעט תבלין נוסף לפונקציה הזו, עם t קבוע נזרק למכנה של אותם מעריכים.",
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "translatedText": "אנו קוראים לזה הטמפרטורה, מכיוון שהיא דומה במעורפל לתפקיד הטמפרטורה במשוואות תרמודינמיות מסוימות, וההשפעה היא שכאשר t גדול יותר, אתה נותן יותר משקל לערכים הנמוכים יותר, כלומר ההתפלגות היא קצת יותר אחידה, ואם t קטן יותר, אז הערכים הגדולים יותר ישלטו בצורה אגרסיבית יותר, כאשר בקיצוניות, הגדרת t שווה לאפס פירושה שכל המשקל עובר לערך המקסימלי.",
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "translatedText": "לדוגמה, אני אגרום ל-GPT-3 ליצור סיפור עם טקסט ה-Seed, פעם היה A, אבל אני אשתמש בטמפרטורות שונות בכל מקרה.",
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "translatedText": "טמפרטורה אפס פירושה שזה תמיד הולך עם המילה הכי צפויה, ומה שאתה מקבל בסופו של דבר הוא נגזרת נדושה של זהבה.",
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "translatedText": "טמפרטורה גבוהה יותר נותנת לו הזדמנות לבחור מילים פחות סבירות, אבל זה כרוך בסיכון.",
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "translatedText": "במקרה הזה, הסיפור מתחיל יותר במקור, על אמן אינטרנט צעיר מדרום קוריאה, אבל הוא מידרדר במהירות לשטויות.",
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "translatedText": "מבחינה טכנית, ה-API לא מאפשר לך לבחור טמפרטורה גדולה מ-2.",
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "translatedText": "אין לכך סיבה מתמטית, זה רק אילוץ שרירותי שהוטל כדי למנוע מהכלי שלהם להיראות מייצר דברים שטותיים מדי.",
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "translatedText": "אז אם אתה סקרן, הדרך שבה האנימציה הזו עובדת היא שאני לוקח את 20 האסימונים הבאים הסבירים ביותר ש-GPT-3 מייצר, וזה נראה כמקסימום שהם יתנו לי, ואז אני משנה את ההסתברויות על סמך על מעריך של 1 5.",
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "translatedText": "בתור עוד קצת ז&#39;רגון, באותו אופן שאתה יכול לקרוא לרכיבי הפלט של פונקציה זו הסתברויות, אנשים מתייחסים לעתים קרובות לכניסות כלוגיטים, או שיש אנשים שאומרים לוגיטים, יש אנשים שאומרים לוגיטים, אני הולך לומר לוגיטים. .",
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "translatedText": "אז למשל, כשאתה מזין טקסט כלשהו, כל הטמעות המילים האלה זורמות דרך הרשת, ואתה עושה את הכפל הסופי הזה עם המטריצה של ביטול ההטמעה, אנשי למידת מכונה יתייחסו לרכיבים בפלט הגולמי והלא מנורמל הזה בתור הלוגיטים לתחזית המילה הבאה.",
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "translatedText": "חלק גדול מהמטרה עם הפרק הזה הייתה להניח את היסודות להבנת מנגנון הקשב, סגנון שעווה-על-שעווה של קראטה קיד.",
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "translatedText": "אתה מבין, אם יש לך אינטואיציה חזקה להטמעות מילים, ל-softmax, לאופן שבו מוצרי נקודות מודדים דמיון, וגם את הנחת היסוד שרוב החישובים צריכים להיראות כמו כפל מטריצה עם מטריצות מלאות בפרמטרים הניתנים לכוונון, אז להבין את תשומת הלב מנגנון, חלק אבן הפינה הזה בכל הבום המודרני ב-AI, צריך להיות חלק יחסית.",
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "translatedText": "בשביל זה, בוא הצטרף אליי בפרק הבא.",
  "input": "For that, come join me in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "translatedText": "בזמן שאני מפרסם את זה, טיוטה של הפרק הבא זמינה לסקירה של תומכי Patreon.",
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "translatedText": "גרסה סופית אמורה לעלות לציבור בעוד שבוע או שבועיים, זה בדרך כלל תלוי בכמה בסופו של דבר אשנה בהתבסס על הביקורת הזו.",
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "translatedText": "בינתיים, אם אתה רוצה לצלול לתשומת הלב, ואם אתה רוצה לעזור לערוץ קצת, הוא שם ומחכה.",
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]