1
00:00:00,000 --> 00:00:04,560
ראשי התיבות GPT מייצגים Generative Pretrained Transformer.

2
00:00:05,220 --> 00:00:09,020
אז המילה הראשונה היא פשוטה מספיק, אלו הם בוטים שמייצרים טקסט חדש.

3
00:00:09,800 --> 00:00:15,023
Pretrained מתייחס לאופן שבו המודל עבר תהליך של למידה מכמות עצומה של נתונים, 

4
00:00:15,023 --> 00:00:20,040
והקידומת מרמזת שיש יותר מקום לכוונן אותו במשימות ספציפיות עם הכשרה נוספת.

5
00:00:20,720 --> 00:00:22,900
אבל המילה האחרונה, זו חתיכת המפתח האמיתית.

6
00:00:23,380 --> 00:00:27,266
שנאי הוא סוג מסוים של רשת עצבית, מודל למידת מכונה, 

7
00:00:27,266 --> 00:00:31,000
וזו ההמצאה המרכזית העומדת בבסיס הבום הנוכחי ב-AI.

8
00:00:31,740 --> 00:00:35,359
מה שאני רוצה לעשות עם הסרטון הזה והפרקים הבאים הוא 

9
00:00:35,359 --> 00:00:39,120
לעבור על הסבר מונע ויזואלי למה שקורה בפועל בתוך שנאי.

10
00:00:39,700 --> 00:00:42,820
אנחנו הולכים לעקוב אחר הנתונים שזורמים בו וללכת צעד אחר צעד.

11
00:00:43,440 --> 00:00:47,380
ישנם סוגים רבים ושונים של דגמים שניתן לבנות באמצעות שנאים.

12
00:00:47,800 --> 00:00:50,800
דגמים מסוימים קולטים אודיו ומפיקים תמליל.

13
00:00:51,340 --> 00:00:56,220
המשפט הזה מגיע ממודל שהולך הפוך, ומייצר דיבור סינתטי רק מטקסט.

14
00:00:56,660 --> 00:01:00,647
כל הכלים האלה שכבשו את העולם בסערה בשנת 2022 כמו דולי 

15
00:01:00,647 --> 00:01:05,519
ומידג&#39;ורני שמקבלים תיאור טקסט ומייצרים תמונה מבוססים על שנאים.

16
00:01:06,000 --> 00:01:09,872
גם אם אני לא ממש מצליח להבין מה אמור להיות יצור עוגה, 

17
00:01:09,872 --> 00:01:13,100
אני עדיין המום מכך שדבר כזה אפשרי אפילו במעט.

18
00:01:13,900 --> 00:01:18,124
והשנאי המקורי שהוצג ב-2017 על ידי גוגל הומצא למקרה 

19
00:01:18,124 --> 00:01:22,100
השימוש הספציפי של תרגום טקסט משפה אחת לשפה אחרת.

20
00:01:22,660 --> 00:01:28,318
אבל הגרסה שבה אתה ואני נתמקד, שהוא הסוג שעומד בבסיס כלים כמו ChatGPT, 

21
00:01:28,318 --> 00:01:35,430
תהיה מודל שמאומן לקלוט פיסת טקסט, אולי אפילו עם כמה תמונות מסביב או סאונד שמלווים אותה, 

22
00:01:35,430 --> 00:01:38,260
ולייצר חיזוי למה שיבוא אחר כך בקטע.

23
00:01:38,600 --> 00:01:43,800
התחזית הזו לובשת צורה של התפלגות הסתברות על פני חלקי טקסט רבים ושונים שעשויים להופיע.

24
00:01:45,040 --> 00:01:49,940
במבט ראשון, אולי תחשוב שחיזוי המילה הבאה מרגיש כמו מטרה שונה מאוד מיצירת טקסט חדש.

25
00:01:50,180 --> 00:01:55,037
אבל ברגע שיש לך מודל חיזוי כזה, דבר פשוט שאתה מייצר קטע טקסט ארוך יותר 

26
00:01:55,037 --> 00:02:00,988
הוא לתת לו קטע טקסט ראשוני לעבוד איתו, לבקש ממנו לקחת דגימה אקראית מההפצה שזה עתה יצר, 

27
00:02:00,988 --> 00:02:05,572
לצרף את המדגם הזה לטקסט , ולאחר מכן הפעל שוב את כל התהליך כדי לבצע 

28
00:02:05,572 --> 00:02:09,539
חיזוי חדש המבוסס על כל הטקסט החדש, כולל מה שזה הרגע הוסיף.

29
00:02:10,100 --> 00:02:13,000
אני לא יודע מה איתך, אבל זה ממש לא מרגיש שזה אמור לעבוד.

30
00:02:13,420 --> 00:02:17,990
באנימציה הזו, למשל, אני מריץ את GPT-2 במחשב הנייד שלי ושהוא מנבא 

31
00:02:17,990 --> 00:02:22,420
שוב ושוב את קטע הטקסט הבא כדי ליצור סיפור המבוסס על טקסט המקור.

32
00:02:22,420 --> 00:02:26,120
הסיפור פשוט לא ממש הגיוני.

33
00:02:26,500 --> 00:02:31,839
אבל אם אני מחליף את זה בקריאות API ל-GPT-3 במקום זאת, שהוא אותו דגם בסיסי, 

34
00:02:31,839 --> 00:02:36,679
רק הרבה יותר גדול, פתאום כמעט באופן קסום אנחנו מקבלים סיפור הגיוני, 

35
00:02:36,679 --> 00:02:40,880
כזה שאפילו נראה להסיק שיצור פי יחיה ב ארץ המתמטיקה והחישוב.

36
00:02:41,580 --> 00:02:46,700
התהליך הזה כאן של חיזוי ודגימה חוזרים ונשנים הוא בעצם מה שקורה כשאתה מקיים אינטראקציה 

37
00:02:46,700 --> 00:02:51,880
עם ChatGPT או עם כל אחד מדגמי השפה הגדולים האלה ואתה רואה אותם מפיקים מילה אחת בכל פעם.

38
00:02:52,480 --> 00:02:55,751
למעשה, תכונה אחת שהייתי מאוד נהנה ממנה היא היכולת 

39
00:02:55,751 --> 00:02:59,220
לראות את ההתפלגות הבסיסית של כל מילה חדשה שהיא בוחרת.

40
00:03:03,820 --> 00:03:08,180
בואו נתחיל עם תצוגה מקדימה ברמה גבוהה מאוד של האופן שבו הנתונים זורמים דרך שנאי.

41
00:03:08,640 --> 00:03:13,805
נקדיש הרבה יותר זמן למוטיבציה ולפרש ולהרחיב את הפרטים של כל שלב, אבל במילים רחבות, 

42
00:03:13,805 --> 00:03:18,660
כאשר אחד מהצ&#39;אטבוטים הללו מייצר מילה נתונה, הנה מה שקורה מתחת למכסה המנוע.

43
00:03:19,080 --> 00:03:22,040
ראשית, הקלט מפורק לחבורה של חתיכות קטנות.

44
00:03:22,620 --> 00:03:26,158
חלקים אלה נקראים אסימונים, ובמקרה של טקסט אלה נוטים להיות 

45
00:03:26,158 --> 00:03:29,820
מילים או חתיכות קטנות של מילים או צירופי תווים נפוצים אחרים.

46
00:03:30,740 --> 00:03:33,819
אם מדובר בתמונות או צליל, אז אסימונים יכולים להיות 

47
00:03:33,819 --> 00:03:37,080
כתמים קטנים של התמונה הזו או נתחים קטנים של הצליל הזה.

48
00:03:37,580 --> 00:03:42,399
כל אחד מהאסימונים הללו משויך אז לווקטור, כלומר רשימה כלשהי של מספרים, 

49
00:03:42,399 --> 00:03:45,360
שנועדה איכשהו לקודד את המשמעות של אותו חלק.

50
00:03:45,880 --> 00:03:50,250
אם אתה חושב על הוקטורים האלה כאל נותנים קואורדינטות במרחב ממדי מאוד גבוה, 

51
00:03:50,250 --> 00:03:54,680
מילים בעלות משמעויות דומות נוטות לנחות על וקטורים שקרובים זה לזה במרחב הזה.

52
00:03:55,280 --> 00:03:59,165
רצף זה של וקטורים עובר לאחר מכן דרך פעולה המכונה בלוק קשב, 

53
00:03:59,165 --> 00:04:04,500
וזה מאפשר לוקטורים לדבר זה עם זה ולהעביר מידע הלוך ושוב כדי לעדכן את הערכים שלהם.

54
00:04:04,880 --> 00:04:11,800
לדוגמה, המשמעות של המילה מודל בביטוי מודל למידת מכונה שונה ממשמעותה בביטוי מודל אופנה.

55
00:04:12,260 --> 00:04:16,930
חסימת הקשב היא מה שאחראי לגלות אילו מילים בהקשר רלוונטיות לעדכון 

56
00:04:16,930 --> 00:04:21,959
המשמעויות של אילו מילים אחרות, וכיצד בדיוק יש לעדכן את המשמעויות הללו.

57
00:04:22,500 --> 00:04:28,040
ושוב, בכל פעם שאני משתמש במילה משמעות, זה איכשהו מקודד לחלוטין בערכים של אותם וקטורים.

58
00:04:29,180 --> 00:04:32,863
לאחר מכן, הוקטורים הללו עוברים סוג אחר של פעולה, 

59
00:04:32,863 --> 00:04:38,200
ובהתאם למקור שאתה קורא זה יקרא תפיסת רב שכבתית או אולי שכבת הזנה קדימה.

60
00:04:38,580 --> 00:04:42,660
וכאן הוקטורים לא מדברים זה עם זה, כולם עוברים את אותה פעולה במקביל.

61
00:04:43,060 --> 00:04:48,565
ולמרות שהגוש הזה קצת יותר קשה לפרש, בהמשך נדבר על איך השלב הוא קצת כמו לשאול 

62
00:04:48,565 --> 00:04:54,000
רשימה ארוכה של שאלות על כל וקטור, ואז לעדכן אותן על סמך התשובות לשאלות האלה.

63
00:04:54,900 --> 00:05:00,071
כל הפעולות בשני הבלוקים הללו נראות כמו ערימה ענקית של כפל מטריצות, 

64
00:05:00,071 --> 00:05:05,320
והתפקיד העיקרי שלנו הולך להיות להבין איך לקרוא את המטריצות הבסיסיות.

65
00:05:06,980 --> 00:05:10,505
אני מעלה כמה פרטים על כמה שלבי נורמליזציה שקורים ביניהם, 

66
00:05:10,505 --> 00:05:12,980
אבל אחרי הכל זו תצוגה מקדימה ברמה גבוהה.

67
00:05:13,680 --> 00:05:18,545
לאחר מכן, התהליך בעצם חוזר על עצמו, אתה עובר הלוך ושוב בין בלוקי 

68
00:05:18,545 --> 00:05:23,485
קשב ובין בלוקים פרצפטונים רב-שכבתיים, עד שבסופו של דבר התקווה היא 

69
00:05:23,485 --> 00:05:28,500
שכל המשמעות המהותית של הקטע נאפתה איכשהו לתוך הווקטור האחרון. הרצף.

70
00:05:28,920 --> 00:05:33,827
לאחר מכן אנו מבצעים פעולה מסויימת על אותו וקטור אחרון שמייצרת התפלגות הסתברות 

71
00:05:33,827 --> 00:05:38,420
על כל האסימונים האפשריים, כל נתחי טקסט קטנים אפשריים שעשויים לבוא אחר כך.

72
00:05:38,980 --> 00:05:43,702
וכמו שאמרתי, ברגע שיש לך כלי שמנבא את מה שבא לאחר מכן בהינתן קטע טקסט, 

73
00:05:43,702 --> 00:05:48,357
אתה יכול להזין אותו במעט טקסט ראשוני ולגרום לו לשחק שוב ושוב את המשחק 

74
00:05:48,357 --> 00:05:53,080
הזה של חיזוי מה יבוא אחר כך, דגימה מההפצה, הוספה זה, ואז חוזר שוב ושוב.

75
00:05:53,640 --> 00:05:58,169
כמה מכם היודעים אולי זוכרים כמה זמן לפני ש-ChatGPT נכנס לסצנה, 

76
00:05:58,169 --> 00:06:04,640
כך נראו הדגמות מוקדמות של GPT-3, הייתם רוצים שהוא ישלים סיפורים ומאמרים על סמך קטע ראשוני.

77
00:06:05,580 --> 00:06:10,798
כדי להפוך כלי כזה לצ&#39;אט בוט, נקודת ההתחלה הקלה ביותר היא לקבל מעט טקסט 

78
00:06:10,798 --> 00:06:15,807
שיקבע את ההגדרה של משתמש המקיים אינטראקציה עם עוזר בינה מלאכותית מועיל, 

79
00:06:15,807 --> 00:06:21,234
מה שתקרא להנחיית המערכת, ואז תשתמש ב- השאלה או ההנחיה הראשונית של המשתמש בתור 

80
00:06:21,234 --> 00:06:26,940
החלק הראשון בדיאלוג, ואז אתה צריך להתחיל לחזות מה עוזר AI כל כך מועיל יגיד בתגובה.

81
00:06:27,720 --> 00:06:33,940
יש עוד מה לומר על שלב של הכשרה שנדרש כדי לגרום לזה לעבוד היטב, אבל ברמה גבוהה זה הרעיון.

82
00:06:35,720 --> 00:06:41,537
בפרק זה, אתה ואני הולכים להרחיב את הפרטים של מה שקורה ממש בתחילת הרשת, 

83
00:06:41,537 --> 00:06:47,929
ממש בסוף הרשת, ואני גם רוצה להקדיש זמן רב לסקור כמה פיסות חשובות של ידע רקע , 

84
00:06:47,929 --> 00:06:52,600
דברים שהיו טבע שני לכל מהנדס למידת מכונה עד שהגיעו שנאים.

85
00:06:53,060 --> 00:06:58,959
אם אתה מרגיש בנוח עם ידע הרקע הזה וקצת חסר סבלנות, אתה יכול להרגיש חופשי לדלג לפרק הבא, 

86
00:06:58,959 --> 00:07:02,780
שעומד להתמקד בחסימות הקשב, הנחשבות בדרך כלל ללב של השנאי.

87
00:07:03,360 --> 00:07:07,721
לאחר מכן אני רוצה לדבר יותר על בלוקי הפרצפטרון הרב-שכבתיים הללו, 

88
00:07:07,721 --> 00:07:11,680
כיצד האימון עובד, ועוד מספר פרטים שדילג עליהם עד לנקודה זו.

89
00:07:12,180 --> 00:07:16,423
להקשר רחב יותר, הסרטונים האלה הם תוספות למיני-סדרה על למידה עמוקה, 

90
00:07:16,423 --> 00:07:21,299
וזה בסדר אם לא צפית בקודמים, אני חושב שאתה יכול לעשות את זה בצורה לא מסודרת, 

91
00:07:21,299 --> 00:07:26,746
אבל לפני שצולל לתוך שנאים ספציפית, אני כן חושב כדאי לוודא שאנחנו באותו עמוד לגבי הנחת 

92
00:07:26,746 --> 00:07:28,520
היסוד והמבנה של למידה עמוקה.

93
00:07:29,020 --> 00:07:32,931
בסיכון של הצהרה ברורה, זוהי גישה אחת ללמידת מכונה, 

94
00:07:32,931 --> 00:07:38,300
שמתארת כל מודל שבו אתה משתמש בנתונים כדי לקבוע איכשהו כיצד מודל מתנהג.

95
00:07:39,140 --> 00:07:43,794
מה שאני מתכוון בזה הוא, נניח שאתה רוצה פונקציה שמקבלת תמונה והיא מייצרת 

96
00:07:43,794 --> 00:07:48,254
תווית המתארת אותה, או הדוגמה שלנו לניבוי המילה הבאה בהינתן קטע טקסט, 

97
00:07:48,254 --> 00:07:52,780
או כל משימה אחרת שנראית דורשת אלמנט כלשהו של אינטואיציה וזיהוי דפוסים.

98
00:07:53,200 --> 00:07:58,409
אנחנו כמעט לוקחים את זה כמובן מאליו בימינו, אבל הרעיון עם למידת מכונה הוא שבמקום 

99
00:07:58,409 --> 00:08:01,947
לנסות להגדיר במפורש נוהל כיצד לבצע את המשימה הזו בקוד, 

100
00:08:01,947 --> 00:08:07,028
וזה מה שאנשים היו עושים בימים הראשונים של AI, במקום זאת הגדר מבנה מאוד גמיש עם 

101
00:08:07,028 --> 00:08:10,373
פרמטרים ניתנים לשינוי, כמו חבורה של כפתורים וחוגים, 

102
00:08:10,373 --> 00:08:15,840
ואז איכשהו אתה משתמש בדוגמאות רבות של איך הפלט צריך להיראות עבור קלט נתון כדי לכוונן 

103
00:08:15,840 --> 00:08:19,700
ולכוון את הערכים של הפרמטרים האלה כדי לחקות את ההתנהגות הזו.

104
00:08:19,700 --> 00:08:24,138
לדוגמה, אולי הצורה הפשוטה ביותר של למידת מכונה היא רגרסיה ליניארית, 

105
00:08:24,138 --> 00:08:27,401
כאשר התשומות והפלטים שלך הם כל אחד מספרים בודדים, 

106
00:08:27,401 --> 00:08:33,145
משהו כמו השטח הריבועי של בית והמחיר שלו, ומה שאתה רוצה זה למצוא קו המתאים ביותר באמצעות 

107
00:08:33,145 --> 00:08:36,799
זה נתונים, אתה יודע, כדי לחזות את מחירי הדירות העתידיים.

108
00:08:37,440 --> 00:08:42,264
הקו הזה מתואר על ידי שני פרמטרים רציפים, נניח השיפוע וחתך ה-y, 

109
00:08:42,264 --> 00:08:48,160
והמטרה של רגרסיה ליניארית היא לקבוע את הפרמטרים האלה כך שיתאימו היטב לנתונים.

110
00:08:48,880 --> 00:08:52,100
מיותר לציין שמודלים של למידה עמוקה הופכים הרבה יותר מסובכים.

111
00:08:52,620 --> 00:08:57,660
ל-GPT-3, למשל, אין שניים אלא 175 מיליארד פרמטרים.

112
00:08:58,120 --> 00:09:03,696
אבל זה העניין, זה לא מובן מאליו שאתה יכול ליצור איזה מודל ענק עם מספר עצום של 

113
00:09:03,696 --> 00:09:09,560
פרמטרים מבלי שהוא יתאים יותר מדי את נתוני האימון או יהיה בלתי נסבל לחלוטין לאימון.

114
00:09:10,260 --> 00:09:16,180
למידה עמוקה מתארת כיתה של מודלים שבעשורים האחרונים הוכיחו כי הם מתקדמים בצורה יוצאת דופן.

115
00:09:16,480 --> 00:09:21,413
מה שמאחד אותם הוא אותו אלגוריתם אימון, הנקרא &#39;התפשטות לאחור&#39;, 

116
00:09:21,413 --> 00:09:26,417
וההקשר שאני רוצה שיהיה לכם כשנכנסים אליו הוא שכדי שאלגוריתם האימון הזה 

117
00:09:26,417 --> 00:09:31,280
יעבוד טוב בקנה מידה, המודלים האלה צריכים לפעול לפי פורמט מסוים מסוים.

118
00:09:31,800 --> 00:09:38,175
אם אתה יודע שהפורמט הזה נכנס, זה עוזר להסביר רבות מהאפשרויות לאופן שבו שנאי מעבד שפה, 

119
00:09:38,175 --> 00:09:40,400
שאחרת מסתכנים בתחושה שרירותית.

120
00:09:41,440 --> 00:09:46,740
ראשית, כל דגם שאתה עושה, הקלט צריך להיות מעוצב כמערך של מספרים אמיתיים.

121
00:09:46,740 --> 00:09:50,598
זה יכול להיות רשימה של מספרים, זה יכול להיות מערך דו מימדי, 

122
00:09:50,598 --> 00:09:56,000
או לעתים קרובות מאוד אתה עוסק במערכים גבוהים יותר, כאשר המונח הכללי המשמש הוא טנזור.

123
00:09:56,560 --> 00:10:01,649
לעתים קרובות אתה חושב על נתוני הקלט האלה ככאלה שהופכים בהדרגה לשכבות שונות, 

124
00:10:01,649 --> 00:10:05,666
כאשר שוב, כל שכבה בנויה תמיד כאיזשהו מערך של מספרים ממשיים, 

125
00:10:05,666 --> 00:10:08,680
עד שאתה מגיע לשכבה הסופית שאתה מחשיב את הפלט.

126
00:10:09,280 --> 00:10:13,261
לדוגמה, השכבה האחרונה במודל עיבוד הטקסט שלנו היא רשימה של מספרים 

127
00:10:13,261 --> 00:10:17,060
המייצגים את התפלגות ההסתברות עבור כל האסימונים הבאים האפשריים.

128
00:10:17,820 --> 00:10:21,651
בלמידה עמוקה, כמעט תמיד מתייחסים לפרמטרי מודל אלה כמשקלות, 

129
00:10:21,651 --> 00:10:25,483
וזאת משום שמאפיין מרכזי של מודלים אלה הוא שהדרך היחידה שבה 

130
00:10:25,483 --> 00:10:29,900
פרמטרים אלה מתקשרים עם הנתונים המעובדים היא באמצעות סכומים משוקללים.

131
00:10:30,340 --> 00:10:34,360
אתה גם מפזר כמה פונקציות לא ליניאריות לאורך, אבל הן לא יהיו תלויות בפרמטרים.

132
00:10:35,200 --> 00:10:41,193
עם זאת, בדרך כלל, במקום לראות את הסכומים המשוקללים כולם חשופים וכתובים בצורה מפורשת כך, 

133
00:10:41,193 --> 00:10:45,620
במקום זאת תמצאו אותם ארוזים יחד כרכיבים שונים במוצר וקטור מטריצה.

134
00:10:46,740 --> 00:10:51,856
זה מסתכם באמירת אותו הדבר, אם אתה חושב לאחור כיצד פועל כפל וקטור מטריצה, 

135
00:10:51,856 --> 00:10:54,240
כל רכיב בפלט נראה כמו סכום משוקלל.

136
00:10:54,780 --> 00:11:00,174
זה פשוט לעתים קרובות יותר נקי מבחינה רעיונית עבורך ולי לחשוב על מטריצות 

137
00:11:00,174 --> 00:11:05,420
שמלאות בפרמטרים ניתנים לשינוי הממיר וקטורים שנמשכים מהנתונים המעובדים.

138
00:11:06,340 --> 00:11:14,160
לדוגמה, אותם 175 מיליארד משקלים ב-GPT-3 מאורגנים בקצת פחות מ-28,000 מטריצות נפרדות.

139
00:11:14,660 --> 00:11:17,588
המטריצות האלה מתחלקות בתורן לשמונה קטגוריות שונות, 

140
00:11:17,588 --> 00:11:22,700
ומה שאתה ואני הולכים לעשות זה לעבור על כל אחת מהקטגוריות האלה כדי להבין מה הסוג הזה עושה.

141
00:11:23,160 --> 00:11:26,923
בזמן שאנחנו עוברים, אני חושב שזה די כיף להתייחס למספרים 

142
00:11:26,923 --> 00:11:31,360
הספציפיים מ-GPT-3 כדי לספור בדיוק מאיפה מגיעים ה-175 מיליארד האלה.

143
00:11:31,880 --> 00:11:36,175
גם אם בימינו יש דגמים גדולים וטובים יותר, יש לזה קסם מסוים בתור 

144
00:11:36,175 --> 00:11:40,740
המודל בשפה הגדולה שבאמת ללכוד את תשומת הלב של העולם מחוץ לקהילות ML.

145
00:11:41,440 --> 00:11:44,019
כמו כן, באופן מעשי, חברות נוטות לשמור על שפתיים הדוקות 

146
00:11:44,019 --> 00:11:46,740
הרבה יותר סביב המספרים הספציפיים עבור רשתות מודרניות יותר.

147
00:11:47,360 --> 00:11:52,400
אני רק רוצה להגדיר את הסצנה, שכאשר אתה מציץ מתחת למכסה המנוע כדי לראות מה 

148
00:11:52,400 --> 00:11:57,440
קורה בתוך כלי כמו ChatGPT, כמעט כל החישוב בפועל נראה כמו כפל וקטור מטריצה.

149
00:11:57,900 --> 00:12:01,882
יש סיכון קטן ללכת לאיבוד בים של מיליארדי המספרים, 

150
00:12:01,882 --> 00:12:08,494
אבל כדאי שתעשה הבחנה חדה מאוד בנפשך בין משקלי הדגם, שאותו תמיד אצבע בכחול או אדום, 

151
00:12:08,494 --> 00:12:11,840
לבין הנתונים מעובד, שאותו תמיד אצבע באפור.

152
00:12:12,180 --> 00:12:17,920
המשקולות הן המוח האמיתי, הן הדברים שנלמדו במהלך האימון, והן קובעות כיצד הוא מתנהג.

153
00:12:18,280 --> 00:12:24,751
הנתונים המעובדים פשוט מקודדים כל קלט ספציפי שמוזן למודל עבור הפעלה נתונה, 

154
00:12:24,751 --> 00:12:26,500
כמו קטע טקסט לדוגמה.

155
00:12:27,480 --> 00:12:32,089
עם כל זה כבסיס, בואו נחפור בשלב הראשון של דוגמה זו של עיבוד טקסט, 

156
00:12:32,089 --> 00:12:36,420
שהוא לפרק את הקלט לנתחים קטנים ולהפוך את הנתחים הללו לוקטורים.

157
00:12:37,020 --> 00:12:41,948
הזכרתי איך הנתחים האלה נקראים אסימונים, שיכולים להיות פיסות מילים או סימני פיסוק, 

158
00:12:41,948 --> 00:12:45,675
אבל מדי פעם בפרק הזה ובמיוחד בפרק הבא, הייתי רוצה פשוט להעמיד 

159
00:12:45,675 --> 00:12:48,080
פנים שהוא מפורק בצורה נקייה יותר למילים.

160
00:12:48,600 --> 00:12:51,340
מכיוון שאנו בני האדם חושבים במילים, זה פשוט יקל 

161
00:12:51,340 --> 00:12:54,080
הרבה יותר להתייחס לדוגמאות קטנות ולהבהיר כל שלב.

162
00:12:55,260 --> 00:13:01,492
למודל יש אוצר מילים מוגדר מראש, רשימה כלשהי של כל המילים האפשריות, נניח 50,000 מהן, 

163
00:13:01,492 --> 00:13:07,800
והמטריצה הראשונה שנתקל בה, המכונה מטריצת הטבעה, כוללת עמודה אחת לכל אחת מהמילים הללו.

164
00:13:08,940 --> 00:13:13,760
העמודות הללו הן שקובעות לאיזה וקטור הופכת כל מילה בשלב הראשון.

165
00:13:15,100 --> 00:13:18,608
אנחנו מתייגים את זה אנחנו, וכמו כל המטריצות שאנחנו רואים, 

166
00:13:18,608 --> 00:13:22,360
הערכים שלה מתחילים באקראי, אבל הם הולכים להילמד על סמך נתונים.

167
00:13:23,620 --> 00:13:28,364
הפיכת מילים לוקטורים הייתה נוהג נפוץ בלמידת מכונה הרבה לפני השנאים, 

168
00:13:28,364 --> 00:13:33,736
אבל זה קצת מוזר אם מעולם לא ראית את זה קודם, וזה קובע את הבסיס לכל מה שאחרי, 

169
00:13:33,736 --> 00:13:35,760
אז בואו ניקח רגע להכיר את זה.

170
00:13:36,040 --> 00:13:39,772
לעתים קרובות אנו קוראים להטמעה זו מילה, אשר מזמינה אתכם לחשוב על 

171
00:13:39,772 --> 00:13:43,620
הווקטורים הללו בצורה מאוד גיאומטרית כנקודות במרחב בעל ממדים גבוהים.

172
00:13:44,180 --> 00:13:48,751
הצגת רשימה של שלושה מספרים כקואורדינטות עבור נקודות במרחב תלת-ממד לא תהיה בעיה, 

173
00:13:48,751 --> 00:13:51,780
אבל הטמעות מילים נוטות להיות ממדיות הרבה יותר גבוהות.

174
00:13:52,280 --> 00:14:00,440
ב-GPT-3 יש להם 12,288 ממדים, וכפי שתראו, חשוב לעבוד בחלל שיש לו הרבה כיוונים ברורים.

175
00:14:01,180 --> 00:14:07,415
באותו אופן שאתה יכול לקחת פרוסה דו מימדית דרך חלל תלת מימד ולהקרין את כל הנקודות על 

176
00:14:07,415 --> 00:14:11,498
הפרוסה הזו, למען הטבעת מילים הנפשת שמודל פשוט נותן לי, 

177
00:14:11,498 --> 00:14:17,510
אני הולך לעשות דבר מקביל על ידי בחירת פרוסה תלת מימדית דרך המרחב הגבוה מאוד הזה, 

178
00:14:17,510 --> 00:14:20,480
והשלכת וקטורי המילה על זה והצגת התוצאות.

179
00:14:21,280 --> 00:14:25,620
הרעיון הגדול כאן הוא שכשמודל מכוונן ומכוון את המשקולות שלו כדי 

180
00:14:25,620 --> 00:14:29,341
לקבוע איך בדיוק מילים מוטבעות כווקטורים במהלך האימון, 

181
00:14:29,341 --> 00:14:34,440
הוא נוטה להסתפק בסט של הטבעות שבהן לכיוונים במרחב יש סוג של משמעות סמנטית.

182
00:14:34,980 --> 00:14:38,071
עבור המודל הפשוט של מילה לוקטור שאני מריץ כאן, 

183
00:14:38,071 --> 00:14:42,742
אם אני אפעיל חיפוש אחר כל המילים שההטבעות שלהן הכי קרובות לזו של מגדל, 

184
00:14:42,742 --> 00:14:45,900
תבחין איך כולן נותנות אווירה דומות מאוד של מגדל.

185
00:14:46,340 --> 00:14:48,834
ואם אתה רוצה להרים קצת פייתון ולשחק יחד בבית, זה 

186
00:14:48,834 --> 00:14:51,380
הדגם הספציפי שבו אני משתמש כדי ליצור את האנימציות.

187
00:14:51,620 --> 00:14:57,600
זה לא שנאי, אבל זה מספיק כדי להמחיש את הרעיון שכיוונים במרחב יכולים לשאת משמעות סמנטית.

188
00:14:58,300 --> 00:15:04,875
דוגמה מאוד קלאסית לכך היא איך אם אתה לוקח את ההבדל בין הווקטורים של אישה וגבר, 

189
00:15:04,875 --> 00:15:10,120
משהו שהיית מדמיין כווקטור קטן המחבר את קצה האחד לקצהו של השני, 

190
00:15:10,120 --> 00:15:13,200
זה דומה מאוד להבדל בין מלך ל מַלכָּה.

191
00:15:15,080 --> 00:15:20,198
אז נניח שלא ידעת את המילה למלכה נשית, תוכל למצוא אותה על ידי נטילת מלך, 

192
00:15:20,198 --> 00:15:25,460
הוספת הכיוון הזה של אישה-גבר וחיפוש אחר ההטבעות הקרובות ביותר לאותה נקודה.

193
00:15:27,000 --> 00:15:28,200
לפחות, סוג של.

194
00:15:28,480 --> 00:15:32,469
למרות שזו דוגמה קלאסית למודל שאיתו אני משחק, ההטבעה האמיתית 

195
00:15:32,469 --> 00:15:35,660
של המלכה היא למעשה קצת יותר רחוקה ממה שזה מרמז, 

196
00:15:35,660 --> 00:15:40,780
ככל הנראה משום שהדרך שבה משתמשים במלכה בנתוני אימון אינה רק גרסה נשית של מלך.

197
00:15:41,620 --> 00:15:45,260
כששיחקתי, נראה היה שיחסי המשפחה המחישו את הרעיון הרבה יותר טוב.

198
00:15:46,340 --> 00:15:50,368
הנקודה היא, שנראה שבמהלך האימון המודל מצא יתרון 

199
00:15:50,368 --> 00:15:54,900
לבחור בהטמעות כך שכיוון אחד במרחב הזה מקודד מידע מגדר.

200
00:15:56,800 --> 00:16:02,631
דוגמה נוספת היא שאם לוקחים את ההטבעה של איטליה, ומפחיתים את ההטבעה של גרמניה, 

201
00:16:02,631 --> 00:16:08,090
ומוסיפים את זה להטבעה של היטלר, מקבלים משהו מאוד קרוב להטבעה של מוסוליני.

202
00:16:08,570 --> 00:16:12,271
זה כאילו המודל למד לקשר כיוונים מסוימים עם סגנון 

203
00:16:12,271 --> 00:16:15,670
איטלקי ואחרים למנהיגי ציר מלחמת העולם השנייה.

204
00:16:16,470 --> 00:16:20,138
אולי הדוגמה האהובה עליי ברוח זו היא איך בדגמים מסוימים, 

205
00:16:20,138 --> 00:16:23,806
אם לוקחים את ההבדל בין גרמניה ליפן ומוסיפים אותו לסושי, 

206
00:16:23,806 --> 00:16:26,230
בסופו של דבר מתקרבים מאוד לברוטוורסט.

207
00:16:27,350 --> 00:16:30,882
גם במשחק הזה של מציאת השכנים הקרובים ביותר, שמחתי 

208
00:16:30,882 --> 00:16:33,850
לראות עד כמה קאט קרובה גם לחיה וגם למפלצת.

209
00:16:34,690 --> 00:16:38,917
טיפה אחת של אינטואיציה מתמטית שמועיל לזכור, במיוחד עבור הפרק הבא, 

210
00:16:38,917 --> 00:16:43,850
היא כיצד ניתן לחשוב על מכפלת הנקודה של שני וקטורים כדרך למדוד את מידת התאמתם.

211
00:16:44,870 --> 00:16:49,842
מבחינה חישובית, מוצרי נקודה כוללים הכפלה של כל הרכיבים התואמים ואז הוספת התוצאות, 

212
00:16:49,842 --> 00:16:54,330
וזה טוב, מכיוון שכל כך הרבה מהחישוב שלנו צריך להיראות כמו סכומים משוקללים.

213
00:16:55,190 --> 00:17:00,542
מבחינה גיאומטרית, מכפלת הנקודה חיובית כאשר וקטורים מצביעים לכיוונים דומים, 

214
00:17:00,542 --> 00:17:05,609
הוא אפס אם הם מאונכים, והוא שלילי בכל פעם שהם מצביעים בכיוונים מנוגדים.

215
00:17:06,550 --> 00:17:12,065
לדוגמה, נניח ששיחקת עם המודל הזה, ואתה משער שהטבעת חתולים 

216
00:17:12,065 --> 00:17:17,010
מינוס חתול עשויה לייצג סוג של כיוון ריבוי במרחב הזה.

217
00:17:17,430 --> 00:17:22,113
כדי לבדוק זאת, אני הולך לקחת את הווקטור הזה ולחשב את תוצר הנקודות שלו מול 

218
00:17:22,113 --> 00:17:27,050
ההטמעות של שמות עצם מסוימים, ולהשוות אותו למוצרי הנקודה עם שמות העצם המקבילים.

219
00:17:27,270 --> 00:17:32,894
אם תשחקו עם זה, תשימו לב שהריבים אכן נותנים באופן עקבי ערכים גבוהים יותר מאלה ביחיד, 

220
00:17:32,894 --> 00:17:36,070
מה שמצביע על כך שהם מתיישבים יותר עם הכיוון הזה.

221
00:17:37,070 --> 00:17:42,843
זה גם כיף איך אם לוקחים את המוצר הנקודות הזה עם ההטמעות של המילים 1, 2, 3 וכן הלאה, 

222
00:17:42,843 --> 00:17:49,030
הם נותנים ערכים הולכים וגדלים, אז זה כאילו נוכל למדוד כמותית עד כמה המודל מוצא מילה נתונה.

223
00:17:50,250 --> 00:17:53,570
שוב, הספציפיות לאופן שבו מילים מוטבעות נלמדות באמצעות נתונים.

224
00:17:54,050 --> 00:17:57,417
מטריצת הטבעה הזו, שהעמודות שלה מספרות לנו מה קורה לכל מילה, 

225
00:17:57,417 --> 00:17:59,550
היא ערימת המשקולות הראשונה במודל שלנו.

226
00:18:00,030 --> 00:18:04,712
באמצעות מספרי GPT-3, גודל אוצר המילים הוא 50,257, 

227
00:18:04,712 --> 00:18:09,770
ושוב, טכנית זה לא מורכב ממילים כשלעצמן, אלא מאסימונים.

228
00:18:10,630 --> 00:18:17,790
מימד ההטמעה הוא 12,288, וכפל זה אומר לנו שזה מורכב מכ-617 מיליון משקלים.

229
00:18:18,250 --> 00:18:23,810
בוא נמשיך ונוסיף את זה למספר שוטף, ונזכור שעד הסוף אנחנו צריכים לספור עד 175 מיליארד.

230
00:18:25,430 --> 00:18:28,852
במקרה של שנאים, אתה באמת רוצה לחשוב שהווקטורים 

231
00:18:28,852 --> 00:18:32,130
בחלל ההטמעה הזה אינם מייצגים רק מילים בודדות.

232
00:18:32,550 --> 00:18:38,227
דבר אחד, הם גם מקודדים מידע על המיקום של המילה הזו, שעליו נדבר בהמשך, 

233
00:18:38,227 --> 00:18:42,770
אבל חשוב מכך, כדאי לחשוב עליהם כבעלי יכולת להשרות בהקשר.

234
00:18:43,350 --> 00:18:50,341
וקטור שהתחיל את חייו כהטמעת המילה מלך, למשל, עלול להימשך ולהימשך בהדרגה על ידי בלוקים 

235
00:18:50,341 --> 00:18:57,332
שונים ברשת הזו, כך שבסופו הוא מצביע לכיוון הרבה יותר ספציפי וניואנסי שמקודד איכשהו את 

236
00:18:57,332 --> 00:19:02,778
זה היה מלך שחי בסקוטלנד, ושהשיג את תפקידו לאחר שרצח את המלך הקודם, 

237
00:19:02,778 --> 00:19:04,730
ושמתואר בשפה שייקספירית.

238
00:19:05,210 --> 00:19:07,790
חשבו על ההבנה שלכם של מילה נתונה.

239
00:19:08,250 --> 00:19:12,134
המשמעות של המילה הזו ניתנת בבירור על ידי הסביבה, 

240
00:19:12,134 --> 00:19:17,127
ולפעמים זה כולל הקשר ממרחק רב, אז בהרכבת מודל שיש לו את היכולת 

241
00:19:17,127 --> 00:19:23,390
לחזות איזו מילה מגיעה לאחר מכן, המטרה היא איכשהו להעצים אותו לשלב הקשר ביעילות.

242
00:19:24,050 --> 00:19:29,151
שיהיה ברור, בשלב הראשון הזה, כשאתה יוצר את מערך הוקטורים על סמך טקסט הקלט, 

243
00:19:29,151 --> 00:19:33,300
כל אחד מהם פשוט נלקח מהמטריצת ההטמעה, כך שבהתחלה כל אחד יכול 

244
00:19:33,300 --> 00:19:36,770
רק לקודד את המשמעות של מילה אחת בלי כל קלט מסביבתו.

245
00:19:37,710 --> 00:19:43,447
אבל כדאי לחשוב על המטרה העיקרית של הרשת הזו שהיא זורמת דרכה לאפשר לכל אחד מאותם 

246
00:19:43,447 --> 00:19:48,970
וקטורים לספוג משמעות הרבה יותר עשירה וספציפית ממה שמילים בודדות יכולות לייצג.

247
00:19:49,510 --> 00:19:54,170
הרשת יכולה לעבד רק מספר קבוע של וקטורים בכל פעם, המכונה גודל ההקשר שלה.

248
00:19:54,510 --> 00:19:59,760
עבור GPT-3 הוא הוכשר עם גודל הקשר של 2048, כך שהנתונים הזורמים ברשת 

249
00:19:59,760 --> 00:20:05,010
נראים תמיד כמו מערך זה של 2048 עמודות, שלכל אחת מהן יש 12,000 ממדים.

250
00:20:05,590 --> 00:20:11,830
גודל ההקשר הזה מגביל את כמות הטקסט שהשנאי יכול לשלב כשהוא מבצע חיזוי של המילה הבאה.

251
00:20:12,370 --> 00:20:17,458
זו הסיבה ששיחות ארוכות עם צ&#39;אטבוטים מסוימים, כמו הגרסאות המוקדמות של ChatGPT, 

252
00:20:17,458 --> 00:20:22,050
נתנו לעתים קרובות את התחושה שהבוט מאבד את חוט השיחה ככל שהמשכת זמן רב מדי.

253
00:20:23,030 --> 00:20:25,982
אנחנו ניכנס לפרטי תשומת הלב בבוא העת, אבל בדלג 

254
00:20:25,982 --> 00:20:28,810
קדימה אני רוצה לדבר דקה על מה שקורה ממש בסוף.

255
00:20:29,450 --> 00:20:34,870
זכור, הפלט הרצוי הוא התפלגות הסתברות על כל האסימונים שעשויים להגיע בהמשך.

256
00:20:35,170 --> 00:20:40,142
לדוגמה, אם המילה האחרונה היא פרופסור, וההקשר כולל מילים כמו הארי פוטר, 

257
00:20:40,142 --> 00:20:45,184
ומיד לפני כן אנו רואים את המורה הפחות אהוב, וגם אם אתה נותן לי קצת מרחב 

258
00:20:45,184 --> 00:20:50,297
פעולה בכך שאתה נותן לי להעמיד פנים שאסימונים פשוט נראים כמו מילים מלאות, 

259
00:20:50,297 --> 00:20:55,830
אז יש להניח שרשת מאומנת היטב שבנתה ידע על הארי פוטר תקצה מספר גבוה למילה סנייפ.

260
00:20:56,510 --> 00:20:57,970
זה כרוך בשני שלבים שונים.

261
00:20:58,310 --> 00:21:05,365
הראשון הוא להשתמש במטריצה אחרת שממפה את הווקטור האחרון בהקשר זה לרשימה של 50,000 ערכים, 

262
00:21:05,365 --> 00:21:07,610
אחד לכל אסימון באוצר המילים.

263
00:21:08,170 --> 00:21:13,364
אז יש פונקציה שמנרמלת את זה להתפלגות הסתברות, היא נקראת Softmax ואנחנו 

264
00:21:13,364 --> 00:21:18,412
נדבר עליה רק בעוד שנייה, אבל לפני זה זה אולי נראה קצת מוזר להשתמש רק 

265
00:21:18,412 --> 00:21:23,461
בהטבעה האחרונה הזו כדי לבצע חיזוי, כאשר אחרי הכל בשלב האחרון יש אלפי 

266
00:21:23,461 --> 00:21:28,290
וקטורים אחרים בשכבה שפשוט יושבים שם עם משמעויות עשירות ההקשר שלהם.

267
00:21:28,930 --> 00:21:34,639
זה קשור לעובדה שבתהליך האימון מתברר שזה הרבה יותר יעיל אם תשתמש בכל אחד 

268
00:21:34,639 --> 00:21:40,270
מאותם וקטורים בשכבה הסופית כדי לבצע בו זמנית חיזוי למה שיבוא מיד אחריו.

269
00:21:40,970 --> 00:21:45,090
יש עוד הרבה מה לומר על אימונים בהמשך, אבל אני רק רוצה לומר זאת עכשיו.

270
00:21:45,730 --> 00:21:49,690
מטריצה זו נקראת מטריצת Unembedding ואנחנו נותנים לה את התווית WU.

271
00:21:50,210 --> 00:21:53,990
שוב, כמו כל מטריצות המשקל שאנו רואים, הערכים שלו מתחילים באקראי, 

272
00:21:53,990 --> 00:21:55,910
אבל הם נלמדים במהלך תהליך האימון.

273
00:21:56,470 --> 00:21:59,410
תוך שמירה על ציון על ספירת הפרמטרים הכוללת שלנו, 

274
00:21:59,410 --> 00:22:02,890
למטריצת Unembedding זו יש שורה אחת לכל מילה באוצר המילים, 

275
00:22:02,890 --> 00:22:05,650
ולכל שורה יש אותו מספר אלמנטים כמו ממד ההטבעה.

276
00:22:06,410 --> 00:22:10,159
זה מאוד דומה למטריצת ההטמעה, רק כשההזמנה הוחלפה, 

277
00:22:10,159 --> 00:22:16,892
אז זה מוסיף עוד 617 מיליון פרמטרים לרשת, כלומר הספירה שלנו עד כה היא קצת יותר ממיליארד, 

278
00:22:16,892 --> 00:22:21,790
חלק קטן אך לא זניח לגמרי מ-175 מיליארד שאנחנו. יגמור עם בסך הכל.

279
00:22:22,550 --> 00:22:27,034
בתור המיני-שיעור האחרון לפרק זה, אני רוצה לדבר יותר על פונקציית ה-softmax הזו, 

280
00:22:27,034 --> 00:22:30,610
מכיוון שהיא מופיעה עבורנו שוב ברגע שאנו צוללים לתוך בלוקי הקשב.

281
00:22:31,430 --> 00:22:36,315
הרעיון הוא שאם אתה רוצה שרצף של מספרים יפעל כהתפלגות הסתברות, 

282
00:22:36,315 --> 00:22:42,147
נניח התפלגות על כל המילים הבאות האפשריות, אז כל ערך צריך להיות בין 0 ל-1, 

283
00:22:42,147 --> 00:22:44,590
ואתה גם צריך שכולם יצטרפו ל-1 .

284
00:22:45,250 --> 00:22:51,225
עם זאת, אם אתה משחק במשחק הלמידה שבו כל מה שאתה עושה נראה כמו כפל מטריצה-וקטור, 

285
00:22:51,225 --> 00:22:54,810
הפלטים שאתה מקבל כברירת מחדל לא עומדים בזה בכלל.

286
00:22:55,330 --> 00:22:59,870
הערכים הם לרוב שליליים, או הרבה יותר גדולים מ-1, והם כמעט בוודאות אינם מסתכמים ב-1.

287
00:23:00,510 --> 00:23:05,778
Softmax היא הדרך הסטנדרטית להפוך רשימה שרירותית של מספרים להתפלגות חוקית באופן שהערכים 

288
00:23:05,778 --> 00:23:08,625
הגדולים בסופו של דבר מגיעים הקרובים ביותר ל-1, 

289
00:23:08,625 --> 00:23:11,290
והערכים הקטנים בסופו של דבר קרובים מאוד ל-0.

290
00:23:11,830 --> 00:23:13,070
זה כל מה שאתה באמת צריך לדעת.

291
00:23:13,090 --> 00:23:18,616
אבל אם אתה סקרן, הדרך שבה זה עובד היא קודם כל להעלות את e לחזק של כל אחד מהמספרים, 

292
00:23:18,616 --> 00:23:23,876
מה שאומר שיש לך עכשיו רשימה של ערכים חיוביים, ואז אתה יכול לקחת את הסכום של כל 

293
00:23:23,876 --> 00:23:29,470
הערכים החיוביים האלה ולחלק כל מונח לפי הסכום הזה, מה שמנרמל אותו לרשימה שמצטברת ל-1.

294
00:23:30,170 --> 00:23:37,198
תבחין שאם אחד המספרים בקלט גדול משמעותית מהשאר, אז בפלט המונח המתאים שולט בהתפלגות, 

295
00:23:37,198 --> 00:23:42,470
כך שאם היית דוגמת ממנו כמעט בטוח היית בוחר רק את הקלט המקסימלי.

296
00:23:42,990 --> 00:23:48,684
אבל זה רך יותר מסתם לבחור את המקסימום במובן זה שכאשר ערכים אחרים גדולים באופן דומה, 

297
00:23:48,684 --> 00:23:54,650
הם גם מקבלים משקל משמעותי בהתפלגות, והכל משתנה ללא הרף כאשר אתה משנה ברציפות את התשומות.

298
00:23:55,130 --> 00:24:00,770
במצבים מסוימים, כמו כאשר ChatGPT משתמש בהפצה הזו כדי ליצור מילה הבאה, 

299
00:24:00,770 --> 00:24:05,928
יש מקום למעט כיף נוסף על ידי הוספת מעט תבלין נוסף לפונקציה הזו, 

300
00:24:05,928 --> 00:24:08,910
עם t קבוע נזרק למכנה של אותם מעריכים.

301
00:24:09,550 --> 00:24:15,633
אנו קוראים לזה הטמפרטורה, מכיוון שהיא דומה במעורפל לתפקיד הטמפרטורה במשוואות תרמודינמיות 

302
00:24:15,633 --> 00:24:21,101
מסוימות, וההשפעה היא שכאשר t גדול יותר, אתה נותן יותר משקל לערכים הנמוכים יותר, 

303
00:24:21,101 --> 00:24:24,587
כלומר ההתפלגות היא קצת יותר אחידה, ואם t קטן יותר, 

304
00:24:24,587 --> 00:24:29,098
אז הערכים הגדולים יותר ישלטו בצורה אגרסיבית יותר, כאשר בקיצוניות, 

305
00:24:29,098 --> 00:24:32,790
הגדרת t שווה לאפס פירושה שכל המשקל עובר לערך המקסימלי.

306
00:24:33,470 --> 00:24:38,345
לדוגמה, אני אגרום ל-GPT-3 ליצור סיפור עם טקסט ה-Seed, 

307
00:24:38,345 --> 00:24:42,950
פעם היה A, אבל אני אשתמש בטמפרטורות שונות בכל מקרה.

308
00:24:43,630 --> 00:24:48,124
טמפרטורה אפס פירושה שזה תמיד הולך עם המילה הכי צפויה, 

309
00:24:48,124 --> 00:24:52,370
ומה שאתה מקבל בסופו של דבר הוא נגזרת נדושה של זהבה.

310
00:24:53,010 --> 00:24:57,910
טמפרטורה גבוהה יותר נותנת לו הזדמנות לבחור מילים פחות סבירות, אבל זה כרוך בסיכון.

311
00:24:58,230 --> 00:25:03,622
במקרה הזה, הסיפור מתחיל יותר במקור, על אמן אינטרנט צעיר מדרום קוריאה, 

312
00:25:03,622 --> 00:25:06,010
אבל הוא מידרדר במהירות לשטויות.

313
00:25:06,950 --> 00:25:10,830
מבחינה טכנית, ה-API לא מאפשר לך לבחור טמפרטורה גדולה מ-2.

314
00:25:11,170 --> 00:25:15,341
אין לכך סיבה מתמטית, זה רק אילוץ שרירותי שהוטל כדי 

315
00:25:15,341 --> 00:25:19,350
למנוע מהכלי שלהם להיראות מייצר דברים שטותיים מדי.

316
00:25:19,870 --> 00:25:24,282
אז אם אתה סקרן, הדרך שבה האנימציה הזו עובדת היא שאני לוקח את 20 

317
00:25:24,282 --> 00:25:29,522
האסימונים הבאים הסבירים ביותר ש-GPT-3 מייצר, וזה נראה כמקסימום שהם יתנו לי, 

318
00:25:29,522 --> 00:25:32,970
ואז אני משנה את ההסתברויות על סמך על מעריך של 1 5.

319
00:25:33,130 --> 00:25:38,519
בתור עוד קצת ז&#39;רגון, באותו אופן שאתה יכול לקרוא לרכיבי הפלט של פונקציה זו הסתברויות, 

320
00:25:38,519 --> 00:25:43,122
אנשים מתייחסים לעתים קרובות לכניסות כלוגיטים, או שיש אנשים שאומרים לוגיטים, 

321
00:25:43,122 --> 00:25:46,150
יש אנשים שאומרים לוגיטים, אני הולך לומר לוגיטים. .

322
00:25:46,530 --> 00:25:51,325
אז למשל, כשאתה מזין טקסט כלשהו, כל הטמעות המילים האלה זורמות דרך הרשת, 

323
00:25:51,325 --> 00:25:55,108
ואתה עושה את הכפל הסופי הזה עם המטריצה של ביטול ההטמעה, 

324
00:25:55,108 --> 00:26:00,174
אנשי למידת מכונה יתייחסו לרכיבים בפלט הגולמי והלא מנורמל הזה בתור הלוגיטים 

325
00:26:00,174 --> 00:26:01,390
לתחזית המילה הבאה.

326
00:26:03,330 --> 00:26:08,068
חלק גדול מהמטרה עם הפרק הזה הייתה להניח את היסודות להבנת מנגנון הקשב, 

327
00:26:08,068 --> 00:26:10,370
סגנון שעווה-על-שעווה של קראטה קיד.

328
00:26:10,850 --> 00:26:15,266
אתה מבין, אם יש לך אינטואיציה חזקה להטמעות מילים, ל-softmax, 

329
00:26:15,266 --> 00:26:20,769
לאופן שבו מוצרי נקודות מודדים דמיון, וגם את הנחת היסוד שרוב החישובים צריכים 

330
00:26:20,769 --> 00:26:25,548
להיראות כמו כפל מטריצה עם מטריצות מלאות בפרמטרים הניתנים לכוונון, 

331
00:26:25,548 --> 00:26:30,689
אז להבין את תשומת הלב מנגנון, חלק אבן הפינה הזה בכל הבום המודרני ב-AI, 

332
00:26:30,689 --> 00:26:32,210
צריך להיות חלק יחסית.

333
00:26:32,650 --> 00:26:34,510
בשביל זה, בוא הצטרף אליי בפרק הבא.

334
00:26:36,390 --> 00:26:41,210
בזמן שאני מפרסם את זה, טיוטה של הפרק הבא זמינה לסקירה של תומכי Patreon.

335
00:26:41,770 --> 00:26:44,324
גרסה סופית אמורה לעלות לציבור בעוד שבוע או שבועיים, 

336
00:26:44,324 --> 00:26:47,370
זה בדרך כלל תלוי בכמה בסופו של דבר אשנה בהתבסס על הביקורת הזו.

337
00:26:47,810 --> 00:26:52,410
בינתיים, אם אתה רוצה לצלול לתשומת הלב, ואם אתה רוצה לעזור לערוץ קצת, הוא שם ומחכה.

