1
00:00:00,000 --> 00:00:04,560
ראשי התיבות GPT מייצגים באנגלית טרנספורמר מאומן מראש בעל יכולת יצירה.

2
00:00:05,220 --> 00:00:09,020
"בעל יכולת יצירה" מסביר שאלו הם בוטים שמייצרים טקסט חדש.

3
00:00:09,800 --> 00:00:15,464
"מאומן מראש" מתייחס לאופן שבו המודל עבר תהליך של למידה מכמות עצומה של נתונים, 

4
00:00:15,464 --> 00:00:20,040
ומרמז שיש עוד מקום לכוונן אותו במשימות ספציפיות עם הכשרה נוספת.

5
00:00:20,720 --> 00:00:22,900
אבל טרנספורמר היא מילת המפתח האמיתית.

6
00:00:23,380 --> 00:00:27,443
טרנספורמר הוא סוג מסוים של רשת עצבית, מודל למידת מכונה, 

7
00:00:27,443 --> 00:00:31,000
וזו ההמצאה המרכזית העומדת בבסיס הבום הנוכחי ב-AI.

8
00:00:31,740 --> 00:00:35,359
מה שאני רוצה לעשות עם הסרטון הזה והפרקים הבאים הוא 

9
00:00:35,359 --> 00:00:39,120
לעבור על הסבר ויזואלי למה שקורה בפועל בתוך טרנספורמר.

10
00:00:39,700 --> 00:00:42,820
אנחנו הולכים לעקוב צעד צעד אחר הנתונים שעוברים בו.

11
00:00:43,440 --> 00:00:47,380
ישנם סוגים רבים ושונים של מודלים שניתן לבנות באמצעות טרנספורמרים.

12
00:00:47,800 --> 00:00:50,800
מודלים מסוימים קולטים אודיו ומפיקים תמליל.

13
00:00:51,340 --> 00:00:56,220
המשפט הזה מגיע ממודל שפועל הפוך, ומייצר דיבור סינתטי רק מטקסט.

14
00:00:56,660 --> 00:01:01,441
כל הכלים האלה שכבשו את העולם בסערה בשנת 2022 כמו Dolly ו-Midjourney 

15
00:01:01,441 --> 00:01:05,519
שמקבלים תיאור בטקסט ומייצרים תמונה מבוססים על טרנספורמרים.

16
00:01:06,000 --> 00:01:10,180
גם אם אני לא ממש מצליח לגרום לו להבין מה אמור להיות חייית פאי, 

17
00:01:10,180 --> 00:01:13,100
אני עדיין המום מכך שדבר כזה אפשרי אפילו קצת.

18
00:01:13,900 --> 00:01:18,083
והטרנספורמר המקורי שהוצג ב-2017 על ידי גוגל הומצא 

19
00:01:18,083 --> 00:01:22,100
לשימוש הספציפי של תרגום טקסט משפה אחת לשפה אחרת.

20
00:01:22,660 --> 00:01:28,829
אבל הגרסה שבה אתם ואני נתמקד, שהוא הסוג שעומד בבסיס כלים כמו ChatGPT, 

21
00:01:28,829 --> 00:01:36,321
יהיה מודל שמאומן לקלוט קטע טקסט, אולי אפילו עם כמה תמונות מסביב או קול שמלווים אותו, 

22
00:01:36,321 --> 00:01:38,260
ולחזות מה יבוא אחר כך.

23
00:01:38,600 --> 00:01:43,800
התחזית הזו היא בצורה של התפלגות הסתברותית על פני חלקי טקסט רבים ושונים שעשויים להופיע.

24
00:01:45,040 --> 00:01:49,940
במבט ראשון, אולי תחשבו שחיזוי המילה הבאה נראה כמו מטרה שונה מאוד מיצירת טקסט חדש.

25
00:01:50,180 --> 00:01:54,933
אבל ברגע שיש לכם מודל חיזוי כזה, דבר פשוט שאתם יכולים לנסות זה לגרום 

26
00:01:54,933 --> 00:01:59,687
לו לייצר קטע טקסט ארוך יותר. זאת על ידי  בקשה ממנו לקחת דגימה אקראית 

27
00:01:59,687 --> 00:02:03,545
מההתפלגות שיצר לגבי קטע הטקסט הראשוני ולצרף אותה לטקסט. 

28
00:02:03,545 --> 00:02:09,539
לאחר מכן להפעיל שוב את כל התהליך כדי לבצע חיזוי חדש המבוסס על הטקסט שכולל את מה שהוסיף.

29
00:02:10,100 --> 00:02:13,000
אני לא יודע מה איתכם, אבל זה ממש לא מרגיש שזה אמור לעבוד.

30
00:02:13,420 --> 00:02:17,748
באנימציה הזו, למשל, אני מריץ את GPT-2 במחשב הנייד שלי וגורם לו 

31
00:02:17,748 --> 00:02:22,420
לנבא שוב ושוב את קטע הטקסט הבא כדי ליצור סיפור המבוסס על טקסט המקור.

32
00:02:22,420 --> 00:02:26,120
הסיפור פשוט לא ממש הגיוני.

33
00:02:26,500 --> 00:02:31,939
אבל אם אני מחליף ל-GPT-3, שבאופן בסיסי הוא אותו מודל, רק הרבה יותר גדול, 

34
00:02:31,939 --> 00:02:35,589
פתאום כמעט באופן קסום אנחנו מקבלים סיפור הגיוני, 

35
00:02:35,589 --> 00:02:40,880
כזה שאפילו נראה שמשתמע ממנו שיצור פאי חי וקיים בארץ המתמטיקה והחישובים.

36
00:02:41,580 --> 00:02:46,730
התהליך כאן של חיזוי ודגימה חוזרים ונשנים הוא בעצם מה שקורה כשאתם מקיימים אינטראקציה עם 

37
00:02:46,730 --> 00:02:51,880
ChatGPT או עם כל אחד ממודלי השפה הגדולים האלה ושאתם רואים אותם מפיקים מילה אחת בכל פעם.

38
00:02:52,480 --> 00:02:55,816
למעשה, תכונה אחת שהייתי מאוד נהנה ממנה היא היכולת 

39
00:02:55,816 --> 00:02:59,220
לראות את ההתפלגות הבסיסית לכל מילה חדשה שהיא בוחרת.

40
00:03:03,820 --> 00:03:08,180
בואו נתחיל עם תצוגה מקדימה ברמה גבוהה מאוד של האופן שבו הנתונים זורמים דרך טרנספורמר.

41
00:03:08,640 --> 00:03:13,716
נקדיש בהמשך הרבה יותר זמן לפרש ולהרחיב את הפרטים של כל שלב, אבל באופן כללי, 

42
00:03:13,716 --> 00:03:18,660
כאשר אחד מהצ'אטבוטים הללו מייצר מילה נתונה, הנה מה שקורה בקרביים של המודל.

43
00:03:19,080 --> 00:03:22,040
ראשית, הקלט מפורק לחתיכות קטנות.

44
00:03:22,620 --> 00:03:26,454
חלקים אלה נקראים טוקנים, ובמקרה של טקסט הם מילים 

45
00:03:26,454 --> 00:03:29,820
או חלקי מילים או צירופי תווים נפוצים אחרים.

46
00:03:30,740 --> 00:03:37,080
אם מדובר בתמונה או בצליל, אז טוקנים יכולים להיות חלקים קטנים של התמונה או של הצליל.

47
00:03:37,580 --> 00:03:42,346
כל אחד מהטוקנים הללו משויך אז לווקטור, כלומר רשימה כלשהי של מספרים, 

48
00:03:42,346 --> 00:03:45,360
שנועדה לקודד איכשהו את המשמעות של אותו חלק.

49
00:03:45,880 --> 00:03:49,901
אם אתם חושבים על הוקטורים האלה כקואורדינטות במרחב עם מימד מאוד גבוה, 

50
00:03:49,901 --> 00:03:54,680
מילים בעלות משמעויות דומות נוטות להיות משוייכות לוקטורים שקרובים זה לזה במרחב הזה.

51
00:03:55,280 --> 00:04:00,023
רצף זה של וקטורים עובר לאחר מכן דרך פעולה המכונה בלוק מיקוד (attention 

52
00:04:00,023 --> 00:04:04,500
block) שמאפשר לוקטורים להעביר מידע ביניהם כדי לעדכן את הערכים שלהם.

53
00:04:04,880 --> 00:04:11,800
לדוגמה, המשמעות של המילה מודל בביטוי מודל למידת מכונה שונה ממשמעותה בביטוי מודל אופנה.

54
00:04:12,260 --> 00:04:16,854
בלוק המיקוד הוא האחראי לגלות אילו מילים בהקשר רלוונטיות לעדכון 

55
00:04:16,854 --> 00:04:21,959
המשמעויות של אילו מילים אחרות, וכיצד בדיוק יש לעדכן את המשמעויות הללו.

56
00:04:22,500 --> 00:04:28,040
ושוב, בכל פעם שאני משתמש במילה משמעות, היא איכשהו מקודדת באופן מלא בערכים של אותם וקטורים.

57
00:04:29,180 --> 00:04:32,292
לאחר מכן, הוקטורים הללו עוברים סוג אחר של פעולה, 

58
00:04:32,292 --> 00:04:36,866
ובהתאם למקור שאתם קוראים היא תיקרא פרספטרון רב שכבתי או אולי שכבת זרימה 

59
00:04:36,866 --> 00:04:38,200
קדימה (feed-forward).

60
00:04:38,580 --> 00:04:42,660
וכאן הוקטורים לא מתקשרים ביניהם, כולם עוברים את אותה פעולה במקביל.

61
00:04:43,060 --> 00:04:48,286
ולמרות שהבלוק הזה קצת יותר קשה לפרוש, בהמשך נדבר על איך שלב זה הוא קצת כמו 

62
00:04:48,286 --> 00:04:54,000
לשאול רשימה ארוכה של שאלות על כל וקטור, ואז לעדכן אותן על סמך התשובות לשאלות האלה.

63
00:04:54,900 --> 00:05:00,033
כל הפעולות בשני הבלוקים הללו נראות כמו ערימה ענקית של כפל מטריצות, 

64
00:05:00,033 --> 00:05:05,320
והתפקיד העיקרי שלנו הולך להיות איך להבין ולקרוא את המטריצות הבסיסיות.

65
00:05:06,980 --> 00:05:10,786
אני פוסח כמה פרטים לגבי כמה שלבי נורמליזציה שקורים ביניהם, 

66
00:05:10,786 --> 00:05:12,980
אבל אחרי הכל זו הצגה מקדימה כללית.

67
00:05:13,680 --> 00:05:18,547
לאחר מכן, התהליך בעצם חוזר על עצמו, אתם עוברים הלוך ושוב בין בלוקי 

68
00:05:18,547 --> 00:05:23,414
מיקוד ובין בלוקים של פרספטונים רב-שכבתיים, עד שבסופו של דבר התקווה 

69
00:05:23,414 --> 00:05:28,500
היא שכל המשמעות המהותית של הקטע נמצאת איכשהו בתוך הווקטור האחרון ברצף.

70
00:05:28,920 --> 00:05:33,637
לאחר מכן אנחנו מבצעים פעולה מסויימת על אותו וקטור אחרון שמייצרת התפלגות 

71
00:05:33,637 --> 00:05:38,420
הסתברותית לכל הטוקנים האפשריים, כל חלקי הטקסט הקטנים שעשויים לבוא אחר כך.

72
00:05:38,980 --> 00:05:43,930
וכמו שאמרתי, ברגע שיש לכם כלי שמנבא את מה שבא לאחר קטע טקסט נתון, 

73
00:05:43,930 --> 00:05:49,855
אתם יכולים להזין אותו בקטע טקסט ראשוני ולגרום לו לנבא שוב ושוב מה יבוא אחר כך: 

74
00:05:49,855 --> 00:05:53,080
דגימה מההתפלגות, הוספתו לטקסט, וחוזר חלילה.

75
00:05:53,640 --> 00:05:58,118
כמה מכם אולי זוכרים שכמה זמן לפני ש-ChatGPT נכנס לתמונה, 

76
00:05:58,118 --> 00:06:04,640
כך נראו הדגמות מוקדמות של GPT-3. גרמתם לו להשלים סיפורים ומאמרים על סמך קטע ראשוני.

77
00:06:05,580 --> 00:06:10,844
כדי להפוך כלי כזה לצ'אטבוט, נקודת ההתחלה הקלה ביותר היא לקבל מעט טקסט 

78
00:06:10,844 --> 00:06:16,560
שיקבע את ההגדרה של משתמש המקיים אינטראקציה עם הכלי, מה שנקרא הנחיית המערכת, 

79
00:06:16,560 --> 00:06:21,976
ואז תשתמשו בשאלה או ההנחיה הראשונית של המשתמש בתור החלק הראשון בדיאלוג, 

80
00:06:21,976 --> 00:06:26,940
ואז אתם צריכים להתחיל לחזות מה כלי בינה מלאכותית יעיל יענה בתגובה.

81
00:06:27,720 --> 00:06:32,285
יש עוד מה לומר על שלב נוסף של האימון שנדרש כדי לגרום לזה לעבוד היטב, 

82
00:06:32,285 --> 00:06:33,940
אבל זה הרעיון באופן כללי.

83
00:06:35,720 --> 00:06:41,481
בפרק זה, אתם ואני הולכים להרחיב את הפרטים של מה שקורה ממש בתחילת הרשת, 

84
00:06:41,481 --> 00:06:47,487
ממש בסוף הרשת, ואני גם רוצה להקדיש זמן רב לסקור כמה פיסות ידע רקע חשובות, 

85
00:06:47,487 --> 00:06:52,600
דברים שהיו טבע שני לכל מהנדס למידת מכונה עד שהגיעו טרנספורמרים.

86
00:06:53,060 --> 00:06:58,381
אם אתם מרגישים נוח עם ידע הרקע הזה, אתם יכולים להרגיש חופשי לדלג לפרק הבא, 

87
00:06:58,381 --> 00:07:02,780
שעומד להתמקד בבלוקי מיקוד, הנחשבים בדרך כלל ללב של הטרנספורמר.

88
00:07:03,360 --> 00:07:07,652
לאחר מכן אני רוצה לדבר יותר על בלוקי הפרספטרון הרב-שכבתיים הללו, 

89
00:07:07,652 --> 00:07:11,680
כיצד האימון עובד, ועוד מספר פרטים שדילגתי עליהם עד לנקודה זו.

90
00:07:12,180 --> 00:07:17,178
בהקשר רחב יותר, הסרטונים האלה הם תוספות למיני-סדרה על למידה עמוקה, 

91
00:07:17,178 --> 00:07:22,327
וזה בסדר אם לא צפיתם בקודמים, אבל לפני שאני צולל לעומק לטרנספורמרים, 

92
00:07:22,327 --> 00:07:28,520
אני כן חושב שכדאי לוודא שיש לנו הבנה משותפת לגבי הנחות היסוד והמבנה של למידה עמוקה.

93
00:07:29,020 --> 00:07:33,057
בסיכון של הצהרת הברור מאליו, זוהי גישה אחת ללמידת מכונה, 

94
00:07:33,057 --> 00:07:38,300
שמתארת מודל כלשהו שבו אתם משתמש בנתונים כדי לקבוע איכשהו כיצד המודל מתנהג.

95
00:07:39,140 --> 00:07:44,948
מה שאני מתכוון בזה הוא, נניח שאתם רוצים פונקציה שמקבלת תמונה ושמייצרת תווית המתארת אותה, 

96
00:07:44,948 --> 00:07:48,211
או הדוגמה שלנו לחיזוי המילה הבאה בהינתן קטע טקסט, 

97
00:07:48,211 --> 00:07:52,780
או כל משימה אחרת שנראה שדורשת אלמנט כלשהו של אינטואיציה וזיהוי דפוסים.

98
00:07:53,200 --> 00:07:58,320
אנחנו כמעט לוקחים את זה כמובן מאליו בימינו, אבל הרעיון עם למידת מכונה הוא 

99
00:07:58,320 --> 00:08:04,408
שבמקום לנסות להגדיר במפורש נוהל כיצד לבצע את המשימה הזו בקוד, כמו בימים הראשונים של AI, 

100
00:08:04,408 --> 00:08:10,013
אתם מגדירים מבנה מאוד גמיש עם פרמטרים ניתנים לשינוי, כמו אוסף של כפתורים וחוגות, 

101
00:08:10,013 --> 00:08:15,271
ואז איכשהו אתם משתמשים בדוגמאות רבות של איך הפלט צריך להיראות עבור קלט נתון 

102
00:08:15,271 --> 00:08:19,700
כדי לכוונן את הערכים של הפרמטרים האלה כדי לחקות את ההתנהגות הזו.

103
00:08:19,700 --> 00:08:24,733
לדוגמה, אולי הצורה הפשוטה ביותר של למידת מכונה היא רגרסיה ליניארית, 

104
00:08:24,733 --> 00:08:30,507
כאשר כל אחד הקלטים והפלטים הם מספרים בודדים, משהו כמו השטח של בית והמחיר שלו, 

105
00:08:30,507 --> 00:08:36,799
ומה שאתם רוצים זה למצוא את הקו המתאים ביותר שעובר בנתונים אלו, כדי לחזות מחירי דירות.

106
00:08:37,440 --> 00:08:42,347
הקו הזה מתואר על ידי שני פרמטרים רציפים, למשל השיפוע והעתקת ה-y, 

107
00:08:42,347 --> 00:08:48,160
והמטרה של רגרסיה ליניארית היא לקבוע את הפרמטרים האלה כך שיתאימו היטב לנתונים.

108
00:08:48,880 --> 00:08:52,100
מיותר לציין שמודלים של למידה עמוקה הרבה יותר מסובכים.

109
00:08:52,620 --> 00:08:57,660
ל-GPT-3, למשל, אין שניים אלא 175 מיליארד פרמטרים.

110
00:08:58,120 --> 00:09:03,878
אבל זה העניין, זה לא מובן מאליו שאתם יכולים ליצור מודל ענק עם מספר עצום של 

111
00:09:03,878 --> 00:09:09,560
פרמטרים מבלי שהוא יתאים יותר מדי את נתוני האימון או יהיה בלתי ניתן לאימון.

112
00:09:10,260 --> 00:09:13,408
למידה עמוקה מתארת סוג של מודלים שבעשורים האחרונים 

113
00:09:13,408 --> 00:09:16,180
הוכיחו כי הם ניתנים להרחבה בצורה יוצאת דופן.

114
00:09:16,480 --> 00:09:21,836
מה שמאחד אותם הוא אותו אלגוריתם אימון, הנקרא פעפוע לאחור (backpropagation), 

115
00:09:21,836 --> 00:09:26,628
וההקשר שאני רוצה שיהיה לכם כשנעמיק בו הוא שכדי שאלגוריתם האימון הזה 

116
00:09:26,628 --> 00:09:31,280
יעבוד טוב בכל קנה מידה, המודלים האלה צריכים לפעול לפי פורמט מסוים.

117
00:09:31,800 --> 00:09:38,092
אם אתם מכירים את הפורמט הזה, זה עוזר להסביר רבות מהאפשרויות לאופן שבו טרנספורמר מעבד שפה. 

118
00:09:38,092 --> 00:09:40,400
אחרת מסתכנים בתחושה של שרירותיות.

119
00:09:41,440 --> 00:09:46,740
ראשית, בכל מודל שאתם בונים, הקלט צריך להיות בפורמט של מערך של מספרים ממשיים.

120
00:09:46,740 --> 00:09:49,848
הוא יכול להיות רשימה של מספרים, מערך דו מימדי, 

121
00:09:49,848 --> 00:09:53,685
או לעתים קרובות מאוד אתם עוסקים במערכים בדרגה גבוהה יותר, 

122
00:09:53,685 --> 00:09:56,000
כאשר המונח הכללי שבשימוש הוא טנזור.

123
00:09:56,560 --> 00:10:02,551
לעתים קרובות אתם חושבים על נתוני הקלט האלה ככאלה שהופכים בהדרגה לשכבות שונות, כאשר שוב, 

124
00:10:02,551 --> 00:10:08,680
כל שכבה בנויה תמיד כאיזשהו מערך של מספרים ממשיים, עד שאתם מגיעים לשכבה הסופית שנחשבת כפלט.

125
00:10:09,280 --> 00:10:13,325
לדוגמה, השכבה האחרונה במודל עיבוד הטקסט שלנו היא רשימה של מספרים 

126
00:10:13,325 --> 00:10:17,060
המייצגים את התפלגות ההסתברות עבור כל הטוקנים הבאים האפשריים.

127
00:10:17,820 --> 00:10:21,889
בלמידה עמוקה, כמעט תמיד מתייחסים לפרמטרי מודל האלו כאל משקלים, 

128
00:10:21,889 --> 00:10:25,701
וזאת משום שמאפיין מרכזי של מודלים אלה הוא שהדרך היחידה שבה 

129
00:10:25,701 --> 00:10:29,900
הפרמטרים מתקשרים עם הנתונים המעובדים היא באמצעות סכומים משוקללים.

130
00:10:30,340 --> 00:10:34,360
יהיו גם כמה פונקציות לא ליניאריות לאורך הדרך, אבל הן לא יהיו תלויות בפרמטרים.

131
00:10:35,200 --> 00:10:41,523
עם זאת, בדרך כלל, במקום לראות את הסכומים המשוקללים כולם חשופים וכתובים בצורה מפורשת כך, 

132
00:10:41,523 --> 00:10:45,620
תמצאו אותם ארוזים יחד כרכיבים שונים במכפלת וקטור ומטריצה.

133
00:10:46,740 --> 00:10:51,788
זה מסתכם באמירת אותו הדבר, אם אתם חושבים כיצד פועל כפל וקטור ומטריצה, 

134
00:10:51,788 --> 00:10:54,240
כל רכיב בפלט נראה כמו סכום משוקלל.

135
00:10:54,780 --> 00:11:00,026
פשוט, לעתים קרובות יותר נקי מבחינה רעיונית עבורכם ולי לחשוב על מטריצות 

136
00:11:00,026 --> 00:11:05,420
שמלאות בפרמטרים שניתנים לשינוי ושממירות וקטורים שנוצרו מהנתונים המעובדים.

137
00:11:06,340 --> 00:11:14,160
לדוגמה, אותם 175 מיליארד משקלים ב-GPT-3 מאורגנים בקצת פחות מ-28,000 מטריצות נפרדות.

138
00:11:14,660 --> 00:11:18,561
המטריצות האלה מתחלקות לשמונה קטגוריות שונות, ומה שאתם ואני הולכים 

139
00:11:18,561 --> 00:11:22,700
לעשות זה לעבור על כל אחת מהקטגוריות האלה כדי להבין מה כל אחת מהן עושה.

140
00:11:23,160 --> 00:11:26,960
בזמן שאנחנו עוברים, אני חושב שזה די מהנה להתייחס למספרים 

141
00:11:26,960 --> 00:11:31,360
הספציפיים מ-GPT-3 כדי לספור בדיוק מאיפה מגיעים ה-175 מיליארד האלה.

142
00:11:31,880 --> 00:11:36,310
גם אם בימינו יש מודלים גדולים וטובים יותר, ל-GPT-3 יש קסם מסוים בתור 

143
00:11:36,310 --> 00:11:40,740
מודל השפה הגדול (ML) שבאמת לכד את תשומת הלב של העולם מחוץ לקהילות ML.

144
00:11:41,440 --> 00:11:43,990
כמו כן, באופן מעשי, חברות נוטות לשמור הרבה יותר על 

145
00:11:43,990 --> 00:11:46,740
סודיות סביב המספרים הספציפיים עבור רשתות מודרניות יותר.

146
00:11:47,360 --> 00:11:52,433
אני רק רוצה להגדיר את הזירה, שכאשר אתם מציצים בקרביים של המודל כדי לראות מה 

147
00:11:52,433 --> 00:11:57,440
קורה בתוך כלי כמו ChatGPT, כמעט כל החישוב בפועל נראה כמו כפל וקטור ומטריצה.

148
00:11:57,900 --> 00:12:01,928
יש סיכון קטן ללכת לאיבוד בים של מיליארדי המספרים, 

149
00:12:01,928 --> 00:12:08,213
אבל כדאי שתעשו הבחנה חדה מאוד בין משקלי המודל, שאותם תמיד אצבע בכחול או אדום, 

150
00:12:08,213 --> 00:12:11,840
לבין הנתונים המעובדים, שאותם תמיד אצבע באפור.

151
00:12:12,180 --> 00:12:17,920
המשקלים הם המוח האמיתי, הם הדברים שנלמדו במהלך האימון, והם קובעים כיצד המודל מתנהג.

152
00:12:18,280 --> 00:12:24,751
הנתונים המעובדים פשוט מקודדים כל קלט ספציפי שמוזן למודל עבור הפעלה נתונה, 

153
00:12:24,751 --> 00:12:26,500
כמו קטע טקסט לדוגמה.

154
00:12:27,480 --> 00:12:32,089
עם כל זה כבסיס, בואו נעמיק בשלב הראשון של דוגמה זו של עיבוד טקסט, 

155
00:12:32,089 --> 00:12:36,420
שהוא לפרק את הקלט לנתחים קטנים ולהפוך את הנתחים הללו לוקטורים.

156
00:12:37,020 --> 00:12:42,382
הזכרתי איך הנתחים האלה נקראים טוקנים, שיכולים להיות פיסות מילים או סימני פיסוק, 

157
00:12:42,382 --> 00:12:48,080
אבל מדי פעם בפרק הזה ובמיוחד בפרק הבא, הייתי רוצה פשוט להעמיד פנים שהוא מפורק למילים.

158
00:12:48,600 --> 00:12:51,340
מכיוון שאנחנו בני האדם חושבים במילים, זה פשוט 

159
00:12:51,340 --> 00:12:54,080
יקל על התייחסות לדוגמאות קטנות ולהבהרת כל שלב.

160
00:12:55,260 --> 00:12:59,901
למודל יש אוצר מילים מוגדר מראש, רשימה כלשהי של כל המילים האפשריות, 

161
00:12:59,901 --> 00:13:05,236
נניח 50,000 מהן, והמטריצה הראשונה שנתקל בה, המכונה מטריצת שיכון (embedding), 

162
00:13:05,236 --> 00:13:07,800
כוללת עמודה אחת לכל אחת מהמילים הללו.

163
00:13:08,940 --> 00:13:13,760
העמודות הללו הן שקובעות לאיזה וקטור הופכת כל מילה בשלב הראשון.

164
00:13:15,100 --> 00:13:18,762
אנחנו מתייגים אותה ב-We, וכמו כל המטריצות שאנחנו רואים, 

165
00:13:18,762 --> 00:13:22,360
הערכים שלה מתחילים באקראי, אבל הם נלמדים על סמך נתונים.

166
00:13:23,620 --> 00:13:28,373
הפיכת מילים לוקטורים הייתה נוהג נפוץ בלמידת מכונה הרבה לפני הטרנספורמרים, 

167
00:13:28,373 --> 00:13:31,584
אבל זה קצת מוזר אם מעולם לא ראיתם את התהליך קודם, 

168
00:13:31,584 --> 00:13:35,760
וזה קובע את הבסיס לכל מה שבא אחר כך, אז בואו ניקח רגע להכיר אותו.

169
00:13:36,040 --> 00:13:39,743
לעתים קרובות אנו קוראים לזה שיכון של מילה, שמזמין אתכם לחשוב על 

170
00:13:39,743 --> 00:13:43,620
הווקטורים הללו בצורה מאוד גיאומטרית כנקודות במרחב בעל ממדים גבוהים.

171
00:13:44,180 --> 00:13:48,751
הצגת רשימה של שלושה מספרים כקואורדינטות עבור נקודות במרחב תלת-ממד לא תהיה בעיה, 

172
00:13:48,751 --> 00:13:51,780
אבל שיכוני מילים נוטים להיות בממדים הרבה יותר גבוהים.

173
00:13:52,280 --> 00:14:00,440
ב-GPT-3 יש להם 12,288 ממדים, וכפי שתראו, חשוב לעבוד במרחב שיש לו הרבה כיוונים שונים.

174
00:14:01,180 --> 00:14:07,835
באותו אופן שבו אתם יכולים לקחת פרוסה דו מימדית במרחב תלת מימדי ולהטיל עליה את כל הנקודות, 

175
00:14:07,835 --> 00:14:11,458
למען אנימציה של שיכון מילים שמודל פשוט מאפשר לי, 

176
00:14:11,458 --> 00:14:17,448
אני הולך לעשות דבר מקביל על ידי בחירת פרוסה תלת מימדית דרך המרחב הגבוה מאוד הזה, 

177
00:14:17,448 --> 00:14:20,480
הטלה של וקטורי המילים עליה והצגת התוצאות.

178
00:14:21,280 --> 00:14:25,878
הרעיון המרכזי כאן הוא שכשמודל מכוונן את המשקלים שלו במהלך האימון 

179
00:14:25,878 --> 00:14:30,265
 כדי לקבוע איך בדיוק מילים משוכנות כווקטורים, הוא נוטה להסתפק 

180
00:14:30,265 --> 00:14:34,440
בסט של שיכונים שבהם לכיוונים במרחב יש סוג של משמעות סמנטית.

181
00:14:34,980 --> 00:14:38,207
עבור המודל הפשוט של מילה-לוקטור שאני מריץ כאן, 

182
00:14:38,207 --> 00:14:42,878
אם אפעיל חיפוש אחר כל המילים שהשיכונים שלהן הכי קרובים לזה של מגדל, 

183
00:14:42,878 --> 00:14:45,900
תבחינו איך כולן נותנות תחושה דומה מאד למגדל.

184
00:14:46,340 --> 00:14:48,941
ואם אתם רוצים לשחק קצת עם פייתון בבית, זה המודל 

185
00:14:48,941 --> 00:14:51,380
הספציפי שבו אני משתמש כדי ליצור את האנימציות.

186
00:14:51,620 --> 00:14:54,935
זה לא טרנספורמר, אבל זה מספיק כדי להמחיש את הרעיון 

187
00:14:54,935 --> 00:14:57,600
שכיוונים במרחב יכולים לשאת משמעות סמנטית.

188
00:14:58,300 --> 00:15:04,756
דוגמה מאוד קלאסית לכך היא שאם אתם לוקחים את ההבדל בין הווקטורים של אישה וגבר, 

189
00:15:04,756 --> 00:15:10,220
משהו שהייתם מדמיינים כווקטור קטן המחבר את קצה האחד לקצהו של השני, 

190
00:15:10,220 --> 00:15:13,200
זה דומה מאוד להבדל בין מלך למַלכָּה.

191
00:15:15,080 --> 00:15:20,050
אז נניח שלא ידעתם את המילה למלך נשית, תוכלו למצוא אותה על ידי הוספת 

192
00:15:20,050 --> 00:15:25,460
הכיוון הזה של אישה-גבר למלך וחיפוש אחר השיכונים הקרובים ביותר לאותה נקודה.

193
00:15:27,000 --> 00:15:28,200
לפחות, סוג של.

194
00:15:28,480 --> 00:15:32,467
למרות שזו דוגמה קלאסית למודל שאיתו אני משחק, השיכון האמיתי 

195
00:15:32,467 --> 00:15:36,589
של מלכה הוא למעשה קצת יותר רחוק ממה שזה מרמז, ככל הנראה משום 

196
00:15:36,589 --> 00:15:40,780
שהדרך שבה משתמשים במלכה בנתוני אימון אינה רק גרסה נשית של מלך.

197
00:15:41,620 --> 00:15:45,260
כששיחקתי, נראה היה שיחסי המשפחה המחישו את הרעיון הרבה יותר טוב.

198
00:15:46,340 --> 00:15:50,784
הנקודה היא, שנראה שבמהלך האימון המודל מצא יתרון לבחור 

199
00:15:50,784 --> 00:15:54,900
בשיכונים כך שכיוון אחד במרחב הזה מקודד מידע מגדרי.

200
00:15:56,800 --> 00:16:02,631
דוגמה נוספת היא שאם לוקחים את השיכון של איטליה, ומפחיתים את השיכון של גרמניה, 

201
00:16:02,631 --> 00:16:08,090
ומוסיפים את זה לשיכון של היטלר, מקבלים משהו מאוד קרוב לשיכון של מוסוליני.

202
00:16:08,570 --> 00:16:12,271
זה כאילו המודל למד לקשר כיוונים מסוימים עם סגנון 

203
00:16:12,271 --> 00:16:15,670
איטלקי ואחרים למנהיגי ציר מלחמת העולם השנייה.

204
00:16:16,470 --> 00:16:19,904
אולי הדוגמה האהובה עליי ברוח זו היא איך במודלים מסוימים, 

205
00:16:19,904 --> 00:16:23,277
אם לוקחים את ההבדל בין גרמניה ליפן ומוסיפים אותו לסושי, 

206
00:16:23,277 --> 00:16:26,230
מתקרבים מאוד ל-bratwurst (סוג של נקניקיה גרמנית).

207
00:16:27,350 --> 00:16:30,632
גם במשחק הזה של מציאת השכנים הקרובים ביותר, שמחתי 

208
00:16:30,632 --> 00:16:33,850
לראות עד כמה המילה חתול קרובה גם לחיה וגם למפלצת.

209
00:16:34,690 --> 00:16:38,409
אינטואיציה מתמטית שמועיל לזכור, במיוחד עבור הפרק הבא, 

210
00:16:38,409 --> 00:16:43,850
היא כיצד ניתן לחשוב על המכפלה הסקלרית של שני וקטורים כדרך למדוד את מידת התאמתם.

211
00:16:44,870 --> 00:16:49,907
מבחינה חישובית, מכפלה סקלרית כוללת הכפלה של כל הרכיבים התואמים ואז חיבור התוצאות, 

212
00:16:49,907 --> 00:16:54,330
וזה טוב, מכיוון שהרבה מהחישובים שלנו צריכים להיראות כמו סכומים משוקללים.

213
00:16:55,190 --> 00:17:00,655
מבחינה גיאומטרית, מכפלה סקלרית חיובית כאשר וקטורים מצביעים לכיוונים דומים, 

214
00:17:00,655 --> 00:17:05,609
היא אפס אם הם מאונכים, והיא שלילית כאשר הם מצביעים בכיוונים מנוגדים.

215
00:17:06,550 --> 00:17:11,461
לדוגמה, נניח ששיחקתם עם המודל הזה, ואתם משערים ששיכון 

216
00:17:11,461 --> 00:17:17,010
חתולים מינוס חתול עשוי לייצג סוג של כיוון של ריבוי במרחב הזה.

217
00:17:17,430 --> 00:17:22,146
כדי לבדוק זאת, אני הולך לקחת את הווקטור הזה ולחשב את המכפלה הסקלרית שלו מול 

218
00:17:22,146 --> 00:17:27,050
השיכונים של שמות עצם מסוימים ביחיד, ולהשוות למכפלה הסקלרית עם שמות העצם בריבוי.

219
00:17:27,270 --> 00:17:32,960
אם תשחקו עם זה, תשימו לב שאלו בריבוי נותנים באופן עקבי ערכים גבוהים יותר מאלה שביחיד, 

220
00:17:32,960 --> 00:17:36,070
מה שמצביע על כך שהם מתישרים יותר עם הכיוון הזה.

221
00:17:37,070 --> 00:17:41,866
זה גם מהנה איך אם לוקחים את המכפלה הסקלרית הזאת עם השיכונים של המילים אחת, 

222
00:17:41,866 --> 00:17:44,936
שתיים שלוש וכו', הם נותנים ערכים הולכים וגדלים, 

223
00:17:44,936 --> 00:17:49,030
כאילו שנוכל למדוד כמותית את מידת הריבוי שהמודל מוצא למילה נתונה.

224
00:17:50,250 --> 00:17:53,570
שוב, האופן שבו מילים משוכנות נלמד באמצעות נתונים.

225
00:17:54,050 --> 00:17:57,544
מטריצת השיכון הזו, שהעמודות שלה מספרות לנו מה קורה לכל מילה, 

226
00:17:57,544 --> 00:17:59,550
היא אוסף המשקלים הראשון במודל שלנו.

227
00:18:00,030 --> 00:18:04,804
באמצעות מספרי GPT-3, גודל אוצר המילים הוא 50,257, 

228
00:18:04,804 --> 00:18:09,770
ושוב, טכנית זה לא מורכב ממילים כשלעצמן, אלא מטוקנים.

229
00:18:10,630 --> 00:18:17,790
מימד השיכון הוא 12,288, וכפל שלהם אומר לנו שהוא מורכב מכ-617 מיליון משקלים.

230
00:18:18,250 --> 00:18:23,810
בואו נמשיך ונוסיף את זה לסיכום שוטף, ונזכור שעד הסוף אנחנו צריכים לספור עד 175 מיליארד.

231
00:18:25,430 --> 00:18:29,048
במקרה של טרנספורמרים, אתם באמת רוצים לחשוב שהווקטורים 

232
00:18:29,048 --> 00:18:32,130
במרחב השיכון הזה אינם מייצגים רק מילים בודדות.

233
00:18:32,550 --> 00:18:38,273
דבר אחד, הם גם מקודדים מידע על המיקום של המילה הזו, שעליו נדבר בהמשך, 

234
00:18:38,273 --> 00:18:42,770
אבל חשוב מכך, כדאי לחשוב עליהם כבעלי יכולת להספג בהקשר.

235
00:18:43,350 --> 00:18:50,243
וקטור שהתחיל את חייו כשיכון המילה מלך, למשל, עשוי להשתנות בהדרגה על ידי בלוקים 

236
00:18:50,243 --> 00:18:57,399
שונים ברשת הזו, כך שבסופו הוא מצביע לכיוון הרבה יותר ספציפי שמקודד איכשהו שזה היה 

237
00:18:57,399 --> 00:19:04,730
מלך שחי בסקוטלנד, ושהשיג את תפקידו לאחר שרצח את המלך הקודם, ושמתואר בשפה שייקספירית.

238
00:19:05,210 --> 00:19:07,790
חשבו על ההבנה שלכם של מילה נתונה.

239
00:19:08,250 --> 00:19:12,196
המשמעות של המילה הזו ניתנת בבירור על ידי הסביבה, 

240
00:19:12,196 --> 00:19:17,269
ולפעמים זה כולל הקשר ממרחק רב, אז בבניית מודל שיש לו את היכולת 

241
00:19:17,269 --> 00:19:23,390
לחזות איזו מילה מגיעה לאחר מכן, המטרה היא איכשהו לאפשר לו לשלב הקשר ביעילות.

242
00:19:24,050 --> 00:19:29,259
שיהיה ברור, בשלב הראשון הזה, כשאתם יוצרים את מערך הוקטורים על סמך טקסט הקלט, 

243
00:19:29,259 --> 00:19:33,522
כל אחד מהם פשוט נלקח ממטריצת השיכון, כך שבהתחלה כל אחד יכול רק 

244
00:19:33,522 --> 00:19:36,770
לקודד את המשמעות של מילה אחת בלי כל קלט מסביבתו.

245
00:19:37,710 --> 00:19:43,200
אבל כדאי לחשוב על המטרה העיקרית של הרשת הזו שדרכה הם עוברים שהיא לאפשר לכל אחד 

246
00:19:43,200 --> 00:19:48,970
מאותם וקטורים לספוג משמעות הרבה יותר עשירה וספציפית ממה שמילים בודדות יכולות לייצג.

247
00:19:49,510 --> 00:19:54,170
הרשת יכולה לעבד רק מספר קבוע של וקטורים בכל פעם, המכונה גודל ההקשר שלה.

248
00:19:54,510 --> 00:19:59,760
עבור GPT-3 הוא הוכשר עם גודל הקשר של 2048, כך שהנתונים הזורמים ברשת 

249
00:19:59,760 --> 00:20:05,010
נראים תמיד כמו מערך זה של 2048 עמודות, שלכל אחת מהן יש 12,000 ממדים.

250
00:20:05,590 --> 00:20:11,830
גודל ההקשר הזה מגביל את כמות הטקסט שהטרנספורמר יכול לשלב כשהוא מבצע חיזוי של המילה הבאה.

251
00:20:12,370 --> 00:20:17,304
זו הסיבה ששיחות ארוכות עם צ'אטבוטים מסוימים, כמו הגרסאות המוקדמות של ChatGPT, 

252
00:20:17,304 --> 00:20:22,050
נתנו לעתים קרובות את התחושה שהבוט מאבד את חוט השיחה ככל שהמשכתם זמן רב מדי.

253
00:20:23,030 --> 00:20:25,693
אנחנו ניכנס לפרטי המיקוד (attention) בבוא העת, 

254
00:20:25,693 --> 00:20:28,810
אבל בדלוג קדימה אני רוצה לדבר רגע על מה שקורה ממש בסוף.

255
00:20:29,450 --> 00:20:34,870
זכרו, הפלט הרצוי הוא התפלגות הסתברותית על כל הטוקנים  שעשויים להגיע בהמשך.

256
00:20:35,170 --> 00:20:40,465
לדוגמה, אם המילה האחרונה היא פרופסור, וההקשר כולל מילים כמו הארי פוטר, 

257
00:20:40,465 --> 00:20:45,462
ומיד לפני כן אנחנו רואים את המורה הפחות אהוב, וגם אם אתם נותנים לי 

258
00:20:45,462 --> 00:20:49,937
קצת מרחב פעולה כדי להעמיד פנים שטוקנים הם פשוט מילים שלמות, 

259
00:20:49,937 --> 00:20:55,830
אז יש להניח שרשת מאומנת היטב שבנתה ידע על הארי פוטר תקצה מספר גבוה למילה סנייפ.

260
00:20:56,510 --> 00:20:57,970
זה כרוך בשני שלבים שונים.

261
00:20:58,310 --> 00:21:05,525
הראשון הוא להשתמש במטריצה נוספת שממפה את הווקטור האחרון בהקשר הזה לרשימה של 50,000 ערכים, 

262
00:21:05,525 --> 00:21:07,610
אחד לכל טוקן באוצר המילים.

263
00:21:08,170 --> 00:21:15,006
אז יש פונקציה שמנרמלת את הערכים האלו להתפלגות הסתברותית שנקראת סופטמקס ושנדבר עליה בעוד 

264
00:21:15,006 --> 00:21:20,910
רגע, אבל לפני כן אולי נראה קצת מוזר להשתמש רק בשיכון האחרון כדי לבצע חיזוי, 

265
00:21:20,910 --> 00:21:27,823
כאשר אחרי הכל בשלב האחרון יש אלפי וקטורים אחרים בשכבה שפשוט יושבים שם עם משמעויות עשירות 

266
00:21:27,823 --> 00:21:28,290
משלהם.

267
00:21:28,930 --> 00:21:34,520
זה קשור לעובדה שבתהליך האימון מתברר שהרבה יותר יעיל אם תשתמשו בכל אחד 

268
00:21:34,520 --> 00:21:40,270
מאותם וקטורים בשכבה הסופית כדי לבצע בו זמנית חיזוי למה שיבוא מיד אחר כך.

269
00:21:40,970 --> 00:21:45,090
יש עוד הרבה מה לומר על אימונים בהמשך, אבל אני רק רוצה לומר זאת עכשיו.

270
00:21:45,730 --> 00:21:49,690
מטריצה זו נקראת מטריצת ביטול-שיכון (Unembedding) ואנחנו נותנים לה את התווית Wu.

271
00:21:50,210 --> 00:21:54,029
שוב, כמו כל מטריצות המשקל שאנחנו רואים, הערכים שלה מתחילים באקראי, 

272
00:21:54,029 --> 00:21:55,910
אבל הם נלמדים במהלך תהליך האימון.

273
00:21:56,470 --> 00:22:01,093
לגבי ספירת הפרמטרים הכוללת שלנו, למטריצת ביטול-השיכון הזו יש שורה אחת 

274
00:22:01,093 --> 00:22:05,650
לכל מילה באוצר המילים, ולכל שורה יש אותו מספר אלמנטים כמו ממד השיכון.

275
00:22:06,410 --> 00:22:10,353
זה מאוד דומה למטריצת השיכון, רק עם החלפה של הסדר, 

276
00:22:10,353 --> 00:22:17,294
אז זה מוסיף עוד 617 מיליון פרמטרים לרשת, כלומר הספירה שלנו עד כה היא קצת יותר ממיליארד, 

277
00:22:17,294 --> 00:22:21,790
חלק קטן אך לא זניח לגמרי מ-175 המיליארד שנגיע אליהם בסוף.

278
00:22:22,550 --> 00:22:27,173
בתור המיני-שיעור האחרון לפרק זה, אני רוצה לדבר יותר על פונקציית הסופטמקס, 

279
00:22:27,173 --> 00:22:30,610
מכיוון שהיא מופיעה שוב ושוב כשצוללים לתוך בלוקי השיכון.

280
00:22:31,430 --> 00:22:36,346
הרעיון הוא שאם אתם רוצים שרצף של מספרים יפעל כהתפלגות הסתברותית, 

281
00:22:36,346 --> 00:22:42,094
נניח התפלגות לגבי כל המילים הבאות האפשריות, אז כל ערך צריך להיות בין 0 ל-1, 

282
00:22:42,094 --> 00:22:44,590
ואתם גם צריכים שכולם יסתכמו ל-1 .

283
00:22:45,250 --> 00:22:51,307
עם זאת, אם אתם משחקים במשחק הלמידה שבו כל מה שאתם עושים נראה כמו כפל מטריצה-וקטור, 

284
00:22:51,307 --> 00:22:54,810
הפלטים שתקבלו ברוב רובם של המקרים לא עומדים בזה.

285
00:22:55,330 --> 00:22:59,870
הערכים הם לרוב שליליים, או הרבה יותר גדולים מ-1, והם כמעט בוודאות לא מסתכמים ב-1.

286
00:23:00,510 --> 00:23:06,004
סופטמקס היא הדרך הסטנדרטית להפוך רשימה שרירותית של מספרים להתפלגות חוקית באופן 

287
00:23:06,004 --> 00:23:11,290
שהערכים הגדולים יהיו הקרובים ביותר ל-1, והערכים הקטנים יהיו קרובים מאוד ל-0.

288
00:23:11,830 --> 00:23:13,070
זה באמת כל מה שאתם צריכים לדעת.

289
00:23:13,090 --> 00:23:18,770
אבל אם אתם סקרנים, הדרך שבה זה עובד היא קודם כל להעלות את e לחזקה של כל אחד מהמספרים, 

290
00:23:18,770 --> 00:23:21,874
מה שאומר שיש לכם עכשיו רשימה של ערכים חיוביים, 

291
00:23:21,874 --> 00:23:27,224
ואז אתם יכולים לקחת את הסכום של כל הערכים החיוביים האלה ולחלק כל מספר בסכום הזה, 

292
00:23:27,224 --> 00:23:29,470
מה שמנרמל אותם לרשימה שמצטברת ל-1.

293
00:23:30,170 --> 00:23:34,475
תוכלו לשים לב שאם אחד המספרים בקלט גדול משמעותית מהשאר, 

294
00:23:34,475 --> 00:23:40,394
אז בפלט הערך המתאים שולט בהתפלגות, כך שאם הייתם דוגמים ממנה כמעט בטוח שהייתם 

295
00:23:40,394 --> 00:23:42,470
בוחרים רק את הקלט המקסימלי.

296
00:23:42,990 --> 00:23:48,393
אבל זה לא כמו לבחור את המקסימום מכיוון שכאשר ערכים אחרים גדולים באופן דומה, 

297
00:23:48,393 --> 00:23:54,650
הם גם מקבלים משקל משמעותי בהתפלגות, והכל משתנה ללא הרף כאשר אתם משנים ברציפות את הקלטים.

298
00:23:55,130 --> 00:24:01,168
במצבים מסוימים, כמו כאשר ChatGPT משתמשת בהתפלגות הזו כדי ליצור את המילה הבאה, 

299
00:24:01,168 --> 00:24:05,890
יש מקום למעט הנאה נוספת על ידי הוספת מעט תבלין לפונקציה הזו, 

300
00:24:05,890 --> 00:24:08,910
עם הקבוע T שנוסף למכנה של אותם מעריכים.

301
00:24:09,550 --> 00:24:15,254
אנו קוראים לו טמפרטורה, מכיוון שבמעועם התפקיד שלו דומה לתפקיד הטמפרטורה במשוואות 

302
00:24:15,254 --> 00:24:21,522
תרמודינמיות מסוימות, וההשפעה היא שכאשר T גדול יותר, אתם נותנים יותר משקל לערכים הנמוכים, 

303
00:24:21,522 --> 00:24:25,113
כלומר ההתפלגות היא קצת יותר אחידה, ואם T קטן יותר, 

304
00:24:25,113 --> 00:24:29,409
אז הערכים הגדולים ישלטו בצורה אגרסיבית יותר, כאשר בקיצוניות, 

305
00:24:29,409 --> 00:24:32,790
T שווה לאפס פירושו שכל המשקל עובר לערך המקסימלי.

306
00:24:33,470 --> 00:24:39,303
לדוגמה, אני אגרום ל-GPT-3 ליצור סיפור עם טקסט התחלתי "היו היה", 

307
00:24:39,303 --> 00:24:42,950
אבל אני אשתמש בטמפרטורות שונות בכל מקרה.

308
00:24:43,630 --> 00:24:47,663
טמפרטורה אפס פירושה הבחירה תמיד תהיה במילה הכי צפויה, 

309
00:24:47,663 --> 00:24:52,370
ומה שתקבלו בסופו של דבר הוא נגזרת משעממת של זהבה ושלושת הדובים.

310
00:24:53,010 --> 00:24:57,910
טמפרטורה גבוהה יותר נותנת הזדמנות לבחור מילים פחות סבירות, אבל זה כרוך בסיכון.

311
00:24:58,230 --> 00:25:03,645
במקרה הזה, התחלת הסיפור יותר מקורית, על אמן אינטרנט צעיר מדרום קוריאה, 

312
00:25:03,645 --> 00:25:06,010
אבל הוא מידרדר במהירות לשטויות.

313
00:25:06,950 --> 00:25:10,830
מבחינה טכנית, לא מאפשרים לכם לבחור טמפרטורה גדולה מ-2.

314
00:25:11,170 --> 00:25:15,391
אין לכך סיבה מתמטית, זהו רק אילוץ שרירותי שהוטל 

315
00:25:15,391 --> 00:25:19,350
כדי למנוע מהכלי שלהם לייצר דברים שטותיים מדי.

316
00:25:19,870 --> 00:25:24,303
אז אם אתם סקרנים, הדרך שבה האנימציה הזו עובדת היא שאני לוקח את 20 

317
00:25:24,303 --> 00:25:29,275
הטוקנים הבאים הסבירים ביותר ש-GPT-3 מייצר, וזה נראה כמקסימום שהם יתנו לי, 

318
00:25:29,275 --> 00:25:32,970
ואז אני משנה את ההסתברויות על סמך על אקפוננט של חמישית.

319
00:25:33,130 --> 00:25:37,492
בתור עוד קצת ז'רגון, באותו אופן שבו אתם יכולים לקרוא לרכיבי הפלט 

320
00:25:37,492 --> 00:25:42,458
של הפונקציה הזו הסתברויות, אנשים מתייחסים לעתים קרובות לכניסות כלוג'יטים, 

321
00:25:42,458 --> 00:25:46,150
או שיש אנשים שאומרים לוגיטים, אני הולך לומר לוג'יטים. .

322
00:25:46,530 --> 00:25:51,552
אז למשל, כשאתם מזינים טקסט כלשהו, כל שיכוני המילים האלו זורמים דרך הרשת, 

323
00:25:51,552 --> 00:25:55,198
ואתם עושים את הכפל הסופי עם המטריצה של ביטול-השיכון, 

324
00:25:55,198 --> 00:26:01,390
אנשי למידת מכונה יתייחסו לרכיבים בפלט הגולמי והלא מנורמל בתור הלוג'יטים לחיזוי המילה הבאה.

325
00:26:03,330 --> 00:26:08,217
חלק גדול מהמטרה של הפרק הזה הייתה להניח את היסודות להבנת מנגנון המיקוד (attention), 

326
00:26:08,217 --> 00:26:10,370
בסגנון wax-on-wax-off בסרט קראטה קיד.

327
00:26:10,850 --> 00:26:15,400
אתם מבינים, אם יש לכם אינטואיציה חזקה לשיכון מילים, לסופטמקס, 

328
00:26:15,400 --> 00:26:20,685
לאופן שבו מכפלות סקאלריות מודדות דמיון, וגם את הנחת היסוד שרוב החישובים 

329
00:26:20,685 --> 00:26:25,897
צריכים להיראות כמו כפל מטריצה עם מטריצות שהפרמטרים בהן ניתנים לכוונון, 

330
00:26:25,897 --> 00:26:32,210
אז להבין את מנגנון המיקוד, שהוא אבן הפינה לכל הבום המודרני ב-AI, צריך להיות חלק יחסית.

331
00:26:32,650 --> 00:26:34,510
בשביל זה, הצטרפו אליי בפרק הבא.

332
00:26:36,390 --> 00:26:41,210
בזמן שאני מפרסם את זה, טיוטה של הפרק הבא זמינה לסקירה של תומכי Patreon.

333
00:26:41,770 --> 00:26:44,302
גרסה סופית אמורה לעלות לציבור בעוד שבוע או שבועיים, 

334
00:26:44,302 --> 00:26:47,370
זה בדרך כלל תלוי בסופו של דבר כמה אשנה בהתבסס על הביקורות האלו.

335
00:26:47,810 --> 00:26:52,410
בינתיים, אם אתם רוצה לצלול למנגנון המיקוד, ואם אתם רוצים לעזור לערוץ קצת, הוא שם ומחכה.

