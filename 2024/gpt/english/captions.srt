1
00:00:00,000 --> 00:00:04,560
The initials GPT stand for Generative Pretrained Transformer.

2
00:00:05,220 --> 00:00:09,020
So that first word is straightforward enough, these are bots that generate new text.

3
00:00:09,800 --> 00:00:13,229
Pretrained refers to how the model went through a process of learning 

4
00:00:13,229 --> 00:00:16,659
from a massive amount of data, and the prefix insinuates that there's 

5
00:00:16,659 --> 00:00:20,040
more room to fine-tune it on specific tasks with additional training.

6
00:00:20,720 --> 00:00:22,900
But the last word, that's the real key piece.

7
00:00:23,380 --> 00:00:27,625
A transformer is a specific kind of neural network, a machine learning model, 

8
00:00:27,625 --> 00:00:31,000
and it's the core invention underlying the current boom in AI.

9
00:00:31,740 --> 00:00:35,381
What I want to do with this video and the following chapters is go through 

10
00:00:35,381 --> 00:00:39,120
a visually-driven explanation for what actually happens inside a transformer.

11
00:00:39,700 --> 00:00:42,820
We're going to follow the data that flows through it and go step by step.

12
00:00:43,440 --> 00:00:47,380
There are many different kinds of models that you can build using transformers.

13
00:00:47,800 --> 00:00:50,800
Some models take in audio and produce a transcript.

14
00:00:51,340 --> 00:00:54,230
This sentence comes from a model going the other way around, 

15
00:00:54,230 --> 00:00:56,220
producing synthetic speech just from text.

16
00:00:56,660 --> 00:01:01,089
All those tools that took the world by storm in 2022 like Dolly and Midjourney 

17
00:01:01,089 --> 00:01:05,519
that take in a text description and produce an image are based on transformers.

18
00:01:06,000 --> 00:01:09,805
Even if I can't quite get it to understand what a pie creature is supposed to be, 

19
00:01:09,805 --> 00:01:13,100
I'm still blown away that this kind of thing is even remotely possible.

20
00:01:13,900 --> 00:01:18,055
And the original transformer introduced in 2017 by Google was invented for 

21
00:01:18,055 --> 00:01:22,100
the specific use case of translating text from one language into another.

22
00:01:22,660 --> 00:01:26,450
But the variant that you and I will focus on, which is the type that 

23
00:01:26,450 --> 00:01:31,338
underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, 

24
00:01:31,338 --> 00:01:34,964
maybe even with some surrounding images or sound accompanying it, 

25
00:01:34,964 --> 00:01:38,260
and produce a prediction for what comes next in the passage.

26
00:01:38,600 --> 00:01:41,382
That prediction takes the form of a probability distribution 

27
00:01:41,382 --> 00:01:43,800
over many different chunks of text that might follow.

28
00:01:45,040 --> 00:01:47,388
At first glance, you might think that predicting the next 

29
00:01:47,388 --> 00:01:49,940
word feels like a very different goal from generating new text.

30
00:01:50,180 --> 00:01:52,719
But once you have a prediction model like this, 

31
00:01:52,719 --> 00:01:56,739
a simple thing you generate a longer piece of text is to give it an initial 

32
00:01:56,739 --> 00:02:00,600
snippet to work with, have it take a random sample from the distribution 

33
00:02:00,600 --> 00:02:03,298
it just generated, append that sample to the text, 

34
00:02:03,298 --> 00:02:08,006
and then run the whole process again to make a new prediction based on all the new text, 

35
00:02:08,006 --> 00:02:09,539
including what it just added.

36
00:02:10,100 --> 00:02:13,000
I don't know about you, but it really doesn't feel like this should actually work.

37
00:02:13,420 --> 00:02:17,998
In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly 

38
00:02:17,998 --> 00:02:22,420
predict and sample the next chunk of text to generate a story based on the seed text.

39
00:02:22,420 --> 00:02:26,120
The story just doesn't really make that much sense.

40
00:02:26,500 --> 00:02:31,351
But if I swap it out for API calls to GPT-3 instead, which is the same basic model, 

41
00:02:31,351 --> 00:02:35,509
just much bigger, suddenly almost magically we do get a sensible story, 

42
00:02:35,509 --> 00:02:40,186
one that even seems to infer that a pi creature would live in a land of math and 

43
00:02:40,186 --> 00:02:40,880
computation.

44
00:02:41,580 --> 00:02:45,013
This process here of repeated prediction and sampling is essentially 

45
00:02:45,013 --> 00:02:48,496
what's happening when you interact with ChatGPT or any of these other 

46
00:02:48,496 --> 00:02:51,880
large language models and you see them producing one word at a time.

47
00:02:52,480 --> 00:02:55,900
In fact, one feature that I would very much enjoy is the ability to 

48
00:02:55,900 --> 00:02:59,220
see the underlying distribution for each new word that it chooses.

49
00:03:03,820 --> 00:03:06,304
Let's kick things off with a very high level preview 

50
00:03:06,304 --> 00:03:08,180
of how data flows through a transformer.

51
00:03:08,640 --> 00:03:12,011
We will spend much more time motivating and interpreting and expanding 

52
00:03:12,011 --> 00:03:14,433
on the details of each step, but in broad strokes, 

53
00:03:14,433 --> 00:03:18,660
when one of these chatbots generates a given word, here's what's going on under the hood.

54
00:03:19,080 --> 00:03:22,040
First, the input is broken up into a bunch of little pieces.

55
00:03:22,620 --> 00:03:26,269
These pieces are called tokens, and in the case of text these tend to be 

56
00:03:26,269 --> 00:03:29,820
words or little pieces of words or other common character combinations.

57
00:03:30,740 --> 00:03:33,743
If images or sound are involved, then tokens could be 

58
00:03:33,743 --> 00:03:37,080
little patches of that image or little chunks of that sound.

59
00:03:37,580 --> 00:03:40,681
Each one of these tokens is then associated with a vector, 

60
00:03:40,681 --> 00:03:45,360
meaning some list of numbers, which is meant to somehow encode the meaning of that piece.

61
00:03:45,880 --> 00:03:50,136
If you think of these vectors as giving coordinates in some very high dimensional space, 

62
00:03:50,136 --> 00:03:53,053
words with similar meanings tend to land on vectors that are 

63
00:03:53,053 --> 00:03:54,680
close to each other in that space.

64
00:03:55,280 --> 00:03:58,232
This sequence of vectors then passes through an operation that's 

65
00:03:58,232 --> 00:04:01,320
known as an attention block, and this allows the vectors to talk to 

66
00:04:01,320 --> 00:04:04,500
each other and pass information back and forth to update their values.

67
00:04:04,880 --> 00:04:08,145
For example, the meaning of the word model in the phrase a machine 

68
00:04:08,145 --> 00:04:11,800
learning model is different from its meaning in the phrase a fashion model.

69
00:04:12,260 --> 00:04:15,561
The attention block is what's responsible for figuring out which 

70
00:04:15,561 --> 00:04:19,471
words in context are relevant to updating the meanings of which other words, 

71
00:04:19,471 --> 00:04:21,959
and how exactly those meanings should be updated.

72
00:04:22,500 --> 00:04:25,142
And again, whenever I use the word meaning, this is 

73
00:04:25,142 --> 00:04:28,040
somehow entirely encoded in the entries of those vectors.

74
00:04:29,180 --> 00:04:32,321
After that, these vectors pass through a different kind of operation, 

75
00:04:32,321 --> 00:04:35,462
and depending on the source that you're reading this will be referred 

76
00:04:35,462 --> 00:04:38,200
to as a multi-layer perceptron or maybe a feed-forward layer.

77
00:04:38,580 --> 00:04:40,536
And here the vectors don't talk to each other, 

78
00:04:40,536 --> 00:04:42,660
they all go through the same operation in parallel.

79
00:04:43,060 --> 00:04:45,795
And while this block is a little bit harder to interpret, 

80
00:04:45,795 --> 00:04:49,520
later on we'll talk about how the step is a little bit like asking a long list 

81
00:04:49,520 --> 00:04:53,104
of questions about each vector, and then updating them based on the answers 

82
00:04:53,104 --> 00:04:54,000
to those questions.

83
00:04:54,900 --> 00:04:58,239
All of the operations in both of these blocks look like a 

84
00:04:58,239 --> 00:05:01,750
giant pile of matrix multiplications, and our primary job is 

85
00:05:01,750 --> 00:05:05,320
going to be to understand how to read the underlying matrices.

86
00:05:06,980 --> 00:05:10,980
I'm glossing over some details about some normalization steps that happen in between, 

87
00:05:10,980 --> 00:05:12,980
but this is after all a high-level preview.

88
00:05:13,680 --> 00:05:17,290
After that, the process essentially repeats, you go back and forth 

89
00:05:17,290 --> 00:05:20,524
between attention blocks and multi-layer perceptron blocks, 

90
00:05:20,524 --> 00:05:24,188
until at the very end the hope is that all of the essential meaning 

91
00:05:24,188 --> 00:05:28,500
of the passage has somehow been baked into the very last vector in the sequence.

92
00:05:28,920 --> 00:05:33,378
We then perform a certain operation on that last vector that produces a probability 

93
00:05:33,378 --> 00:05:38,154
distribution over all possible tokens, all possible little chunks of text that might come 

94
00:05:38,154 --> 00:05:38,420
next.

95
00:05:38,980 --> 00:05:42,367
And like I said, once you have a tool that predicts what comes next 

96
00:05:42,367 --> 00:05:45,905
given a snippet of text, you can feed it a little bit of seed text and 

97
00:05:45,905 --> 00:05:49,143
have it repeatedly play this game of predicting what comes next, 

98
00:05:49,143 --> 00:05:53,080
sampling from the distribution, appending it, and then repeating over and over.

99
00:05:53,640 --> 00:05:57,997
Some of you in the know may remember how long before ChatGPT came into the scene, 

100
00:05:57,997 --> 00:06:00,495
this is what early demos of GPT-3 looked like, 

101
00:06:00,495 --> 00:06:04,640
you would have it autocomplete stories and essays based on an initial snippet.

102
00:06:05,580 --> 00:06:09,767
To make a tool like this into a chatbot, the easiest starting point is to have 

103
00:06:09,767 --> 00:06:13,954
a little bit of text that establishes the setting of a user interacting with a 

104
00:06:13,954 --> 00:06:17,187
helpful AI assistant, what you would call the system prompt, 

105
00:06:17,187 --> 00:06:21,480
and then you would use the user's initial question or prompt as the first bit of 

106
00:06:21,480 --> 00:06:25,773
dialogue, and then you have it start predicting what such a helpful AI assistant 

107
00:06:25,773 --> 00:06:26,940
would say in response.

108
00:06:27,720 --> 00:06:32,084
There is more to say about an step of training that's required to make this work well, 

109
00:06:32,084 --> 00:06:33,940
but at a high level this is the idea.

110
00:06:35,720 --> 00:06:40,011
In this chapter, you and I are going to expand on the details of what happens at the very 

111
00:06:40,011 --> 00:06:42,777
beginning of the network, at the very end of the network, 

112
00:06:42,777 --> 00:06:46,734
and I also want to spend a lot of time reviewing some important bits of background 

113
00:06:46,734 --> 00:06:50,978
knowledge, things that would have been second nature to any machine learning engineer by 

114
00:06:50,978 --> 00:06:52,600
the time transformers came around.

115
00:06:53,060 --> 00:06:56,386
If you're comfortable with that background knowledge and a little impatient, 

116
00:06:56,386 --> 00:06:58,503
you could feel free to skip to the next chapter, 

117
00:06:58,503 --> 00:07:00,619
which is going to focus on the attention blocks, 

118
00:07:00,619 --> 00:07:02,780
generally considered the heart of the transformer.

119
00:07:03,360 --> 00:07:07,003
After that I want to talk more about these multi-layer perceptron blocks, 

120
00:07:07,003 --> 00:07:11,138
how training works, and a number of other details that will have been skipped up to 

121
00:07:11,138 --> 00:07:11,680
that point.

122
00:07:12,180 --> 00:07:16,253
For broader context, these videos are additions to a mini-series about deep learning, 

123
00:07:16,253 --> 00:07:18,905
and it's okay if you haven't watched the previous ones, 

124
00:07:18,905 --> 00:07:22,978
I think you can do it out of order, but before diving into transformers specifically, 

125
00:07:22,978 --> 00:07:27,051
I do think it's worth making sure that we're on the same page about the basic premise 

126
00:07:27,051 --> 00:07:28,520
and structure of deep learning.

127
00:07:29,020 --> 00:07:33,303
At the risk of stating the obvious, this is one approach to machine learning, 

128
00:07:33,303 --> 00:07:37,860
which describes any model where you're using data to somehow determine how a model 

129
00:07:37,860 --> 00:07:38,300
behaves.

130
00:07:39,140 --> 00:07:42,525
What I mean by that is, let's say you want a function that takes in 

131
00:07:42,525 --> 00:07:44,914
an image and it produces a label describing it, 

132
00:07:44,914 --> 00:07:48,299
or our example of predicting the next word given a passage of text, 

133
00:07:48,299 --> 00:07:52,780
or any other task that seems to require some element of intuition and pattern recognition.

134
00:07:53,200 --> 00:07:57,502
We almost take this for granted these days, but the idea with machine learning is 

135
00:07:57,502 --> 00:08:02,225
that rather than trying to explicitly define a procedure for how to do that task in code, 

136
00:08:02,225 --> 00:08:05,636
which is what people would have done in the earliest days of AI, 

137
00:08:05,636 --> 00:08:09,309
instead you set up a very flexible structure with tunable parameters, 

138
00:08:09,309 --> 00:08:13,717
like a bunch of knobs and dials, and then somehow you use many examples of what the 

139
00:08:13,717 --> 00:08:17,915
output should look like for a given input to tweak and tune the values of those 

140
00:08:17,915 --> 00:08:19,700
parameters to mimic this behavior.

141
00:08:19,700 --> 00:08:24,173
For example, maybe the simplest form of machine learning is linear regression, 

142
00:08:24,173 --> 00:08:27,287
where your inputs and outputs are each single numbers, 

143
00:08:27,287 --> 00:08:30,684
something like the square footage of a house and its price, 

144
00:08:30,684 --> 00:08:35,044
and what you want is to find a line of best fit through this data, you know, 

145
00:08:35,044 --> 00:08:36,799
to predict future house prices.

146
00:08:37,440 --> 00:08:40,579
That line is described by two continuous parameters, 

147
00:08:40,579 --> 00:08:44,014
say the slope and the y-intercept, and the goal of linear 

148
00:08:44,014 --> 00:08:48,160
regression is to determine those parameters to closely match the data.

149
00:08:48,880 --> 00:08:52,100
Needless to say, deep learning models get much more complicated.

150
00:08:52,620 --> 00:08:57,660
GPT-3, for example, has not two, but 175 billion parameters.

151
00:08:58,120 --> 00:09:02,007
But here's the thing, it's not a given that you can create some giant 

152
00:09:02,007 --> 00:09:05,617
model with a huge number of parameters without it either grossly 

153
00:09:05,617 --> 00:09:09,560
overfitting the training data or being completely intractable to train.

154
00:09:10,260 --> 00:09:13,140
Deep learning describes a class of models that in the 

155
00:09:13,140 --> 00:09:16,180
last couple decades have proven to scale remarkably well.

156
00:09:16,480 --> 00:09:20,986
What unifies them is the same training algorithm, called backpropagation, 

157
00:09:20,986 --> 00:09:25,981
and the context I want you to have as we go in is that in order for this training 

158
00:09:25,981 --> 00:09:31,280
algorithm to work well at scale, these models have to follow a certain specific format.

159
00:09:31,800 --> 00:09:36,048
If you know this format going in, it helps to explain many of the choices for how 

160
00:09:36,048 --> 00:09:40,400
a transformer processes language, which otherwise run the risk of feeling arbitrary.

161
00:09:41,440 --> 00:09:44,062
First, whatever model you're making, the input 

162
00:09:44,062 --> 00:09:46,740
has to be formatted as an array of real numbers.

163
00:09:46,740 --> 00:09:50,780
This could mean a list of numbers, it could be a two-dimensional array, 

164
00:09:50,780 --> 00:09:53,867
or very often you deal with higher dimensional arrays, 

165
00:09:53,867 --> 00:09:56,000
where the general term used is tensor.

166
00:09:56,560 --> 00:10:00,566
You often think of that input data as being progressively transformed into many 

167
00:10:00,566 --> 00:10:04,473
distinct layers, where again, each layer is always structured as some kind of 

168
00:10:04,473 --> 00:10:08,680
array of real numbers, until you get to a final layer which you consider the output.

169
00:10:09,280 --> 00:10:12,962
For example, the final layer in our text processing model is a list of 

170
00:10:12,962 --> 00:10:17,060
numbers representing the probability distribution for all possible next tokens.

171
00:10:17,820 --> 00:10:22,086
In deep learning, these model parameters are almost always referred to as weights, 

172
00:10:22,086 --> 00:10:26,044
and this is because a key feature of these models is that the only way these 

173
00:10:26,044 --> 00:10:29,900
parameters interact with the data being processed is through weighted sums.

174
00:10:30,340 --> 00:10:32,786
You also sprinkle some non-linear functions throughout, 

175
00:10:32,786 --> 00:10:34,360
but they won't depend on parameters.

176
00:10:35,200 --> 00:10:38,637
Typically though, instead of seeing the weighted sums all naked 

177
00:10:38,637 --> 00:10:42,021
and written out explicitly like this, you'll instead find them 

178
00:10:42,021 --> 00:10:45,620
packaged together as various components in a matrix vector product.

179
00:10:46,740 --> 00:10:50,465
It amounts to saying the same thing, if you think back to how matrix vector 

180
00:10:50,465 --> 00:10:54,240
multiplication works, each component in the output looks like a weighted sum.

181
00:10:54,780 --> 00:10:58,307
It's just often conceptually cleaner for you and me to think 

182
00:10:58,307 --> 00:11:01,776
about matrices that are filled with tunable parameters that 

183
00:11:01,776 --> 00:11:05,420
transform vectors that are drawn from the data being processed.

184
00:11:06,340 --> 00:11:10,287
For example, those 175 billion weights in GPT-3 are 

185
00:11:10,287 --> 00:11:14,160
organized into just under 28,000 distinct matrices.

186
00:11:14,660 --> 00:11:17,462
Those matrices in turn fall into eight different categories, 

187
00:11:17,462 --> 00:11:21,275
and what you and I are going to do is step through each one of those categories to 

188
00:11:21,275 --> 00:11:22,700
understand what that type does.

189
00:11:23,160 --> 00:11:27,144
As we go through, I think it's kind of fun to reference the specific 

190
00:11:27,144 --> 00:11:31,360
numbers from GPT-3 to count up exactly where those 175 billion come from.

191
00:11:31,880 --> 00:11:34,548
Even if nowadays there are bigger and better models, 

192
00:11:34,548 --> 00:11:38,927
this one has a certain charm as the large-language model to really capture the world's 

193
00:11:38,927 --> 00:11:40,740
attention outside of ML communities.

194
00:11:41,440 --> 00:11:44,220
Also, practically speaking, companies tend to keep much tighter 

195
00:11:44,220 --> 00:11:46,740
lips around the specific numbers for more modern networks.

196
00:11:47,360 --> 00:11:50,754
I just want to set the scene going in, that as you peek under the 

197
00:11:50,754 --> 00:11:53,480
hood to see what happens inside a tool like ChatGPT, 

198
00:11:53,480 --> 00:11:57,440
almost all of the actual computation looks like matrix vector multiplication.

199
00:11:57,900 --> 00:12:01,933
There's a little bit of a risk getting lost in the sea of billions of numbers, 

200
00:12:01,933 --> 00:12:05,304
but you should draw a very sharp distinction in your mind between 

201
00:12:05,304 --> 00:12:08,674
the weights of the model, which I'll always color in blue or red, 

202
00:12:08,674 --> 00:12:11,840
and the data being processed, which I'll always color in gray.

203
00:12:12,180 --> 00:12:16,208
The weights are the actual brains, they are the things learned during training, 

204
00:12:16,208 --> 00:12:17,920
and they determine how it behaves.

205
00:12:18,280 --> 00:12:22,359
The data being processed simply encodes whatever specific input is 

206
00:12:22,359 --> 00:12:26,500
fed into the model for a given run, like an example snippet of text.

207
00:12:27,480 --> 00:12:31,751
With all of that as foundation, let's dig into the first step of this text processing 

208
00:12:31,751 --> 00:12:36,022
example, which is to break up the input into little chunks and turn those chunks into 

209
00:12:36,022 --> 00:12:36,420
vectors.

210
00:12:37,020 --> 00:12:39,308
I mentioned how those chunks are called tokens, 

211
00:12:39,308 --> 00:12:41,548
which might be pieces of words or punctuation, 

212
00:12:41,548 --> 00:12:44,933
but every now and then in this chapter and especially in the next one, 

213
00:12:44,933 --> 00:12:48,080
I'd like to just pretend that it's broken more cleanly into words.

214
00:12:48,600 --> 00:12:51,431
Because we humans think in words, this will just make it much 

215
00:12:51,431 --> 00:12:54,080
easier to reference little examples and clarify each step.

216
00:12:55,260 --> 00:12:59,479
The model has a predefined vocabulary, some list of all possible words, 

217
00:12:59,479 --> 00:13:03,170
say 50,000 of them, and the first matrix that we'll encounter, 

218
00:13:03,170 --> 00:13:07,800
known as the embedding matrix, has a single column for each one of these words.

219
00:13:08,940 --> 00:13:13,760
These columns are what determines what vector each word turns into in that first step.

220
00:13:15,100 --> 00:13:18,100
We label it We, and like all the matrices we see, 

221
00:13:18,100 --> 00:13:22,360
its values begin random, but they're going to be learned based on data.

222
00:13:23,620 --> 00:13:27,425
Turning words into vectors was common practice in machine learning long before 

223
00:13:27,425 --> 00:13:30,798
transformers, but it's a little weird if you've never seen it before, 

224
00:13:30,798 --> 00:13:33,495
and it sets the foundation for everything that follows, 

225
00:13:33,495 --> 00:13:35,760
so let's take a moment to get familiar with it.

226
00:13:36,040 --> 00:13:39,641
We often call this embedding a word, which invites you to think of 

227
00:13:39,641 --> 00:13:43,620
these vectors very geometrically as points in some high dimensional space.

228
00:13:44,180 --> 00:13:47,808
Visualizing a list of three numbers as coordinates for points in 3D space 

229
00:13:47,808 --> 00:13:51,780
would be no problem, but word embeddings tend to be much much higher dimensional.

230
00:13:52,280 --> 00:13:56,000
In GPT-3 they have 12,288 dimensions, and as you'll see, 

231
00:13:56,000 --> 00:14:00,440
it matters to work in a space that has a lot of distinct directions.

232
00:14:01,180 --> 00:14:05,109
In the same way that you could take a two-dimensional slice through a 3D space 

233
00:14:05,109 --> 00:14:08,840
and project all the points onto that slice, for the sake of animating word 

234
00:14:08,840 --> 00:14:12,521
embeddings that a simple model is giving me, I'm going to do an analogous 

235
00:14:12,521 --> 00:14:16,799
thing by choosing a three-dimensional slice through this very high dimensional space, 

236
00:14:16,799 --> 00:14:20,480
and projecting the word vectors down onto that and displaying the results.

237
00:14:21,280 --> 00:14:25,576
The big idea here is that as a model tweaks and tunes its weights to determine 

238
00:14:25,576 --> 00:14:28,784
how exactly words get embedded as vectors during training, 

239
00:14:28,784 --> 00:14:33,080
it tends to settle on a set of embeddings where directions in the space have a 

240
00:14:33,080 --> 00:14:34,440
kind of semantic meaning.

241
00:14:34,980 --> 00:14:37,842
For the simple word-to-vector model I'm running here, 

242
00:14:37,842 --> 00:14:42,242
if I run a search for all the words whose embeddings are closest to that of tower, 

243
00:14:42,242 --> 00:14:45,900
you'll notice how they all seem to give very similar tower-ish vibes.

244
00:14:46,340 --> 00:14:48,820
And if you want to pull up some Python and play along at home, 

245
00:14:48,820 --> 00:14:51,380
this is the specific model that I'm using to make the animations.

246
00:14:51,620 --> 00:14:54,534
It's not a transformer, but it's enough to illustrate the 

247
00:14:54,534 --> 00:14:57,600
idea that directions in the space can carry semantic meaning.

248
00:14:58,300 --> 00:15:03,115
A very classic example of this is how if you take the difference between the vectors 

249
00:15:03,115 --> 00:15:08,044
for woman and man, something you would visualize as a little vector connecting the tip 

250
00:15:08,044 --> 00:15:12,860
of one to the tip of the other, it's very similar to the difference between king and 

251
00:15:12,860 --> 00:15:13,200
queen.

252
00:15:15,080 --> 00:15:18,501
So let's say you didn't know the word for a female monarch, 

253
00:15:18,501 --> 00:15:22,323
you could find it by taking king, adding this woman-man direction, 

254
00:15:22,323 --> 00:15:25,460
and searching for the embeddings closest to that point.

255
00:15:27,000 --> 00:15:28,200
At least, kind of.

256
00:15:28,480 --> 00:15:31,821
Despite this being a classic example for the model I'm playing with, 

257
00:15:31,821 --> 00:15:35,985
the true embedding of queen is actually a little farther off than this would suggest, 

258
00:15:35,985 --> 00:15:40,005
presumably because the way queen is used in training data is not merely a feminine 

259
00:15:40,005 --> 00:15:40,780
version of king.

260
00:15:41,620 --> 00:15:45,260
When I played around, family relations seemed to illustrate the idea much better.

261
00:15:46,340 --> 00:15:50,514
The point is, it looks like during training the model found it advantageous to 

262
00:15:50,514 --> 00:15:54,900
choose embeddings such that one direction in this space encodes gender information.

263
00:15:56,800 --> 00:16:00,136
Another example is that if you take the embedding of Italy, 

264
00:16:00,136 --> 00:16:04,808
and you subtract the embedding of Germany, and add that to the embedding of Hitler, 

265
00:16:04,808 --> 00:16:08,090
you get something very close to the embedding of Mussolini.

266
00:16:08,570 --> 00:16:13,495
It's as if the model learned to associate some directions with Italian-ness, 

267
00:16:13,495 --> 00:16:15,670
and others with WWII axis leaders.

268
00:16:16,470 --> 00:16:19,988
Maybe my favorite example in this vein is how in some models, 

269
00:16:19,988 --> 00:16:24,243
if you take the difference between Germany and Japan, and add it to sushi, 

270
00:16:24,243 --> 00:16:26,230
you end up very close to bratwurst.

271
00:16:27,350 --> 00:16:30,358
Also in playing this game of finding nearest neighbors, 

272
00:16:30,358 --> 00:16:33,850
I was pleased to see how close Kat was to both beast and monster.

273
00:16:34,690 --> 00:16:37,790
One bit of mathematical intuition that's helpful to have in mind, 

274
00:16:37,790 --> 00:16:40,749
especially for the next chapter, is how the dot product of two 

275
00:16:40,749 --> 00:16:43,850
vectors can be thought of as a way to measure how well they align.

276
00:16:44,870 --> 00:16:47,742
Computationally, dot products involve multiplying all the 

277
00:16:47,742 --> 00:16:51,160
corresponding components and then adding the results, which is good, 

278
00:16:51,160 --> 00:16:54,330
since so much of our computation has to look like weighted sums.

279
00:16:55,190 --> 00:17:00,056
Geometrically, the dot product is positive when vectors point in similar directions, 

280
00:17:00,056 --> 00:17:03,663
it's zero if they're perpendicular, and it's negative whenever 

281
00:17:03,663 --> 00:17:05,609
they point in opposite directions.

282
00:17:06,550 --> 00:17:09,976
For example, let's say you were playing with this model, 

283
00:17:09,976 --> 00:17:14,966
and you hypothesize that the embedding of cats minus cat might represent a sort of 

284
00:17:14,966 --> 00:17:17,010
plurality direction in this space.

285
00:17:17,430 --> 00:17:20,620
To test this, I'm going to take this vector and compute its dot 

286
00:17:20,620 --> 00:17:23,511
product against the embeddings of certain singular nouns, 

287
00:17:23,511 --> 00:17:27,050
and compare it to the dot products with the corresponding plural nouns.

288
00:17:27,270 --> 00:17:30,264
If you play around with this, you'll notice that the plural ones 

289
00:17:30,264 --> 00:17:33,674
do indeed seem to consistently give higher values than the singular ones, 

290
00:17:33,674 --> 00:17:36,070
indicating that they align more with this direction.

291
00:17:37,070 --> 00:17:41,730
It's also fun how if you take this dot product with the embeddings of the words 1, 

292
00:17:41,730 --> 00:17:45,492
2, 3, and so on, they give increasing values, so it's as if we can 

293
00:17:45,492 --> 00:17:49,030
quantitatively measure how plural the model finds a given word.

294
00:17:50,250 --> 00:17:53,570
Again, the specifics for how words get embedded is learned using data.

295
00:17:54,050 --> 00:17:57,523
This embedding matrix, whose columns tell us what happens to each word, 

296
00:17:57,523 --> 00:17:59,550
is the first pile of weights in our model.

297
00:18:00,030 --> 00:18:04,796
Using the GPT-3 numbers, the vocabulary size specifically is 50,257, 

298
00:18:04,796 --> 00:18:09,770
and again, technically this consists not of words per se, but of tokens.

299
00:18:10,630 --> 00:18:13,980
The embedding dimension is 12,288, and multiplying 

300
00:18:13,980 --> 00:18:17,790
those tells us this consists of about 617 million weights.

301
00:18:18,250 --> 00:18:20,676
Let's go ahead and add this to a running tally, 

302
00:18:20,676 --> 00:18:23,810
remembering that by the end we should count up to 175 billion.

303
00:18:25,430 --> 00:18:28,804
In the case of transformers, you really want to think of the vectors 

304
00:18:28,804 --> 00:18:32,130
in this embedding space as not merely representing individual words.

305
00:18:32,550 --> 00:18:36,565
For one thing, they also encode information about the position of that word, 

306
00:18:36,565 --> 00:18:39,276
which we'll talk about later, but more importantly, 

307
00:18:39,276 --> 00:18:42,770
you should think of them as having the capacity to soak in context.

308
00:18:43,350 --> 00:18:47,459
A vector that started its life as the embedding of the word king, for example, 

309
00:18:47,459 --> 00:18:51,465
might progressively get tugged and pulled by various blocks in this network, 

310
00:18:51,465 --> 00:18:55,626
so that by the end it points in a much more specific and nuanced direction that 

311
00:18:55,626 --> 00:18:58,643
somehow encodes that it was a king who lived in Scotland, 

312
00:18:58,643 --> 00:19:02,024
and who had achieved his post after murdering the previous king, 

313
00:19:02,024 --> 00:19:04,730
and who's being described in Shakespearean language.

314
00:19:05,210 --> 00:19:07,790
Think about your own understanding of a given word.

315
00:19:08,250 --> 00:19:11,780
The meaning of that word is clearly informed by the surroundings, 

316
00:19:11,780 --> 00:19:15,151
and sometimes this includes context from a long distance away, 

317
00:19:15,151 --> 00:19:19,698
so in putting together a model that has the ability to predict what word comes next, 

318
00:19:19,698 --> 00:19:23,390
the goal is to somehow empower it to incorporate context efficiently.

319
00:19:24,050 --> 00:19:27,183
To be clear, in that very first step, when you create the array of 

320
00:19:27,183 --> 00:19:30,409
vectors based on the input text, each one of those is simply plucked 

321
00:19:30,409 --> 00:19:33,543
out of the embedding matrix, so initially each one can only encode 

322
00:19:33,543 --> 00:19:36,770
the meaning of a single word without any input from its surroundings.

323
00:19:37,710 --> 00:19:41,611
But you should think of the primary goal of this network that it flows through 

324
00:19:41,611 --> 00:19:45,463
as being to enable each one of those vectors to soak up a meaning that's much 

325
00:19:45,463 --> 00:19:48,970
more rich and specific than what mere individual words could represent.

326
00:19:49,510 --> 00:19:52,853
The network can only process a fixed number of vectors at a time, 

327
00:19:52,853 --> 00:19:54,170
known as its context size.

328
00:19:54,510 --> 00:19:57,731
For GPT-3 it was trained with a context size of 2048, 

329
00:19:57,731 --> 00:20:02,862
so the data flowing through the network always looks like this array of 2048 columns, 

330
00:20:02,862 --> 00:20:05,010
each of which has 12,000 dimensions.

331
00:20:05,590 --> 00:20:08,710
This context size limits how much text the transformer can 

332
00:20:08,710 --> 00:20:11,830
incorporate when it's making a prediction of the next word.

333
00:20:12,370 --> 00:20:15,092
This is why long conversations with certain chatbots, 

334
00:20:15,092 --> 00:20:18,218
like the early versions of ChatGPT, often gave the feeling of 

335
00:20:18,218 --> 00:20:22,050
the bot kind of losing the thread of conversation as you continued too long.

336
00:20:23,030 --> 00:20:25,272
We'll go into the details of attention in due time, 

337
00:20:25,272 --> 00:20:28,810
but skipping ahead I want to talk for a minute about what happens at the very end.

338
00:20:29,450 --> 00:20:32,047
Remember, the desired output is a probability 

339
00:20:32,047 --> 00:20:34,870
distribution over all tokens that might come next.

340
00:20:35,170 --> 00:20:37,813
For example, if the very last word is Professor, 

341
00:20:37,813 --> 00:20:40,510
and the context includes words like Harry Potter, 

342
00:20:40,510 --> 00:20:43,585
and immediately preceding we see least favorite teacher, 

343
00:20:43,585 --> 00:20:47,738
and also if you give me some leeway by letting me pretend that tokens simply 

344
00:20:47,738 --> 00:20:51,946
look like full words, then a well-trained network that had built up knowledge 

345
00:20:51,946 --> 00:20:55,830
of Harry Potter would presumably assign a high number to the word Snape.

346
00:20:56,510 --> 00:20:57,970
This involves two different steps.

347
00:20:58,310 --> 00:21:02,806
The first one is to use another matrix that maps the very last vector in 

348
00:21:02,806 --> 00:21:07,610
that context to a list of 50,000 values, one for each token in the vocabulary.

349
00:21:08,170 --> 00:21:12,225
Then there's a function that normalizes this into a probability distribution, 

350
00:21:12,225 --> 00:21:15,708
it's called Softmax and we'll talk more about it in just a second, 

351
00:21:15,708 --> 00:21:19,919
but before that it might seem a little bit weird to only use this last embedding 

352
00:21:19,919 --> 00:21:23,974
to make a prediction, when after all in that last step there are thousands of 

353
00:21:23,974 --> 00:21:28,290
other vectors in the layer just sitting there with their own context-rich meanings.

354
00:21:28,930 --> 00:21:32,726
This has to do with the fact that in the training process it turns out to be 

355
00:21:32,726 --> 00:21:36,473
much more efficient if you use each one of those vectors in the final layer 

356
00:21:36,473 --> 00:21:40,270
to simultaneously make a prediction for what would come immediately after it.

357
00:21:40,970 --> 00:21:43,282
There's a lot more to be said about training later on, 

358
00:21:43,282 --> 00:21:45,090
but I just want to call that out right now.

359
00:21:45,730 --> 00:21:49,690
This matrix is called the Unembedding matrix and we give it the label WU.

360
00:21:50,210 --> 00:21:53,620
Again, like all the weight matrices we see, its entries begin at random, 

361
00:21:53,620 --> 00:21:55,910
but they are learned during the training process.

362
00:21:56,470 --> 00:21:59,496
Keeping score on our total parameter count, this Unembedding 

363
00:21:59,496 --> 00:22:02,077
matrix has one row for each word in the vocabulary, 

364
00:22:02,077 --> 00:22:05,650
and each row has the same number of elements as the embedding dimension.

365
00:22:06,410 --> 00:22:10,436
It's very similar to the embedding matrix, just with the order swapped, 

366
00:22:10,436 --> 00:22:13,680
so it adds another 617 million parameters to the network, 

367
00:22:13,680 --> 00:22:16,644
meaning our count so far is a little over a billion, 

368
00:22:16,644 --> 00:22:20,279
a small but not wholly insignificant fraction of the 175 billion 

369
00:22:20,279 --> 00:22:21,790
we'll end up with in total.

370
00:22:22,550 --> 00:22:26,367
As the last mini-lesson for this chapter, I want to talk more about this softmax 

371
00:22:26,367 --> 00:22:30,610
function, since it makes another appearance for us once we dive into the attention blocks.

372
00:22:31,430 --> 00:22:36,612
The idea is that if you want a sequence of numbers to act as a probability distribution, 

373
00:22:36,612 --> 00:22:39,465
say a distribution over all possible next words, 

374
00:22:39,465 --> 00:22:44,590
then each value has to be between 0 and 1, and you also need all of them to add up to 1.

375
00:22:45,250 --> 00:22:49,802
However, if you're playing the learning game where everything you do looks like 

376
00:22:49,802 --> 00:22:54,810
matrix-vector multiplication, the outputs you get by default don't abide by this at all.

377
00:22:55,330 --> 00:22:57,831
The values are often negative, or much bigger than 1, 

378
00:22:57,831 --> 00:22:59,870
and they almost certainly don't add up to 1.

379
00:23:00,510 --> 00:23:04,085
Softmax is the standard way to turn an arbitrary list of numbers 

380
00:23:04,085 --> 00:23:08,760
into a valid distribution in such a way that the largest values end up closest to 1, 

381
00:23:08,760 --> 00:23:11,290
and the smaller values end up very close to 0.

382
00:23:11,830 --> 00:23:13,070
That's all you really need to know.

383
00:23:13,090 --> 00:23:17,185
But if you're curious, the way it works is to first raise e to the power 

384
00:23:17,185 --> 00:23:21,448
of each of the numbers, which means you now have a list of positive values, 

385
00:23:21,448 --> 00:23:25,655
and then you can take the sum of all those positive values and divide each 

386
00:23:25,655 --> 00:23:29,470
term by that sum, which normalizes it into a list that adds up to 1.

387
00:23:30,170 --> 00:23:34,140
You'll notice that if one of the numbers in the input is meaningfully bigger than 

388
00:23:34,140 --> 00:23:38,014
the rest, then in the output the corresponding term dominates the distribution, 

389
00:23:38,014 --> 00:23:42,179
so if you were sampling from it you'd almost certainly just be picking the maximizing 

390
00:23:42,179 --> 00:23:42,470
input.

391
00:23:42,990 --> 00:23:46,685
But it's softer than just picking the max in the sense that when other 

392
00:23:46,685 --> 00:23:50,902
values are similarly large, they also get meaningful weight in the distribution, 

393
00:23:50,902 --> 00:23:54,650
and everything changes continuously as you continuously vary the inputs.

394
00:23:55,130 --> 00:24:00,039
In some situations, like when ChatGPT is using this distribution to create a next word, 

395
00:24:00,039 --> 00:24:04,725
there's room for a little bit of extra fun by adding a little extra spice into this 

396
00:24:04,725 --> 00:24:08,910
function, with a constant t thrown into the denominator of those exponents.

397
00:24:09,550 --> 00:24:14,055
We call it the temperature, since it vaguely resembles the role of temperature in 

398
00:24:14,055 --> 00:24:18,175
certain thermodynamics equations, and the effect is that when t is larger, 

399
00:24:18,175 --> 00:24:22,735
you give more weight to the lower values, meaning the distribution is a little bit 

400
00:24:22,735 --> 00:24:26,966
more uniform, and if t is smaller, then the bigger values will dominate more 

401
00:24:26,966 --> 00:24:31,581
aggressively, where in the extreme, setting t equal to zero means all of the weight 

402
00:24:31,581 --> 00:24:32,790
goes to maximum value.

403
00:24:33,470 --> 00:24:37,785
For example, I'll have GPT-3 generate a story with the seed text, 

404
00:24:37,785 --> 00:24:42,950
once upon a time there was A, but I'll use different temperatures in each case.

405
00:24:43,630 --> 00:24:48,345
Temperature zero means that it always goes with the most predictable word, 

406
00:24:48,345 --> 00:24:52,370
and what you get ends up being a trite derivative of Goldilocks.

407
00:24:53,010 --> 00:24:56,592
A higher temperature gives it a chance to choose less likely words, 

408
00:24:56,592 --> 00:24:57,910
but it comes with a risk.

409
00:24:58,230 --> 00:25:01,204
In this case, the story starts out more originally, 

410
00:25:01,204 --> 00:25:06,010
about a young web artist from South Korea, but it quickly degenerates into nonsense.

411
00:25:06,950 --> 00:25:10,830
Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.

412
00:25:11,170 --> 00:25:15,387
There's no mathematical reason for this, it's just an arbitrary constraint imposed 

413
00:25:15,387 --> 00:25:19,350
to keep their tool from being seen generating things that are too nonsensical.

414
00:25:19,870 --> 00:25:24,289
So if you're curious, the way this animation is actually working is I'm taking the 

415
00:25:24,289 --> 00:25:27,005
20 most probable next tokens that GPT-3 generates, 

416
00:25:27,005 --> 00:25:29,508
which seems to be the maximum they'll give me, 

417
00:25:29,508 --> 00:25:32,970
and then I tweak the probabilities based on an exponent of 1 5th.

418
00:25:33,130 --> 00:25:37,488
As another bit of jargon, in the same way that you might call the components of 

419
00:25:37,488 --> 00:25:42,227
the output of this function probabilities, people often refer to the inputs as logits, 

420
00:25:42,227 --> 00:25:46,150
or some people say logits, some people say logits, I'm gonna say logits.

421
00:25:46,530 --> 00:25:50,466
So for instance, when you feed in some text, you have all these word embeddings 

422
00:25:50,466 --> 00:25:54,009
flow through the network, and you do this final multiplication with the 

423
00:25:54,009 --> 00:25:58,290
unembedding matrix, machine learning people would refer to the components in that raw, 

424
00:25:58,290 --> 00:26:01,390
unnormalized output as the logits for the next word prediction.

425
00:26:03,330 --> 00:26:06,747
A lot of the goal with this chapter was to lay the foundations for 

426
00:26:06,747 --> 00:26:10,370
understanding the attention mechanism, Karate Kid wax-on-wax-off style.

427
00:26:10,850 --> 00:26:14,944
You see, if you have a strong intuition for word embeddings, for softmax, 

428
00:26:14,944 --> 00:26:19,261
for how dot products measure similarity, and also the underlying premise that 

429
00:26:19,261 --> 00:26:23,632
most of the calculations have to look like matrix multiplication with matrices 

430
00:26:23,632 --> 00:26:27,617
full of tunable parameters, then understanding the attention mechanism, 

431
00:26:27,617 --> 00:26:32,210
this cornerstone piece in the whole modern boom in AI, should be relatively smooth.

432
00:26:32,650 --> 00:26:34,510
For that, come join me in the next chapter.

433
00:26:36,390 --> 00:26:38,970
As I'm publishing this, a draft of that next chapter 

434
00:26:38,970 --> 00:26:41,210
is available for review by Patreon supporters.

435
00:26:41,770 --> 00:26:44,283
A final version should be up in public in a week or two, 

436
00:26:44,283 --> 00:26:47,370
it usually depends on how much I end up changing based on that review.

437
00:26:47,810 --> 00:26:49,744
In the meantime, if you want to dive into attention, 

438
00:26:49,744 --> 00:26:52,410
and if you want to help the channel out a little bit, it's there waiting.

