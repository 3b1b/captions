[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Die Initialen GPT stehen für \"Generative Pretrained Transformer\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Das erste Wort ist also ganz einfach: Das sind Bots, die neuen Text generieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Pretrained bezieht sich darauf, dass das Modell einen Lernprozess aus einer riesigen Datenmenge durchlaufen hat, und das Präfix deutet an, dass es mehr Spielraum gibt, um es durch zusätzliches Training auf bestimmte Aufgaben abzustimmen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Aber das letzte Wort, das ist der eigentliche Schlüsselteil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Ein Transformator ist eine bestimmte Art von neuronalem Netzwerk, ein maschinelles Lernmodell, und es ist die zentrale Erfindung, die dem aktuellen Boom der KI zugrunde liegt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "In diesem Video und den folgenden Kapiteln möchte ich dir auf visuelle Weise erklären, was im Inneren eines Transformators passiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Wir werden den Daten folgen, die durch sie fließen, und Schritt für Schritt vorgehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Es gibt viele verschiedene Arten von Modellen, die du mit Transformatoren bauen kannst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Einige Modelle nehmen Audiodaten auf und erstellen eine Abschrift.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Dieser Satz stammt von einem Modell, das den umgekehrten Weg geht und synthetische Sprache nur aus Text erzeugt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "All die Tools, die 2022 die Welt im Sturm eroberten, wie Dolly und Midjourney, die eine Textbeschreibung aufnehmen und ein Bild erzeugen, basieren auf Transformatoren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Auch wenn ich es nicht ganz dazu bringe, zu verstehen, was ein Kuchenwesen sein soll, bin ich immer noch begeistert, dass so etwas überhaupt möglich ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "Und der ursprüngliche Transformator, der 2017 von Google eingeführt wurde, wurde für den speziellen Anwendungsfall der Übersetzung von Text von einer Sprache in eine andere erfunden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Aber die Variante, auf die du und ich uns konzentrieren werden und die Tools wie ChatGPT zugrunde liegt, ist ein Modell, das darauf trainiert ist, einen Text zu lesen, vielleicht sogar mit Bildern oder Geräuschen, die ihn umgeben, und eine Vorhersage darüber zu machen, was als Nächstes in dem Text kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Diese Vorhersage hat die Form einer Wahrscheinlichkeitsverteilung über viele verschiedene Textabschnitte, die folgen könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "Auf den ersten Blick könnte man meinen, dass das Vorhersagen des nächsten Wortes ein ganz anderes Ziel ist als das Erstellen von neuem Text.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Wenn du ein solches Vorhersagemodell hast, kannst du einen längeren Text ganz einfach generieren, indem du dem Modell einen ersten Textausschnitt gibst, es eine Zufallsstichprobe aus der soeben erstellten Verteilung nehmen lässt, diese an den Text anhängt und dann den gesamten Prozess noch einmal durchlaufen lässt, um eine neue Vorhersage auf der Grundlage des gesamten neuen Textes zu treffen, einschließlich dessen, was es gerade hinzugefügt hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Ich weiß nicht, wie es dir geht, aber es fühlt sich wirklich nicht so an, als ob das wirklich funktionieren sollte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "In dieser Animation lasse ich zum Beispiel GPT-2 auf meinem Laptop laufen und lasse es wiederholt den nächsten Textabschnitt vorhersagen und abtasten, um eine Geschichte auf der Grundlage des Starttextes zu erstellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "Die Geschichte macht einfach nicht wirklich viel Sinn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Aber wenn ich stattdessen API-Aufrufe zu GPT-3 einsetze, was das gleiche Grundmodell ist, nur viel größer, bekommen wir plötzlich auf fast magische Weise eine vernünftige Geschichte, die sogar darauf schließen lässt, dass ein Pi-Wesen in einem Land der Mathematik und des Rechnens lebt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Dieser Prozess der wiederholten Vorhersage und des Samplings ist im Wesentlichen das, was passiert, wenn du mit ChatGPT oder einem anderen großen Sprachmodell interagierst und siehst, wie sie ein Wort nach dem anderen produzieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Eine Funktion, die mir sehr gefallen würde, ist die Möglichkeit, die zugrundeliegende Verteilung für jedes neu gewählte Wort zu sehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Beginnen wir mit einem Überblick darüber, wie Daten durch einen Transformer fließen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Wir werden noch viel mehr Zeit darauf verwenden, die Details der einzelnen Schritte zu erklären und zu interpretieren, aber in groben Zügen, wenn einer dieser Chatbots ein bestimmtes Wort generiert, passiert folgendes unter der Haube.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Zuerst wird der Input in viele kleine Teile zerlegt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Diese Teile werden als Token bezeichnet, und im Fall von Text sind das in der Regel Wörter oder kleine Teile von Wörtern oder andere gängige Zeichenkombinationen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Wenn es sich um Bilder oder Töne handelt, können die Token kleine Teile des Bildes oder kleine Teile des Tons sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Jedes dieser Token wird dann mit einem Vektor verknüpft, also einer Liste von Zahlen, die die Bedeutung des Stücks irgendwie kodieren sollen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Wenn du dir diese Vektoren als Koordinaten in einem sehr hochdimensionalen Raum vorstellst, landen Wörter mit ähnlichen Bedeutungen meist auf Vektoren, die in diesem Raum nahe beieinander liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Diese Abfolge von Vektoren durchläuft dann eine Operation, die als Aufmerksamkeitsblock bezeichnet wird und die es den Vektoren ermöglicht, miteinander zu kommunizieren und Informationen hin- und herzugeben, um ihre Werte zu aktualisieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Die Bedeutung des Wortes Modell in der Formulierung Modell für maschinelles Lernen unterscheidet sich zum Beispiel von der Bedeutung in der Formulierung Modell für Mode.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Der Aufmerksamkeitsblock ist dafür verantwortlich, herauszufinden, welche Wörter im Kontext relevant sind, um die Bedeutungen der anderen Wörter zu aktualisieren, und wie genau diese Bedeutungen aktualisiert werden sollten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "Und noch einmal: Wenn ich das Wort Bedeutung verwende, ist diese irgendwie vollständig in den Einträgen dieser Vektoren kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Danach durchlaufen diese Vektoren eine andere Art von Operation. Je nachdem, welche Quelle du liest, wird dies als mehrschichtiges Perzeptron oder vielleicht als Feed-Forward-Schicht bezeichnet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "Und hier reden die Vektoren nicht miteinander, sondern sie durchlaufen alle parallel dieselbe Operation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Auch wenn dieser Block etwas schwieriger zu interpretieren ist, werden wir später darüber sprechen, dass der Schritt ein bisschen so ist, als ob man eine lange Liste von Fragen zu jedem Vektor stellt und sie dann auf der Grundlage der Antworten auf diese Fragen aktualisiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Alle Operationen in diesen beiden Blöcken sehen aus wie ein riesiger Haufen von Matrizenmultiplikationen, und unsere Hauptaufgabe besteht darin, zu verstehen, wie man die zugrunde liegenden Matrizen liest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Ich beschönige ein paar Details zu einigen Normalisierungsschritten, die zwischendurch stattfinden, aber das ist ja nur eine Vorschau auf hohem Niveau.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Danach wiederholt sich der Prozess im Wesentlichen, du wechselst zwischen Aufmerksamkeitsblöcken und Mehrschicht-Perzeptron-Blöcken hin und her, bis am Ende die Hoffnung besteht, dass alle wesentlichen Bedeutungen der Passage irgendwie in den allerletzten Vektor der Sequenz eingeflossen sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Dann führen wir eine bestimmte Operation an diesem letzten Vektor durch, die eine Wahrscheinlichkeitsverteilung über alle möglichen Token, also alle möglichen kleinen Textabschnitte, die als nächstes kommen könnten, ergibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "Und wie gesagt, sobald du ein Tool hast, das aus einem Textausschnitt vorhersagen kann, was als Nächstes kommt, kannst du es mit einem kleinen Teil des Ausgangstextes füttern und es immer wieder dieses Spiel spielen lassen, bei dem es vorhersagt, was als Nächstes kommt, Stichproben aus der Verteilung nimmt, sie anfügt und dann immer wieder wiederholt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Einige von euch erinnern sich vielleicht noch daran, wie lange bevor ChatGPT aufkam, frühe Demos von GPT-3 aussahen: Du konntest Geschichten und Aufsätze auf der Grundlage eines ersten Snippets automatisch vervollständigen lassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Um ein solches Tool in einen Chatbot zu verwandeln, ist es am einfachsten, mit einem kleinen Text zu beginnen, der den Rahmen für die Interaktion eines Nutzers mit einem hilfreichen KI-Assistenten absteckt, dem sogenannten Systemprompt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Es gäbe noch mehr zu sagen über den Schritt der Ausbildung, der nötig ist, damit das gut funktioniert, aber im Großen und Ganzen ist das die Idee.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "In diesem Kapitel werden wir uns näher mit den Details befassen, was am Anfang und am Ende des Netzes passiert. Außerdem möchte ich einige wichtige Hintergrundinformationen wiederholen, die jedem Ingenieur für maschinelles Lernen spätestens seit der Entwicklung von Transformers ein Begriff sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Wenn du mit diesem Hintergrundwissen gut zurechtkommst und ein bisschen ungeduldig bist, kannst du das nächste Kapitel überspringen, in dem es um die Aufmerksamkeitsblöcke geht, die allgemein als das Herzstück des Transformators gelten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Danach möchte ich mehr über diese mehrschichtigen Perzeptron-Blöcke, die Funktionsweise des Trainings und eine Reihe anderer Details erzählen, die wir bis zu diesem Punkt übersprungen haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Diese Videos sind Teil einer Miniserie über Deep Learning und es ist nicht schlimm, wenn du die vorherigen Videos noch nicht gesehen hast. Ich denke, du kannst sie auch in umgekehrter Reihenfolge ansehen, aber bevor wir uns mit den Transformers beschäftigen, sollten wir uns vergewissern, dass wir über die Grundvoraussetzungen und die Struktur von Deep Learning auf dem gleichen Stand sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Auch auf die Gefahr hin, das Offensichtliche zu sagen, ist dies ein Ansatz für maschinelles Lernen, der jedes Modell beschreibt, bei dem du Daten verwendest, um irgendwie zu bestimmen, wie sich ein Modell verhält.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Was ich damit meine, ist, dass du eine Funktion brauchst, die ein Bild aufnimmt und eine Beschriftung erzeugt, die es beschreibt, oder unser Beispiel, das nächste Wort in einem Text vorherzusagen, oder jede andere Aufgabe, die ein gewisses Element von Intuition und Mustererkennung erfordert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Die Idee des maschinellen Lernens ist, dass man nicht versucht, ein Verfahren für diese Aufgabe explizit im Code zu definieren, wie es in den Anfängen der KI der Fall war, sondern dass man eine sehr flexible Struktur mit einstellbaren Parametern aufbaut, wie eine Reihe von Knöpfen und Reglern, und dass man dann anhand von vielen Beispielen, die zeigen, wie die Ausgabe für eine bestimmte Eingabe aussehen sollte, die Werte dieser Parameter optimiert, um dieses Verhalten zu imitieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Die einfachste Form des maschinellen Lernens ist zum Beispiel die lineare Regression, bei der die Eingaben und Ausgaben jeweils einzelne Zahlen sind, zum Beispiel die Quadratmeterzahl eines Hauses und der Preis, und du willst eine Linie finden, die sich am besten an diese Daten anpasst, um zukünftige Hauspreise vorherzusagen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Diese Linie wird durch zwei kontinuierliche Parameter beschrieben, z. B. die Steigung und den y-Achsenabschnitt, und das Ziel der linearen Regression ist es, diese Parameter so zu bestimmen, dass sie genau zu den Daten passen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Unnötig zu erwähnen, dass Deep Learning-Modelle viel komplizierter werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "GPT-3 hat zum Beispiel nicht zwei, sondern 175 Milliarden Parameter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Aber die Sache ist die: Es ist nicht selbstverständlich, dass du ein riesiges Modell mit einer riesigen Anzahl von Parametern erstellen kannst, ohne dass es die Trainingsdaten übermäßig gut anpasst oder völlig unpraktisch zu trainieren ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "Deep Learning beschreibt eine Klasse von Modellen, die sich in den letzten Jahrzehnten als bemerkenswert gut skalierbar erwiesen haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Was sie eint, ist derselbe Trainingsalgorithmus, der Backpropagation genannt wird. Damit dieser Trainingsalgorithmus in großem Maßstab gut funktioniert, müssen diese Modelle einem bestimmten Format folgen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Wenn du dieses Format kennst, hilft es dir, viele der Entscheidungen zu erklären, wie ein Transformator die Sprache verarbeitet, die sonst willkürlich wirken könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Erstens: Egal, welches Modell du erstellst, die Eingabe muss als Array mit reellen Zahlen formatiert werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Das kann eine Liste von Zahlen sein, ein zweidimensionales Array oder sehr oft handelt es sich um höherdimensionale Arrays, für die der allgemeine Begriff \"Tensor\" verwendet wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Du stellst dir oft vor, dass die Eingabedaten nach und nach in viele verschiedene Schichten umgewandelt werden, wobei jede Schicht immer als eine Art Array aus reellen Zahlen strukturiert ist, bis du zu einer letzten Schicht gelangst, die du als Ausgabe betrachtest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Die letzte Schicht in unserem Textverarbeitungsmodell ist zum Beispiel eine Liste von Zahlen, die die Wahrscheinlichkeitsverteilung für alle möglichen nächsten Token darstellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "Beim Deep Learning werden diese Modellparameter fast immer als Gewichte bezeichnet, denn ein wesentliches Merkmal dieser Modelle ist, dass diese Parameter nur durch gewichtete Summen mit den zu verarbeitenden Daten interagieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Du streust auch einige nicht-lineare Funktionen ein, die aber nicht von Parametern abhängig sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "In der Regel werden die gewichteten Summen aber nicht nackt und explizit ausgeschrieben, sondern als verschiedene Komponenten in einem Matrix-Vektorprodukt zusammengefasst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Es läuft auf dasselbe hinaus, wenn du daran denkst, wie eine Matrix-Vektor-Multiplikation funktioniert: Jede Komponente des Ergebnisses sieht wie eine gewichtete Summe aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Für dich und mich ist es einfach konzeptionell klarer, über Matrizen nachzudenken, die mit einstellbaren Parametern gefüllt sind und Vektoren transformieren, die aus den zu verarbeitenden Daten gezogen werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Zum Beispiel sind die 175 Milliarden Gewichte in GPT-3 in knapp 28.000 verschiedenen Matrizen organisiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Diese Matrizen werden wiederum in acht verschiedene Kategorien eingeteilt, und wir werden jede dieser Kategorien durchgehen, um zu verstehen, was der jeweilige Typ macht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Während wir das durchgehen, macht es Spaß, die spezifischen Zahlen von GPT-3 heranzuziehen und genau zu zählen, woher diese 175 Milliarden kommen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Auch wenn es heutzutage größere und bessere Modelle gibt, hat dieses Modell einen gewissen Charme als das Modell für große Sprachen, das die Aufmerksamkeit der Welt außerhalb der ML-Gemeinschaften auf sich zieht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Außerdem neigen die Unternehmen dazu, die genauen Zahlen für modernere Netze zu verschweigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Wenn du unter die Haube schaust, um zu sehen, was in einem Tool wie ChatGPT passiert, sieht fast die gesamte eigentliche Berechnung wie eine Matrix-Vektor-Multiplikation aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Es besteht die Gefahr, dass du dich im Meer der Milliarden von Zahlen verlierst, aber du solltest in deinem Kopf eine klare Unterscheidung treffen zwischen den Gewichten des Modells, die ich immer blau oder rot einfärbe, und den zu verarbeitenden Daten, die ich immer grau einfärbe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Die Gewichte sind die eigentlichen Gehirne, sie sind die Dinge, die beim Training gelernt wurden, und sie bestimmen, wie es sich verhält.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Die zu verarbeitenden Daten kodieren einfach die spezifischen Eingaben, die für einen bestimmten Lauf in das Modell eingegeben werden, wie zum Beispiel einen Textausschnitt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Mit all dem als Grundlage wollen wir uns nun dem ersten Schritt dieses Textverarbeitungsbeispiels widmen, nämlich der Zerlegung der Eingabe in kleine Teile und der Umwandlung dieser Teile in Vektoren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Ich habe schon erwähnt, dass diese Stücke Token genannt werden, die aus Wörtern oder Satzzeichen bestehen können, aber ab und zu möchte ich in diesem und vor allem im nächsten Kapitel so tun, als ob sie sauberer in Wörter unterteilt sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Da wir Menschen in Worten denken, wird es dadurch viel einfacher, kleine Beispiele anzuführen und jeden Schritt zu verdeutlichen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Das Modell hat ein vordefiniertes Vokabular, eine Liste mit allen möglichen Wörtern, z.B. 50.000, und die erste Matrix, die wir kennenlernen werden, die so genannte Einbettungsmatrix, hat eine einzelne Spalte für jedes dieser Wörter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Diese Spalten bestimmen, in welchen Vektor sich jedes Wort in diesem ersten Schritt verwandelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Wir bezeichnen sie als Wir, und wie alle Matrizen, die wir sehen, beginnen ihre Werte zufällig, aber sie werden anhand von Daten gelernt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Die Umwandlung von Wörtern in Vektoren war beim maschinellen Lernen schon lange vor den Transformatoren üblich, aber es ist etwas seltsam, wenn du es noch nie gesehen hast, und es bildet die Grundlage für alles, was folgt, also lass uns einen Moment Zeit, um uns damit vertraut zu machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Wir nennen diese Einbettung oft ein Wort, das dich dazu einlädt, dir diese Vektoren ganz geometrisch als Punkte in einem hochdimensionalen Raum vorzustellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Eine Liste von drei Zahlen als Koordinaten für Punkte im 3D-Raum zu visualisieren, wäre kein Problem, aber Worteinbettungen sind in der Regel sehr viel höher dimensioniert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "In GPT-3 haben sie 12.288 Dimensionen, und wie du sehen wirst, ist es wichtig, in einem Raum zu arbeiten, der viele verschiedene Richtungen hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Genauso wie du einen zweidimensionalen Schnitt durch einen 3D-Raum nehmen und alle Punkte auf diesen Schnitt projizieren könntest, werde ich, um die Worteinbettungen, die mir ein einfaches Modell liefert, zu animieren, einen dreidimensionalen Schnitt durch diesen sehr hochdimensionalen Raum wählen und die Wortvektoren darauf projizieren und die Ergebnisse anzeigen lassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "Die Idee dahinter ist, dass ein Modell, das seine Gewichte optimiert, um zu bestimmen, wie genau Wörter während des Trainings als Vektoren eingebettet werden, dazu neigt, sich auf eine Reihe von Einbettungen festzulegen, bei denen die Richtungen im Raum eine Art semantische Bedeutung haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Wenn ich für das einfache Wort-Vektor-Modell, das ich hier verwende, nach allen Wörtern suche, deren Einbettung der von Turm am nächsten kommt, wirst du feststellen, dass sie alle sehr ähnlich aussehen wie ein Turm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Und wenn du zu Hause mit Python nachspielen willst, ist das das Modell, das ich für die Animationen verwende.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Es ist kein Transformator, aber es reicht aus, um die Idee zu veranschaulichen, dass Richtungen im Raum eine semantische Bedeutung haben können.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Ein sehr klassisches Beispiel dafür ist, dass der Unterschied zwischen den Vektoren für Frau und Mann, den du dir als kleinen Vektor vorstellen kannst, der die Spitze des einen mit der Spitze des anderen verbindet, dem Unterschied zwischen König und Königin sehr ähnlich ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Angenommen, du kennst das Wort für einen weiblichen Monarchen nicht, dann könntest du es finden, indem du König nimmst, diese Frau-Mann-Richtung hinzufügst und nach den Einbettungen suchst, die diesem Punkt am nächsten sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Zumindest so ähnlich.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Obwohl dies ein klassisches Beispiel für das Modell ist, mit dem ich spiele, ist die tatsächliche Einbettung von Königin ein wenig weiter entfernt, als dies vermuten lässt, vermutlich weil die Art und Weise, wie Königin in den Trainingsdaten verwendet wird, nicht nur eine weibliche Version von König ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Als ich ein wenig herumspielte, schienen mir die Familienbeziehungen die Idee viel besser zu veranschaulichen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Es sieht so aus, als hätte das Modell beim Training einen Vorteil darin gesehen, die Einbettungen so zu wählen, dass eine Richtung in diesem Raum die Geschlechtsinformation kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Ein anderes Beispiel: Wenn du die Einbettung Italiens nimmst, die Einbettung Deutschlands abziehst und diese zur Einbettung Hitlers addierst, erhältst du etwas, das der Einbettung Mussolinis sehr ähnlich ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Es ist, als ob das Modell gelernt hätte, einige Richtungen mit dem Italienertum und andere mit den Achsenführern des Zweiten Weltkriegs zu assoziieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Mein Lieblingsbeispiel in diesem Zusammenhang ist vielleicht, dass der Unterschied zwischen Deutschland und Japan in einigen Modellen, wenn man ihn zu Sushi hinzufügt, der Bratwurst sehr nahe kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Auch bei diesem Spiel, bei dem es darum geht, die nächsten Nachbarn zu finden, war ich erfreut zu sehen, wie nah Kat sowohl dem Tier als auch dem Monster war.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Eine mathematische Intuition, die du dir vor allem für das nächste Kapitel merken solltest, ist, dass das Punktprodukt zweier Vektoren ein Maß dafür ist, wie gut sie zusammenpassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Bei Punktprodukten werden alle entsprechenden Komponenten multipliziert und die Ergebnisse dann addiert. Das ist gut, denn ein Großteil unserer Berechnungen muss wie gewichtete Summen aussehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geometrisch gesehen ist das Punktprodukt positiv, wenn Vektoren in ähnliche Richtungen zeigen, es ist null, wenn sie senkrecht zueinander stehen, und es ist negativ, wenn sie in entgegengesetzte Richtungen zeigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Nehmen wir zum Beispiel an, du spielst mit diesem Modell und stellst die Hypothese auf, dass die Einbettung von Katzen minus Katze eine Art Pluralitätsrichtung in diesem Raum darstellen könnte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Um das zu testen, werde ich diesen Vektor nehmen und sein Punktprodukt mit den Einbettungen bestimmter Singularnomen berechnen und es mit den Punktprodukten der entsprechenden Pluralnomen vergleichen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Wenn du damit herumspielst, wirst du feststellen, dass die Pluralform durchweg höhere Werte liefert als die Singularform, was darauf hindeutet, dass sie mehr in diese Richtung geht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Lustig ist auch, dass das Punktprodukt mit den Einbettungen der Wörter 1, 2, 3 usw. steigende Werte ergibt, so als ob wir quantitativ messen könnten, wie plural das Modell ein bestimmtes Wort findet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Auch hier wird anhand von Daten gelernt, wie die Wörter eingebettet werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Diese Einbettungsmatrix, deren Spalten uns sagen, was mit jedem Wort passiert, ist der erste Stapel von Gewichten in unserem Modell.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Wenn du die GPT-3-Zahlen verwendest, beträgt der Wortschatz 50.257, und auch hier handelt es sich technisch gesehen nicht um Wörter an sich, sondern um Token.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "Die Einbettungsdimension beträgt 12.288 und die Multiplikation dieser Werte zeigt, dass es sich um etwa 617 Millionen Gewichte handelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Fügen wir dies zu einer laufenden Rechnung hinzu und denken wir daran, dass wir am Ende auf 175 Milliarden kommen sollten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "Im Fall von Transformatoren solltest du die Vektoren in diesem Einbettungsraum nicht nur als einzelne Wörter betrachten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Zum einen kodieren sie auch Informationen über die Position des Wortes, worauf wir später noch zu sprechen kommen, aber noch wichtiger ist, dass sie die Fähigkeit haben, den Kontext zu erfassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Ein Vektor, der zum Beispiel als Einbettung des Wortes \"König\" begann, könnte nach und nach von verschiedenen Blöcken in diesem Netzwerk beeinflusst werden, so dass er am Ende in eine viel spezifischere und nuanciertere Richtung zeigt, die irgendwie kodiert, dass es sich um einen König handelt, der in Schottland lebte, der sein Amt nach der Ermordung des vorherigen Königs erlangt hatte und der in Shakespeare'scher Sprache beschrieben wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Denke über dein eigenes Verständnis eines bestimmten Wortes nach.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Die Bedeutung dieses Wortes wird eindeutig von der Umgebung beeinflusst, und manchmal gehört dazu auch der Kontext aus großer Entfernung. Wenn man also ein Modell entwickelt, das vorhersagen kann, welches Wort als nächstes kommt, muss man es irgendwie befähigen, den Kontext effizient zu berücksichtigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Um das klarzustellen: Wenn du in diesem ersten Schritt eine Reihe von Vektoren auf der Grundlage des Eingabetextes erstellst, wird jeder dieser Vektoren einfach aus der Einbettungsmatrix herausgezogen, so dass jeder von ihnen zunächst nur die Bedeutung eines einzelnen Wortes kodieren kann, ohne dass er von seiner Umgebung beeinflusst wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Aber du solltest dir vorstellen, dass das Hauptziel dieses Netzwerks, durch das es fließt, darin besteht, jedem dieser Vektoren eine Bedeutung zu geben, die viel reichhaltiger und spezifischer ist als das, was einzelne Wörter darstellen könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Das Netzwerk kann nur eine bestimmte Anzahl von Vektoren gleichzeitig verarbeiten, die so genannte Kontextgröße.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Für GPT-3 wurde es mit einer Kontextgröße von 2048 trainiert, sodass die Daten, die durch das Netz fließen, immer wie dieses Array aus 2048 Spalten aussehen, von denen jede 12.000 Dimensionen hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Diese Kontextgröße begrenzt, wie viel Text der Transformator bei der Vorhersage des nächsten Wortes einbeziehen kann.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Aus diesem Grund hatten lange Gespräche mit bestimmten Chatbots, wie die frühen Versionen von ChatGPT, oft das Gefühl, dass der Bot den Gesprächsfaden verliert, wenn du zu lange weitermachst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Wir werden zu gegebener Zeit auf die Details der Aufmerksamkeit eingehen, aber ich möchte kurz darüber sprechen, was ganz am Ende passiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Denk daran, dass die gewünschte Ausgabe eine Wahrscheinlichkeitsverteilung über alle Token ist, die als nächstes kommen könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Wenn das allerletzte Wort zum Beispiel Professor ist und der Kontext Wörter wie Harry Potter enthält, und direkt davor sehen wir den unbeliebtesten Lehrer, und wenn du mir etwas Spielraum gibst, indem du mir erlaubst, so zu tun, als ob die Token einfach wie ganze Wörter aussehen, dann würde ein gut trainiertes Netzwerk, das Wissen über Harry Potter aufgebaut hat, dem Wort Snape vermutlich eine hohe Zahl zuordnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Dies umfasst zwei verschiedene Schritte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Die erste besteht darin, eine andere Matrix zu verwenden, die den allerletzten Vektor in diesem Kontext auf eine Liste von 50.000 Werten abbildet, einen für jedes Token im Vokabular.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Dann gibt es eine Funktion, die diese in eine Wahrscheinlichkeitsverteilung normalisiert. Sie heißt Softmax und wir werden gleich noch mehr darüber sprechen, aber vorher mag es etwas seltsam erscheinen, nur diese letzte Einbettung für eine Vorhersage zu verwenden, wo es doch in diesem letzten Schritt tausende anderer Vektoren in der Schicht gibt, die ihre eigenen kontextreichen Bedeutungen haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Das hat damit zu tun, dass es sich im Trainingsprozess als viel effizienter erweist, wenn du jeden dieser Vektoren in der letzten Schicht nutzt, um gleichzeitig eine Vorhersage für das zu treffen, was unmittelbar danach kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Es gibt später noch viel mehr über die Ausbildung zu sagen, aber ich möchte das jetzt nur kurz erwähnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Diese Matrix wird Unembedding-Matrix genannt und wir geben ihr die Bezeichnung WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Wie alle anderen Gewichtungsmatrizen, die wir sehen, beginnen auch diese mit zufälligen Einträgen, die aber während des Trainings gelernt werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Diese Unembedding-Matrix hat eine Zeile für jedes Wort des Vokabulars und jede Zeile hat die gleiche Anzahl von Elementen wie die Einbettungsdimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Sie ist der Einbettungsmatrix sehr ähnlich, nur die Reihenfolge ist vertauscht. Das bedeutet, dass dem Netzwerk weitere 617 Millionen Parameter hinzugefügt werden, was bedeutet, dass wir bisher etwas mehr als eine Milliarde gezählt haben - ein kleiner, aber nicht ganz unbedeutender Teil der 175 Milliarden, die wir am Ende insgesamt haben werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "In der letzten Minilektion dieses Kapitels möchte ich mehr über die Softmax-Funktion sprechen, denn sie taucht noch einmal auf, wenn wir uns mit den Aufmerksamkeitsblöcken beschäftigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Wenn du willst, dass eine Zahlenfolge als Wahrscheinlichkeitsverteilung fungiert, z. B. eine Verteilung über alle möglichen nächsten Wörter, dann muss jeder Wert zwischen 0 und 1 liegen und alle Werte müssen sich zu 1 addieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Wenn du jedoch das Lernspiel spielst, bei dem alles, was du tust, wie eine Matrix-Vektor-Multiplikation aussieht, halten sich die Ausgaben, die du standardmäßig erhältst, überhaupt nicht daran.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Die Werte sind oft negativ oder viel größer als 1 und sie ergeben mit ziemlicher Sicherheit nicht die Summe 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax ist die Standardmethode, um eine beliebige Liste von Zahlen in eine gültige Verteilung umzuwandeln, und zwar so, dass die größten Werte möglichst nahe bei 1 und die kleineren Werte sehr nahe bei 0 enden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Das ist alles, was du wirklich wissen musst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Aber falls du neugierig bist: Es funktioniert so, dass du zuerst e mit jeder der Zahlen potenzierst, was bedeutet, dass du jetzt eine Liste positiver Werte hast. Dann kannst du die Summe all dieser positiven Werte nehmen und jeden Term durch diese Summe teilen, was sie zu einer Liste normalisiert, die sich zu 1 addiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Du wirst feststellen, dass, wenn eine der Zahlen in der Eingabe deutlich größer ist als der Rest, der entsprechende Term in der Ausgabe die Verteilung dominiert. Wenn du also eine Stichprobe ziehst, würdest du mit ziemlicher Sicherheit nur die maximierende Eingabe auswählen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Aber es ist weicher, als nur den Maximalwert zu wählen, denn wenn andere Werte ähnlich groß sind, bekommen sie auch ein sinnvolles Gewicht in der Verteilung, und alles ändert sich kontinuierlich, wenn du die Eingaben ständig variierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "In manchen Situationen, z. B. wenn ChatGPT diese Verteilung verwendet, um ein nächstes Wort zu bilden, kann man diese Funktion noch ein wenig aufpeppen, indem man eine Konstante t in den Nenner der Exponenten einfügt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Wir nennen sie die Temperatur, da sie vage an die Rolle der Temperatur in bestimmten Gleichungen der Thermodynamik erinnert. Der Effekt ist, dass, wenn t größer ist, du den niedrigeren Werten mehr Gewicht gibst, was bedeutet, dass die Verteilung etwas gleichmäßiger ist, und wenn t kleiner ist, dann dominieren die größeren Werte aggressiver, wobei im Extremfall, wenn t gleich Null ist, das gesamte Gewicht auf den maximalen Wert geht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Ich lasse GPT-3 zum Beispiel eine Geschichte mit dem Ausgangstext \"Es war einmal A\" erstellen, aber ich verwende jeweils unterschiedliche Temperaturen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Temperatur Null bedeutet, dass sie immer das vorhersehbarste Wort wählt, und das, was du bekommst, ist am Ende eine banale Ableitung von Goldlöckchen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Eine höhere Temperatur gibt ihm die Chance, weniger wahrscheinliche Wörter zu wählen, aber sie ist mit einem Risiko verbunden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "In diesem Fall fängt die Geschichte über einen jungen Webkünstler aus Südkorea eher originell an, aber sie artet schnell in Unsinn aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Technisch gesehen kannst du mit der API eigentlich keine höhere Temperatur als 2 wählen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Dafür gibt es keinen mathematischen Grund, es ist nur eine willkürliche Einschränkung, die sie auferlegt haben, um zu verhindern, dass ihr Werkzeug Dinge erzeugt, die zu unsinnig sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Wenn du also neugierig bist: Die Animation funktioniert so, dass ich die 20 wahrscheinlichsten nächsten Token nehme, die GPT-3 generiert, was das Maximum zu sein scheint, das sie mir geben, und dann die Wahrscheinlichkeiten mit einem Exponenten von 1,5 korrigiere.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "So wie du die Komponenten der Ausgabe dieser Funktion als Wahrscheinlichkeiten bezeichnest, bezeichnen die Leute die Eingaben oft als Logits, oder manche sagen Logits, manche sagen Logits, ich werde Logits sagen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Wenn du also zum Beispiel einen Text einspeist, all diese Worteinbettungen durch das Netzwerk fließen lässt und diese abschließende Multiplikation mit der Matrix ohne Einbettung durchführst, bezeichnen Menschen, die sich mit maschinellem Lernen beschäftigen, die Komponenten in dieser rohen, nicht normalisierten Ausgabe als die Logits für die nächste Wortvorhersage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "In diesem Kapitel ging es vor allem darum, die Grundlagen für das Verständnis des Aufmerksamkeitsmechanismus zu legen, im Stil von Karate Kid: Wachs-auf-Wachs-ab.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Wenn du ein gutes Gespür für Worteinbettungen, für Softmax und dafür hast, wie Punktprodukte Ähnlichkeit messen, und wenn du weißt, dass die meisten Berechnungen wie eine Matrixmultiplikation mit Matrizen voller einstellbarer Parameter aussehen müssen, dann sollte es relativ einfach sein, den Aufmerksamkeitsmechanismus zu verstehen, diesen Eckpfeiler des ganzen modernen KI-Booms.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Dafür kommst du im nächsten Kapitel zu mir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Während ich dies veröffentliche, steht ein Entwurf des nächsten Kapitels für Patreon-Unterstützer zur Ansicht bereit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Eine endgültige Version sollte in ein oder zwei Wochen veröffentlicht werden. Das hängt davon ab, wie viel ich aufgrund der Überprüfung ändere.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "In der Zwischenzeit, wenn du in die Aufmerksamkeit eintauchen und dem Kanal ein bisschen helfen willst, ist er da und wartet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]