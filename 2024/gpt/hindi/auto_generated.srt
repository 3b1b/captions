1
00:00:00,000 --> 00:00:04,560
प्रारंभिक GPT का मतलब जेनरेटिव प्रीट्रेन्ड ट्रांसफार्मर है।

2
00:00:05,220 --> 00:00:09,020
इसलिए पहला शब्द काफी सीधा है, ये बॉट हैं जो नया टेक्स्ट उत्पन्न करते हैं।

3
00:00:09,800 --> 00:00:13,146
पूर्व-प्रशिक्षित से तात्पर्य यह है कि मॉडल भारी मात्रा में डेटा से 

4
00:00:13,146 --> 00:00:16,643
सीखने की प्रक्रिया से कैसे गुजरा, और उपसर्ग संकेत देता है कि अतिरिक्त 

5
00:00:16,643 --> 00:00:20,040
प्रशिक्षण के साथ विशिष्ट कार्यों पर इसे ठीक करने के लिए अधिक जगह है।

6
00:00:20,720 --> 00:00:22,900
लेकिन आखिरी शब्द, यही असली कुंजी है।

7
00:00:23,380 --> 00:00:26,575
ट्रांसफार्मर एक विशिष्ट प्रकार का तंत्रिका नेटवर्क, 

8
00:00:26,575 --> 00:00:31,000
एक मशीन लर्निंग मॉडल है, और यह एआई में मौजूदा उछाल का मुख्य आविष्कार है।

9
00:00:31,740 --> 00:00:35,624
इस वीडियो और आगामी अध्यायों के माध्यम से मैं एक दृश्य-आधारित व्याख्या 

10
00:00:35,624 --> 00:00:39,120
करना चाहता हूँ कि ट्रांसफार्मर के अंदर वास्तव में क्या होता है।

11
00:00:39,700 --> 00:00:42,820
हम इसके माध्यम से प्रवाहित होने वाले डेटा का अनुसरण करेंगे और चरण दर चरण आगे बढ़ेंगे।

12
00:00:43,440 --> 00:00:47,380
ट्रांसफार्मरों का उपयोग करके आप कई प्रकार के मॉडल बना सकते हैं।

13
00:00:47,800 --> 00:00:50,800
कुछ मॉडल ऑडियो लेते हैं और एक प्रतिलेख तैयार करते हैं।

14
00:00:51,340 --> 00:00:53,967
यह वाक्य एक मॉडल से आता है जो दूसरी तरफ जाता है, 

15
00:00:53,967 --> 00:00:56,220
केवल पाठ से सिंथेटिक भाषण उत्पन्न करता है।

16
00:00:56,660 --> 00:01:01,292
डॉली और मिडजर्नी जैसे वे सभी उपकरण जिन्होंने 2022 में दुनिया में तूफान ला दिया, 

17
00:01:01,292 --> 00:01:05,519
जो एक पाठ विवरण लेते हैं और एक छवि बनाते हैं, ट्रांसफार्मर पर आधारित हैं।

18
00:01:06,000 --> 00:01:09,654
भले ही मैं यह समझ नहीं पा रहा हूं कि एक पाई प्राणी क्या माना जाता है, 

19
00:01:09,654 --> 00:01:13,100
फिर भी मैं इस बात से हैरान हूं कि इस तरह की चीज दूर से भी संभव है।

20
00:01:13,900 --> 00:01:17,947
और Google द्वारा 2017 में पेश किए गए मूल ट्रांसफार्मर का आविष्कार एक भाषा से 

21
00:01:17,947 --> 00:01:22,100
दूसरी भाषा में पाठ का अनुवाद करने के विशिष्ट उपयोग के मामले के लिए किया गया था।

22
00:01:22,660 --> 00:01:25,525
लेकिन जिस वैरिएंट पर आप और मैं ध्यान केंद्रित करेंगे, 

23
00:01:25,525 --> 00:01:29,345
जो कि चैटजीपीटी जैसे टूल का आधार है, वह एक ऐसा मॉडल होगा जिसे पाठ के एक 

24
00:01:29,345 --> 00:01:31,839
टुकड़े को लेने के लिए प्रशिक्षित किया जाता है, 

25
00:01:31,839 --> 00:01:34,492
शायद इसके साथ कुछ आसपास की छवियां या ध्वनि भी हो, 

26
00:01:34,492 --> 00:01:38,260
और एक भविष्यवाणी उत्पन्न करता है परिच्छेद में आगे क्या आता है इसके लिए।

27
00:01:38,600 --> 00:01:41,501
वह भविष्यवाणी पाठ के कई अलग-अलग हिस्सों पर संभाव्यता 

28
00:01:41,501 --> 00:01:43,800
वितरण का रूप लेती है जो अनुसरण कर सकती है।

29
00:01:45,040 --> 00:01:47,917
पहली नज़र में, आपको लग सकता है कि अगले शब्द की भविष्यवाणी करना, 

30
00:01:47,917 --> 00:01:49,940
नया टेक्स्ट तैयार करने से बहुत अलग लक्ष्य है।

31
00:01:50,180 --> 00:01:53,356
लेकिन एक बार जब आपके पास इस तरह का एक भविष्यवाणी मॉडल होता है, 

32
00:01:53,356 --> 00:01:56,482
तो एक साधारण चीज़ जो आप पाठ का एक लंबा टुकड़ा तैयार करते हैं, 

33
00:01:56,482 --> 00:01:59,154
वह है इसे काम करने के लिए एक प्रारंभिक स्निपेट देना, 

34
00:01:59,154 --> 00:02:03,137
इसे अभी उत्पन्न वितरण से एक यादृच्छिक नमूना लेना, उस नमूने को पाठ में जोड़ना , 

35
00:02:03,137 --> 00:02:07,674
और फिर सभी नए पाठ के आधार पर एक नई भविष्यवाणी करने के लिए पूरी प्रक्रिया को फिर से चलाएं, 

36
00:02:07,674 --> 00:02:09,539
जिसमें अभी जोड़ा गया पाठ भी शामिल है।

37
00:02:10,100 --> 00:02:11,550
मैं आपके बारे में नहीं जानता, लेकिन वास्तव में 

38
00:02:11,550 --> 00:02:13,000
ऐसा नहीं लगता कि इसे वास्तव में काम करना चाहिए।

39
00:02:13,420 --> 00:02:16,353
इस एनीमेशन में, उदाहरण के लिए, मैं अपने लैपटॉप पर जीपीटी-2 

40
00:02:16,353 --> 00:02:19,386
चला रहा हूं और बीज पाठ के आधार पर एक कहानी तैयार करने के लिए 

41
00:02:19,386 --> 00:02:22,420
पाठ के अगले हिस्से की बार-बार भविष्यवाणी और नमूना ले रहा हूं।

42
00:02:22,420 --> 00:02:26,120
कहानी वास्तव में उतनी अर्थपूर्ण नहीं है।

43
00:02:26,500 --> 00:02:31,413
लेकिन अगर मैं इसे एपीआई कॉल के लिए जीपीटी-3 में बदल दूं, जो कि एक ही मूल मॉडल है, 

44
00:02:31,413 --> 00:02:35,906
बस बहुत बड़ा है, तो अचानक लगभग जादुई रूप से हमें एक समझदार कहानी मिलती है, 

45
00:02:35,906 --> 00:02:40,880
जो यह भी अनुमान लगाती है कि एक पीआई प्राणी एक में रहता होगा गणित और संगणना की भूमि.

46
00:02:41,580 --> 00:02:44,899
बार-बार भविष्यवाणी और नमूने लेने की यह प्रक्रिया अनिवार्य रूप से तब 

47
00:02:44,899 --> 00:02:48,218
होती है जब आप चैटजीपीटी या इनमें से किसी अन्य बड़े भाषा मॉडल के साथ 

48
00:02:48,218 --> 00:02:51,880
बातचीत करते हैं और आप उन्हें एक समय में एक शब्द उत्पन्न करते हुए देखते हैं।

49
00:02:52,480 --> 00:02:55,283
वास्तव में, एक विशेषता जो मुझे बहुत पसंद आएगी, 

50
00:02:55,283 --> 00:02:59,220
वह है प्रत्येक नए शब्द के लिए अंतर्निहित वितरण को देखने की क्षमता।

51
00:03:03,820 --> 00:03:06,044
आइये, एक ट्रांसफार्मर के माध्यम से डेटा प्रवाह के 

52
00:03:06,044 --> 00:03:08,180
उच्च स्तरीय पूर्वावलोकन के साथ इसकी शुरुआत करें।

53
00:03:08,640 --> 00:03:11,949
हम प्रत्येक चरण के विवरण को प्रेरित करने, व्याख्या करने और विस्तार देने 

54
00:03:11,949 --> 00:03:14,109
में अधिक समय व्यतीत करेंगे, लेकिन मोटे तौर पर, 

55
00:03:14,109 --> 00:03:17,924
जब इनमें से कोई चैटबॉट कोई शब्द उत्पन्न करता है, तो उसके पीछे क्या चल रहा होता है, 

56
00:03:17,924 --> 00:03:18,660
यह बताया गया है।

57
00:03:19,080 --> 00:03:22,040
सबसे पहले, इनपुट को छोटे टुकड़ों के समूह में तोड़ दिया जाता है।

58
00:03:22,620 --> 00:03:26,160
इन टुकड़ों को टोकन कहा जाता है, और पाठ के मामले में ये शब्द 

59
00:03:26,160 --> 00:03:29,820
या शब्दों के छोटे टुकड़े या अन्य सामान्य वर्ण संयोजन होते हैं।

60
00:03:30,740 --> 00:03:33,813
यदि छवियाँ या ध्वनि शामिल है, तो टोकन उस छवि के 

61
00:03:33,813 --> 00:03:37,080
छोटे टुकड़े या उस ध्वनि के छोटे टुकड़े हो सकते हैं।

62
00:03:37,580 --> 00:03:40,413
इनमें से प्रत्येक टोकन फिर एक वेक्टर से जुड़ा होता है, 

63
00:03:40,413 --> 00:03:44,174
जिसका अर्थ है संख्याओं की कुछ सूची, जिसका उद्देश्य किसी तरह उस टुकड़े के 

64
00:03:44,174 --> 00:03:45,360
अर्थ को एन्कोड करना है।

65
00:03:45,880 --> 00:03:48,797
यदि आप इन सदिशों को किसी बहुत उच्च आयामी स्थान में निर्देशांक 

66
00:03:48,797 --> 00:03:51,715
देने वाले के रूप में सोचते हैं, तो समान अर्थ वाले शब्द प्रायः 

67
00:03:51,715 --> 00:03:54,680
उन सदिशों पर आते हैं जो उस स्थान में एक दूसरे के करीब होते हैं।

68
00:03:55,280 --> 00:03:58,367
इसके बाद सदिशों का यह क्रम एक ऑपरेशन से गुजरता है जिसे अटेंशन ब्लॉक के 

69
00:03:58,367 --> 00:04:01,281
रूप में जाना जाता है, और यह सदिशों को एक दूसरे से बात करने और अपने 

70
00:04:01,281 --> 00:04:04,500
मूल्यों को अद्यतन करने के लिए सूचनाओं को आगे-पीछे भेजने की अनुमति देता है।

71
00:04:04,880 --> 00:04:08,435
उदाहरण के लिए, मशीन लर्निंग मॉडल वाक्यांश में मॉडल शब्द 

72
00:04:08,435 --> 00:04:11,800
का अर्थ फैशन मॉडल वाक्यांश में इसके अर्थ से भिन्न है।

73
00:04:12,260 --> 00:04:15,395
ध्यान ब्लॉक वह है जो यह पता लगाने के लिए जिम्मेदार है कि संदर्भ 

74
00:04:15,395 --> 00:04:19,265
में कौन से शब्द किन अन्य शब्दों के अर्थों को अद्यतन करने के लिए प्रासंगिक हैं, 

75
00:04:19,265 --> 00:04:21,959
और वास्तव में उन अर्थों को कैसे अद्यतन किया जाना चाहिए।

76
00:04:22,500 --> 00:04:24,634
और फिर, जब भी मैं अर्थ शब्द का उपयोग करता हूं, 

77
00:04:24,634 --> 00:04:28,040
यह किसी तरह उन वैक्टरों की प्रविष्टियों में पूरी तरह से एन्कोड किया गया है।

78
00:04:29,180 --> 00:04:31,940
इसके बाद, ये वेक्टर एक अलग तरह के ऑपरेशन से गुजरते हैं, 

79
00:04:31,940 --> 00:04:34,848
और जिस स्रोत को आप पढ़ रहे हैं उसके आधार पर इसे मल्टी-लेयर 

80
00:04:34,848 --> 00:04:38,200
परसेप्ट्रॉन या शायद फीड-फॉरवर्ड लेयर के रूप में संदर्भित किया जाएगा।

81
00:04:38,580 --> 00:04:40,598
और यहां वेक्टर एक-दूसरे से बात नहीं करते हैं, 

82
00:04:40,598 --> 00:04:42,660
वे सभी समानांतर में एक ही ऑपरेशन से गुजरते हैं।

83
00:04:43,060 --> 00:04:46,655
और यद्यपि इस ब्लॉक को समझना थोड़ा कठिन है, बाद में हम इस बारे में बात 

84
00:04:46,655 --> 00:04:50,250
करेंगे कि यह चरण प्रत्येक वेक्टर के बारे में प्रश्नों की एक लंबी सूची 

85
00:04:50,250 --> 00:04:54,000
पूछने और फिर उन प्रश्नों के उत्तरों के आधार पर उन्हें अपडेट करने जैसा है।

86
00:04:54,900 --> 00:05:00,143
इन दोनों ब्लॉकों में सभी ऑपरेशन मैट्रिक्स गुणन के विशाल ढेर की तरह दिखते हैं, 

87
00:05:00,143 --> 00:05:05,320
और हमारा प्राथमिक काम यह समझना होगा कि अंतर्निहित मैट्रिसेस को कैसे पढ़ा जाए।

88
00:05:06,980 --> 00:05:10,810
मैं बीच में होने वाले कुछ सामान्यीकरण चरणों के बारे में कुछ विवरण नहीं दे रहा हूँ, 

89
00:05:10,810 --> 00:05:12,980
लेकिन यह आखिरकार एक उच्च-स्तरीय पूर्वावलोकन है।

90
00:05:13,680 --> 00:05:16,973
उसके बाद, प्रक्रिया अनिवार्य रूप से दोहराई जाती है, 

91
00:05:16,973 --> 00:05:22,039
आप ध्यान ब्लॉकों और मल्टी-लेयर परसेप्ट्रॉन ब्लॉकों के बीच आगे और पीछे जाते हैं, 

92
00:05:22,039 --> 00:05:27,043
जब तक कि अंत में आशा न हो कि मार्ग के सभी आवश्यक अर्थ किसी तरह से अंतिम वेक्टर 

93
00:05:27,043 --> 00:05:28,500
में बेक हो गए हैं क्रम।

94
00:05:28,920 --> 00:05:33,286
फिर हम उस अंतिम वेक्टर पर एक निश्चित ऑपरेशन करते हैं जो सभी संभावित टोकन, 

95
00:05:33,286 --> 00:05:38,420
पाठ के सभी संभावित छोटे हिस्सों पर एक संभाव्यता वितरण उत्पन्न करता है जो आगे आ सकता है।

96
00:05:38,980 --> 00:05:42,441
और जैसा कि मैंने कहा, एक बार आपके पास ऐसा उपकरण हो जो पाठ के एक टुकड़े के आधार पर 

97
00:05:42,441 --> 00:05:44,425
यह पूर्वानुमान लगा दे कि आगे क्या आने वाला है, 

98
00:05:44,425 --> 00:05:47,845
तो आप इसमें थोड़ा सा बीज पाठ डाल सकते हैं और इसे बार-बार यह पूर्वानुमान लगाने का 

99
00:05:47,845 --> 00:05:51,011
खेल खेलने दे सकते हैं कि आगे क्या आने वाला है, वितरण से नमूना ले सकते हैं, 

100
00:05:51,011 --> 00:05:53,080
उसे जोड़ सकते हैं, और फिर बार-बार दोहरा सकते हैं।

101
00:05:53,640 --> 00:05:58,162
आप में से कुछ लोगों को याद होगा कि चैटजीपीटी के दृश्य में आने से कितने समय पहले, 

102
00:05:58,162 --> 00:06:02,964
जीपीटी-3 के शुरुआती डेमो इस तरह दिखते थे, आपके पास प्रारंभिक स्निपेट के आधार पर स्वत: 

103
00:06:02,964 --> 00:06:04,640
पूर्ण कहानियां और निबंध होंगे।

104
00:06:05,580 --> 00:06:09,703
इस तरह के एक टूल को चैटबॉट में बनाने के लिए, सबसे आसान शुरुआती बिंदु थोड़ा सा 

105
00:06:09,703 --> 00:06:14,092
टेक्स्ट होना है जो एक सहायक एआई सहायक के साथ उपयोगकर्ता के इंटरैक्ट करने की सेटिंग 

106
00:06:14,092 --> 00:06:18,216
स्थापित करता है, जिसे आप सिस्टम प्रॉम्प्ट कहेंगे, और फिर आप इसका उपयोग करेंगे।

107
00:06:18,216 --> 00:06:21,970
 संवाद के पहले अंश के रूप में उपयोगकर्ता का प्रारंभिक प्रश्न या संकेत, 

108
00:06:21,970 --> 00:06:26,146
और फिर आपके पास यह भविष्यवाणी करना शुरू हो जाता है कि ऐसा सहायक एआई सहायक जवाब 

109
00:06:26,146 --> 00:06:26,940
में क्या कहेगा।

110
00:06:27,720 --> 00:06:30,875
इस कार्य को अच्छी तरह से करने के लिए आवश्यक प्रशिक्षण के चरण के बारे 

111
00:06:30,875 --> 00:06:33,940
में कहने के लिए और भी बहुत कुछ है, लेकिन उच्च स्तर पर यही विचार है।

112
00:06:35,720 --> 00:06:39,966
इस अध्याय में, आप और मैं नेटवर्क की शुरुआत में, नेटवर्क के अंत में क्या होता है, 

113
00:06:39,966 --> 00:06:44,160
इसके विवरण पर विस्तार करने जा रहे हैं, और मैं पृष्ठभूमि ज्ञान के कुछ महत्वपूर्ण 

114
00:06:44,160 --> 00:06:47,305
हिस्सों की समीक्षा करने में भी काफी समय बिताना चाहता हूं। , 

115
00:06:47,305 --> 00:06:51,341
ट्रांसफार्मर आने के समय तक ऐसी चीजें जो किसी भी मशीन लर्निंग इंजीनियर के लिए 

116
00:06:51,341 --> 00:06:52,600
दूसरी प्रकृति रही होंगी।

117
00:06:53,060 --> 00:06:56,005
यदि आप उस पृष्ठभूमि ज्ञान के साथ सहज हैं और थोड़े अधीर हैं, 

118
00:06:56,005 --> 00:07:00,423
तो आप बेझिझक अगले अध्याय पर जा सकते हैं, जो ध्यान ब्लॉकों पर ध्यान केंद्रित करने वाला है, 

119
00:07:00,423 --> 00:07:02,780
जिसे आम तौर पर ट्रांसफार्मर का दिल माना जाता है।

120
00:07:03,360 --> 00:07:07,746
उसके बाद मैं इन मल्टी-लेयर परसेप्ट्रॉन ब्लॉकों के बारे में और अधिक बात करना चाहता हूं, 

121
00:07:07,746 --> 00:07:11,680
प्रशिक्षण कैसे काम करता है, और कई अन्य विवरण जो उस बिंदु तक छोड़ दिए गए होंगे।

122
00:07:12,180 --> 00:07:16,323
व्यापक संदर्भ के लिए, ये वीडियो गहन शिक्षा के बारे में एक लघु-श्रृंखला के अतिरिक्त हैं, 

123
00:07:16,323 --> 00:07:18,584
और यह ठीक है यदि आपने पिछले वाले नहीं देखे हैं, 

124
00:07:18,584 --> 00:07:22,586
मुझे लगता है कि आप इसे क्रम से कर सकते हैं, लेकिन विशेष रूप से ट्रांसफार्मर में गोता 

125
00:07:22,586 --> 00:07:26,589
लगाने से पहले, मुझे लगता है यह सुनिश्चित करना ज़रूरी है कि गहन शिक्षण के मूल आधार और 

126
00:07:26,589 --> 00:07:28,520
संरचना के बारे में हम एक ही पृष्ठ पर हों।

127
00:07:29,020 --> 00:07:32,032
स्पष्ट बताने के जोखिम पर, यह मशीन लर्निंग का एक दृष्टिकोण है, 

128
00:07:32,032 --> 00:07:35,141
जो किसी भी मॉडल का वर्णन करता है जहां आप डेटा का उपयोग किसी तरह 

129
00:07:35,141 --> 00:07:38,300
यह निर्धारित करने के लिए कर रहे हैं कि मॉडल कैसे व्यवहार करता है।

130
00:07:39,140 --> 00:07:42,515
मेरे कहने का मतलब यह है कि, मान लें कि आप एक ऐसा फ़ंक्शन चाहते हैं जो एक 

131
00:07:42,515 --> 00:07:45,382
छवि लेता है और यह उसका वर्णन करने वाला एक लेबल तैयार करता है, 

132
00:07:45,382 --> 00:07:48,896
या पाठ के एक अंश को देखते हुए अगले शब्द की भविष्यवाणी करने का हमारा उदाहरण, 

133
00:07:48,896 --> 00:07:52,780
या कोई अन्य कार्य जिसमें कुछ तत्व की आवश्यकता होती है अंतर्ज्ञान और पैटर्न पहचान की।

134
00:07:53,200 --> 00:07:57,558
इन दिनों हम इसे लगभग मान लेते हैं, लेकिन मशीन लर्निंग के साथ विचार यह है कि कोड में उस 

135
00:07:57,558 --> 00:08:01,966
कार्य को करने के लिए एक प्रक्रिया को स्पष्ट रूप से परिभाषित करने की कोशिश करने के बजाय, 

136
00:08:01,966 --> 00:08:04,521
जो कि लोगों ने एआई के शुरुआती दिनों में किया होगा, 

137
00:08:04,521 --> 00:08:08,729
इसके बजाय आप ट्यून करने योग्य मापदंडों के साथ एक बहुत ही लचीली संरचना स्थापित करें, 

138
00:08:08,729 --> 00:08:12,937
जैसे कि नॉब और डायल का एक समूह, और फिर किसी तरह आप इस व्यवहार की नकल करने के लिए उन 

139
00:08:12,937 --> 00:08:17,295
मापदंडों के मूल्यों को ट्विक और ट्यून करने के लिए किसी दिए गए इनपुट के लिए आउटपुट कैसा 

140
00:08:17,295 --> 00:08:19,700
दिखना चाहिए, इसके कई उदाहरणों का उपयोग करते हैं।

141
00:08:19,700 --> 00:08:23,563
उदाहरण के लिए, शायद मशीन लर्निंग का सबसे सरल रूप रैखिक प्रतिगमन है, 

142
00:08:23,563 --> 00:08:26,574
जहां आपके इनपुट और आउटपुट प्रत्येक एकल संख्याएं हैं, 

143
00:08:26,574 --> 00:08:30,664
एक घर के वर्ग फुटेज और इसकी कीमत की तरह कुछ, और आप जो चाहते हैं वह इसके 

144
00:08:30,664 --> 00:08:33,959
माध्यम से सबसे उपयुक्त लाइन ढूंढना है डेटा, आप जानते हैं, 

145
00:08:33,959 --> 00:08:36,799
भविष्य में घर की कीमतों की भविष्यवाणी करने के लिए।

146
00:08:37,440 --> 00:08:42,406
उस रेखा को दो निरंतर मापदंडों द्वारा वर्णित किया गया है, ढलान और वाई-अवरोधन कहें, 

147
00:08:42,406 --> 00:08:47,675
और रैखिक प्रतिगमन का लक्ष्य डेटा से निकटता से मेल खाने के लिए उन मापदंडों को निर्धारित 

148
00:08:47,675 --> 00:08:48,160
करना है।

149
00:08:48,880 --> 00:08:52,100
कहने की जरूरत नहीं है, गहन शिक्षण मॉडल बहुत अधिक जटिल हो जाते हैं।

150
00:08:52,620 --> 00:08:57,660
उदाहरण के लिए, GPT-3 में दो नहीं, बल्कि 175 बिलियन पैरामीटर हैं।

151
00:08:58,120 --> 00:09:01,856
लेकिन यहाँ एक बात है, यह दी गई बात नहीं है कि आप बड़ी संख्या में 

152
00:09:01,856 --> 00:09:05,593
मापदंडों के साथ कुछ विशाल मॉडल बना सकते हैं, बिना प्रशिक्षण डेटा 

153
00:09:05,593 --> 00:09:09,560
को पूरी तरह से ओवरफिट किए या प्रशिक्षित करने के लिए पूरी तरह से कठिन।

154
00:09:10,260 --> 00:09:13,142
डीप लर्निंग मॉडलों के एक वर्ग का वर्णन करता है जो पिछले 

155
00:09:13,142 --> 00:09:16,180
कुछ दशकों में उल्लेखनीय रूप से बड़े पैमाने पर साबित हुआ है।

156
00:09:16,480 --> 00:09:19,751
जो चीज़ उन्हें एकीकृत करती है वह वही प्रशिक्षण एल्गोरिदम है, 

157
00:09:19,751 --> 00:09:23,504
जिसे बैकप्रॉपैगेशन कहा जाता है, और जिस संदर्भ में हम आपको बताना चाहते 

158
00:09:23,504 --> 00:09:28,169
हैं वह यह है कि इस प्रशिक्षण एल्गोरिदम को बड़े पैमाने पर अच्छी तरह से काम करने के लिए, 

159
00:09:28,169 --> 00:09:31,280
इन मॉडलों को एक निश्चित विशिष्ट प्रारूप का पालन करना होगा।

160
00:09:31,800 --> 00:09:35,868
यदि आप इस प्रारूप को जानते हैं, तो यह कई विकल्पों को समझाने में मदद करता है कि 

161
00:09:35,868 --> 00:09:40,400
ट्रांसफार्मर भाषा को कैसे संसाधित करता है, जो अन्यथा मनमाना महसूस करने का जोखिम रखता है।

162
00:09:41,440 --> 00:09:44,066
सबसे पहले, आप जो भी मॉडल बना रहे हों, इनपुट को वास्तविक 

163
00:09:44,066 --> 00:09:46,740
संख्याओं की एक सारणी के रूप में स्वरूपित किया जाना चाहिए।

164
00:09:46,740 --> 00:09:50,761
इसका मतलब संख्याओं की एक सूची हो सकता है, यह एक द्वि-आयामी सरणी हो सकती है, 

165
00:09:50,761 --> 00:09:55,259
या अक्सर आप उच्च आयामी सरणी से निपटते हैं, जहां इस्तेमाल किया जाने वाला सामान्य शब्द 

166
00:09:55,259 --> 00:09:56,000
टेंसर होता है।

167
00:09:56,560 --> 00:10:00,630
आप अक्सर उस इनपुट डेटा के बारे में सोचते हैं जो उत्तरोत्तर कई अलग-अलग परतों में परिवर्तित 

168
00:10:00,630 --> 00:10:04,564
हो रहा है, जहां फिर से, प्रत्येक परत हमेशा वास्तविक संख्याओं की किसी प्रकार की सरणी के 

169
00:10:04,564 --> 00:10:07,549
रूप में संरचित होती है, जब तक कि आप अंतिम परत तक नहीं पहुंच जाते, 

170
00:10:07,549 --> 00:10:08,680
जिसे आप आउटपुट मानते हैं।

171
00:10:09,280 --> 00:10:13,118
उदाहरण के लिए, हमारे पाठ प्रसंस्करण मॉडल में अंतिम परत संख्याओं की एक सूची 

172
00:10:13,118 --> 00:10:17,060
है जो सभी संभावित अगले टोकनों के लिए संभाव्यता वितरण का प्रतिनिधित्व करती है।

173
00:10:17,820 --> 00:10:21,878
गहन शिक्षण में, इन मॉडल मापदंडों को लगभग हमेशा वजन के रूप में संदर्भित किया जाता है, 

174
00:10:21,878 --> 00:10:26,032
और ऐसा इसलिए है क्योंकि इन मॉडलों की एक प्रमुख विशेषता यह है कि जिस तरह से ये पैरामीटर 

175
00:10:26,032 --> 00:10:29,900
संसाधित किए जा रहे डेटा के साथ बातचीत करते हैं वह भारित रकम के माध्यम से होता है।

176
00:10:30,340 --> 00:10:34,360
आप कुछ गैर-रेखीय फ़ंक्शंस भी छिड़कते हैं, लेकिन वे मापदंडों पर निर्भर नहीं होंगे।

177
00:10:35,200 --> 00:10:38,505
हालाँकि, आम तौर पर, भारित योगों को पूरी तरह नग्न और इस तरह 

178
00:10:38,505 --> 00:10:42,146
स्पष्ट रूप से लिखे हुए देखने के बजाय, आप उन्हें मैट्रिक्स वेक्टर 

179
00:10:42,146 --> 00:10:45,620
उत्पाद में विभिन्न घटकों के रूप में एक साथ पैक किए हुए पाएंगे।

180
00:10:46,740 --> 00:10:51,240
यह वही बात कहने के समान है, यदि आप सोचें कि मैट्रिक्स वेक्टर गुणन कैसे काम करता है, 

181
00:10:51,240 --> 00:10:54,240
तो आउटपुट में प्रत्येक घटक एक भारित योग की तरह दिखता है।

182
00:10:54,780 --> 00:10:58,418
आपके और मेरे लिए उन मैट्रिक्स के बारे में सोचना अक्सर अवधारणात्मक 

183
00:10:58,418 --> 00:11:02,001
रूप से साफ़ होता है जो ट्यून करने योग्य मापदंडों से भरे होते हैं 

184
00:11:02,001 --> 00:11:05,420
जो संसाधित किए जा रहे डेटा से खींचे गए वैक्टर को बदल देते हैं।

185
00:11:06,340 --> 00:11:10,070
उदाहरण के लिए, GPT-3 में उन 175 बिलियन वज़न को केवल 

186
00:11:10,070 --> 00:11:14,160
28,000 से कम अलग-अलग मैट्रिक्स में व्यवस्थित किया गया है।

187
00:11:14,660 --> 00:11:17,146
ये मैट्रिसेस आठ अलग-अलग श्रेणियों में आते हैं, 

188
00:11:17,146 --> 00:11:21,113
और आप और मैं उन श्रेणियों में से प्रत्येक के माध्यम से यह समझने जा रहे हैं 

189
00:11:21,113 --> 00:11:22,700
कि कौन सा प्रकार क्या करता है।

190
00:11:23,160 --> 00:11:27,177
जैसे-जैसे हम आगे बढ़ते हैं, मुझे लगता है कि जीपीटी-3 से विशिष्ट संख्याओं 

191
00:11:27,177 --> 00:11:31,360
को संदर्भित करना मजेदार है ताकि यह गिना जा सके कि वे 175 अरब कहां से आए हैं।

192
00:11:31,880 --> 00:11:36,124
भले ही आजकल बड़े और बेहतर मॉडल उपलब्ध हैं, लेकिन इस बड़े भाषा मॉडल में एक विशेष 

193
00:11:36,124 --> 00:11:40,740
आकर्षण है, जो वास्तव में मशीन लर्निंग समुदायों के बाहर दुनिया का ध्यान आकर्षित करता है।

194
00:11:41,440 --> 00:11:44,004
इसके अलावा, व्यावहारिक रूप से कहें तो, कंपनियां अधिक आधुनिक 

195
00:11:44,004 --> 00:11:46,740
नेटवर्क के लिए विशिष्ट संख्याओं के बारे में अधिक सतर्क रहती हैं।

196
00:11:47,360 --> 00:11:50,754
मैं बस उस दृश्य को सेट करना चाहता हूं, जिसमें आप यह देखने के लिए 

197
00:11:50,754 --> 00:11:54,254
हुड के नीचे झांकते हैं कि चैटजीपीटी जैसे टूल के अंदर क्या होता है, 

198
00:11:54,254 --> 00:11:57,440
लगभग सभी वास्तविक गणना मैट्रिक्स वेक्टर गुणन की तरह दिखती है।

199
00:11:57,900 --> 00:12:00,947
अरबों संख्याओं के सागर में खो जाने का थोड़ा जोखिम है, 

200
00:12:00,947 --> 00:12:05,631
लेकिन आपको अपने दिमाग में मॉडल के भार, जिसे मैं हमेशा नीले या लाल रंग में रंगूंगा, 

201
00:12:05,631 --> 00:12:09,300
और संसाधित किए जा रहे डेटा, जिसे मैं हमेशा ग्रे रंग में रंगूंगा, 

202
00:12:09,300 --> 00:12:11,840
के बीच एक बहुत ही स्पष्ट अंतर बना लेना चाहिए।

203
00:12:12,180 --> 00:12:15,548
भार ही वास्तविक मस्तिष्क हैं, वे प्रशिक्षण के दौरान सीखी गई चीजें हैं, 

204
00:12:15,548 --> 00:12:17,920
और वे निर्धारित करते हैं कि यह कैसे व्यवहार करेगा।

205
00:12:18,280 --> 00:12:20,916
संसाधित किया जा रहा डेटा किसी भी विशिष्ट इनपुट को, 

206
00:12:20,916 --> 00:12:24,793
जो किसी दिए गए रन के लिए मॉडल में डाला जाता है, केवल उसी को एनकोड करता है, 

207
00:12:24,793 --> 00:12:26,500
जैसे कि पाठ का एक उदाहरण स्निपेट।

208
00:12:27,480 --> 00:12:31,922
इन सब बातों को आधार मानकर, आइए इस पाठ प्रसंस्करण उदाहरण के पहले चरण पर चलते हैं, 

209
00:12:31,922 --> 00:12:36,420
जिसमें इनपुट को छोटे-छोटे टुकड़ों में तोड़ना और उन टुकड़ों को वेक्टर में बदलना है।

210
00:12:37,020 --> 00:12:39,439
मैंने बताया कि उन टुकड़ों को टोकन्स कहा जाता है, 

211
00:12:39,439 --> 00:12:41,908
जो शब्दों या विराम चिह्नों के टुकड़े हो सकते हैं, 

212
00:12:41,908 --> 00:12:44,278
लेकिन इस अध्याय में और विशेषकर अगले अध्याय में, 

213
00:12:44,278 --> 00:12:48,080
मैं बस यह दिखाना चाहता हूँ कि इसे शब्दों में अधिक स्पष्ट रूप से तोड़ा गया है।

214
00:12:48,600 --> 00:12:51,340
क्योंकि हम मनुष्य शब्दों में सोचते हैं, इससे छोटे उदाहरणों का 

215
00:12:51,340 --> 00:12:54,080
संदर्भ लेना और प्रत्येक चरण को स्पष्ट करना बहुत आसान हो जाएगा।

216
00:12:55,260 --> 00:12:59,083
मॉडल में एक पूर्वनिर्धारित शब्दावली है, सभी संभावित शब्दों की कुछ सूची है, 

217
00:12:59,083 --> 00:13:02,753
मान लीजिए उनमें से 50,000 हैं, और पहला मैट्रिक्स जिसका हम सामना करेंगे, 

218
00:13:02,753 --> 00:13:05,302
जिसे एम्बेडिंग मैट्रिक्स के रूप में जाना जाता है, 

219
00:13:05,302 --> 00:13:07,800
में इनमें से प्रत्येक शब्द के लिए एक एकल कॉलम है।

220
00:13:08,940 --> 00:13:13,760
ये कॉलम ही निर्धारित करते हैं कि प्रत्येक शब्द पहले चरण में किस वेक्टर में बदल जाएगा।

221
00:13:15,100 --> 00:13:18,856
हम इसे &#39;We&#39; लेबल देते हैं, तथा हमारे द्वारा देखे जाने वाले सभी मैट्रिसेस की तरह, 

222
00:13:18,856 --> 00:13:22,360
इसके मान यादृच्छिक रूप से शुरू होते हैं, लेकिन इन्हें डेटा के आधार पर सीखा जाता है।

223
00:13:23,620 --> 00:13:27,858
ट्रांसफॉर्मर से बहुत पहले मशीन लर्निंग में शब्दों को वेक्टर में बदलना आम बात थी, 

224
00:13:27,858 --> 00:13:31,102
लेकिन अगर आपने इसे पहले कभी नहीं देखा है तो यह थोड़ा अजीब है, 

225
00:13:31,102 --> 00:13:35,760
और यह आगे आने वाली हर चीज की नींव तय करता है, तो आइए इससे परिचित होने के लिए कुछ समय लें।

226
00:13:36,040 --> 00:13:39,709
हम अक्सर इसे एम्बेडिंग शब्द कहते हैं, जो आपको इन वैक्टरों को बहुत ज्यामितीय 

227
00:13:39,709 --> 00:13:43,620
रूप से कुछ उच्च आयामी स्थान में बिंदुओं के रूप में सोचने के लिए आमंत्रित करता है।

228
00:13:44,180 --> 00:13:48,223
3डी स्पेस में बिंदुओं के निर्देशांक के रूप में तीन संख्याओं की सूची को विज़ुअलाइज़ 

229
00:13:48,223 --> 00:13:51,780
करना कोई समस्या नहीं होगी, लेकिन शब्द एम्बेडिंग बहुत अधिक आयामी होते हैं।

230
00:13:52,280 --> 00:13:56,020
GPT-3 में उनके 12,288 आयाम हैं, और जैसा कि आप देखेंगे, 

231
00:13:56,020 --> 00:14:00,440
ऐसे स्थान पर काम करना मायने रखता है जिसमें कई अलग-अलग दिशाएँ हों।

232
00:14:01,180 --> 00:14:04,997
उसी तरह जैसे आप 3डी स्पेस के माध्यम से एक द्वि-आयामी स्लाइस ले सकते हैं 

233
00:14:04,997 --> 00:14:07,860
और उस स्लाइस पर सभी बिंदुओं को प्रोजेक्ट कर सकते हैं, 

234
00:14:07,860 --> 00:14:11,625
शब्द एम्बेडिंग को एनिमेट करने के लिए जो एक साधारण मॉडल मुझे दे रहा है, 

235
00:14:11,625 --> 00:14:15,389
मैं एक समान चीज़ करने जा रहा हूं इस अत्यधिक उच्च आयामी स्थान के माध्यम 

236
00:14:15,389 --> 00:14:19,154
से एक त्रि-आयामी स्लाइस चुनकर, और शब्द वैक्टर को उस पर प्रक्षेपित करके 

237
00:14:19,154 --> 00:14:20,480
और परिणाम प्रदर्शित करके।

238
00:14:21,280 --> 00:14:24,523
यहां बड़ा विचार यह है कि एक मॉडल यह निर्धारित करने के लिए अपने वजन को 

239
00:14:24,523 --> 00:14:27,813
बदलता और समायोजित करता है कि प्रशिक्षण के दौरान शब्द वास्तव में वैक्टर 

240
00:14:27,813 --> 00:14:31,103
के रूप में कैसे एम्बेडेड होते हैं, यह एम्बेडिंग के एक सेट पर व्यवस्थित 

241
00:14:31,103 --> 00:14:34,440
होता है जहां अंतरिक्ष में दिशाओं का एक प्रकार का अर्थपूर्ण अर्थ होता है।

242
00:14:34,980 --> 00:14:37,696
यहां मैं जो सरल शब्द-से-वेक्टर मॉडल चला रहा हूं, 

243
00:14:37,696 --> 00:14:42,186
उसके लिए यदि मैं उन सभी शब्दों की खोज करूं जिनकी एम्बेडिंग टावर के सबसे करीब है, 

244
00:14:42,186 --> 00:14:45,900
तो आप देखेंगे कि वे सभी बहुत ही समान टावर-जैसी अनुभूतियां देते हैं।

245
00:14:46,340 --> 00:14:48,707
और यदि आप कुछ पायथन बनाना चाहते हैं और घर पर खेलना चाहते हैं, 

246
00:14:48,707 --> 00:14:51,380
तो यह विशिष्ट मॉडल है जिसका उपयोग मैं एनिमेशन बनाने के लिए कर रहा हूं।

247
00:14:51,620 --> 00:14:54,587
यह कोई ट्रांसफार्मर नहीं है, लेकिन यह इस विचार को स्पष्ट करने के 

248
00:14:54,587 --> 00:14:57,600
लिए पर्याप्त है कि अंतरिक्ष में दिशाएँ अर्थपूर्ण अर्थ ले सकती हैं।

249
00:14:58,300 --> 00:15:03,308
इसका एक बहुत ही उत्कृष्ट उदाहरण यह है कि यदि आप महिला और पुरुष के लिए वेक्टर के 

250
00:15:03,308 --> 00:15:08,066
बीच अंतर लेते हैं, तो आप एक के सिरे को दूसरे के सिरे से जोड़ने वाले एक छोटे 

251
00:15:08,066 --> 00:15:13,200
वेक्टर के रूप में कल्पना करेंगे, यह राजा और पुरुष के बीच के अंतर के समान है। रानी।

252
00:15:15,080 --> 00:15:20,178
तो मान लीजिए कि आप एक महिला सम्राट के लिए शब्द नहीं जानते हैं, आप इसे राजा को लेकर, 

253
00:15:20,178 --> 00:15:25,460
इस महिला-पुरुष दिशा को जोड़कर, और उस बिंदु के निकटतम एम्बेडिंग की खोज करके पा सकते हैं।

254
00:15:27,000 --> 00:15:28,200
कम से कम, एक तरह का।

255
00:15:28,480 --> 00:15:32,029
यद्यपि यह उस मॉडल का एक उत्कृष्ट उदाहरण है जिसके साथ मैं काम कर रहा हूँ, 

256
00:15:32,029 --> 00:15:34,848
क्वीन का वास्तविक एम्बेडिंग वास्तव में इससे थोड़ा दूर है, 

257
00:15:34,848 --> 00:15:38,835
संभवतः इसलिए क्योंकि प्रशिक्षण डेटा में क्वीन का उपयोग जिस तरह से किया जाता है वह 

258
00:15:38,835 --> 00:15:40,780
किंग का केवल स्त्रीलिंग संस्करण नहीं है।

259
00:15:41,620 --> 00:15:45,260
जब मैंने इधर-उधर खेला, तो पारिवारिक रिश्ते इस विचार को बेहतर ढंग से चित्रित करने लगे।

260
00:15:46,340 --> 00:15:50,647
मुद्दा यह है कि, ऐसा लगता है कि प्रशिक्षण के दौरान मॉडल को ऐसे एम्बेडिंग चुनना 

261
00:15:50,647 --> 00:15:54,900
फायदेमंद लगा, जिससे इस स्थान में एक दिशा लिंग संबंधी जानकारी को एन्कोड कर सके।

262
00:15:56,800 --> 00:16:00,221
एक और उदाहरण यह है कि यदि आप इटली के एम्बेडिंग को लेते हैं, 

263
00:16:00,221 --> 00:16:04,896
और आप जर्मनी के एम्बेडिंग को घटाते हैं, और उसे हिटलर के एम्बेडिंग में जोड़ते हैं, 

264
00:16:04,896 --> 00:16:08,090
तो आपको मुसोलिनी के एम्बेडिंग के बहुत करीब कुछ मिलता है।

265
00:16:08,570 --> 00:16:12,092
यह ऐसा है मानो मॉडल ने कुछ दिशाओं को इटालियन-नेस के साथ और कुछ 

266
00:16:12,092 --> 00:16:15,670
को द्वितीय विश्व युद्ध के धुरी नेताओं के साथ जोड़ना सीख लिया हो।

267
00:16:16,470 --> 00:16:20,076
इस संबंध में शायद मेरा पसंदीदा उदाहरण यह है कि कैसे कुछ मॉडलों में, 

268
00:16:20,076 --> 00:16:23,736
यदि आप जर्मनी और जापान के बीच का अंतर लें, और उसे सुशी में जोड़ दें, 

269
00:16:23,736 --> 00:16:26,230
तो आप ब्रैटवुर्स्ट के बहुत करीब पहुंच जाते हैं।

270
00:16:27,350 --> 00:16:30,426
इसके अलावा, निकटतम पड़ोसियों को खोजने के इस खेल को खेलते समय, 

271
00:16:30,426 --> 00:16:33,850
मुझे यह देखकर खुशी हुई कि कैट जानवर और राक्षस दोनों के कितने करीब थी।

272
00:16:34,690 --> 00:16:37,294
गणितीय अंतर्ज्ञान का एक अंश, जिसे ध्यान में रखना उपयोगी है, 

273
00:16:37,294 --> 00:16:40,290
विशेष रूप से अगले अध्याय के लिए, वह यह है कि दो सदिशों के डॉट उत्पाद 

274
00:16:40,290 --> 00:16:43,850
को यह मापने के तरीके के रूप में देखा जा सकता है कि वे कितनी अच्छी तरह संरेखित हैं।

275
00:16:44,870 --> 00:16:49,653
कम्प्यूटेशनल रूप से, डॉट उत्पाद में सभी संगत घटकों को गुणा करना और फिर परिणामों को जोड़ना 

276
00:16:49,653 --> 00:16:54,330
शामिल होता है, जो कि अच्छा है, क्योंकि हमारी अधिकांश गणना भारित योगों की तरह होनी चाहिए।

277
00:16:55,190 --> 00:16:58,561
ज्यामितीय रूप से, जब वेक्टर समान दिशाओं में इंगित करते हैं तो डॉट 

278
00:16:58,561 --> 00:17:02,085
उत्पाद सकारात्मक होता है, यदि वे लंबवत होते हैं तो यह शून्य होता है, 

279
00:17:02,085 --> 00:17:05,609
और जब भी वे विपरीत दिशाओं में इंगित करते हैं तो यह नकारात्मक होता है।

280
00:17:06,550 --> 00:17:09,794
उदाहरण के लिए, मान लीजिए कि आप इस मॉडल के साथ खेल रहे थे, 

281
00:17:09,794 --> 00:17:13,374
और आप परिकल्पना करते हैं कि बिल्लियों माइनस बिल्ली का एम्बेडिंग 

282
00:17:13,374 --> 00:17:17,010
इस स्थान में एक प्रकार की बहुलता दिशा का प्रतिनिधित्व कर सकता है।

283
00:17:17,430 --> 00:17:20,527
इसका परीक्षण करने के लिए, मैं इस वेक्टर को लेने जा रहा हूं और कुछ 

284
00:17:20,527 --> 00:17:23,858
एकवचन संज्ञाओं के एम्बेडिंग के विरुद्ध इसके डॉट उत्पाद की गणना करूंगा, 

285
00:17:23,858 --> 00:17:27,050
और इसकी तुलना संबंधित बहुवचन संज्ञाओं के साथ डॉट उत्पादों से करूंगा।

286
00:17:27,270 --> 00:17:31,747
यदि आप इसके साथ खेलते हैं, तो आप देखेंगे कि बहुवचन वाले वास्तव में एकवचन वाले की तुलना 

287
00:17:31,747 --> 00:17:36,070
में लगातार उच्च मूल्य देते हैं, जो दर्शाता है कि वे इस दिशा के साथ अधिक संरेखित हैं।

288
00:17:37,070 --> 00:17:39,553
यह भी मजेदार है कि यदि आप इस डॉट उत्पाद को 1, 2, 

289
00:17:39,553 --> 00:17:43,759
3 और इसी तरह के शब्दों के एम्बेडिंग के साथ लेते हैं, तो वे बढ़ते हुए मान देते हैं, 

290
00:17:43,759 --> 00:17:47,813
इसलिए यह ऐसा है जैसे हम मात्रात्मक रूप से माप सकते हैं कि मॉडल किसी दिए गए शब्द 

291
00:17:47,813 --> 00:17:49,030
को कितना बहुवचन पाता है।

292
00:17:50,250 --> 00:17:52,011
पुनः, शब्दों को किस प्रकार अंतर्निहित किया जाता है, 

293
00:17:52,011 --> 00:17:53,570
इसकी विशिष्टता डेटा के माध्यम से सीखी जाती है।

294
00:17:54,050 --> 00:17:57,932
यह एम्बेडिंग मैट्रिक्स, जिसके कॉलम हमें बताते हैं कि प्रत्येक शब्द का क्या होता है, 

295
00:17:57,932 --> 00:17:59,550
हमारे मॉडल में वज़न का पहला ढेर है।

296
00:18:00,030 --> 00:18:05,535
जीपीटी-3 संख्याओं का उपयोग करते हुए, शब्दावली का आकार विशेष रूप से 50,257 है, 

297
00:18:05,535 --> 00:18:09,770
और फिर, तकनीकी रूप से इसमें शब्द नहीं, बल्कि टोकन शामिल हैं।

298
00:18:10,630 --> 00:18:14,312
एम्बेडिंग आयाम 12,288 है, और इन्हें गुणा करने पर हमें 

299
00:18:14,312 --> 00:18:17,790
पता चलता है कि इसमें लगभग 617 मिलियन भार शामिल हैं।

300
00:18:18,250 --> 00:18:21,030
आइए आगे बढ़ें और इसे चालू संख्या में जोड़ें, यह याद 

301
00:18:21,030 --> 00:18:23,810
रखते हुए कि अंत तक हमें 175 अरब तक गिनती करनी चाहिए।

302
00:18:25,430 --> 00:18:28,694
ट्रांसफार्मर के मामले में, आप वास्तव में इस एम्बेडिंग स्पेस में वैक्टरों के 

303
00:18:28,694 --> 00:18:32,130
बारे में सोचना चाहते हैं जो केवल व्यक्तिगत शब्दों का प्रतिनिधित्व नहीं करते हैं।

304
00:18:32,550 --> 00:18:36,138
एक बात के लिए, वे उस शब्द की स्थिति के बारे में भी जानकारी को एन्कोड करते हैं, 

305
00:18:36,138 --> 00:18:39,590
जिसके बारे में हम बाद में बात करेंगे, लेकिन इससे भी महत्वपूर्ण बात यह है कि 

306
00:18:39,590 --> 00:18:42,770
आपको उनके बारे में संदर्भ में सोखने की क्षमता के बारे में सोचना चाहिए।

307
00:18:43,350 --> 00:18:48,130
उदाहरण के लिए, एक वेक्टर जिसने अपना जीवन किंग शब्द के एम्बेडिंग के रूप में शुरू किया था, 

308
00:18:48,130 --> 00:18:52,159
इस नेटवर्क में विभिन्न ब्लॉकों द्वारा धीरे-धीरे खींचा और खींचा जा सकता है, 

309
00:18:52,159 --> 00:18:56,457
ताकि अंत तक यह अधिक विशिष्ट और सूक्ष्म दिशा में इंगित हो जो किसी तरह इसे एन्कोड 

310
00:18:56,457 --> 00:19:00,701
करता है एक राजा था जो स्कॉटलैंड में रहता था, और जिसने पिछले राजा की हत्या करके 

311
00:19:00,701 --> 00:19:04,730
अपना पद हासिल किया था, और जिसका वर्णन शेक्सपियर की भाषा में किया जा रहा है।

312
00:19:05,210 --> 00:19:07,790
किसी दिए गए शब्द की अपनी समझ के बारे में सोचें।

313
00:19:08,250 --> 00:19:11,602
उस शब्द का अर्थ परिवेश द्वारा स्पष्ट रूप से सूचित किया जाता है, 

314
00:19:11,602 --> 00:19:14,431
और कभी-कभी इसमें बहुत दूर से संदर्भ भी शामिल होता है, 

315
00:19:14,431 --> 00:19:18,151
इसलिए एक ऐसे मॉडल को एक साथ रखने में जो भविष्यवाणी करने की क्षमता रखता 

316
00:19:18,151 --> 00:19:21,870
है कि अगला शब्द क्या आएगा, लक्ष्य किसी तरह इसे संदर्भ को शामिल करने के 

317
00:19:21,870 --> 00:19:23,390
लिए सशक्त बनाना है कुशलता से.

318
00:19:24,050 --> 00:19:27,183
स्पष्ट रूप से कहें तो, पहले चरण में, जब आप इनपुट टेक्स्ट के आधार पर 

319
00:19:27,183 --> 00:19:30,409
वैक्टर की सरणी बनाते हैं, तो उनमें से प्रत्येक को एम्बेडिंग मैट्रिक्स 

320
00:19:30,409 --> 00:19:33,451
से आसानी से बाहर निकाल लिया जाता है, इसलिए शुरू में प्रत्येक अपने 

321
00:19:33,451 --> 00:19:36,770
आसपास से किसी भी इनपुट के बिना केवल एक शब्द का अर्थ ही एनकोड कर सकता है।

322
00:19:37,710 --> 00:19:41,506
लेकिन आपको इस नेटवर्क के प्राथमिक लक्ष्य के बारे में सोचना चाहिए जिससे यह प्रवाहित होता 

323
00:19:41,506 --> 00:19:45,216
है कि इनमें से प्रत्येक वैक्टर को एक ऐसे अर्थ को ग्रहण करने में सक्षम बनाया जाए जो कि 

324
00:19:45,216 --> 00:19:48,970
केवल व्यक्तिगत शब्दों का प्रतिनिधित्व करने की तुलना में कहीं अधिक समृद्ध और विशिष्ट हो।

325
00:19:49,510 --> 00:19:52,322
नेटवर्क एक समय में केवल निश्चित संख्या में वैक्टर संसाधित कर सकता है, 

326
00:19:52,322 --> 00:19:54,170
जिसे इसके संदर्भ आकार के रूप में जाना जाता है।

327
00:19:54,510 --> 00:19:57,926
GPT-3 के लिए इसे 2048 के संदर्भ आकार के साथ प्रशिक्षित किया गया था, 

328
00:19:57,926 --> 00:20:01,392
इसलिए नेटवर्क के माध्यम से प्रवाहित होने वाला डेटा हमेशा 2048 कॉलमों 

329
00:20:01,392 --> 00:20:05,010
की इस सरणी की तरह दिखता है, जिनमें से प्रत्येक में 12,000 आयाम होते हैं।

330
00:20:05,590 --> 00:20:08,790
यह संदर्भ आकार इस बात को सीमित करता है कि ट्रांसफार्मर अगले 

331
00:20:08,790 --> 00:20:11,830
शब्द का पूर्वानुमान लगाते समय कितना पाठ शामिल कर सकता है।

332
00:20:12,370 --> 00:20:14,790
यही कारण है कि कुछ चैटबॉट्स के साथ लंबी बातचीत, 

333
00:20:14,790 --> 00:20:17,966
जैसे कि चैटजीपीटी के शुरुआती संस्करण, अक्सर ऐसा महसूस कराते थे 

334
00:20:17,966 --> 00:20:22,050
कि जैसे-जैसे आप बहुत लंबे समय तक बातचीत करते रहे, बॉट बातचीत का सूत्र खो देता है।

335
00:20:23,030 --> 00:20:25,896
हम उचित समय पर ध्यान के विवरण में जाएंगे, लेकिन आगे बढ़ते हुए 

336
00:20:25,896 --> 00:20:28,810
मैं एक मिनट के लिए बात करना चाहता हूं कि आखिर में क्या होता है।

337
00:20:29,450 --> 00:20:34,870
याद रखें, वांछित आउटपुट आगे आने वाले सभी टोकन पर संभाव्यता वितरण है।

338
00:20:35,170 --> 00:20:37,752
उदाहरण के लिए, यदि सबसे अंतिम शब्द प्रोफेसर है, 

339
00:20:37,752 --> 00:20:41,680
और संदर्भ में हैरी पॉटर जैसे शब्द शामिल हैं, और उसके ठीक पहले हम सबसे कम 

340
00:20:41,680 --> 00:20:45,876
पसंदीदा शिक्षक को देखते हैं, और साथ ही यदि आप मुझे यह दिखावा करने की छूट देते 

341
00:20:45,876 --> 00:20:48,459
हैं कि टोकन केवल पूर्ण शब्दों की तरह दिखते हैं, 

342
00:20:48,459 --> 00:20:53,193
तो एक अच्छी तरह से प्रशिक्षित नेटवर्क जिसने हैरी पॉटर के बारे में ज्ञान अर्जित किया है, 

343
00:20:53,193 --> 00:20:55,830
संभवतः स्नेप शब्द को एक उच्च संख्या प्रदान करेगा।

344
00:20:56,510 --> 00:20:57,970
इसमें दो अलग-अलग चरण शामिल हैं।

345
00:20:58,310 --> 00:21:02,762
पहला तरीका एक अन्य मैट्रिक्स का उपयोग करना है जो उस संदर्भ में अंतिम वेक्टर को 

346
00:21:02,762 --> 00:21:07,610
50,000 मानों की सूची में मैप करता है, जो शब्दावली में प्रत्येक टोकन के लिए एक होता है।

347
00:21:08,170 --> 00:21:11,879
फिर एक फ़ंक्शन है जो इसे संभाव्यता वितरण में सामान्यीकृत करता है, 

348
00:21:11,879 --> 00:21:16,375
इसे सॉफ्टमैक्स कहा जाता है और हम बस एक सेकंड में इसके बारे में अधिक बात करेंगे, 

349
00:21:16,375 --> 00:21:21,321
लेकिन इससे पहले भविष्यवाणी करने के लिए केवल इस अंतिम एम्बेडिंग का उपयोग करना थोड़ा अजीब 

350
00:21:21,321 --> 00:21:25,929
लग सकता है, जब आख़िरकार उस अंतिम चरण में परत में हजारों अन्य वेक्टर अपने स्वयं के 

351
00:21:25,929 --> 00:21:28,290
संदर्भ-समृद्ध अर्थों के साथ वहां बैठे हैं।

352
00:21:28,930 --> 00:21:32,766
इसका संबंध इस तथ्य से है कि प्रशिक्षण प्रक्रिया में यह अधिक कुशल हो 

353
00:21:32,766 --> 00:21:36,490
जाता है यदि आप अंतिम परत में उन वैक्टरों में से प्रत्येक का उपयोग 

354
00:21:36,490 --> 00:21:40,270
एक साथ भविष्यवाणी करने के लिए करते हैं कि इसके तुरंत बाद क्या आएगा।

355
00:21:40,970 --> 00:21:43,418
प्रशिक्षण के बारे में बाद में और भी बहुत कुछ कहा जाना बाकी है, 

356
00:21:43,418 --> 00:21:45,090
लेकिन मैं अभी उस पर प्रकाश डालना चाहता हूं।

357
00:21:45,730 --> 00:21:49,690
इस मैट्रिक्स को अनएम्बेडिंग मैट्रिक्स कहा जाता है और हम इसे WU लेबल देते हैं।

358
00:21:50,210 --> 00:21:52,282
फिर, हमारे द्वारा देखे जाने वाले सभी वज़न मैट्रिक्स की तरह, 

359
00:21:52,282 --> 00:21:54,010
इसकी प्रविष्टियाँ यादृच्छिक रूप से शुरू होती हैं, 

360
00:21:54,010 --> 00:21:55,910
लेकिन उन्हें प्रशिक्षण प्रक्रिया के दौरान सीखा जाता है।

361
00:21:56,470 --> 00:21:59,654
हमारे कुल पैरामीटर गणना पर स्कोर रखते हुए, इस अनएम्बेडिंग मैट्रिक्स 

362
00:21:59,654 --> 00:22:02,324
में शब्दावली में प्रत्येक शब्द के लिए एक पंक्ति होती है, 

363
00:22:02,324 --> 00:22:05,650
और प्रत्येक पंक्ति में एम्बेडिंग आयाम के समान तत्वों की संख्या होती है।

364
00:22:06,410 --> 00:22:10,098
यह एम्बेडिंग मैट्रिक्स के समान है, बस ऑर्डर की अदला-बदली के साथ, 

365
00:22:10,098 --> 00:22:12,993
यह नेटवर्क में अन्य 617 मिलियन पैरामीटर जोड़ता है, 

366
00:22:12,993 --> 00:22:16,398
जिसका अर्थ है कि अब तक हमारी गिनती एक अरब से थोड़ी अधिक है, 

367
00:22:16,398 --> 00:22:20,200
हमारे 175 बिलियन का एक छोटा लेकिन पूरी तरह से महत्वहीन अंश नहीं है।

368
00:22:20,200 --> 00:22:21,790
 कुल मिलाकर समाप्त हो जाएगा.

369
00:22:22,550 --> 00:22:25,250
इस अध्याय के अंतिम लघु पाठ के रूप में, मैं इस सॉफ्टमैक्स फ़ंक्शन 

370
00:22:25,250 --> 00:22:27,909
के बारे में अधिक बात करना चाहता हूं, क्योंकि एक बार जब हम ध्यान 

371
00:22:27,909 --> 00:22:30,610
ब्लॉक में गोता लगाते हैं तो यह हमारे लिए एक और उपस्थिति बनाता है।

372
00:22:31,430 --> 00:22:35,604
विचार यह है कि यदि आप संख्याओं के अनुक्रम को संभाव्यता वितरण के रूप में 

373
00:22:35,604 --> 00:22:39,314
कार्य करना चाहते हैं, जैसे कि सभी संभावित अगले शब्दों पर वितरण, 

374
00:22:39,314 --> 00:22:43,662
तो प्रत्येक मान 0 और 1 के बीच होना चाहिए, और आपको उन सभी को 1 तक जोड़ने की 

375
00:22:43,662 --> 00:22:44,590
भी आवश्यकता है .

376
00:22:45,250 --> 00:22:48,298
हालाँकि, यदि आप सीखने का खेल खेल रहे हैं जहाँ आप जो कुछ भी 

377
00:22:48,298 --> 00:22:50,830
करते हैं वह मैट्रिक्स-वेक्टर गुणन जैसा दिखता है, 

378
00:22:50,830 --> 00:22:54,810
तो डिफ़ॉल्ट रूप से आपको मिलने वाले आउटपुट इसका बिल्कुल भी पालन नहीं करते हैं।

379
00:22:55,330 --> 00:22:57,924
ये मान प्रायः ऋणात्मक होते हैं, या 1 से बहुत बड़े होते हैं, 

380
00:22:57,924 --> 00:22:59,870
तथा लगभग निश्चित रूप से इनका योग 1 नहीं होता।

381
00:23:00,510 --> 00:23:05,763
सॉफ्टमैक्स संख्याओं की एक मनमानी सूची को एक वैध वितरण में इस तरह से बदलने का 

382
00:23:05,763 --> 00:23:11,290
मानक तरीका है कि सबसे बड़ा मान 1 के सबसे करीब हो, और छोटे मान 0 के बहुत करीब हों।

383
00:23:11,830 --> 00:23:13,070
वास्तव में आपको बस इतना ही जानने की जरूरत है।

384
00:23:13,090 --> 00:23:17,320
लेकिन यदि आप उत्सुक हैं, तो इसके काम करने का तरीका यह है कि पहले e को प्रत्येक संख्या 

385
00:23:17,320 --> 00:23:21,452
की घात तक बढ़ाया जाए, जिसका अर्थ है कि अब आपके पास सकारात्मक मूल्यों की एक सूची है, 

386
00:23:21,452 --> 00:23:25,682
और फिर आप उन सभी सकारात्मक मूल्यों का योग ले सकते हैं और विभाजित कर सकते हैं प्रत्येक 

387
00:23:25,682 --> 00:23:29,470
पद को उस योग से, जो इसे एक सूची में सामान्यीकृत करता है जो 1 तक जुड़ जाता है।

388
00:23:30,170 --> 00:23:34,672
आप देखेंगे कि यदि इनपुट में से एक संख्या बाकी की तुलना में सार्थक रूप से बड़ी है, 

389
00:23:34,672 --> 00:23:37,473
तो आउटपुट में संबंधित पद वितरण पर हावी हो जाता है, 

390
00:23:37,473 --> 00:23:41,536
इसलिए यदि आप इससे नमूना ले रहे हैं तो आप लगभग निश्चित रूप से अधिकतम इनपुट 

391
00:23:41,536 --> 00:23:42,470
ही चुन रहे होंगे।

392
00:23:42,990 --> 00:23:45,799
लेकिन यह केवल अधिकतम चुनने की तुलना में अधिक सरल है, 

393
00:23:45,799 --> 00:23:48,290
क्योंकि जब अन्य मान समान रूप से बड़े होते हैं, 

394
00:23:48,290 --> 00:23:53,007
तो उन्हें भी वितरण में सार्थक भार मिलता है, और जब आप इनपुट में निरंतर परिवर्तन करते हैं, 

395
00:23:53,007 --> 00:23:54,650
तो सब कुछ निरंतर बदलता रहता है।

396
00:23:55,130 --> 00:23:59,785
कुछ स्थितियों में, जैसे कि जब चैटजीपीटी अगले शब्द बनाने के लिए इस वितरण का 

397
00:23:59,785 --> 00:24:03,758
उपयोग कर रहा है, तो इस फ़ंक्शन में थोड़ा अतिरिक्त मसाला जोड़कर, 

398
00:24:03,758 --> 00:24:08,910
उन घातांकों के हर में एक निरंतर टी डालकर थोड़ा अतिरिक्त मनोरंजन की गुंजाइश होती है।

399
00:24:09,550 --> 00:24:14,105
हम इसे तापमान कहते हैं, क्योंकि यह अस्पष्ट रूप से कुछ थर्मोडायनामिक्स समीकरणों 

400
00:24:14,105 --> 00:24:18,892
में तापमान की भूमिका से मिलता जुलता है, और इसका प्रभाव यह है कि जब t बड़ा होता है, 

401
00:24:18,892 --> 00:24:23,793
तो आप निम्न मानों को अधिक महत्व देते हैं, जिसका अर्थ है कि वितरण थोड़ा अधिक समान है, 

402
00:24:23,793 --> 00:24:28,176
और यदि t छोटा है, तो बड़े मान अधिक आक्रामक रूप से हावी होंगे, जहां चरम में, 

403
00:24:28,176 --> 00:24:32,790
t को शून्य के बराबर सेट करने का मतलब है कि सारा भार अधिकतम मूल्य पर चला जाता है।

404
00:24:33,470 --> 00:24:38,144
उदाहरण के लिए, मैं जीपीटी-3 से बीज पाठ के साथ एक कहानी तैयार करवाऊंगा, 

405
00:24:38,144 --> 00:24:42,950
एक समय ए था, लेकिन मैं प्रत्येक मामले में अलग-अलग तापमान का उपयोग करूंगा।

406
00:24:43,630 --> 00:24:48,150
तापमान शून्य का अर्थ है कि यह हमेशा सबसे पूर्वानुमानित शब्द के साथ आता है, 

407
00:24:48,150 --> 00:24:52,370
और आपको जो मिलता है वह गोल्डीलॉक्स का एक घिसा-पिटा व्युत्पन्न होता है।

408
00:24:53,010 --> 00:24:56,293
उच्च तापमान उसे कम संभावना वाले शब्दों को चुनने का मौका देता है, 

409
00:24:56,293 --> 00:24:57,910
लेकिन यह एक जोखिम के साथ आता है।

410
00:24:58,230 --> 00:25:02,091
इस मामले में, कहानी दक्षिण कोरिया के एक युवा वेब कलाकार के बारे में 

411
00:25:02,091 --> 00:25:06,010
अधिक मूल रूप से शुरू होती है, लेकिन यह जल्द ही बकवास में बदल जाती है।

412
00:25:06,950 --> 00:25:08,953
तकनीकी रूप से कहें तो, एपीआई वास्तव में आपको 2 

413
00:25:08,953 --> 00:25:10,830
से अधिक तापमान चुनने की अनुमति नहीं देता है।

414
00:25:11,170 --> 00:25:15,178
इसका कोई गणितीय कारण नहीं है, यह केवल एक मनमाना अवरोध है जो उनके उपकरण को 

415
00:25:15,178 --> 00:25:19,350
उन चीजों को उत्पन्न करने से रोकने के लिए लगाया गया है जो बहुत ही निरर्थक हैं।

416
00:25:19,870 --> 00:25:23,917
इसलिए यदि आप उत्सुक हैं, तो यह एनीमेशन वास्तव में जिस तरह से काम कर रहा है, 

417
00:25:23,917 --> 00:25:28,550
मैं जीपीटी-3 द्वारा उत्पन्न 20 सबसे संभावित अगले टोकन ले रहा हूं, जो कि वे मुझे देंगे, 

418
00:25:28,550 --> 00:25:32,970
ऐसा लगता है, और फिर मैं संभावनाओं के आधार पर बदलाव करता हूं 1 5वें के प्रतिपादक पर।

419
00:25:33,130 --> 00:25:37,452
शब्दजाल के एक अन्य अंश के रूप में, जिस तरह से आप इस फ़ंक्शन के आउटपुट के घटकों को 

420
00:25:37,452 --> 00:25:41,774
प्रायिकताएँ कह सकते हैं, लोग अक्सर इनपुट को लॉगिट्स के रूप में संदर्भित करते हैं, 

421
00:25:41,774 --> 00:25:46,150
या कुछ लोग लॉगिट्स कहते हैं, कुछ लोग लॉगिट्स कहते हैं, मैं लॉगिट्स कहने जा रहा हूँ।

422
00:25:46,530 --> 00:25:50,331
उदाहरण के लिए, जब आप कुछ पाठ फीड करते हैं, तो आपके पास ये सभी शब्द एम्बेडिंग 

423
00:25:50,331 --> 00:25:54,034
नेटवर्क के माध्यम से प्रवाहित होते हैं, और आप अनएम्बेडिंग मैट्रिक्स के साथ 

424
00:25:54,034 --> 00:25:56,798
यह अंतिम गुणन करते हैं, मशीन लर्निंग वाले लोग उस कच्चे, 

425
00:25:56,798 --> 00:26:00,402
असामान्य आउटपुट में घटकों को अगले शब्द की भविष्यवाणी के लिए लॉगिट के रूप 

426
00:26:00,402 --> 00:26:01,390
में संदर्भित करेंगे।

427
00:26:03,330 --> 00:26:06,815
इस अध्याय का अधिकांश लक्ष्य ध्यान तंत्र, कराटे किड 

428
00:26:06,815 --> 00:26:10,370
वैक्स-ऑन-वैक्स-ऑफ शैली को समझने के लिए नींव रखना था।

429
00:26:10,850 --> 00:26:14,780
आप देखते हैं, यदि आपके पास शब्द एम्बेडिंग के लिए, सॉफ्टमैक्स के लिए, 

430
00:26:14,780 --> 00:26:18,767
डॉट उत्पाद समानता को कैसे मापते हैं, इसके लिए एक मजबूत अंतर्ज्ञान है, 

431
00:26:18,767 --> 00:26:23,039
और यह अंतर्निहित आधार भी है कि अधिकांश गणनाओं को ट्यून करने योग्य पैरामीटर 

432
00:26:23,039 --> 00:26:27,653
से भरे मैट्रिक्स के साथ मैट्रिक्स गुणन की तरह दिखना है, तो ध्यान को समझें तंत्र, 

433
00:26:27,653 --> 00:26:32,210
एआई में संपूर्ण आधुनिक उछाल में यह आधारशिला टुकड़ा, अपेक्षाकृत चिकना होना चाहिए।

434
00:26:32,650 --> 00:26:34,510
इसके लिए, अगले अध्याय में मेरे साथ आइए।

435
00:26:36,390 --> 00:26:38,842
जैसा कि मैं इसे प्रकाशित कर रहा हूं, उस अगले अध्याय का एक 

436
00:26:38,842 --> 00:26:41,210
मसौदा पैट्रियन समर्थकों द्वारा समीक्षा के लिए उपलब्ध है।

437
00:26:41,770 --> 00:26:44,096
अंतिम संस्करण एक या दो सप्ताह में सार्वजनिक हो जाना चाहिए, 

438
00:26:44,096 --> 00:26:47,370
यह आमतौर पर इस पर निर्भर करता है कि मैं उस समीक्षा के आधार पर कितना बदलाव करता हूँ।

439
00:26:47,810 --> 00:26:51,148
इस बीच, यदि आप ध्यान आकर्षित करना चाहते हैं, और चैनल की थोड़ी मदद करना चाहते हैं, 

440
00:26:51,148 --> 00:26:52,410
तो यह आपकी प्रतीक्षा कर रहा है।

