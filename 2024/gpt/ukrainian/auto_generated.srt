1
00:00:00,000 --> 00:00:02,141
Ініціали GPT розшифровуються як Generative Pretrained 

2
00:00:02,141 --> 00:00:04,560
Transformer (генеративний попередньо навчений трансформатор).

3
00:00:05,220 --> 00:00:09,020
Щоб перше слово було зрозумілим, це боти, які генерують новий текст.

4
00:00:09,800 --> 00:00:13,687
Pretrained означає, що модель пройшла процес навчання на великій кількості даних, 

5
00:00:13,687 --> 00:00:17,005
а префікс вказує на те, що за допомогою додаткового навчання є більше 

6
00:00:17,005 --> 00:00:20,040
можливостей для її точного налаштування на конкретних завданнях.

7
00:00:20,720 --> 00:00:22,900
Але останнє слово - це справді ключовий момент.

8
00:00:23,380 --> 00:00:27,395
Трансформатор - це особливий вид нейронної мережі, модель машинного навчання, 

9
00:00:27,395 --> 00:00:31,000
і це ключовий винахід, що лежить в основі нинішнього буму в галузі ШІ.

10
00:00:31,740 --> 00:00:35,666
Цим відео та наступними розділами я хочу наочно пояснити, 

11
00:00:35,666 --> 00:00:39,120
що насправді відбувається всередині трансформатора.

12
00:00:39,700 --> 00:00:42,820
Ми будемо стежити за даними, які проходять через нього, і робити крок за кроком.

13
00:00:43,440 --> 00:00:47,380
Існує багато різних типів моделей, які можна побудувати за допомогою трансформаторів.

14
00:00:47,800 --> 00:00:50,800
Деякі моделі приймають аудіо і створюють транскрипт.

15
00:00:51,340 --> 00:00:53,929
Це речення походить від моделі, яка працює навпаки, 

16
00:00:53,929 --> 00:00:56,220
створюючи синтетичне мовлення просто з тексту.

17
00:00:56,660 --> 00:01:01,117
Всі ті інструменти, які підкорили світ у 2022 році, такі як Dolly і Midjourney, 

18
00:01:01,117 --> 00:01:05,519
що приймають текстовий опис і видають зображення, засновані на трансформаторах.

19
00:01:06,000 --> 00:01:10,345
Навіть якщо я не можу змусити його зрозуміти, що таке істота з пирога, 

20
00:01:10,345 --> 00:01:13,100
я все одно вражений, що таке взагалі можливо.

21
00:01:13,900 --> 00:01:17,478
А оригінальний трансформер, представлений у 2017 році компанією Google, 

22
00:01:17,478 --> 00:01:21,453
був винайдений для конкретного випадку використання - перекладу тексту з однієї 

23
00:01:21,453 --> 00:01:22,100
мови на іншу.

24
00:01:22,660 --> 00:01:28,041
Але варіант, на якому ми з вами зосередимося, і який лежить в основі таких інструментів, 

25
00:01:28,041 --> 00:01:32,334
як ChatGPT, - це модель, яка навчена сприймати шматок тексту, можливо, 

26
00:01:32,334 --> 00:01:36,929
навіть із зображеннями або звуком, що його супроводжують, і передбачати те, 

27
00:01:36,929 --> 00:01:38,260
що буде далі в уривку.

28
00:01:38,600 --> 00:01:42,544
Це передбачення має форму розподілу ймовірностей для багатьох різних фрагментів тексту, 

29
00:01:42,544 --> 00:01:43,800
які можуть слідувати за ним.

30
00:01:45,040 --> 00:01:47,650
На перший погляд, ви можете подумати, що передбачення наступного 

31
00:01:47,650 --> 00:01:49,940
слова - це зовсім інша мета, ніж створення нового тексту.

32
00:01:50,180 --> 00:01:53,899
Але коли у вас є така модель прогнозування, ви можете просто згенерувати 

33
00:01:53,899 --> 00:01:57,159
довший фрагмент тексту, дати їй початковий фрагмент для роботи, 

34
00:01:57,159 --> 00:02:01,184
попросити її взяти випадкову вибірку з розподілу, який вона щойно згенерувала, 

35
00:02:01,184 --> 00:02:04,547
додати цю вибірку до тексту, а потім запустити весь процес знову, 

36
00:02:04,547 --> 00:02:08,470
щоб зробити нове передбачення на основі всього нового тексту, включно з тим, 

37
00:02:08,470 --> 00:02:09,539
що вона щойно додала.

38
00:02:10,100 --> 00:02:13,000
Не знаю, як вам, а мені здається, що це не спрацює.

39
00:02:13,420 --> 00:02:16,452
У цій анімації, наприклад, я запускаю GPT-2 на своєму ноутбуці 

40
00:02:16,452 --> 00:02:19,965
і змушую його постійно передбачати і вибирати наступний фрагмент тексту, 

41
00:02:19,965 --> 00:02:22,420
щоб згенерувати історію на основі вихідного тексту.

42
00:02:22,420 --> 00:02:26,120
Історія просто не має особливого сенсу.

43
00:02:26,500 --> 00:02:30,903
Але якщо я заміню його на виклики API до GPT-3, який є тією ж базовою моделлю, 

44
00:02:30,903 --> 00:02:35,640
тільки набагато більшою, раптом, майже магічним чином, ми отримаємо розумну історію, 

45
00:02:35,640 --> 00:02:40,322
яка навіть дозволяє припустити, що істота з числом пі жила б у країні математики та 

46
00:02:40,322 --> 00:02:40,880
обчислень.

47
00:02:41,580 --> 00:02:45,408
Цей процес повторного передбачення і вибірки - це те, що відбувається, 

48
00:02:45,408 --> 00:02:49,884
коли ви взаємодієте з ChatGPT або будь-якою іншою великою мовною моделлю і бачите, 

49
00:02:49,884 --> 00:02:51,880
як вона видає по одному слову за раз.

50
00:02:52,480 --> 00:02:55,195
Насправді, одна з функцій, яка мені б дуже сподобалася, 

51
00:02:55,195 --> 00:02:59,220
- це можливість бачити базову дистрибуцію для кожного нового слова, яке він обирає.

52
00:03:03,820 --> 00:03:06,602
Давайте почнемо з дуже високого рівня попереднього перегляду того, 

53
00:03:06,602 --> 00:03:08,180
як дані проходять через трансформатор.

54
00:03:08,640 --> 00:03:11,312
Ми витрачатимемо набагато більше часу на мотивацію, 

55
00:03:11,312 --> 00:03:14,600
тлумачення та деталізацію кожного кроку, але в загальних рисах, 

56
00:03:14,600 --> 00:03:18,660
коли один з цих чат-ботів генерує певне слово, ось що відбувається під капотом.

57
00:03:19,080 --> 00:03:22,040
Спочатку вхідні дані розбиваються на купу маленьких шматочків.

58
00:03:22,620 --> 00:03:26,321
Ці фрагменти називаються токенами, і у випадку з текстом це, як правило, 

59
00:03:26,321 --> 00:03:29,820
слова, невеликі фрагменти слів або інші поширені комбінації символів.

60
00:03:30,740 --> 00:03:34,015
Якщо йдеться про зображення або звук, то токенами можуть бути 

61
00:03:34,015 --> 00:03:37,080
маленькі фрагменти зображення або маленькі шматочки звуку.

62
00:03:37,580 --> 00:03:41,945
Кожен з цих токенів потім асоціюється з вектором, тобто деяким списком чисел, 

63
00:03:41,945 --> 00:03:45,360
який повинен якимось чином кодувати значення цього фрагменту.

64
00:03:45,880 --> 00:03:49,112
Якщо уявити ці вектори як координати в деякому дуже вимірному просторі, 

65
00:03:49,112 --> 00:03:52,165
то слова зі схожими значеннями, як правило, потрапляють на вектори, 

66
00:03:52,165 --> 00:03:54,680
які знаходяться близько один до одного в цьому просторі.

67
00:03:55,280 --> 00:03:57,901
Потім ця послідовність векторів проходить через операцію, 

68
00:03:57,901 --> 00:04:01,110
яка називається блокуванням уваги, і це дозволяє векторам спілкуватися 

69
00:04:01,110 --> 00:04:04,500
один з одним і передавати інформацію туди-сюди для оновлення своїх значень.

70
00:04:04,880 --> 00:04:08,291
Наприклад, значення слова "модель" у словосполученні "модель машинного 

71
00:04:08,291 --> 00:04:11,800
навчання" відрізняється від його значення у словосполученні "фотомодель".

72
00:04:12,260 --> 00:04:17,203
Блок уваги відповідає за визначення того, які слова в контексті є релевантними 

73
00:04:17,203 --> 00:04:21,959
для оновлення значень інших слів, і як саме ці значення мають бути оновлені.

74
00:04:22,500 --> 00:04:25,270
І знову ж таки, щоразу, коли я використовую слово "значення", 

75
00:04:25,270 --> 00:04:28,040
воно якимось чином повністю закодоване у записах цих векторів.

76
00:04:29,180 --> 00:04:33,148
Після цього ці вектори проходять через інші операції, і залежно від джерела, 

77
00:04:33,148 --> 00:04:37,066
яке ви читаєте, це буде називатися багатошаровим персептроном або, можливо, 

78
00:04:37,066 --> 00:04:38,200
шаром прямого зв'язку.

79
00:04:38,580 --> 00:04:40,663
А тут вектори не розмовляють один з одним, вони 

80
00:04:40,663 --> 00:04:42,660
всі проходять одну і ту ж операцію паралельно.

81
00:04:43,060 --> 00:04:47,010
І хоча цей блок трохи складніше інтерпретувати, пізніше ми поговоримо про те, 

82
00:04:47,010 --> 00:04:51,062
що цей крок трохи схожий на задавання довгого списку запитань про кожен вектор, 

83
00:04:51,062 --> 00:04:54,000
а потім їх оновлення на основі відповідей на ці запитання.

84
00:04:54,900 --> 00:04:59,531
Всі операції в обох цих блоках виглядають як гігантська купа матричних множень, 

85
00:04:59,531 --> 00:05:04,046
і наша основна робота буде полягати в тому, щоб зрозуміти, як читати матриці, 

86
00:05:04,046 --> 00:05:05,320
що лежать в їх основі.

87
00:05:06,980 --> 00:05:09,520
Я не висвітлюю деякі деталі про деякі кроки нормалізації, 

88
00:05:09,520 --> 00:05:12,980
які відбуваються між ними, але це, зрештою, попередній перегляд високого рівня.

89
00:05:13,680 --> 00:05:18,406
Після цього процес по суті повторюється, ви ходите туди-сюди між блоками уваги і 

90
00:05:18,406 --> 00:05:23,132
багатошаровими перцептронними блоками, поки в самому кінці не з'являється надія, 

91
00:05:23,132 --> 00:05:27,683
що весь основний сенс уривка якимось чином був запечений в останньому векторі 

92
00:05:27,683 --> 00:05:28,500
послідовності.

93
00:05:28,920 --> 00:05:31,846
Потім ми виконуємо певну операцію над цим останнім вектором, 

94
00:05:31,846 --> 00:05:34,965
яка створює розподіл ймовірностей над усіма можливими лексемами, 

95
00:05:34,965 --> 00:05:38,420
усіма можливими маленькими шматочками тексту, які можуть з'явитися далі.

96
00:05:38,980 --> 00:05:41,878
І як я вже казав, якщо у вас є інструмент, який прогнозує, 

97
00:05:41,878 --> 00:05:45,366
що буде далі за фрагментом тексту, ви можете дати йому трохи вихідного 

98
00:05:45,366 --> 00:05:48,511
тексту і змусити його повторювати цю гру з прогнозуванням того, 

99
00:05:48,511 --> 00:05:51,458
що буде далі, вибираючи вибірку з дистрибутиву, додаючи її, 

100
00:05:51,458 --> 00:05:53,080
а потім повторюючи знову і знову.

101
00:05:53,640 --> 00:05:57,266
Дехто з вас, можливо, пам'ятає, як задовго до появи ChatGPT 

102
00:05:57,266 --> 00:06:00,953
на сцені виглядали ранні демо-версії GPT-3: вони автоматично 

103
00:06:00,953 --> 00:06:04,640
доповнювали розповіді та есе на основі початкового фрагмента.

104
00:06:05,580 --> 00:06:09,802
Щоб перетворити такий інструмент на чат-бота, найпростіша відправна точка - написати 

105
00:06:09,802 --> 00:06:14,124
невеликий текст, який встановлює параметри взаємодії користувача з корисним помічником 

106
00:06:14,124 --> 00:06:17,303
зі штучним інтелектом, те, що ви називаєте системною підказкою, 

107
00:06:17,303 --> 00:06:21,475
а потім використати початкове запитання або підказку користувача як перший фрагмент 

108
00:06:21,475 --> 00:06:25,499
діалогу, після чого він почне передбачати, що такий корисний помічник зі штучним 

109
00:06:25,499 --> 00:06:26,940
інтелектом скаже у відповідь.

110
00:06:27,720 --> 00:06:31,315
Можна ще багато чого сказати про етап навчання, який необхідний для того, 

111
00:06:31,315 --> 00:06:33,940
щоб це працювало добре, але на високому рівні це ідея.

112
00:06:35,720 --> 00:06:40,111
У цій главі ми з вами детально розглянемо, що відбувається на самому початку 

113
00:06:40,111 --> 00:06:44,217
мережі і в самому кінці мережі, а також я хочу витратити багато часу на 

114
00:06:44,217 --> 00:06:48,608
огляд деяких важливих базових знань, які стали другою натурою для будь-якого 

115
00:06:48,608 --> 00:06:52,600
інженера машинного навчання на той час, коли з'явилися трансформатори.

116
00:06:53,060 --> 00:06:56,129
Якщо вам достатньо цих базових знань і ви трохи нетерплячі, 

117
00:06:56,129 --> 00:06:58,636
можете сміливо переходити до наступного розділу, 

118
00:06:58,636 --> 00:07:02,780
в якому мова піде про блоки уваги, які зазвичай вважаються серцем трансформатора.

119
00:07:03,360 --> 00:07:07,594
Після цього я хочу поговорити більше про ці багатошарові перцептронні блоки, про те, 

120
00:07:07,594 --> 00:07:11,680
як працює навчання, і про низку інших деталей, які ми пропустили до цього моменту.

121
00:07:12,180 --> 00:07:15,310
Для більш широкого контексту, ці відео є доповненням до міні-серіалу про 

122
00:07:15,310 --> 00:07:18,655
глибоке навчання, і нічого страшного, якщо ви не дивилися попередні, я думаю, 

123
00:07:18,655 --> 00:07:21,272
що ви можете зробити це в довільному порядку, але перед тим, 

124
00:07:21,272 --> 00:07:24,617
як зануритися безпосередньо в трансформатори, я думаю, що варто переконатися, 

125
00:07:24,617 --> 00:07:27,705
що ми знаходимося на одній сторінці щодо основних передумов і структури 

126
00:07:27,705 --> 00:07:28,520
глибокого навчання.

127
00:07:29,020 --> 00:07:33,005
Ризикуючи сказати очевидне, це один з підходів до машинного навчання, 

128
00:07:33,005 --> 00:07:36,193
який описує будь-яку модель, де ви використовуєте дані, 

129
00:07:36,193 --> 00:07:38,300
щоб якось визначити поведінку моделі.

130
00:07:39,140 --> 00:07:41,697
Я маю на увазі, що, скажімо, вам потрібна функція, 

131
00:07:41,697 --> 00:07:44,405
яка отримує зображення і видає мітку, що його описує, 

132
00:07:44,405 --> 00:07:47,664
або наш приклад передбачення наступного слова за уривком тексту, 

133
00:07:47,664 --> 00:07:51,075
або будь-яке інше завдання, яке, здається, вимагає певного елементу 

134
00:07:51,075 --> 00:07:52,780
інтуїції та розпізнавання образів.

135
00:07:53,200 --> 00:07:57,494
Сьогодні ми сприймаємо це як належне, але ідея машинного навчання полягає в тому, 

136
00:07:57,494 --> 00:08:01,893
що замість того, щоб намагатися явно визначити процедуру виконання завдання в коді, 

137
00:08:01,893 --> 00:08:06,031
як це робили люди на початку розвитку ШІ, ви створюєте дуже гнучку структуру з 

138
00:08:06,031 --> 00:08:09,592
параметрами, що налаштовуються, на кшталт купи ручок і циферблатів, 

139
00:08:09,592 --> 00:08:12,315
а потім якось використовуєте безліч прикладів того, 

140
00:08:12,315 --> 00:08:15,143
як повинен виглядати вихід при заданих вхідних даних, 

141
00:08:15,143 --> 00:08:19,700
щоб підлаштовувати і налаштовувати значення цих параметрів, щоб імітувати цю поведінку.

142
00:08:19,700 --> 00:08:24,216
Наприклад, найпростішою формою машинного навчання є лінійна регресія, 

143
00:08:24,216 --> 00:08:29,895
де вхідні та вихідні дані - це окремі числа, щось на кшталт площі будинку та його ціни, 

144
00:08:29,895 --> 00:08:34,154
і ви хочете знайти лінію найкращої відповідності між цими даними, 

145
00:08:34,154 --> 00:08:36,799
тобто передбачити майбутні ціни на житло.

146
00:08:37,440 --> 00:08:42,590
Ця лінія описується двома неперервними параметрами, скажімо, нахилом та y-інтервалом, 

147
00:08:42,590 --> 00:08:45,884
і метою лінійної регресії є визначення цих параметрів, 

148
00:08:45,884 --> 00:08:48,160
щоб вони якнайкраще відповідали даним.

149
00:08:48,880 --> 00:08:52,100
Зрозуміло, що моделі глибокого навчання стають набагато складнішими.

150
00:08:52,620 --> 00:08:57,660
GPT-3, наприклад, має не два, а 175 мільярдів параметрів.

151
00:08:58,120 --> 00:09:01,844
Але справа в тому, що не факт, що ви можете створити якусь гігантську 

152
00:09:01,844 --> 00:09:04,558
модель з величезною кількістю параметрів без того, 

153
00:09:04,558 --> 00:09:07,112
щоб вона або сильно перекривала навчальні дані, 

154
00:09:07,112 --> 00:09:09,560
або була абсолютно непіддатливою для навчання.

155
00:09:10,260 --> 00:09:14,590
Глибоке навчання описує клас моделей, які за останні кілька десятиліть довели, 

156
00:09:14,590 --> 00:09:16,180
що вони чудово масштабуються.

157
00:09:16,480 --> 00:09:21,197
Їх об'єднує однаковий алгоритм навчання, який називається зворотним поширенням, 

158
00:09:21,197 --> 00:09:25,442
і контекст, який я хочу, щоб ви зрозуміли, полягає в тому, що для того, 

159
00:09:25,442 --> 00:09:28,567
щоб цей алгоритм навчання добре працював у масштабі, 

160
00:09:28,567 --> 00:09:31,280
ці моделі повинні відповідати певному формату.

161
00:09:31,800 --> 00:09:35,850
Якщо ви знаєте цей формат, він допомагає пояснити багато варіантів того, 

162
00:09:35,850 --> 00:09:40,400
як трансформатор обробляє мову, які в іншому випадку ризикують здатися довільними.

163
00:09:41,440 --> 00:09:44,090
По-перше, яку б модель ви не будували, вхідні дані 

164
00:09:44,090 --> 00:09:46,740
повинні бути відформатовані як масив дійсних чисел.

165
00:09:46,740 --> 00:09:50,310
Це може бути список чисел, це може бути двовимірний масив, 

166
00:09:50,310 --> 00:09:54,123
або дуже часто ви маєте справу з масивами більшої розмірності, 

167
00:09:54,123 --> 00:09:56,000
де загальним терміном є тензор.

168
00:09:56,560 --> 00:10:01,225
Ви часто думаєте, що вхідні дані поступово трансформуються у багато різних шарів, 

169
00:10:01,225 --> 00:10:04,981
де кожен шар завжди структурований як певний масив дійсних чисел, 

170
00:10:04,981 --> 00:10:08,680
доки ви не дійдете до останнього шару, який ви вважаєте вихідним.

171
00:10:09,280 --> 00:10:13,196
Наприклад, останній шар у нашій моделі обробки тексту - це список чисел, 

172
00:10:13,196 --> 00:10:17,060
що представляє розподіл ймовірностей для всіх можливих наступних лексем.

173
00:10:17,820 --> 00:10:21,883
У глибокому навчанні ці параметри моделі майже завжди називаються вагами, 

174
00:10:21,883 --> 00:10:25,781
і це тому, що ключовою особливістю цих моделей є те, що єдиний спосіб, 

175
00:10:25,781 --> 00:10:29,900
яким ці параметри взаємодіють з даними, що обробляються, - це зважені суми.

176
00:10:30,340 --> 00:10:34,360
Ви також розкидаєте деякі нелінійні функції, але вони не будуть залежати від параметрів.

177
00:10:35,200 --> 00:10:40,562
Зазвичай, замість того, щоб бачити зважені суми "голими" і виписаними в явному вигляді, 

178
00:10:40,562 --> 00:10:45,620
ви бачите їх упакованими разом як різні компоненти в матричному векторному добутку.

179
00:10:46,740 --> 00:10:50,970
Це те ж саме, якщо згадати, як працює матричне множення векторів, 

180
00:10:50,970 --> 00:10:54,240
кожен компонент на виході виглядає як зважена сума.

181
00:10:54,780 --> 00:10:58,917
Просто часто концептуально чистіше для нас з вами думати про матриці, 

182
00:10:58,917 --> 00:11:03,410
які заповнюються параметрами, що налаштовуються, які трансформують вектори, 

183
00:11:03,410 --> 00:11:05,420
отримані з даних, що обробляються.

184
00:11:06,340 --> 00:11:14,160
Наприклад, 175 мільярдів ваг у GPT-3 організовано у трохи менше ніж 28 000 різних матриць.

185
00:11:14,660 --> 00:11:18,560
Ці матриці, в свою чергу, поділяються на вісім різних категорій, 

186
00:11:18,560 --> 00:11:22,700
і ми з вами розглянемо кожну з них, щоб зрозуміти, що робить цей тип.

187
00:11:23,160 --> 00:11:27,370
Я думаю, що буде цікаво згадати конкретні цифри з GPT-3, 

188
00:11:27,370 --> 00:11:31,360
щоб підрахувати, звідки саме взялися ці 175 мільярдів.

189
00:11:31,880 --> 00:11:35,271
Навіть якщо сьогодні існують більші та кращі моделі, ця модель має певний шарм, 

190
00:11:35,271 --> 00:11:37,984
оскільки вона є великомовною і справді привертає увагу світової 

191
00:11:37,984 --> 00:11:40,740
спільноти поза межами спільнот, що займаються відмиванням грошей.

192
00:11:41,440 --> 00:11:44,009
Крім того, на практиці компанії, як правило, не 

193
00:11:44,009 --> 00:11:46,740
називають конкретні цифри для більш сучасних мереж.

194
00:11:47,360 --> 00:11:50,768
Я просто хочу пояснити, що коли ви зазирнете під капот, щоб побачити, 

195
00:11:50,768 --> 00:11:54,226
що відбувається всередині такого інструменту, як ChatGPT, то побачите, 

196
00:11:54,226 --> 00:11:57,440
що майже всі обчислення виглядають як множення матричних векторів.

197
00:11:57,900 --> 00:12:01,022
Існує невеликий ризик загубитися в морі мільярдів цифр, 

198
00:12:01,022 --> 00:12:04,758
але ви повинні провести дуже чітке розмежування між вагами моделі, 

199
00:12:04,758 --> 00:12:09,442
які я завжди буду позначати синім або червоним кольором, і даними, що обробляються, 

200
00:12:09,442 --> 00:12:11,840
які я завжди буду позначати сірим кольором.

201
00:12:12,180 --> 00:12:15,654
Обтяження - це власне мозок, це те, що вивчається під час тренувань, 

202
00:12:15,654 --> 00:12:17,920
і саме вони визначають, як він себе поводить.

203
00:12:18,280 --> 00:12:22,419
Дані, що обробляються, просто кодують будь-які конкретні вхідні дані, 

204
00:12:22,419 --> 00:12:26,500
що подаються в модель для даного прогону, наприклад, фрагмент тексту.

205
00:12:27,480 --> 00:12:30,320
Маючи все це в якості основи, давайте розглянемо перший крок 

206
00:12:30,320 --> 00:12:33,253
цього прикладу обробки тексту, який полягає в розбитті вхідних 

207
00:12:33,253 --> 00:12:36,420
даних на невеликі фрагменти і перетворенні цих фрагментів у вектори.

208
00:12:37,020 --> 00:12:39,549
Я вже згадував, що ці шматки називаються токенами, 

209
00:12:39,549 --> 00:12:43,765
які можуть бути шматками слів або розділовими знаками, але час від часу в цій главі, 

210
00:12:43,765 --> 00:12:48,080
а особливо в наступній, я хотів би просто уявити, що вона розбита на слова більш чітко.

211
00:12:48,600 --> 00:12:51,291
Оскільки ми, люди, мислимо словами, це значно полегшить 

212
00:12:51,291 --> 00:12:54,080
посилання на маленькі приклади та пояснення кожного кроку.

213
00:12:55,260 --> 00:12:59,902
Модель має заздалегідь визначений словник, деякий список усіх можливих слів, 

214
00:12:59,902 --> 00:13:03,398
скажімо, 50 000, і перша матриця, з якою ми зустрінемося, 

215
00:13:03,398 --> 00:13:07,800
відома як матриця вбудовування, має один стовпчик для кожного з цих слів.

216
00:13:08,940 --> 00:13:13,760
Саме ці стовпчики визначають, на який вектор перетворюється кожне слово на першому кроці.

217
00:13:15,100 --> 00:13:18,180
Ми позначимо її We, і, як і всі матриці, які ми бачимо, 

218
00:13:18,180 --> 00:13:22,360
її значення спочатку випадкові, але вони будуть визначатися на основі даних.

219
00:13:23,620 --> 00:13:26,491
Перетворення слів на вектори було поширеною практикою в машинному 

220
00:13:26,491 --> 00:13:29,320
навчанні задовго до появи трансформаторів, але буде трохи дивно, 

221
00:13:29,320 --> 00:13:32,627
якщо ви ніколи не бачили його раніше, і воно закладає фундамент для всього, 

222
00:13:32,627 --> 00:13:35,760
що буде далі, тому давайте приділимо трохи часу, щоб ознайомитися з ним.

223
00:13:36,040 --> 00:13:39,829
Ми часто називаємо це вбудовування словом, яке запрошує вас думати про ці 

224
00:13:39,829 --> 00:13:43,620
вектори дуже геометрично, як про точки в деякому високовимірному просторі.

225
00:13:44,180 --> 00:13:47,980
Візуалізація списку з трьох чисел як координат точок у тривимірному просторі не 

226
00:13:47,980 --> 00:13:51,780
буде проблемою, але вставки слів, як правило, мають набагато більшу розмірність.

227
00:13:52,280 --> 00:13:55,921
У GPT-3 вони мають 12 288 вимірів, і, як ви побачите, 

228
00:13:55,921 --> 00:14:00,440
це важливо для роботи в просторі, який має багато різних напрямків.

229
00:14:01,180 --> 00:14:06,212
Так само, як ви можете взяти двовимірний зріз у тривимірному просторі і спроектувати 

230
00:14:06,212 --> 00:14:11,481
всі точки на цей зріз, для того, щоб анімувати вставки слів, які дає мені проста модель, 

231
00:14:11,481 --> 00:14:16,809
я зроблю аналогічну річ, вибравши тривимірний зріз у цьому дуже високовимірному просторі, 

232
00:14:16,809 --> 00:14:20,480
спроектувавши на нього вектори слів і відобразивши результати.

233
00:14:21,280 --> 00:14:25,630
Основна ідея полягає в тому, що коли модель налаштовує свої ваги, щоб визначити, 

234
00:14:25,630 --> 00:14:28,638
як саме слова вбудовуються як вектори під час навчання, 

235
00:14:28,638 --> 00:14:31,485
вона має тенденцію зупинятися на наборі вбудовувань, 

236
00:14:31,485 --> 00:14:34,440
де напрямки в просторі мають певне семантичне значення.

237
00:14:34,980 --> 00:14:38,217
Для простої моделі "слово-вектор", яку я використовую тут, 

238
00:14:38,217 --> 00:14:42,497
якщо я запущу пошук всіх слів, які мають найближче входження до слова "вежа", 

239
00:14:42,497 --> 00:14:45,900
ви помітите, що всі вони створюють дуже схожі вежові відчуття.

240
00:14:46,340 --> 00:14:48,642
І якщо ви хочете завантажити Python і пограти вдома, 

241
00:14:48,642 --> 00:14:51,380
це конкретна модель, яку я використовую для створення анімації.

242
00:14:51,620 --> 00:14:54,972
Це не трансформатор, але цього достатньо, щоб проілюструвати ідею про те, 

243
00:14:54,972 --> 00:14:57,600
що напрямки в просторі можуть нести смислове навантаження.

244
00:14:58,300 --> 00:15:03,391
Класичним прикладом цього є різниця між векторами жінки і чоловіка, 

245
00:15:03,391 --> 00:15:09,531
яку можна уявити як маленький вектор, що з'єднує кінчик одного з кінчиком іншого, 

246
00:15:09,531 --> 00:15:13,200
це дуже схоже на різницю між королем і королевою.

247
00:15:15,080 --> 00:15:18,327
Скажімо, якщо ви не знаєте слова, що позначає жінку-монарха, 

248
00:15:18,327 --> 00:15:21,680
ви можете знайти його, взявши слово "король", додавши до нього 

249
00:15:21,680 --> 00:15:25,460
напрям "жінка-чоловік" і пошукавши найближчі до цього слова вкраплення.

250
00:15:27,000 --> 00:15:28,200
Принаймні, начебто.

251
00:15:28,480 --> 00:15:31,744
Незважаючи на те, що це класичний приклад для моделі, з якою я граю, 

252
00:15:31,744 --> 00:15:34,677
справжнє вбудовування ферзя насправді знаходиться трохи далі, 

253
00:15:34,677 --> 00:15:38,793
ніж можна було б припустити, ймовірно, тому, що спосіб використання ферзя в навчальних 

254
00:15:38,793 --> 00:15:40,780
даних - це не просто жіноча версія короля.

255
00:15:41,620 --> 00:15:45,260
Коли я гралася, сімейні стосунки, здавалося, набагато краще ілюстрували цю ідею.

256
00:15:46,340 --> 00:15:50,406
Справа в тому, що, схоже, під час навчання модель вважала вигідним вибирати 

257
00:15:50,406 --> 00:15:54,900
вставки таким чином, щоб один напрямок у цьому просторі кодував гендерну інформацію.

258
00:15:56,800 --> 00:16:00,105
Інший приклад: якщо ви візьмете вбудовування Італії, 

259
00:16:00,105 --> 00:16:04,534
віднімете вбудовування Німеччини і додасте це до вбудовування Гітлера, 

260
00:16:04,534 --> 00:16:08,090
то отримаєте щось дуже близьке до вбудовування Муссоліні.

261
00:16:08,570 --> 00:16:12,856
Наче модель навчилася асоціювати одні напрямки з італійськістю, 

262
00:16:12,856 --> 00:16:15,670
а інші - з лідерами Другої світової війни.

263
00:16:16,470 --> 00:16:19,236
Мабуть, мій улюблений приклад у цьому сенсі - це те, 

264
00:16:19,236 --> 00:16:23,776
як у деяких моделях, якщо взяти різницю між Німеччиною та Японією і додати її до суші, 

265
00:16:23,776 --> 00:16:26,230
то в підсумку виходить дуже схоже на сардельки.

266
00:16:27,350 --> 00:16:30,150
Крім того, граючи в цю гру з пошуку найближчих сусідів, 

267
00:16:30,150 --> 00:16:33,850
я був радий побачити, наскільки Кет була близька і до звіра, і до монстра.

268
00:16:34,690 --> 00:16:38,266
Математична інтуїція, яку корисно мати на увазі, особливо для наступного розділу, 

269
00:16:38,266 --> 00:16:41,320
полягає в тому, що точковий добуток двох векторів можна розглядати як 

270
00:16:41,320 --> 00:16:43,850
спосіб вимірювання того, наскільки добре вони співпадають.

271
00:16:44,870 --> 00:16:48,067
З обчислювальної точки зору, точкові продукти передбачають перемноження 

272
00:16:48,067 --> 00:16:51,221
всіх відповідних компонентів, а потім додавання результатів, що добре, 

273
00:16:51,221 --> 00:16:54,330
оскільки так багато наших обчислень повинні виглядати як зважені суми.

274
00:16:55,190 --> 00:17:00,170
Геометрично, точковий добуток додатний, коли вектори спрямовані в однакових напрямках, 

275
00:17:00,170 --> 00:17:03,033
нульовий, якщо вони перпендикулярні, і від'ємний, 

276
00:17:03,033 --> 00:17:05,609
коли вони спрямовані в протилежних напрямках.

277
00:17:06,550 --> 00:17:10,720
Наприклад, припустимо, ви гралися з цією моделлю і припустили, 

278
00:17:10,720 --> 00:17:15,884
що вбудовування котів мінус кіт може представляти певний напрямок множинності 

279
00:17:15,884 --> 00:17:17,010
в цьому просторі.

280
00:17:17,430 --> 00:17:20,636
Щоб перевірити це, я візьму цей вектор і обчислю його точковий 

281
00:17:20,636 --> 00:17:23,843
добуток на вставки певних іменників в однині, а потім порівняю 

282
00:17:23,843 --> 00:17:27,050
його з точковими добутками з відповідними іменниками у множині.

283
00:17:27,270 --> 00:17:31,699
Якщо ви пограєтеся з цим, то помітите, що множина дійсно дає вищі значення, 

284
00:17:31,699 --> 00:17:36,070
ніж однина, що свідчить про те, що вони більше відповідають цьому напрямку.

285
00:17:37,070 --> 00:17:41,153
Також цікаво, що якщо взяти цей точковий добуток зі вставками слів 1, 

286
00:17:41,153 --> 00:17:44,129
2, 3 і так далі, то вони дають зростаючі значення, 

287
00:17:44,129 --> 00:17:49,030
тож ми ніби можемо кількісно виміряти, у якій множині модель знаходить задане слово.

288
00:17:50,250 --> 00:17:53,570
Знову ж таки, специфіка того, як слова вбудовуються, вивчається за допомогою даних.

289
00:17:54,050 --> 00:17:56,373
Ця матриця вбудовування, стовпці якої показують, 

290
00:17:56,373 --> 00:17:59,550
що відбувається з кожним словом, є першою купою ваг у нашій моделі.

291
00:18:00,030 --> 00:18:04,503
Використовуючи цифри GPT-3, розмір словника становить 50 257, 

292
00:18:04,503 --> 00:18:09,770
і знову ж таки, технічно він складається не зі слів як таких, а з лексем.

293
00:18:10,630 --> 00:18:14,631
Розмірність вбудовування становить 12 288, і якщо помножити її на це число, 

294
00:18:14,631 --> 00:18:17,790
то вийде, що вона складається з приблизно 617 мільйонів ваг.

295
00:18:18,250 --> 00:18:21,134
Давайте додамо це до поточного підрахунку, пам'ятаючи, 

296
00:18:21,134 --> 00:18:23,810
що до кінця ми повинні нарахувати до 175 мільярдів.

297
00:18:25,430 --> 00:18:28,626
У випадку з трансформаторами ви дійсно хочете думати про вектори в цьому 

298
00:18:28,626 --> 00:18:32,130
просторі вбудовування як про щось більше, ніж просто представлення окремих слів.

299
00:18:32,550 --> 00:18:36,091
З одного боку, вони також кодують інформацію про позицію цього слова, 

300
00:18:36,091 --> 00:18:38,621
про яку ми поговоримо пізніше, але, що важливіше, 

301
00:18:38,621 --> 00:18:42,770
ви повинні думати про них як про такі, що мають здатність вбирати в себе контекст.

302
00:18:43,350 --> 00:18:47,301
Вектор, який починав своє життя як вбудовування слова король, наприклад, 

303
00:18:47,301 --> 00:18:51,306
може поступово підтягуватися і витягуватися різними блоками в цій мережі, 

304
00:18:51,306 --> 00:18:55,636
так що в кінці він вказує в набагато більш конкретному і нюансованому напрямку, 

305
00:18:55,636 --> 00:18:59,209
який так чи інакше кодує, що це був король, який жив у Шотландії, 

306
00:18:59,209 --> 00:19:02,619
і який отримав свою посаду після вбивства попереднього короля, 

307
00:19:02,619 --> 00:19:04,730
і який описується шекспірівською мовою.

308
00:19:05,210 --> 00:19:07,790
Подумайте, як ви розумієте те чи інше слово.

309
00:19:08,250 --> 00:19:11,150
Значення цього слова чітко залежить від оточення, 

310
00:19:11,150 --> 00:19:15,326
а іноді й від контексту на великій відстані, тому при створенні моделі, 

311
00:19:15,326 --> 00:19:19,329
яка може передбачити, яке слово буде наступним, мета полягає в тому, 

312
00:19:19,329 --> 00:19:23,390
щоб якимось чином надати їй можливість ефективно враховувати контекст.

313
00:19:24,050 --> 00:19:28,106
Щоб було зрозуміло, на першому кроці, коли ви створюєте масив векторів на основі 

314
00:19:28,106 --> 00:19:31,762
вхідного тексту, кожен з них просто висмикується з матриці вбудовування, 

315
00:19:31,762 --> 00:19:35,968
тому спочатку кожен з них може кодувати значення лише одного слова без жодних даних 

316
00:19:35,968 --> 00:19:36,770
з його оточення.

317
00:19:37,710 --> 00:19:41,384
Але ви повинні думати про основну мету цієї мережі, через яку вона проходить, 

318
00:19:41,384 --> 00:19:44,871
як про те, щоб дозволити кожному з цих векторів увібрати в себе значення, 

319
00:19:44,871 --> 00:19:48,970
яке набагато багатше і конкретніше, ніж те, що можуть представляти просто окремі слова.

320
00:19:49,510 --> 00:19:52,859
Мережа може обробляти лише фіксовану кількість векторів за один раз, 

321
00:19:52,859 --> 00:19:54,170
відому як розмір контексту.

322
00:19:54,510 --> 00:19:58,572
Для GPT-3 він був навчений з розміром контексту 2048, тому дані, 

323
00:19:58,572 --> 00:20:03,010
що проходять через мережу, завжди виглядають як масив з 2048 стовпців, 

324
00:20:03,010 --> 00:20:05,010
кожен з яких має 12 000 вимірів.

325
00:20:05,590 --> 00:20:08,657
Розмір контексту обмежує обсяг тексту, який трансформатор 

326
00:20:08,657 --> 00:20:11,830
може включити в себе під час прогнозування наступного слова.

327
00:20:12,370 --> 00:20:15,316
Ось чому довгі розмови з деякими чат-ботами, наприклад, 

328
00:20:15,316 --> 00:20:18,209
з ранніми версіями ChatGPT, часто створювали відчуття, 

329
00:20:18,209 --> 00:20:22,050
що бот втрачає нитку розмови, коли ви продовжуєте говорити занадто довго.

330
00:20:23,030 --> 00:20:26,275
Ми ще поговоримо про увагу в деталях у свій час, але, забігаючи наперед, 

331
00:20:26,275 --> 00:20:28,810
я хочу поговорити про те, що відбувається в самому кінці.

332
00:20:29,450 --> 00:20:33,401
Пам'ятайте, що бажаний результат - це розподіл ймовірностей для всіх токенів, 

333
00:20:33,401 --> 00:20:34,870
які можуть прийти наступними.

334
00:20:35,170 --> 00:20:39,982
Наприклад, якщо останнє слово - професор, а в контексті зустрічаються такі слова, 

335
00:20:39,982 --> 00:20:44,208
як Гаррі Поттер, а безпосередньо перед ним - найменш улюблений вчитель, 

336
00:20:44,208 --> 00:20:49,256
і якщо ви дозволите мені припустити, що токени просто виглядають як повноцінні слова, 

337
00:20:49,256 --> 00:20:53,717
то добре навчена мережа, яка накопичила знання про Гаррі Поттера, ймовірно, 

338
00:20:53,717 --> 00:20:55,830
присвоїть високий номер слову Снейп.

339
00:20:56,510 --> 00:20:57,970
Це передбачає два різних кроки.

340
00:20:58,310 --> 00:21:02,787
Перший полягає у використанні іншої матриці, яка відображає останній вектор у 

341
00:21:02,787 --> 00:21:07,610
цьому контексті на список з 50 000 значень, по одному для кожної лексеми у словнику.

342
00:21:08,170 --> 00:21:12,641
Потім є функція, яка нормалізує це в розподіл ймовірностей, вона називається Softmax, 

343
00:21:12,641 --> 00:21:15,240
і ми поговоримо про неї докладніше через секунду, 

344
00:21:15,240 --> 00:21:19,087
але перед цим може здатися трохи дивним використовувати тільки це останнє 

345
00:21:19,087 --> 00:21:21,531
вбудовування для прогнозування, коли, зрештою, 

346
00:21:21,531 --> 00:21:24,546
на цьому останньому кроці в шарі є тисячі інших векторів, 

347
00:21:24,546 --> 00:21:28,290
які просто лежать там зі своїми власними контекстно-багатими значеннями.

348
00:21:28,930 --> 00:21:33,239
Це пов'язано з тим, що в процесі навчання виявляється набагато ефективніше, 

349
00:21:33,239 --> 00:21:36,868
якщо використовувати кожен з цих векторів у фінальному шарі для 

350
00:21:36,868 --> 00:21:40,270
одночасного прогнозування того, що буде відразу після нього.

351
00:21:40,970 --> 00:21:45,090
Пізніше я ще багато розповім про тренінги, але зараз я просто хочу наголосити на цьому.

352
00:21:45,730 --> 00:21:49,690
Ця матриця називається матрицею вилучення (Unembedding matrix) і ми позначимо її WU.

353
00:21:50,210 --> 00:21:52,612
Знову ж таки, як і у всіх вагових матрицях, які ми бачимо, 

354
00:21:52,612 --> 00:21:55,910
її елементи починаються випадковим чином, але вони вивчаються в процесі навчання.

355
00:21:56,470 --> 00:21:59,595
Зважаючи на загальну кількість параметрів, ця матриця вилучення 

356
00:21:59,595 --> 00:22:02,671
має один рядок для кожного слова в словнику, і кожен рядок має 

357
00:22:02,671 --> 00:22:05,650
таку саму кількість елементів, як і розмірність вбудовування.

358
00:22:06,410 --> 00:22:10,386
Це дуже схоже на матрицю вбудовування, тільки зі зміненим порядком, 

359
00:22:10,386 --> 00:22:13,602
тому вона додає до мережі ще 617 мільйонів параметрів, 

360
00:22:13,602 --> 00:22:17,170
тобто поки що ми нарахували трохи більше мільярда, невелику, 

361
00:22:17,170 --> 00:22:21,790
але не зовсім незначну частку від 175 мільярдів, які ми отримаємо в результаті.

362
00:22:22,550 --> 00:22:26,526
В останньому міні-уроці цієї глави я хочу поговорити про функцію softmax, 

363
00:22:26,526 --> 00:22:30,610
оскільки вона ще раз з'явиться перед нами, коли ми зануримося в блоки уваги.

364
00:22:31,430 --> 00:22:35,712
Ідея полягає в тому, що якщо ви хочете, щоб послідовність чисел діяла як розподіл 

365
00:22:35,712 --> 00:22:39,472
ймовірностей, скажімо, розподіл над усіма можливими наступними словами, 

366
00:22:39,472 --> 00:22:42,814
то кожне значення повинно бути між 0 і 1, і вам також потрібно, 

367
00:22:42,814 --> 00:22:44,590
щоб всі вони в сумі дорівнювали 1.

368
00:22:45,250 --> 00:22:48,385
Однак, якщо ви граєте в навчальну гру, де все, що ви робите, 

369
00:22:48,385 --> 00:22:51,057
виглядає як матрично-векторне множення, результати, 

370
00:22:51,057 --> 00:22:54,810
які ви отримуєте за замовчуванням, зовсім не відповідають цьому принципу.

371
00:22:55,330 --> 00:22:59,870
Значення часто від'ємні, або набагато більші за 1, і майже напевно не дорівнюють 1.

372
00:23:00,510 --> 00:23:04,161
Softmax - це стандартний спосіб перетворення довільного списку 

373
00:23:04,161 --> 00:23:09,377
чисел у дійсний розподіл таким чином, що найбільші значення виявляються найближчими до 1, 

374
00:23:09,377 --> 00:23:11,290
а найменші - дуже близькими до 0.

375
00:23:11,830 --> 00:23:13,070
Це все, що вам потрібно знати.

376
00:23:13,090 --> 00:23:17,932
Але якщо вам цікаво, то спочатку потрібно піднести e до степеня кожного з чисел, 

377
00:23:17,932 --> 00:23:21,160
що означає, що тепер у вас є список додатних значень, 

378
00:23:21,160 --> 00:23:25,165
а потім ви можете взяти суму всіх цих додатних значень і розділити 

379
00:23:25,165 --> 00:23:29,470
кожен доданок на цю суму, що нормалізує його до списку, який додає до 1.

380
00:23:30,170 --> 00:23:33,962
Ви помітите, що якщо одне з чисел у вхідних даних значно більше за решту, 

381
00:23:33,962 --> 00:23:37,242
то у вихідних даних відповідний член домінує у розподілі, тому, 

382
00:23:37,242 --> 00:23:41,393
якщо б ви робили вибірку з нього, то майже напевно просто вибрали б вхідні дані, 

383
00:23:41,393 --> 00:23:42,470
що максимізують його.

384
00:23:42,990 --> 00:23:46,185
Але це м'якше, ніж просто вибір максимуму, в тому сенсі, 

385
00:23:46,185 --> 00:23:50,838
що коли інші значення також великі, вони також отримують значущу вагу в розподілі, 

386
00:23:50,838 --> 00:23:54,650
і все змінюється безперервно, коли ви постійно змінюєте вхідні дані.

387
00:23:55,130 --> 00:23:59,640
У деяких ситуаціях, наприклад, коли ChatGPT використовує цей дистрибутив 

388
00:23:59,640 --> 00:24:03,163
для створення наступного слова, можна трохи розважитися, 

389
00:24:03,163 --> 00:24:06,129
додавши у цю функцію трохи додаткової гостроти, 

390
00:24:06,129 --> 00:24:08,910
додавши у знаменник цієї функції константу t.

391
00:24:09,550 --> 00:24:14,163
Ми називаємо її температурою, оскільки вона віддалено нагадує роль температури в 

392
00:24:14,163 --> 00:24:18,378
деяких рівняннях термодинаміки, і ефект полягає в тому, що коли t більша, 

393
00:24:18,378 --> 00:24:22,992
ви надаєте більшу вагу меншим значенням, тобто розподіл трохи більш рівномірний, 

394
00:24:22,992 --> 00:24:26,980
а якщо t менша, то більші значення будуть домінувати більш агресивно, 

395
00:24:26,980 --> 00:24:30,397
де в крайньому випадку, встановлення t рівною нулю означає, 

396
00:24:30,397 --> 00:24:32,790
що вся вага йде до максимального значення.

397
00:24:33,470 --> 00:24:38,802
Наприклад, я попрошу GPT-3 згенерувати історію з початковим текстом "Жив-був А", 

398
00:24:38,802 --> 00:24:42,950
але в кожному випадку я буду використовувати різні температури.

399
00:24:43,630 --> 00:24:48,426
Нульова температура означає, що вона завжди поєднується з найбільш передбачуваним словом, 

400
00:24:48,426 --> 00:24:52,370
і те, що ви отримуєте, виявляється банальним похідним від "Золотоволоска".

401
00:24:53,010 --> 00:24:56,260
Вища температура дає йому можливість вибирати менш ймовірні слова, 

402
00:24:56,260 --> 00:24:57,910
але це пов'язано з певним ризиком.

403
00:24:58,230 --> 00:25:01,308
У цьому випадку історія починається більш оригінально, 

404
00:25:01,308 --> 00:25:06,010
про молодого веб-художника з Південної Кореї, але швидко вироджується в нісенітницю.

405
00:25:06,950 --> 00:25:10,830
Технічно кажучи, API не дозволяє вибрати температуру, більшу за 2.

406
00:25:11,170 --> 00:25:14,888
Для цього немає жодної математичної причини, це просто довільне обмеження, 

407
00:25:14,888 --> 00:25:19,350
накладене для того, щоб не допустити, щоб їхній інструмент генерував надто безглузді речі.

408
00:25:19,870 --> 00:25:22,645
Якщо вам цікаво, як насправді працює ця анімація, 

409
00:25:22,645 --> 00:25:26,864
то я беру 20 найбільш ймовірних наступних токенів, які генерує GPT-3, а це, 

410
00:25:26,864 --> 00:25:31,193
здається, максимум, який він мені дасть, а потім я підлаштовую ймовірності на 

411
00:25:31,193 --> 00:25:32,970
основі показника експоненти 1 5.

412
00:25:33,130 --> 00:25:37,387
Як ще один жаргон, так само, як ви могли б назвати компоненти виходу 

413
00:25:37,387 --> 00:25:41,398
цієї функції ймовірностями, люди часто називають входи логітами, 

414
00:25:41,398 --> 00:25:46,150
або деякі люди кажуть логіти, деякі люди кажуть логіти, я буду казати логіти.

415
00:25:46,530 --> 00:25:51,025
Наприклад, коли ви вводите якийсь текст, всі ці вставки слів проходять через мережу, 

416
00:25:51,025 --> 00:25:54,938
і ви робите остаточне множення з матрицею вилучення, люди, що навчаються, 

417
00:25:54,938 --> 00:25:58,587
посилатимуться на компоненти в цьому сирому, ненормованому виході як 

418
00:25:58,587 --> 00:26:01,390
на логічні логіти для прогнозування наступного слова.

419
00:26:03,330 --> 00:26:06,778
Значною мірою метою цієї глави було закласти основи розуміння механізму 

420
00:26:06,778 --> 00:26:10,370
уваги в стилі карате-малюка "wax-on-wax-off" (воск на воску, віск з воску).

421
00:26:10,850 --> 00:26:15,098
Розумієте, якщо у вас є сильна інтуїція щодо вбудовування слів, softmax, 

422
00:26:15,098 --> 00:26:19,347
того, як точкові продукти вимірюють схожість, а також базова передумова, 

423
00:26:19,347 --> 00:26:23,712
що більшість обчислень повинні виглядати як матричне множення з матрицями, 

424
00:26:23,712 --> 00:26:27,728
повними параметрів, що налаштовуються, то розуміння механізму уваги, 

425
00:26:27,728 --> 00:26:32,210
цього наріжного каменю в усьому сучасному бумі ШІ, має бути відносно простим.

426
00:26:32,650 --> 00:26:34,510
Для цього приєднуйтесь до мене в наступному розділі.

427
00:26:36,390 --> 00:26:38,926
Поки я публікую цю статтю, проект наступної глави 

428
00:26:38,926 --> 00:26:41,210
доступний для перегляду прихильникам Patreon.

429
00:26:41,770 --> 00:26:44,253
Остаточна версія має бути опублікована за тиждень-два, 

430
00:26:44,253 --> 00:26:47,370
зазвичай це залежить від того, як багато я змінюю на основі рецензії.

431
00:26:47,810 --> 00:26:51,658
Тим часом, якщо ви хочете зануритися в увагу, і якщо ви хочете трохи допомогти каналу, 

432
00:26:51,658 --> 00:26:52,410
він чекає на вас.

