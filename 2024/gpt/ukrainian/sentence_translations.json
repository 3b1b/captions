[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Ініціали GPT розшифровуються як Generative Pretrained Transformer (генеративний попередньо навчений трансформатор).",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Щоб перше слово було зрозумілим, це боти, які генерують новий текст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Pretrained означає, що модель пройшла процес навчання на великій кількості даних, а префікс вказує на те, що за допомогою додаткового навчання є більше можливостей для її точного налаштування на конкретних завданнях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Але останнє слово - це справді ключовий момент.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Трансформатор - це особливий вид нейронної мережі, модель машинного навчання, і це ключовий винахід, що лежить в основі нинішнього буму в галузі ШІ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Цим відео та наступними розділами я хочу наочно пояснити, що насправді відбувається всередині трансформатора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Ми будемо стежити за даними, які проходять через нього, і робити крок за кроком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Існує багато різних типів моделей, які можна побудувати за допомогою трансформаторів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Деякі моделі приймають аудіо і створюють транскрипт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Це речення походить від моделі, яка працює навпаки, створюючи синтетичне мовлення просто з тексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Всі ті інструменти, які підкорили світ у 2022 році, такі як Dolly і Midjourney, що приймають текстовий опис і видають зображення, засновані на трансформаторах.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Навіть якщо я не можу змусити його зрозуміти, що таке істота з пирога, я все одно вражений, що таке взагалі можливо.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "А оригінальний трансформер, представлений у 2017 році компанією Google, був винайдений для конкретного випадку використання - перекладу тексту з однієї мови на іншу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Але варіант, на якому ми з вами зосередимося, і який лежить в основі таких інструментів, як ChatGPT, - це модель, яка навчена сприймати шматок тексту, можливо, навіть із зображеннями або звуком, що його супроводжують, і передбачати те, що буде далі в уривку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Це передбачення має форму розподілу ймовірностей для багатьох різних фрагментів тексту, які можуть слідувати за ним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "На перший погляд, ви можете подумати, що передбачення наступного слова - це зовсім інша мета, ніж створення нового тексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Але коли у вас є така модель прогнозування, ви можете просто згенерувати довший фрагмент тексту, дати їй початковий фрагмент для роботи, попросити її взяти випадкову вибірку з розподілу, який вона щойно згенерувала, додати цю вибірку до тексту, а потім запустити весь процес знову, щоб зробити нове передбачення на основі всього нового тексту, включно з тим, що вона щойно додала.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Не знаю, як вам, а мені здається, що це не спрацює.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "У цій анімації, наприклад, я запускаю GPT-2 на своєму ноутбуці і змушую його постійно передбачати і вибирати наступний фрагмент тексту, щоб згенерувати історію на основі вихідного тексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "Історія просто не має особливого сенсу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Але якщо я заміню його на виклики API до GPT-3, який є тією ж базовою моделлю, тільки набагато більшою, раптом, майже магічним чином, ми отримаємо розумну історію, яка навіть дозволяє припустити, що істота з числом пі жила б у країні математики та обчислень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Цей процес повторного передбачення і вибірки - це те, що відбувається, коли ви взаємодієте з ChatGPT або будь-якою іншою великою мовною моделлю і бачите, як вона видає по одному слову за раз.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Насправді, одна з функцій, яка мені б дуже сподобалася, - це можливість бачити базову дистрибуцію для кожного нового слова, яке він обирає.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Давайте почнемо з дуже високого рівня попереднього перегляду того, як дані проходять через трансформатор.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Ми витрачатимемо набагато більше часу на мотивацію, тлумачення та деталізацію кожного кроку, але в загальних рисах, коли один з цих чат-ботів генерує певне слово, ось що відбувається під капотом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Спочатку вхідні дані розбиваються на купу маленьких шматочків.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Ці фрагменти називаються токенами, і у випадку з текстом це, як правило, слова, невеликі фрагменти слів або інші поширені комбінації символів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Якщо йдеться про зображення або звук, то токенами можуть бути маленькі фрагменти зображення або маленькі шматочки звуку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Кожен з цих токенів потім асоціюється з вектором, тобто деяким списком чисел, який повинен якимось чином кодувати значення цього фрагменту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Якщо уявити ці вектори як координати в деякому дуже вимірному просторі, то слова зі схожими значеннями, як правило, потрапляють на вектори, які знаходяться близько один до одного в цьому просторі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Потім ця послідовність векторів проходить через операцію, яка називається блокуванням уваги, і це дозволяє векторам спілкуватися один з одним і передавати інформацію туди-сюди для оновлення своїх значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Наприклад, значення слова \"модель\" у словосполученні \"модель машинного навчання\" відрізняється від його значення у словосполученні \"фотомодель\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Блок уваги відповідає за визначення того, які слова в контексті є релевантними для оновлення значень інших слів, і як саме ці значення мають бути оновлені.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "І знову ж таки, щоразу, коли я використовую слово \"значення\", воно якимось чином повністю закодоване у записах цих векторів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Після цього ці вектори проходять через інші операції, і залежно від джерела, яке ви читаєте, це буде називатися багатошаровим персептроном або, можливо, шаром прямого зв'язку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "А тут вектори не розмовляють один з одним, вони всі проходять одну і ту ж операцію паралельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "І хоча цей блок трохи складніше інтерпретувати, пізніше ми поговоримо про те, що цей крок трохи схожий на задавання довгого списку запитань про кожен вектор, а потім їх оновлення на основі відповідей на ці запитання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Всі операції в обох цих блоках виглядають як гігантська купа матричних множень, і наша основна робота буде полягати в тому, щоб зрозуміти, як читати матриці, що лежать в їх основі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Я не висвітлюю деякі деталі про деякі кроки нормалізації, які відбуваються між ними, але це, зрештою, попередній перегляд високого рівня.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Після цього процес по суті повторюється, ви ходите туди-сюди між блоками уваги і багатошаровими перцептронними блоками, поки в самому кінці не з'являється надія, що весь основний сенс уривка якимось чином був запечений в останньому векторі послідовності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Потім ми виконуємо певну операцію над цим останнім вектором, яка створює розподіл ймовірностей над усіма можливими лексемами, усіма можливими маленькими шматочками тексту, які можуть з'явитися далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "І як я вже казав, якщо у вас є інструмент, який прогнозує, що буде далі за фрагментом тексту, ви можете дати йому трохи вихідного тексту і змусити його повторювати цю гру з прогнозуванням того, що буде далі, вибираючи вибірку з дистрибутиву, додаючи її, а потім повторюючи знову і знову.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Дехто з вас, можливо, пам'ятає, як задовго до появи ChatGPT на сцені виглядали ранні демо-версії GPT-3: вони автоматично доповнювали розповіді та есе на основі початкового фрагмента.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Щоб перетворити такий інструмент на чат-бота, найпростіша відправна точка - написати невеликий текст, який встановлює параметри взаємодії користувача з корисним помічником зі штучним інтелектом, те, що ви називаєте системною підказкою, а потім використати початкове запитання або підказку користувача як перший фрагмент діалогу, після чого він почне передбачати, що такий корисний помічник зі штучним інтелектом скаже у відповідь.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Можна ще багато чого сказати про етап навчання, який необхідний для того, щоб це працювало добре, але на високому рівні це ідея.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "У цій главі ми з вами детально розглянемо, що відбувається на самому початку мережі і в самому кінці мережі, а також я хочу витратити багато часу на огляд деяких важливих базових знань, які стали другою натурою для будь-якого інженера машинного навчання на той час, коли з'явилися трансформатори.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Якщо вам достатньо цих базових знань і ви трохи нетерплячі, можете сміливо переходити до наступного розділу, в якому мова піде про блоки уваги, які зазвичай вважаються серцем трансформатора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Після цього я хочу поговорити більше про ці багатошарові перцептронні блоки, про те, як працює навчання, і про низку інших деталей, які ми пропустили до цього моменту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Для більш широкого контексту, ці відео є доповненням до міні-серіалу про глибоке навчання, і нічого страшного, якщо ви не дивилися попередні, я думаю, що ви можете зробити це в довільному порядку, але перед тим, як зануритися безпосередньо в трансформатори, я думаю, що варто переконатися, що ми знаходимося на одній сторінці щодо основних передумов і структури глибокого навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Ризикуючи сказати очевидне, це один з підходів до машинного навчання, який описує будь-яку модель, де ви використовуєте дані, щоб якось визначити поведінку моделі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Я маю на увазі, що, скажімо, вам потрібна функція, яка отримує зображення і видає мітку, що його описує, або наш приклад передбачення наступного слова за уривком тексту, або будь-яке інше завдання, яке, здається, вимагає певного елементу інтуїції та розпізнавання образів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Сьогодні ми сприймаємо це як належне, але ідея машинного навчання полягає в тому, що замість того, щоб намагатися явно визначити процедуру виконання завдання в коді, як це робили люди на початку розвитку ШІ, ви створюєте дуже гнучку структуру з параметрами, що налаштовуються, на кшталт купи ручок і циферблатів, а потім якось використовуєте безліч прикладів того, як повинен виглядати вихід при заданих вхідних даних, щоб підлаштовувати і налаштовувати значення цих параметрів, щоб імітувати цю поведінку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Наприклад, найпростішою формою машинного навчання є лінійна регресія, де вхідні та вихідні дані - це окремі числа, щось на кшталт площі будинку та його ціни, і ви хочете знайти лінію найкращої відповідності між цими даними, тобто передбачити майбутні ціни на житло.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Ця лінія описується двома неперервними параметрами, скажімо, нахилом та y-інтервалом, і метою лінійної регресії є визначення цих параметрів, щоб вони якнайкраще відповідали даним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Зрозуміло, що моделі глибокого навчання стають набагато складнішими.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "GPT-3, наприклад, має не два, а 175 мільярдів параметрів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Але справа в тому, що не факт, що ви можете створити якусь гігантську модель з величезною кількістю параметрів без того, щоб вона або сильно перекривала навчальні дані, або була абсолютно непіддатливою для навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "Глибоке навчання описує клас моделей, які за останні кілька десятиліть довели, що вони чудово масштабуються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Їх об'єднує однаковий алгоритм навчання, який називається зворотним поширенням, і контекст, який я хочу, щоб ви зрозуміли, полягає в тому, що для того, щоб цей алгоритм навчання добре працював у масштабі, ці моделі повинні відповідати певному формату.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Якщо ви знаєте цей формат, він допомагає пояснити багато варіантів того, як трансформатор обробляє мову, які в іншому випадку ризикують здатися довільними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "По-перше, яку б модель ви не будували, вхідні дані повинні бути відформатовані як масив дійсних чисел.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Це може бути список чисел, це може бути двовимірний масив, або дуже часто ви маєте справу з масивами більшої розмірності, де загальним терміном є тензор.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Ви часто думаєте, що вхідні дані поступово трансформуються у багато різних шарів, де кожен шар завжди структурований як певний масив дійсних чисел, доки ви не дійдете до останнього шару, який ви вважаєте вихідним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Наприклад, останній шар у нашій моделі обробки тексту - це список чисел, що представляє розподіл ймовірностей для всіх можливих наступних лексем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "У глибокому навчанні ці параметри моделі майже завжди називаються вагами, і це тому, що ключовою особливістю цих моделей є те, що єдиний спосіб, яким ці параметри взаємодіють з даними, що обробляються, - це зважені суми.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Ви також розкидаєте деякі нелінійні функції, але вони не будуть залежати від параметрів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Зазвичай, замість того, щоб бачити зважені суми \"голими\" і виписаними в явному вигляді, ви бачите їх упакованими разом як різні компоненти в матричному векторному добутку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Це те ж саме, якщо згадати, як працює матричне множення векторів, кожен компонент на виході виглядає як зважена сума.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Просто часто концептуально чистіше для нас з вами думати про матриці, які заповнюються параметрами, що налаштовуються, які трансформують вектори, отримані з даних, що обробляються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Наприклад, 175 мільярдів ваг у GPT-3 організовано у трохи менше ніж 28 000 різних матриць.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Ці матриці, в свою чергу, поділяються на вісім різних категорій, і ми з вами розглянемо кожну з них, щоб зрозуміти, що робить цей тип.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Я думаю, що буде цікаво згадати конкретні цифри з GPT-3, щоб підрахувати, звідки саме взялися ці 175 мільярдів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Навіть якщо сьогодні існують більші та кращі моделі, ця модель має певний шарм, оскільки вона є великомовною і справді привертає увагу світової спільноти поза межами спільнот, що займаються відмиванням грошей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Крім того, на практиці компанії, як правило, не називають конкретні цифри для більш сучасних мереж.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Я просто хочу пояснити, що коли ви зазирнете під капот, щоб побачити, що відбувається всередині такого інструменту, як ChatGPT, то побачите, що майже всі обчислення виглядають як множення матричних векторів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Існує невеликий ризик загубитися в морі мільярдів цифр, але ви повинні провести дуже чітке розмежування між вагами моделі, які я завжди буду позначати синім або червоним кольором, і даними, що обробляються, які я завжди буду позначати сірим кольором.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Обтяження - це власне мозок, це те, що вивчається під час тренувань, і саме вони визначають, як він себе поводить.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Дані, що обробляються, просто кодують будь-які конкретні вхідні дані, що подаються в модель для даного прогону, наприклад, фрагмент тексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Маючи все це в якості основи, давайте розглянемо перший крок цього прикладу обробки тексту, який полягає в розбитті вхідних даних на невеликі фрагменти і перетворенні цих фрагментів у вектори.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Я вже згадував, що ці шматки називаються токенами, які можуть бути шматками слів або розділовими знаками, але час від часу в цій главі, а особливо в наступній, я хотів би просто уявити, що вона розбита на слова більш чітко.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Оскільки ми, люди, мислимо словами, це значно полегшить посилання на маленькі приклади та пояснення кожного кроку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Модель має заздалегідь визначений словник, деякий список усіх можливих слів, скажімо, 50 000, і перша матриця, з якою ми зустрінемося, відома як матриця вбудовування, має один стовпчик для кожного з цих слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Саме ці стовпчики визначають, на який вектор перетворюється кожне слово на першому кроці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Ми позначимо її We, і, як і всі матриці, які ми бачимо, її значення спочатку випадкові, але вони будуть визначатися на основі даних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Перетворення слів на вектори було поширеною практикою в машинному навчанні задовго до появи трансформаторів, але буде трохи дивно, якщо ви ніколи не бачили його раніше, і воно закладає фундамент для всього, що буде далі, тому давайте приділимо трохи часу, щоб ознайомитися з ним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Ми часто називаємо це вбудовування словом, яке запрошує вас думати про ці вектори дуже геометрично, як про точки в деякому високовимірному просторі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Візуалізація списку з трьох чисел як координат точок у тривимірному просторі не буде проблемою, але вставки слів, як правило, мають набагато більшу розмірність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "У GPT-3 вони мають 12 288 вимірів, і, як ви побачите, це важливо для роботи в просторі, який має багато різних напрямків.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Так само, як ви можете взяти двовимірний зріз у тривимірному просторі і спроектувати всі точки на цей зріз, для того, щоб анімувати вставки слів, які дає мені проста модель, я зроблю аналогічну річ, вибравши тривимірний зріз у цьому дуже високовимірному просторі, спроектувавши на нього вектори слів і відобразивши результати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "Основна ідея полягає в тому, що коли модель налаштовує свої ваги, щоб визначити, як саме слова вбудовуються як вектори під час навчання, вона має тенденцію зупинятися на наборі вбудовувань, де напрямки в просторі мають певне семантичне значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Для простої моделі \"слово-вектор\", яку я використовую тут, якщо я запущу пошук всіх слів, які мають найближче входження до слова \"вежа\", ви помітите, що всі вони створюють дуже схожі вежові відчуття.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "І якщо ви хочете завантажити Python і пограти вдома, це конкретна модель, яку я використовую для створення анімації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Це не трансформатор, але цього достатньо, щоб проілюструвати ідею про те, що напрямки в просторі можуть нести смислове навантаження.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Класичним прикладом цього є різниця між векторами жінки і чоловіка, яку можна уявити як маленький вектор, що з'єднує кінчик одного з кінчиком іншого, це дуже схоже на різницю між королем і королевою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Скажімо, якщо ви не знаєте слова, що позначає жінку-монарха, ви можете знайти його, взявши слово \"король\", додавши до нього напрям \"жінка-чоловік\" і пошукавши найближчі до цього слова вкраплення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Принаймні, начебто.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Незважаючи на те, що це класичний приклад для моделі, з якою я граю, справжнє вбудовування ферзя насправді знаходиться трохи далі, ніж можна було б припустити, ймовірно, тому, що спосіб використання ферзя в навчальних даних - це не просто жіноча версія короля.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Коли я гралася, сімейні стосунки, здавалося, набагато краще ілюстрували цю ідею.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Справа в тому, що, схоже, під час навчання модель вважала вигідним вибирати вставки таким чином, щоб один напрямок у цьому просторі кодував гендерну інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Інший приклад: якщо ви візьмете вбудовування Італії, віднімете вбудовування Німеччини і додасте це до вбудовування Гітлера, то отримаєте щось дуже близьке до вбудовування Муссоліні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Наче модель навчилася асоціювати одні напрямки з італійськістю, а інші - з лідерами Другої світової війни.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Мабуть, мій улюблений приклад у цьому сенсі - це те, як у деяких моделях, якщо взяти різницю між Німеччиною та Японією і додати її до суші, то в підсумку виходить дуже схоже на сардельки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Крім того, граючи в цю гру з пошуку найближчих сусідів, я був радий побачити, наскільки Кет була близька і до звіра, і до монстра.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Математична інтуїція, яку корисно мати на увазі, особливо для наступного розділу, полягає в тому, що точковий добуток двох векторів можна розглядати як спосіб вимірювання того, наскільки добре вони співпадають.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "З обчислювальної точки зору, точкові продукти передбачають перемноження всіх відповідних компонентів, а потім додавання результатів, що добре, оскільки так багато наших обчислень повинні виглядати як зважені суми.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Геометрично, точковий добуток додатний, коли вектори спрямовані в однакових напрямках, нульовий, якщо вони перпендикулярні, і від'ємний, коли вони спрямовані в протилежних напрямках.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Наприклад, припустимо, ви гралися з цією моделлю і припустили, що вбудовування котів мінус кіт може представляти певний напрямок множинності в цьому просторі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Щоб перевірити це, я візьму цей вектор і обчислю його точковий добуток на вставки певних іменників в однині, а потім порівняю його з точковими добутками з відповідними іменниками у множині.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Якщо ви пограєтеся з цим, то помітите, що множина дійсно дає вищі значення, ніж однина, що свідчить про те, що вони більше відповідають цьому напрямку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Також цікаво, що якщо взяти цей точковий добуток зі вставками слів 1, 2, 3 і так далі, то вони дають зростаючі значення, тож ми ніби можемо кількісно виміряти, у якій множині модель знаходить задане слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Знову ж таки, специфіка того, як слова вбудовуються, вивчається за допомогою даних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Ця матриця вбудовування, стовпці якої показують, що відбувається з кожним словом, є першою купою ваг у нашій моделі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Використовуючи цифри GPT-3, розмір словника становить 50 257, і знову ж таки, технічно він складається не зі слів як таких, а з лексем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "Розмірність вбудовування становить 12 288, і якщо помножити її на це число, то вийде, що вона складається з приблизно 617 мільйонів ваг.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Давайте додамо це до поточного підрахунку, пам'ятаючи, що до кінця ми повинні нарахувати до 175 мільярдів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "У випадку з трансформаторами ви дійсно хочете думати про вектори в цьому просторі вбудовування як про щось більше, ніж просто представлення окремих слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "З одного боку, вони також кодують інформацію про позицію цього слова, про яку ми поговоримо пізніше, але, що важливіше, ви повинні думати про них як про такі, що мають здатність вбирати в себе контекст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Вектор, який починав своє життя як вбудовування слова король, наприклад, може поступово підтягуватися і витягуватися різними блоками в цій мережі, так що в кінці він вказує в набагато більш конкретному і нюансованому напрямку, який так чи інакше кодує, що це був король, який жив у Шотландії, і який отримав свою посаду після вбивства попереднього короля, і який описується шекспірівською мовою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Подумайте, як ви розумієте те чи інше слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Значення цього слова чітко залежить від оточення, а іноді й від контексту на великій відстані, тому при створенні моделі, яка може передбачити, яке слово буде наступним, мета полягає в тому, щоб якимось чином надати їй можливість ефективно враховувати контекст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Щоб було зрозуміло, на першому кроці, коли ви створюєте масив векторів на основі вхідного тексту, кожен з них просто висмикується з матриці вбудовування, тому спочатку кожен з них може кодувати значення лише одного слова без жодних даних з його оточення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Але ви повинні думати про основну мету цієї мережі, через яку вона проходить, як про те, щоб дозволити кожному з цих векторів увібрати в себе значення, яке набагато багатше і конкретніше, ніж те, що можуть представляти просто окремі слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Мережа може обробляти лише фіксовану кількість векторів за один раз, відому як розмір контексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Для GPT-3 він був навчений з розміром контексту 2048, тому дані, що проходять через мережу, завжди виглядають як масив з 2048 стовпців, кожен з яких має 12 000 вимірів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Розмір контексту обмежує обсяг тексту, який трансформатор може включити в себе під час прогнозування наступного слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Ось чому довгі розмови з деякими чат-ботами, наприклад, з ранніми версіями ChatGPT, часто створювали відчуття, що бот втрачає нитку розмови, коли ви продовжуєте говорити занадто довго.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Ми ще поговоримо про увагу в деталях у свій час, але, забігаючи наперед, я хочу поговорити про те, що відбувається в самому кінці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Пам'ятайте, що бажаний результат - це розподіл ймовірностей для всіх токенів, які можуть прийти наступними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Наприклад, якщо останнє слово - професор, а в контексті зустрічаються такі слова, як Гаррі Поттер, а безпосередньо перед ним - найменш улюблений вчитель, і якщо ви дозволите мені припустити, що токени просто виглядають як повноцінні слова, то добре навчена мережа, яка накопичила знання про Гаррі Поттера, ймовірно, присвоїть високий номер слову Снейп.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Це передбачає два різних кроки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Перший полягає у використанні іншої матриці, яка відображає останній вектор у цьому контексті на список з 50 000 значень, по одному для кожної лексеми у словнику.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Потім є функція, яка нормалізує це в розподіл ймовірностей, вона називається Softmax, і ми поговоримо про неї докладніше через секунду, але перед цим може здатися трохи дивним використовувати тільки це останнє вбудовування для прогнозування, коли, зрештою, на цьому останньому кроці в шарі є тисячі інших векторів, які просто лежать там зі своїми власними контекстно-багатими значеннями.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Це пов'язано з тим, що в процесі навчання виявляється набагато ефективніше, якщо використовувати кожен з цих векторів у фінальному шарі для одночасного прогнозування того, що буде відразу після нього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Пізніше я ще багато розповім про тренінги, але зараз я просто хочу наголосити на цьому.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Ця матриця називається матрицею вилучення (Unembedding matrix) і ми позначимо її WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Знову ж таки, як і у всіх вагових матрицях, які ми бачимо, її елементи починаються випадковим чином, але вони вивчаються в процесі навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Зважаючи на загальну кількість параметрів, ця матриця вилучення має один рядок для кожного слова в словнику, і кожен рядок має таку саму кількість елементів, як і розмірність вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Це дуже схоже на матрицю вбудовування, тільки зі зміненим порядком, тому вона додає до мережі ще 617 мільйонів параметрів, тобто поки що ми нарахували трохи більше мільярда, невелику, але не зовсім незначну частку від 175 мільярдів, які ми отримаємо в результаті.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "В останньому міні-уроці цієї глави я хочу поговорити про функцію softmax, оскільки вона ще раз з'явиться перед нами, коли ми зануримося в блоки уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Ідея полягає в тому, що якщо ви хочете, щоб послідовність чисел діяла як розподіл ймовірностей, скажімо, розподіл над усіма можливими наступними словами, то кожне значення повинно бути між 0 і 1, і вам також потрібно, щоб всі вони в сумі дорівнювали 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Однак, якщо ви граєте в навчальну гру, де все, що ви робите, виглядає як матрично-векторне множення, результати, які ви отримуєте за замовчуванням, зовсім не відповідають цьому принципу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Значення часто від'ємні, або набагато більші за 1, і майже напевно не дорівнюють 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax - це стандартний спосіб перетворення довільного списку чисел у дійсний розподіл таким чином, що найбільші значення виявляються найближчими до 1, а найменші - дуже близькими до 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Це все, що вам потрібно знати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Але якщо вам цікаво, то спочатку потрібно піднести e до степеня кожного з чисел, що означає, що тепер у вас є список додатних значень, а потім ви можете взяти суму всіх цих додатних значень і розділити кожен доданок на цю суму, що нормалізує його до списку, який додає до 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Ви помітите, що якщо одне з чисел у вхідних даних значно більше за решту, то у вихідних даних відповідний член домінує у розподілі, тому, якщо б ви робили вибірку з нього, то майже напевно просто вибрали б вхідні дані, що максимізують його.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Але це м'якше, ніж просто вибір максимуму, в тому сенсі, що коли інші значення також великі, вони також отримують значущу вагу в розподілі, і все змінюється безперервно, коли ви постійно змінюєте вхідні дані.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "У деяких ситуаціях, наприклад, коли ChatGPT використовує цей дистрибутив для створення наступного слова, можна трохи розважитися, додавши у цю функцію трохи додаткової гостроти, додавши у знаменник цієї функції константу t.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Ми називаємо її температурою, оскільки вона віддалено нагадує роль температури в деяких рівняннях термодинаміки, і ефект полягає в тому, що коли t більша, ви надаєте більшу вагу меншим значенням, тобто розподіл трохи більш рівномірний, а якщо t менша, то більші значення будуть домінувати більш агресивно, де в крайньому випадку, встановлення t рівною нулю означає, що вся вага йде до максимального значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Наприклад, я попрошу GPT-3 згенерувати історію з початковим текстом \"Жив-був А\", але в кожному випадку я буду використовувати різні температури.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Нульова температура означає, що вона завжди поєднується з найбільш передбачуваним словом, і те, що ви отримуєте, виявляється банальним похідним від \"Золотоволоска\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Вища температура дає йому можливість вибирати менш ймовірні слова, але це пов'язано з певним ризиком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "У цьому випадку історія починається більш оригінально, про молодого веб-художника з Південної Кореї, але швидко вироджується в нісенітницю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Технічно кажучи, API не дозволяє вибрати температуру, більшу за 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Для цього немає жодної математичної причини, це просто довільне обмеження, накладене для того, щоб не допустити, щоб їхній інструмент генерував надто безглузді речі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Якщо вам цікаво, як насправді працює ця анімація, то я беру 20 найбільш ймовірних наступних токенів, які генерує GPT-3, а це, здається, максимум, який він мені дасть, а потім я підлаштовую ймовірності на основі показника експоненти 1 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Як ще один жаргон, так само, як ви могли б назвати компоненти виходу цієї функції ймовірностями, люди часто називають входи логітами, або деякі люди кажуть логіти, деякі люди кажуть логіти, я буду казати логіти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Наприклад, коли ви вводите якийсь текст, всі ці вставки слів проходять через мережу, і ви робите остаточне множення з матрицею вилучення, люди, що навчаються, посилатимуться на компоненти в цьому сирому, ненормованому виході як на логічні логіти для прогнозування наступного слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Значною мірою метою цієї глави було закласти основи розуміння механізму уваги в стилі карате-малюка \"wax-on-wax-off\" (воск на воску, віск з воску).",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Розумієте, якщо у вас є сильна інтуїція щодо вбудовування слів, softmax, того, як точкові продукти вимірюють схожість, а також базова передумова, що більшість обчислень повинні виглядати як матричне множення з матрицями, повними параметрів, що налаштовуються, то розуміння механізму уваги, цього наріжного каменю в усьому сучасному бумі ШІ, має бути відносно простим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Для цього приєднуйтесь до мене в наступному розділі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Поки я публікую цю статтю, проект наступної глави доступний для перегляду прихильникам Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Остаточна версія має бути опублікована за тиждень-два, зазвичай це залежить від того, як багато я змінюю на основі рецензії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Тим часом, якщо ви хочете зануритися в увагу, і якщо ви хочете трохи допомогти каналу, він чекає на вас.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]