1
00:00:00,000 --> 00:00:04,560
GPT'nin baş harfleri Generative Pretrained Transformer'ın kısaltmasıdır.

2
00:00:05,220 --> 00:00:09,020
Yani ilk kelime yeterince açık, bunlar yeni metin üreten botlar.

3
00:00:09,800 --> 00:00:13,352
Önceden eğitilmiş, modelin büyük miktarda veriden nasıl bir öğrenme 

4
00:00:13,352 --> 00:00:16,539
sürecinden geçtiğini ifade eder ve önek, ek eğitimle belirli 

5
00:00:16,539 --> 00:00:20,040
görevlerde ince ayar yapmak için daha fazla alan olduğunu ima eder.

6
00:00:20,720 --> 00:00:22,900
Ama son söz, asıl anahtar parça budur.

7
00:00:23,380 --> 00:00:27,303
Transformatör, özel bir tür sinir ağı, bir makine öğrenimi modelidir 

8
00:00:27,303 --> 00:00:31,000
ve yapay zekadaki mevcut patlamanın altında yatan temel buluştur.

9
00:00:31,740 --> 00:00:34,795
Bu video ve sonraki bölümlerde yapmak istediğim şey, 

10
00:00:34,795 --> 00:00:39,120
bir transformatörün içinde gerçekte neler olduğunu görsel olarak açıklamak.

11
00:00:39,700 --> 00:00:42,820
İçinden akan verileri takip edeceğiz ve adım adım ilerleyeceğiz.

12
00:00:43,440 --> 00:00:47,380
Transformatörleri kullanarak inşa edebileceğiniz birçok farklı türde model vardır.

13
00:00:47,800 --> 00:00:50,800
Bazı modeller ses alır ve bir transkript üretir.

14
00:00:51,340 --> 00:00:53,584
Bu cümle, tam tersi yönde ilerleyen ve sadece 

15
00:00:53,584 --> 00:00:56,220
metinden sentetik konuşma üreten bir modelden geliyor.

16
00:00:56,660 --> 00:01:01,401
2022'de dünyayı kasıp kavuran Dolly ve Midjourney gibi bir metin açıklaması 

17
00:01:01,401 --> 00:01:05,519
alıp bir görüntü üreten tüm o araçlar transformatörlere dayanıyor.

18
00:01:06,000 --> 00:01:09,755
Bir turta yaratığının ne olması gerektiğini tam olarak anlayamasam bile, 

19
00:01:09,755 --> 00:01:13,100
bu tür bir şeyin uzaktan bile mümkün olması beni hala şaşırtıyor.

20
00:01:13,900 --> 00:01:17,882
Ve 2017 yılında Google tarafından tanıtılan orijinal transformatör, 

21
00:01:17,882 --> 00:01:22,100
metni bir dilden diğerine çevirme özel kullanım durumu için icat edildi.

22
00:01:22,660 --> 00:01:27,518
Ancak, ChatGPT gibi araçların temelini oluşturan, sizin ve benim odaklanacağımız varyant, 

23
00:01:27,518 --> 00:01:31,458
bir metin parçasını, belki de ona eşlik eden bazı çevre görüntüleri veya 

24
00:01:31,458 --> 00:01:35,345
seslerle birlikte almak ve pasajda bir sonraki adımda ne olacağına dair 

25
00:01:35,345 --> 00:01:38,260
bir tahmin üretmek için eğitilmiş bir model olacaktır.

26
00:01:38,600 --> 00:01:41,096
Bu tahmin, takip edebilecek birçok farklı metin 

27
00:01:41,096 --> 00:01:43,800
parçası üzerinde bir olasılık dağılımı şeklini alır.

28
00:01:45,040 --> 00:01:47,490
İlk bakışta, bir sonraki kelimeyi tahmin etmenin yeni metin 

29
00:01:47,490 --> 00:01:49,940
oluşturmaktan çok farklı bir amaç olduğunu düşünebilirsiniz.

30
00:01:50,180 --> 00:01:52,910
Ancak bunun gibi bir tahmin modeline sahip olduğunuzda, 

31
00:01:52,910 --> 00:01:55,690
daha uzun bir metin parçası oluşturmanın basit bir yolu, 

32
00:01:55,690 --> 00:01:58,616
ona üzerinde çalışması için bir başlangıç parçacığı vermek, 

33
00:01:58,616 --> 00:02:01,835
yeni oluşturduğu dağılımdan rastgele bir örnek almasını sağlamak, 

34
00:02:01,835 --> 00:02:05,638
bu örneği metne eklemek ve ardından yeni ekledikleri de dahil olmak üzere tüm 

35
00:02:05,638 --> 00:02:09,539
yeni metne dayalı yeni bir tahmin yapmak için tüm süreci yeniden çalıştırmaktır.

36
00:02:10,100 --> 00:02:13,000
Sizi bilmem ama bunun gerçekten işe yaraması gerekiyormuş gibi gelmiyor.

37
00:02:13,420 --> 00:02:16,521
Örneğin bu animasyonda, GPT-2'yi dizüstü bilgisayarımda çalıştırıyorum 

38
00:02:16,521 --> 00:02:19,449
ve tohum metne dayalı bir hikaye oluşturmak için bir sonraki metin 

39
00:02:19,449 --> 00:02:22,420
parçasını tekrar tekrar tahmin etmesini ve örneklemesini sağlıyorum.

40
00:02:22,420 --> 00:02:26,120
Hikaye o kadar da mantıklı değil.

41
00:02:26,500 --> 00:02:30,303
Ancak bunun yerine aynı temel model olan GPT-3'e API çağrıları yaparsam, 

42
00:02:30,303 --> 00:02:33,898
sadece çok daha büyük, aniden neredeyse sihirli bir şekilde mantıklı 

43
00:02:33,898 --> 00:02:37,649
bir hikaye elde ederiz, hatta bir pi yaratığının matematik ve hesaplama 

44
00:02:37,649 --> 00:02:40,880
diyarında yaşayacağı sonucunu çıkarır gibi görünen bir hikaye.

45
00:02:41,580 --> 00:02:44,018
Buradaki tekrarlanan tahmin ve örnekleme süreci, 

46
00:02:44,018 --> 00:02:47,551
ChatGPT veya diğer büyük dil modellerinden herhangi biriyle etkileşime 

47
00:02:47,551 --> 00:02:51,880
girdiğinizde ve her seferinde bir kelime ürettiklerini gördüğünüzde esasen olan şeydir.

48
00:02:52,480 --> 00:02:55,882
Aslında, çok hoşuma gidecek bir özellik, seçtiği her 

49
00:02:55,882 --> 00:02:59,220
yeni kelime için altta yatan dağılımı görebilmektir.

50
00:03:03,820 --> 00:03:06,189
Verilerin bir dönüştürücüden nasıl geçtiğine dair 

51
00:03:06,189 --> 00:03:08,180
çok üst düzey bir önizleme ile başlayalım.

52
00:03:08,640 --> 00:03:11,927
Her bir adımın ayrıntılarını motive etmek, yorumlamak ve genişletmek için çok daha 

53
00:03:11,927 --> 00:03:13,907
fazla zaman harcayacağız, ancak genel hatlarıyla, 

54
00:03:13,907 --> 00:03:16,323
bu sohbet robotlarından biri belirli bir kelime ürettiğinde, 

55
00:03:16,323 --> 00:03:18,660
kaputun altında neler olup bittiğini burada bulabilirsiniz.

56
00:03:19,080 --> 00:03:22,040
İlk olarak, girdi bir grup küçük parçaya ayrılır.

57
00:03:22,620 --> 00:03:26,052
Bu parçalara belirteç adı verilir ve metin söz konusu olduğunda bunlar genellikle 

58
00:03:26,052 --> 00:03:29,820
kelimeler veya kelimelerin küçük parçaları ya da diğer yaygın karakter kombinasyonlarıdır.

59
00:03:30,740 --> 00:03:34,149
Görüntüler veya sesler söz konusuysa, belirteçler bu görüntünün 

60
00:03:34,149 --> 00:03:37,080
küçük parçaları veya bu sesin küçük parçaları olabilir.

61
00:03:37,580 --> 00:03:40,993
Bu belirteçlerin her biri daha sonra bir vektörle ilişkilendirilir, 

62
00:03:40,993 --> 00:03:45,360
yani o parçanın anlamını bir şekilde kodlamak için bazı sayılar listesi anlamına gelir.

63
00:03:45,880 --> 00:03:50,128
Bu vektörleri çok yüksek boyutlu bir uzayda koordinatlar veriyor gibi düşünürseniz, 

64
00:03:50,128 --> 00:03:54,680
benzer anlamlara sahip kelimeler bu uzayda birbirine yakın vektörlere düşme eğilimindedir.

65
00:03:55,280 --> 00:03:59,177
Bu vektör dizisi daha sonra dikkat bloğu olarak bilinen bir işlemden geçer ve bu, 

66
00:03:59,177 --> 00:04:02,266
vektörlerin birbirleriyle konuşmasına ve değerlerini güncellemek 

67
00:04:02,266 --> 00:04:04,500
için ileri geri bilgi aktarmasına olanak tanır.

68
00:04:04,880 --> 00:04:09,032
Örneğin, model kelimesinin bir makine öğrenimi modeli ifadesindeki anlamı, 

69
00:04:09,032 --> 00:04:11,800
bir moda modeli ifadesindeki anlamından farklıdır.

70
00:04:12,260 --> 00:04:15,525
Dikkat bloğu, bağlamdaki hangi kelimelerin hangi diğer kelimelerin 

71
00:04:15,525 --> 00:04:18,596
anlamlarını güncellemekle ilgili olduğunu ve bu anlamların tam 

72
00:04:18,596 --> 00:04:21,959
olarak nasıl güncellenmesi gerektiğini bulmaktan sorumlu olan şeydir.

73
00:04:22,500 --> 00:04:25,270
Ve yine, ne zaman anlam kelimesini kullansam, bu bir 

74
00:04:25,270 --> 00:04:28,040
şekilde tamamen bu vektörlerin girdilerinde kodlanır.

75
00:04:29,180 --> 00:04:32,429
Bundan sonra, bu vektörler farklı bir işlemden geçer ve okuduğunuz 

76
00:04:32,429 --> 00:04:35,532
kaynağa bağlı olarak bu, çok katmanlı bir algılayıcı veya belki 

77
00:04:35,532 --> 00:04:38,200
de ileri beslemeli bir katman olarak adlandırılacaktır.

78
00:04:38,580 --> 00:04:40,687
Ve burada vektörler birbirleriyle konuşmazlar, 

79
00:04:40,687 --> 00:04:42,660
hepsi paralel olarak aynı işlemden geçerler.

80
00:04:43,060 --> 00:04:46,755
Bu bloğu yorumlamak biraz daha zor olsa da, daha sonra bu adımın nasıl her 

81
00:04:46,755 --> 00:04:50,402
bir vektör hakkında uzun bir soru listesi sormak ve ardından bu soruların 

82
00:04:50,402 --> 00:04:54,000
yanıtlarına göre onları güncellemek gibi bir şey olduğundan bahsedeceğiz.

83
00:04:54,900 --> 00:04:59,871
Bu iki bloktaki tüm işlemler dev bir matris çarpımı yığınına benziyor ve 

84
00:04:59,871 --> 00:05:05,320
bizim öncelikli işimiz altta yatan matrislerin nasıl okunacağını anlamak olacak.

85
00:05:06,980 --> 00:05:11,060
Arada gerçekleşen bazı normalleştirme adımlarıyla ilgili bazı ayrıntıları atlıyorum, 

86
00:05:11,060 --> 00:05:12,980
ancak bu sonuçta üst düzey bir önizleme.

87
00:05:13,680 --> 00:05:18,299
Bundan sonra, süreç esasen tekrarlanır, dikkat blokları ve çok katmanlı 

88
00:05:18,299 --> 00:05:21,635
algılayıcı blokları arasında ileri geri gidersiniz, 

89
00:05:21,635 --> 00:05:26,318
ta ki en sonunda pasajın tüm temel anlamının bir şekilde dizideki en son 

90
00:05:26,318 --> 00:05:28,500
vektörde pişirilmiş olması umulur.

91
00:05:28,920 --> 00:05:32,281
Daha sonra bu son vektör üzerinde belirli bir işlem gerçekleştirerek 

92
00:05:32,281 --> 00:05:35,399
olası tüm belirteçler, bir sonraki adımda gelebilecek tüm olası 

93
00:05:35,399 --> 00:05:38,420
küçük metin parçaları üzerinde bir olasılık dağılımı üretiriz.

94
00:05:38,980 --> 00:05:42,254
Ve dediğim gibi, bir metin parçacığı verildiğinde bir sonraki adımın ne 

95
00:05:42,254 --> 00:05:44,574
olacağını tahmin eden bir araca sahip olduğunuzda, 

96
00:05:44,574 --> 00:05:48,440
onu biraz tohum metinle besleyebilir ve bir sonraki adımın ne olacağını tahmin etme, 

97
00:05:48,440 --> 00:05:51,988
dağıtımdan örnekleme, ekleme ve sonra tekrar tekrar tekrarlama oyununu tekrar 

98
00:05:51,988 --> 00:05:53,080
tekrar oynatabilirsiniz.

99
00:05:53,640 --> 00:05:57,095
Bazılarınız ChatGPT'nin sahneye çıkmasından ne kadar önce GPT-3'ün ilk 

100
00:05:57,095 --> 00:06:00,697
demolarının böyle göründüğünü hatırlayabilir, başlangıçtaki bir parçacığa 

101
00:06:00,697 --> 00:06:04,640
dayalı olarak hikayeleri ve denemeleri otomatik olarak tamamlamasını sağlardınız.

102
00:06:05,580 --> 00:06:09,680
Bunun gibi bir aracı bir sohbet robotuna dönüştürmek için en kolay başlangıç noktası, 

103
00:06:09,680 --> 00:06:13,446
yardımcı bir yapay zeka asistanıyla etkileşime giren bir kullanıcının ortamını 

104
00:06:13,446 --> 00:06:17,499
belirleyen, sistem istemi olarak adlandırabileceğiniz küçük bir metne sahip olmak ve 

105
00:06:17,499 --> 00:06:21,695
ardından kullanıcının ilk sorusunu veya istemini diyaloğun ilk parçası olarak kullanmak 

106
00:06:21,695 --> 00:06:25,700
ve ardından böyle yardımcı bir yapay zeka asistanının yanıt olarak ne söyleyeceğini 

107
00:06:25,700 --> 00:06:26,940
tahmin etmeye başlamaktır.

108
00:06:27,720 --> 00:06:30,854
Bunun iyi işlemesi için gerekli olan bir eğitim adımı hakkında 

109
00:06:30,854 --> 00:06:33,940
söylenecek daha çok şey var, ancak yüksek düzeyde fikir budur.

110
00:06:35,720 --> 00:06:39,940
Bu bölümde, siz ve ben ağın en başında ve en sonunda neler olduğuna dair 

111
00:06:39,940 --> 00:06:44,102
ayrıntıları genişleteceğiz ve ayrıca transformatörler ortaya çıktığında 

112
00:06:44,102 --> 00:06:48,437
herhangi bir makine öğrenimi mühendisi için ikinci doğa olacak bazı önemli 

113
00:06:48,437 --> 00:06:52,600
arka plan bilgilerini gözden geçirmek için çok zaman harcamak istiyorum.

114
00:06:53,060 --> 00:06:56,107
Bu arka plan bilgisine sahipseniz ve biraz sabırsızsanız, 

115
00:06:56,107 --> 00:06:59,259
genellikle transformatörün kalbi olarak kabul edilen dikkat 

116
00:06:59,259 --> 00:07:02,780
bloklarına odaklanacak olan bir sonraki bölüme geçmekte özgürsünüz.

117
00:07:03,360 --> 00:07:05,792
Bundan sonra, bu çok katmanlı algılayıcı bloklar, 

118
00:07:05,792 --> 00:07:09,782
eğitimin nasıl çalıştığı ve bu noktaya kadar atlanmış olan bir dizi diğer ayrıntı 

119
00:07:09,782 --> 00:07:11,680
hakkında daha fazla konuşmak istiyorum.

120
00:07:12,180 --> 00:07:16,178
Daha geniş bir bağlam için, bu videolar derin öğrenmeyle ilgili bir mini dizinin 

121
00:07:16,178 --> 00:07:20,473
ekleridir ve öncekileri izlemediyseniz sorun değil, bence bunu sırasız yapabilirsiniz, 

122
00:07:20,473 --> 00:07:22,892
ancak özellikle transformatörlere dalmadan önce, 

123
00:07:22,892 --> 00:07:26,841
derin öğrenmenin temel öncülü ve yapısı hakkında aynı sayfada olduğumuzdan emin 

124
00:07:26,841 --> 00:07:28,520
olmaya değer olduğunu düşünüyorum.

125
00:07:29,020 --> 00:07:31,974
Bariz olanı belirtme riskini göze alarak, bu, bir modelin nasıl 

126
00:07:31,974 --> 00:07:34,929
davrandığını bir şekilde belirlemek için verileri kullandığınız 

127
00:07:34,929 --> 00:07:38,300
herhangi bir modeli tanımlayan makine öğrenimine yönelik bir yaklaşımdır.

128
00:07:39,140 --> 00:07:42,260
Bununla kastettiğim şey, diyelim ki bir görüntüyü alan ve onu 

129
00:07:42,260 --> 00:07:45,683
tanımlayan bir etiket üreten bir işlev istiyorsunuz ya da bir metin 

130
00:07:45,683 --> 00:07:49,055
parçasına verilen bir sonraki kelimeyi tahmin etme örneğimiz ya da 

131
00:07:49,055 --> 00:07:52,780
bazı sezgi ve örüntü tanıma unsurları gerektiren başka herhangi bir görev.

132
00:07:53,200 --> 00:07:57,328
Bugünlerde bunu neredeyse doğal karşılıyoruz, ancak makine öğrenimi ile ilgili fikir, 

133
00:07:57,328 --> 00:08:00,881
bu görevin kodda nasıl yapılacağına dair bir prosedürü açıkça tanımlamaya 

134
00:08:00,881 --> 00:08:04,769
çalışmak yerine, ki bu insanların yapay zekanın ilk günlerinde yaptıkları şeydi, 

135
00:08:04,769 --> 00:08:08,418
bunun yerine bir grup düğme ve kadran gibi ayarlanabilir parametrelerle çok 

136
00:08:08,418 --> 00:08:12,162
esnek bir yapı kurarsınız ve daha sonra bir şekilde bu davranışı taklit etmek 

137
00:08:12,162 --> 00:08:15,859
için bu parametrelerin değerlerini değiştirmek ve ayarlamak için belirli bir 

138
00:08:15,859 --> 00:08:19,700
girdi için çıktının nasıl görünmesi gerektiğine dair birçok örnek kullanırsınız.

139
00:08:19,700 --> 00:08:23,960
Örneğin, makine öğreniminin belki de en basit biçimi doğrusal regresyondur; 

140
00:08:23,960 --> 00:08:27,549
burada girdileriniz ve çıktılarınızın her biri tek bir sayıdır, 

141
00:08:27,549 --> 00:08:31,305
bir evin metrekaresi ve fiyatı gibi bir şeydir ve istediğiniz şey, 

142
00:08:31,305 --> 00:08:35,678
gelecekteki ev fiyatlarını tahmin etmek için bu veriler üzerinden en iyi uyum 

143
00:08:35,678 --> 00:08:36,799
çizgisini bulmaktır.

144
00:08:37,440 --> 00:08:42,736
Bu çizgi, eğim ve y-kesişimi gibi iki sürekli parametre ile tanımlanır ve doğrusal 

145
00:08:42,736 --> 00:08:48,160
regresyonun amacı, bu parametreleri veriyle yakından eşleşecek şekilde belirlemektir.

146
00:08:48,880 --> 00:08:52,100
Söylemeye gerek yok, derin öğrenme modelleri çok daha karmaşık hale geliyor.

147
00:08:52,620 --> 00:08:57,660
Örneğin GPT-3'ün iki değil 175 milyar parametresi vardır.

148
00:08:58,120 --> 00:09:01,892
Ancak şu da var ki, eğitim verilerine büyük ölçüde aşırı uyum 

149
00:09:01,892 --> 00:09:05,482
sağlamadan ya da eğitilmesi tamamen zor olmadan çok sayıda 

150
00:09:05,482 --> 00:09:09,560
parametreye sahip dev bir model oluşturabileceğiniz kesin değildir.

151
00:09:10,260 --> 00:09:12,909
Derin öğrenme, son birkaç on yılda oldukça iyi 

152
00:09:12,909 --> 00:09:16,180
ölçeklendirildiği kanıtlanmış bir model sınıfını tanımlar.

153
00:09:16,480 --> 00:09:21,311
Bunları birleştiren şey, geriye yayılma adı verilen aynı eğitim algoritmasıdır 

154
00:09:21,311 --> 00:09:24,430
ve içeri girerken sahip olmanızı istediğim bağlam, 

155
00:09:24,430 --> 00:09:29,200
bu eğitim algoritmasının ölçekte iyi çalışması için bu modellerin belirli bir 

156
00:09:29,200 --> 00:09:31,280
formatı takip etmesi gerektiğidir.

157
00:09:31,800 --> 00:09:35,865
Eğer bu formatı biliyorsanız, bir transformatörün dili nasıl işlediğine dair, 

158
00:09:35,865 --> 00:09:40,400
aksi takdirde keyfi hissettirme riski taşıyan birçok seçeneği açıklamaya yardımcı olur.

159
00:09:41,440 --> 00:09:44,066
İlk olarak, yaptığınız model ne olursa olsun, girdinin 

160
00:09:44,066 --> 00:09:46,740
bir gerçek sayı dizisi olarak biçimlendirilmesi gerekir.

161
00:09:46,740 --> 00:09:51,280
Bu, bir sayı listesi anlamına gelebilir, iki boyutlu bir dizi olabilir veya 

162
00:09:51,280 --> 00:09:56,000
genel terimin tensör olduğu daha yüksek boyutlu dizilerle çok sık uğraşırsınız.

163
00:09:56,560 --> 00:10:00,276
Genellikle bu girdi verilerinin aşamalı olarak birçok farklı katmana 

164
00:10:00,276 --> 00:10:02,970
dönüştürüldüğünü düşünürsünüz; burada her katman, 

165
00:10:02,970 --> 00:10:07,117
çıktıyı düşündüğünüz son katmana ulaşana kadar her zaman bir tür gerçek sayı 

166
00:10:07,117 --> 00:10:08,680
dizisi olarak yapılandırılır.

167
00:10:09,280 --> 00:10:12,046
Örneğin, metin işleme modelimizdeki son katman, 

168
00:10:12,046 --> 00:10:17,060
olası tüm sonraki belirteçler için olasılık dağılımını temsil eden bir sayı listesidir.

169
00:10:17,820 --> 00:10:21,563
Derin öğrenmede, bu model parametreleri neredeyse her zaman ağırlık olarak 

170
00:10:21,563 --> 00:10:24,908
adlandırılır ve bunun nedeni, bu modellerin temel bir özelliğinin, 

171
00:10:24,908 --> 00:10:28,901
bu parametrelerin işlenen verilerle etkileşime girmesinin tek yolunun ağırlıklı 

172
00:10:28,901 --> 00:10:29,900
toplamlar olmasıdır.

173
00:10:30,340 --> 00:10:32,678
Ayrıca bazı doğrusal olmayan fonksiyonları da serpiştirirsiniz, 

174
00:10:32,678 --> 00:10:34,360
ancak bunlar parametrelere bağlı olmayacaktır.

175
00:10:35,200 --> 00:10:38,586
Ancak tipik olarak, ağırlıklı toplamları çıplak olarak görmek ve 

176
00:10:38,586 --> 00:10:42,233
bu şekilde açıkça yazmak yerine, bunları bir matris vektör çarpımında 

177
00:10:42,233 --> 00:10:45,620
çeşitli bileşenler olarak bir araya getirilmiş olarak görürsünüz.

178
00:10:46,740 --> 00:10:49,571
Matris vektör çarpımının nasıl çalıştığını düşünürseniz, 

179
00:10:49,571 --> 00:10:53,494
çıktıdaki her bileşenin ağırlıklı bir toplam gibi göründüğü aynı şeyi söylemek 

180
00:10:53,494 --> 00:10:54,240
anlamına gelir.

181
00:10:54,780 --> 00:10:59,852
İşlenen verilerden alınan vektörleri dönüştüren ayarlanabilir parametrelerle dolu 

182
00:10:59,852 --> 00:11:05,420
matrisler hakkında düşünmek sizin ve benim için genellikle kavramsal olarak daha temizdir.

183
00:11:06,340 --> 00:11:10,412
Örneğin, GPT-3'teki 175 milyar ağırlık 28.000'den 

184
00:11:10,412 --> 00:11:14,160
biraz az farklı matris halinde düzenlenmiştir.

185
00:11:14,660 --> 00:11:18,726
Bu matrisler sırayla sekiz farklı kategoriye ayrılır ve sizin ve benim yapacağımız şey, 

186
00:11:18,726 --> 00:11:22,700
bu türün ne yaptığını anlamak için bu kategorilerin her birini adım adım incelemektir.

187
00:11:23,160 --> 00:11:27,141
İlerledikçe, 175 milyarın tam olarak nereden geldiğini saymak için 

188
00:11:27,141 --> 00:11:31,360
GPT-3'teki belirli sayılara başvurmanın eğlenceli olduğunu düşünüyorum.

189
00:11:31,880 --> 00:11:34,675
Günümüzde daha büyük ve daha iyi modeller olsa bile, 

190
00:11:34,675 --> 00:11:39,052
bu model ML toplulukları dışında dünyanın dikkatini çeken geniş dilli model olarak 

191
00:11:39,052 --> 00:11:40,740
belirli bir çekiciliğe sahiptir.

192
00:11:41,440 --> 00:11:44,132
Ayrıca, pratik olarak konuşmak gerekirse, şirketler daha modern 

193
00:11:44,132 --> 00:11:46,740
ağlar için belirli sayıları çok daha sıkı tutma eğilimindedir.

194
00:11:47,360 --> 00:11:50,754
Sadece ChatGPT gibi bir aracın içinde neler olduğunu görmek için 

195
00:11:50,754 --> 00:11:53,836
kaputun altına baktığınızda, gerçek hesaplamanın neredeyse 

196
00:11:53,836 --> 00:11:57,440
tamamının matris vektör çarpımı gibi göründüğünü belirtmek istiyorum.

197
00:11:57,900 --> 00:12:00,940
Milyarlarca sayı denizinde kaybolma riski biraz var, 

198
00:12:00,940 --> 00:12:05,644
ancak her zaman mavi veya kırmızıyla renklendireceğim modelin ağırlıkları ile her 

199
00:12:05,644 --> 00:12:10,119
zaman griyle renklendireceğim işlenmekte olan veriler arasında zihninizde çok 

200
00:12:10,119 --> 00:12:11,840
keskin bir ayrım yapmalısınız.

201
00:12:12,180 --> 00:12:14,854
Ağırlıklar gerçek beyinlerdir, eğitim sırasında 

202
00:12:14,854 --> 00:12:17,920
öğrenilen şeylerdir ve nasıl davranacağını belirlerler.

203
00:12:18,280 --> 00:12:21,693
İşlenen veriler, örnek bir metin parçacığı gibi, 

204
00:12:21,693 --> 00:12:26,500
belirli bir çalıştırma için modele beslenen belirli girdileri kodlar.

205
00:12:27,480 --> 00:12:31,950
Tüm bunları temel alarak, girdiyi küçük parçalara ayırmak ve bu parçaları 

206
00:12:31,950 --> 00:12:36,420
vektörlere dönüştürmek olan bu metin işleme örneğinin ilk adımına geçelim.

207
00:12:37,020 --> 00:12:39,503
Bu parçalara nasıl jeton denildiğinden bahsetmiştim, 

208
00:12:39,503 --> 00:12:42,315
bunlar kelime parçaları veya noktalama işaretleri olabilir, 

209
00:12:42,315 --> 00:12:44,893
ancak bu bölümde ve özellikle bir sonrakinde ara sıra, 

210
00:12:44,893 --> 00:12:48,080
kelimelere daha temiz bir şekilde bölünmüş gibi davranmak istiyorum.

211
00:12:48,600 --> 00:12:51,320
Biz insanlar kelimelerle düşündüğümüz için, bu sadece küçük örneklere 

212
00:12:51,320 --> 00:12:54,080
başvurmayı ve her adımı netleştirmeyi çok daha kolay hale getirecektir.

213
00:12:55,260 --> 00:12:58,327
Modelin önceden tanımlanmış bir kelime dağarcığı vardır, 

214
00:12:58,327 --> 00:13:01,610
tüm olası kelimelerin bir listesi, diyelim ki 50.000 tanesi, 

215
00:13:01,610 --> 00:13:04,947
ve karşılaşacağımız ilk matris, gömme matrisi olarak bilinir, 

216
00:13:04,947 --> 00:13:07,800
bu kelimelerin her biri için tek bir sütuna sahiptir.

217
00:13:08,940 --> 00:13:13,760
Bu sütunlar, her bir kelimenin ilk adımda hangi vektöre dönüşeceğini belirler.

218
00:13:15,100 --> 00:13:18,463
Bunu We olarak etiketliyoruz ve gördüğümüz tüm matrisler gibi, 

219
00:13:18,463 --> 00:13:22,360
değerleri rastgele başlıyor, ancak verilere dayalı olarak öğrenilecekler.

220
00:13:23,620 --> 00:13:27,587
Kelimeleri vektörlere dönüştürmek, transformatörlerden çok önce makine öğreniminde 

221
00:13:27,587 --> 00:13:31,554
yaygın bir uygulamaydı, ancak daha önce hiç görmediyseniz biraz gariptir ve bundan 

222
00:13:31,554 --> 00:13:35,760
sonraki her şeyin temelini oluşturur, bu yüzden bir dakikanızı ayırıp buna aşina olalım.

223
00:13:36,040 --> 00:13:38,792
Bu gömme işlemini genellikle bir kelime olarak adlandırırız, 

224
00:13:38,792 --> 00:13:42,672
bu da sizi bu vektörleri geometrik olarak yüksek boyutlu bir uzaydaki noktalar olarak 

225
00:13:42,672 --> 00:13:43,620
düşünmeye davet eder.

226
00:13:44,180 --> 00:13:46,405
Üç sayıdan oluşan bir listeyi 3B uzaydaki noktaların 

227
00:13:46,405 --> 00:13:48,798
koordinatları olarak görselleştirmek sorun olmayacaktır, 

228
00:13:48,798 --> 00:13:51,780
ancak sözcük katıştırmaları çok daha yüksek boyutlu olma eğilimindedir.

229
00:13:52,280 --> 00:13:56,057
GPT-3'te 12.288 boyut vardır ve göreceğiniz gibi, 

230
00:13:56,057 --> 00:14:00,440
çok sayıda farklı yönü olan bir alanda çalışmak önemlidir.

231
00:14:01,180 --> 00:14:05,948
Tıpkı 3 boyutlu bir uzayda iki boyutlu bir kesit alıp tüm noktaları bu kesit üzerine 

232
00:14:05,948 --> 00:14:10,493
yansıtabileceğiniz gibi, basit bir modelin bana verdiği kelime yerleştirmelerini 

233
00:14:10,493 --> 00:14:15,374
canlandırmak için, bu çok yüksek boyutlu uzayda üç boyutlu bir kesit seçerek ve kelime 

234
00:14:15,374 --> 00:14:19,918
vektörlerini bunun üzerine yansıtarak ve sonuçları görüntüleyerek benzer bir şey 

235
00:14:19,918 --> 00:14:20,480
yapacağım.

236
00:14:21,280 --> 00:14:24,690
Buradaki ana fikir, bir modelin eğitim sırasında kelimelerin vektörler 

237
00:14:24,690 --> 00:14:27,908
olarak tam olarak nasıl gömüleceğini belirlemek için ağırlıklarını 

238
00:14:27,908 --> 00:14:31,077
değiştirip ayarladıkça, uzaydaki yönlerin bir tür anlamsal anlama 

239
00:14:31,077 --> 00:14:34,440
sahip olduğu bir dizi gömme üzerinde karar kılma eğiliminde olmasıdır.

240
00:14:34,980 --> 00:14:37,913
Burada çalıştırdığım basit kelime-vektör modeli için, 

241
00:14:37,913 --> 00:14:42,097
gömülmeleri kuleninkine en yakın olan tüm kelimeler için bir arama yaparsam, 

242
00:14:42,097 --> 00:14:45,900
hepsinin nasıl çok benzer kule-imsi hisler verdiğini fark edeceksiniz.

243
00:14:46,340 --> 00:14:49,385
Eğer Python'u açıp evde oynamak isterseniz, animasyonları 

244
00:14:49,385 --> 00:14:51,380
yapmak için kullandığım özel model bu.

245
00:14:51,620 --> 00:14:54,371
Bu bir transformatör değil, ancak uzaydaki yönlerin 

246
00:14:54,371 --> 00:14:57,600
semantik anlam taşıyabileceği fikrini göstermek için yeterli.

247
00:14:58,300 --> 00:15:04,087
Bunun çok klasik bir örneği, kadın ve erkek vektörleri arasındaki farkı ele alırsanız, 

248
00:15:04,087 --> 00:15:09,674
birinin ucunu diğerinin ucuna bağlayan küçük bir vektör olarak görselleştireceğiniz 

249
00:15:09,674 --> 00:15:13,200
bir şey, kral ve kraliçe arasındaki farka çok benzer.

250
00:15:15,080 --> 00:15:19,185
Diyelim ki bir kadın hükümdar için kullanılan kelimeyi bilmiyordunuz, 

251
00:15:19,185 --> 00:15:24,052
king'i alıp bu kadın-erkek yönünü ekleyerek ve bu noktaya en yakın yerleştirmeleri 

252
00:15:24,052 --> 00:15:25,460
arayarak bulabilirsiniz.

253
00:15:27,000 --> 00:15:28,200
En azından öyle sayılır.

254
00:15:28,480 --> 00:15:31,834
Bu, üzerinde çalıştığım model için klasik bir örnek olmasına rağmen, 

255
00:15:31,834 --> 00:15:35,578
kraliçenin gerçek gömülmesi aslında bunun gösterdiğinden biraz daha uzaktır, 

256
00:15:35,578 --> 00:15:39,661
çünkü muhtemelen kraliçenin eğitim verilerinde kullanılma şekli sadece kralın dişil 

257
00:15:39,661 --> 00:15:40,780
bir versiyonu değildir.

258
00:15:41,620 --> 00:15:45,260
Biraz oynadığımda, aile ilişkileri bu fikri çok daha iyi açıklıyor gibiydi.

259
00:15:46,340 --> 00:15:50,731
Mesele şu ki, eğitim sırasında model, bu uzaydaki bir yönün cinsiyet bilgisini 

260
00:15:50,731 --> 00:15:54,900
kodlayacağı şekilde katıştırmaları seçmeyi avantajlı bulmuş gibi görünüyor.

261
00:15:56,800 --> 00:16:00,441
Bir başka örnek de, İtalya'nın gömülmesini alıp Almanya'nın 

262
00:16:00,441 --> 00:16:04,508
gömülmesini çıkarırsanız ve bunu Hitler'in gömülmesine eklerseniz, 

263
00:16:04,508 --> 00:16:08,090
Mussolini'nin gömülmesine çok yakın bir şey elde edersiniz.

264
00:16:08,570 --> 00:16:12,062
Sanki model bazı yönleri İtalyanlıkla, bazılarını ise İkinci 

265
00:16:12,062 --> 00:16:15,670
Dünya Savaşı eksen liderleriyle ilişkilendirmeyi öğrenmiş gibi.

266
00:16:16,470 --> 00:16:21,016
Belki de bu konudaki en sevdiğim örnek, bazı modellerde Almanya ve Japonya 

267
00:16:21,016 --> 00:16:26,230
arasındaki farkı alıp suşiye eklediğinizde sucuğa çok yakın bir sonuç elde etmenizdir.

268
00:16:27,350 --> 00:16:29,883
Ayrıca bu en yakın komşuları bulma oyununu oynarken, 

269
00:16:29,883 --> 00:16:33,850
Kat'in hem canavara hem de canavara ne kadar yakın olduğunu görmekten memnun oldum.

270
00:16:34,690 --> 00:16:38,714
Özellikle bir sonraki bölüm için akılda tutulması yararlı olan bir matematiksel sezgi, 

271
00:16:38,714 --> 00:16:41,906
iki vektörün nokta çarpımının ne kadar iyi hizalandıklarını ölçmenin 

272
00:16:41,906 --> 00:16:43,850
bir yolu olarak nasıl düşünülebileceğidir.

273
00:16:44,870 --> 00:16:47,891
Hesaplama açısından nokta çarpımlar, karşılık gelen tüm bileşenlerin 

274
00:16:47,891 --> 00:16:51,351
çarpılmasını ve ardından sonuçların toplanmasını içerir; bu da iyi bir şeydir, 

275
00:16:51,351 --> 00:16:54,330
çünkü hesaplamalarımızın çoğu ağırlıklı toplamlar gibi görünmelidir.

276
00:16:55,190 --> 00:17:00,879
Geometrik olarak, vektörler benzer yönleri gösterdiğinde nokta çarpımı pozitiftir, 

277
00:17:00,879 --> 00:17:05,609
dik olduklarında sıfırdır ve zıt yönleri gösterdiklerinde negatiftir.

278
00:17:06,550 --> 00:17:11,888
Örneğin, bu modelle oynadığınızı ve kedi eksi kedi gömülmesinin bu uzayda 

279
00:17:11,888 --> 00:17:17,010
bir tür çoğulluk yönünü temsil edebileceğini varsaydığınızı varsayalım.

280
00:17:17,430 --> 00:17:20,355
Bunu test etmek için, bu vektörü alacağım ve belirli tekil 

281
00:17:20,355 --> 00:17:23,628
isimlerin gömülerine karşı nokta çarpımını hesaplayacağım ve bunu 

282
00:17:23,628 --> 00:17:27,050
karşılık gelen çoğul isimlerle nokta çarpımlarıyla karşılaştıracağım.

283
00:17:27,270 --> 00:17:30,426
Bununla oynarsanız, çoğul olanların gerçekten de tekil olanlardan 

284
00:17:30,426 --> 00:17:33,487
sürekli olarak daha yüksek değerler verdiğini fark edeceksiniz, 

285
00:17:33,487 --> 00:17:36,070
bu da bu yöne daha fazla uyum sağladıklarını gösterir.

286
00:17:37,070 --> 00:17:41,721
Ayrıca, bu nokta çarpımını 1, 2, 3 ve benzeri kelimelerin gömülmeleriyle alırsanız, 

287
00:17:41,721 --> 00:17:45,763
artan değerler vermeleri de eğlencelidir, bu nedenle modelin belirli bir 

288
00:17:45,763 --> 00:17:49,030
kelimeyi ne kadar çoğul bulduğunu nicel olarak ölçebiliriz.

289
00:17:50,250 --> 00:17:53,570
Yine, kelimelerin nasıl gömüldüğüne ilişkin ayrıntılar veriler kullanılarak öğrenilir.

290
00:17:54,050 --> 00:17:57,627
Sütunları bize her kelimeye ne olduğunu söyleyen bu gömme matrisi, 

291
00:17:57,627 --> 00:17:59,550
modelimizdeki ilk ağırlık yığınıdır.

292
00:18:00,030 --> 00:18:04,804
GPT-3 sayılarını kullanarak, kelime haznesi boyutu özellikle 50.257'dir ve 

293
00:18:04,804 --> 00:18:09,770
yine teknik olarak bu, kendi başına kelimelerden değil, belirteçlerden oluşur.

294
00:18:10,630 --> 00:18:13,966
Gömme boyutu 12.288'dir ve bunları çarpmak bize 

295
00:18:13,966 --> 00:18:17,790
bunun yaklaşık 617 milyon ağırlıktan oluştuğunu söyler.

296
00:18:18,250 --> 00:18:21,056
Devam edelim ve bunu bir çeteleye ekleyelim, sonunda 

297
00:18:21,056 --> 00:18:23,810
175 milyara kadar saymamız gerektiğini hatırlayalım.

298
00:18:25,430 --> 00:18:28,854
Dönüştürücüler söz konusu olduğunda, bu gömme uzayındaki vektörlerin 

299
00:18:28,854 --> 00:18:32,130
yalnızca tek tek kelimeleri temsil etmediğini düşünmek istersiniz.

300
00:18:32,550 --> 00:18:35,975
Birincisi, daha sonra bahsedeceğimiz gibi, o kelimenin konumu 

301
00:18:35,975 --> 00:18:38,902
hakkında da bilgi kodlarlar, ancak daha da önemlisi, 

302
00:18:38,902 --> 00:18:42,770
onları bağlam içinde ıslatma kapasitesine sahip olarak düşünmelisiniz.

303
00:18:43,350 --> 00:18:47,685
Örneğin, hayatına kral kelimesinin gömülmesi olarak başlayan bir vektör, 

304
00:18:47,685 --> 00:18:52,020
bu ağdaki çeşitli bloklar tarafından aşamalı olarak çekilip çekilebilir, 

305
00:18:52,020 --> 00:18:57,068
böylece sonunda bir şekilde İskoçya'da yaşayan ve bir önceki kralı öldürdükten sonra 

306
00:18:57,068 --> 00:19:02,176
makamına ulaşan ve Shakespeare dilinde tanımlanan bir kral olduğunu kodlayan çok daha 

307
00:19:02,176 --> 00:19:04,730
spesifik ve incelikli bir yöne işaret eder.

308
00:19:05,210 --> 00:19:07,790
Belirli bir kelime hakkında kendi anlayışınızı düşünün.

309
00:19:08,250 --> 00:19:12,242
Bu kelimenin anlamı açıkça çevreden öğrenilir ve bazen bu çok uzak mesafeden 

310
00:19:12,242 --> 00:19:15,871
gelen bağlamı da içerir, bu nedenle bir sonraki kelimenin ne olduğunu 

311
00:19:15,871 --> 00:19:19,345
tahmin etme yeteneğine sahip bir modeli bir araya getirirken amaç, 

312
00:19:19,345 --> 00:19:23,390
bir şekilde bağlamı verimli bir şekilde dahil etmesi için onu güçlendirmektir.

313
00:19:24,050 --> 00:19:27,575
Açık olmak gerekirse, bu ilk adımda, girdi metnine dayalı vektör dizisini 

314
00:19:27,575 --> 00:19:31,053
oluşturduğunuzda, bunların her biri basitçe gömme matrisinden koparılır, 

315
00:19:31,053 --> 00:19:35,245
bu nedenle başlangıçta her biri çevresinden herhangi bir girdi olmadan yalnızca tek bir 

316
00:19:35,245 --> 00:19:36,770
kelimenin anlamını kodlayabilir.

317
00:19:37,710 --> 00:19:41,422
Ancak bu ağın birincil amacının, bu vektörlerin her birinin, 

318
00:19:41,422 --> 00:19:45,196
tek tek kelimelerin temsil edebileceğinden çok daha zengin ve 

319
00:19:45,196 --> 00:19:48,970
spesifik bir anlamı emmesini sağlamak olduğunu düşünmelisiniz.

320
00:19:49,510 --> 00:19:54,170
Ağ, bir seferde yalnızca bağlam boyutu olarak bilinen sabit sayıda vektörü işleyebilir.

321
00:19:54,510 --> 00:19:57,679
GPT-3 için 2048 bağlam boyutu ile eğitilmiştir, 

322
00:19:57,679 --> 00:20:03,160
bu nedenle ağdan akan veriler her zaman her biri 12.000 boyuta sahip 2048 sütundan 

323
00:20:03,160 --> 00:20:05,010
oluşan bu dizi gibi görünür.

324
00:20:05,590 --> 00:20:08,498
Bu bağlam boyutu, dönüştürücünün bir sonraki kelimenin 

325
00:20:08,498 --> 00:20:11,830
tahminini yaparken ne kadar metni dahil edebileceğini sınırlar.

326
00:20:12,370 --> 00:20:17,349
Bu nedenle ChatGPT'nin ilk sürümleri gibi bazı sohbet botlarıyla yapılan uzun sohbetler, 

327
00:20:17,349 --> 00:20:22,050
siz çok uzun süre devam ettikçe botun sohbetin akışını kaybettiği hissini veriyordu.

328
00:20:23,030 --> 00:20:25,239
Zamanı geldiğinde dikkatin ayrıntılarına gireceğiz, 

329
00:20:25,239 --> 00:20:28,810
ancak ileriye atlayarak en sonunda ne olduğu hakkında bir dakika konuşmak istiyorum.

330
00:20:29,450 --> 00:20:32,132
Unutmayın, istenen çıktı, daha sonra gelebilecek 

331
00:20:32,132 --> 00:20:34,870
tüm belirteçler üzerinde bir olasılık dağılımıdır.

332
00:20:35,170 --> 00:20:39,324
Örneğin, en son kelime Profesör ise ve bağlam Harry Potter gibi kelimeleri 

333
00:20:39,324 --> 00:20:43,478
içeriyorsa ve hemen öncesinde en az sevilen öğretmeni görüyorsak ve ayrıca 

334
00:20:43,478 --> 00:20:47,687
belirteçlerin sadece tam kelimeler gibi göründüğünü varsaymama izin vererek 

335
00:20:47,687 --> 00:20:51,786
bana biraz serbestlik tanırsanız, Harry Potter hakkında bilgi edinmiş iyi 

336
00:20:51,786 --> 00:20:55,830
eğitilmiş bir ağ muhtemelen Snape kelimesine yüksek bir sayı atayacaktır.

337
00:20:56,510 --> 00:20:57,970
Bu iki farklı adımı içerir.

338
00:20:58,310 --> 00:21:02,934
Birincisi, bu bağlamdaki en son vektörü, kelime dağarcığındaki her bir belirteç için bir 

339
00:21:02,934 --> 00:21:07,610
tane olmak üzere 50.000 değerden oluşan bir listeye eşleyen başka bir matris kullanmaktır.

340
00:21:08,170 --> 00:21:12,375
Daha sonra bunu bir olasılık dağılımına normalleştiren bir fonksiyon var, 

341
00:21:12,375 --> 00:21:16,070
buna Softmax deniyor ve birazdan bundan daha fazla bahsedeceğiz, 

342
00:21:16,070 --> 00:21:21,185
ancak bundan önce bir tahmin yapmak için sadece bu son katıştırmayı kullanmak biraz garip 

343
00:21:21,185 --> 00:21:26,130
görünebilir, sonuçta bu son adımda katmanda kendi bağlam açısından zengin anlamlarıyla 

344
00:21:26,130 --> 00:21:28,290
orada duran binlerce başka vektör var.

345
00:21:28,930 --> 00:21:32,556
Bunun nedeni, eğitim sürecinde, son katmandaki vektörlerin her 

346
00:21:32,556 --> 00:21:36,470
birini aynı anda hemen ardından gelecek olan için bir tahmin yapmak 

347
00:21:36,470 --> 00:21:40,270
üzere kullanırsanız çok daha verimli olacağı gerçeğiyle ilgilidir.

348
00:21:40,970 --> 00:21:43,136
Eğitim hakkında daha sonra söylenecek çok şey var, 

349
00:21:43,136 --> 00:21:45,090
ancak şu anda sadece bunu belirtmek istiyorum.

350
00:21:45,730 --> 00:21:49,690
Bu matris Unembedding matrisi olarak adlandırılır ve ona WU etiketi verilir.

351
00:21:50,210 --> 00:21:54,027
Yine, gördüğümüz tüm ağırlık matrisleri gibi, girişleri rastgele başlar, 

352
00:21:54,027 --> 00:21:55,910
ancak eğitim sürecinde öğrenilirler.

353
00:21:56,470 --> 00:22:01,168
Toplam parametre sayımızı koruyarak, bu Gömme matrisi kelime dağarcığındaki her kelime 

354
00:22:01,168 --> 00:22:05,650
için bir satıra sahiptir ve her satır gömme boyutuyla aynı sayıda elemana sahiptir.

355
00:22:06,410 --> 00:22:10,191
Gömme matrisine çok benzer, sadece sırası değiştirilmiştir, 

356
00:22:10,191 --> 00:22:13,154
bu yüzden ağa 617 milyon parametre daha ekler, 

357
00:22:13,154 --> 00:22:16,621
yani şu ana kadarki sayımız bir milyardan biraz fazla, 

358
00:22:16,621 --> 00:22:21,790
toplamda elde edeceğimiz 175 milyarın küçük ama tamamen önemsiz olmayan bir kısmı.

359
00:22:22,550 --> 00:22:26,628
Bu bölümün son mini dersi olarak bu softmax fonksiyonu hakkında daha fazla konuşmak 

360
00:22:26,628 --> 00:22:30,610
istiyorum, çünkü dikkat bloklarına daldığımızda karşımıza bir kez daha çıkacaktır.

361
00:22:31,430 --> 00:22:35,798
Buradaki fikir, bir sayı dizisinin bir olasılık dağılımı olarak hareket etmesini 

362
00:22:35,798 --> 00:22:39,681
istiyorsanız, örneğin tüm olası sonraki kelimeler üzerinde bir dağılım, 

363
00:22:39,681 --> 00:22:44,158
o zaman her değerin 0 ile 1 arasında olması ve ayrıca hepsinin toplamının 1 olması 

364
00:22:44,158 --> 00:22:44,590
gerekir.

365
00:22:45,250 --> 00:22:49,741
Ancak, yaptığınız her şeyin matris-vektör çarpımına benzediği öğrenme 

366
00:22:49,741 --> 00:22:54,810
oyununu oynuyorsanız, varsayılan olarak elde ettiğiniz çıktılar buna hiç uymaz.

367
00:22:55,330 --> 00:22:57,600
Değerler genellikle negatiftir veya 1'den çok daha 

368
00:22:57,600 --> 00:22:59,870
büyüktür ve neredeyse kesinlikle 1'e eşit değildir.

369
00:23:00,510 --> 00:23:05,900
Softmax, rastgele bir sayı listesini, en büyük değerler 1'e en yakın olacak ve daha küçük 

370
00:23:05,900 --> 00:23:11,290
değerler 0'a çok yakın olacak şekilde geçerli bir dağılıma dönüştürmenin standart yoludur.

371
00:23:11,830 --> 00:23:13,070
Gerçekten bilmeniz gereken tek şey bu.

372
00:23:13,090 --> 00:23:17,297
Ancak merak ediyorsanız, bunun çalışma şekli önce e'yi sayıların her birinin gücüne 

373
00:23:17,297 --> 00:23:21,555
yükseltmektir, bu da artık pozitif değerlerin bir listesine sahip olduğunuz anlamına 

374
00:23:21,555 --> 00:23:25,663
gelir ve daha sonra tüm bu pozitif değerlerin toplamını alabilir ve her terimi bu 

375
00:23:25,663 --> 00:23:29,470
toplama bölebilirsiniz, bu da onu 1'e kadar olan bir listeye normalleştirir.

376
00:23:30,170 --> 00:23:33,828
Girdideki sayılardan biri diğerlerinden anlamlı bir şekilde büyükse, 

377
00:23:33,828 --> 00:23:37,274
çıktıda ilgili terimin dağılıma hakim olduğunu fark edeceksiniz, 

378
00:23:37,274 --> 00:23:41,356
bu nedenle örnekleme yapıyorsanız neredeyse kesinlikle sadece maksimize eden 

379
00:23:41,356 --> 00:23:42,470
girdiyi seçeceksiniz.

380
00:23:42,990 --> 00:23:45,818
Ancak, diğer değerler de benzer şekilde büyük olduğunda, 

381
00:23:45,818 --> 00:23:49,986
dağılımda anlamlı bir ağırlık kazanmaları ve girdileri sürekli olarak değiştirdikçe 

382
00:23:49,986 --> 00:23:53,856
her şeyin sürekli olarak değişmesi anlamında sadece maksimum değeri seçmekten 

383
00:23:53,856 --> 00:23:54,650
daha yumuşaktır.

384
00:23:55,130 --> 00:23:59,743
ChatGPT'nin bu dağılımı bir sonraki kelimeyi oluşturmak için kullanması gibi 

385
00:23:59,743 --> 00:24:03,757
bazı durumlarda, bu fonksiyona biraz daha fazla baharat ekleyerek, 

386
00:24:03,757 --> 00:24:08,910
bu üslerin paydasına atılan bir t sabiti ile biraz daha fazla eğlence için yer vardır.

387
00:24:09,550 --> 00:24:14,042
Buna sıcaklık diyoruz, çünkü belli termodinamik denklemlerinde sıcaklığın rolüne 

388
00:24:14,042 --> 00:24:17,647
belli belirsiz benziyor ve etkisi şu ki, t daha büyük olduğunda, 

389
00:24:17,647 --> 00:24:20,532
daha düşük değerlere daha fazla ağırlık verirsiniz, 

390
00:24:20,532 --> 00:24:24,692
bu da dağılımın biraz daha düzgün olduğu anlamına gelir ve t daha küçükse, 

391
00:24:24,692 --> 00:24:28,186
daha büyük değerler daha agresif bir şekilde baskın olacaktır, 

392
00:24:28,186 --> 00:24:32,790
burada t'yi sıfıra eşitlemek, tüm ağırlığın maksimum değere gitmesi anlamına gelir.

393
00:24:33,470 --> 00:24:38,179
Örneğin, GPT-3'ün bir zamanlar A varmış şeklindeki tohum metniyle bir hikaye 

394
00:24:38,179 --> 00:24:42,950
oluşturmasını sağlayacağım, ancak her durumda farklı sıcaklıklar kullanacağım.

395
00:24:43,630 --> 00:24:48,000
Sıfır sıcaklık, her zaman en öngörülebilir kelimeyi seçtiği anlamına 

396
00:24:48,000 --> 00:24:52,370
gelir ve elde ettiğiniz şey Goldilocks'un basmakalıp bir türevi olur.

397
00:24:53,010 --> 00:24:56,372
Daha yüksek bir sıcaklık, daha az olası kelimeleri seçme şansı verir, 

398
00:24:56,372 --> 00:24:57,910
ancak bir riskle birlikte gelir.

399
00:24:58,230 --> 00:25:02,061
Bu durumda, hikaye Güney Kore'den genç bir web sanatçısı hakkında 

400
00:25:02,061 --> 00:25:06,010
daha orijinal bir şekilde başlıyor, ancak hızla saçmalığa dönüşüyor.

401
00:25:06,950 --> 00:25:08,869
Teknik olarak konuşmak gerekirse, API aslında 

402
00:25:08,869 --> 00:25:10,830
2'den büyük bir sıcaklık seçmenize izin vermez.

403
00:25:11,170 --> 00:25:15,318
Bunun matematiksel bir nedeni yok, sadece araçlarının çok saçma şeyler 

404
00:25:15,318 --> 00:25:19,350
ürettiğinin görülmesini engellemek için konulmuş keyfi bir kısıtlama.

405
00:25:19,870 --> 00:25:24,236
Merak ediyorsanız, bu animasyonun çalışma şekli aslında GPT-3'ün ürettiği 

406
00:25:24,236 --> 00:25:28,662
en olası 20 sonraki jetonu alıyorum, ki bu bana verecekleri maksimum değer 

407
00:25:28,662 --> 00:25:32,970
gibi görünüyor ve sonra olasılıkları 15'lik bir üsse göre değiştiriyorum.

408
00:25:33,130 --> 00:25:37,529
Bir başka jargon olarak, bu fonksiyonun çıktısının bileşenlerini olasılık 

409
00:25:37,529 --> 00:25:41,691
olarak adlandırabileceğiniz gibi, insanlar genellikle girdileri logit 

410
00:25:41,691 --> 00:25:46,150
olarak adlandırır veya bazıları logit, bazıları logit, ben logit diyeceğim.

411
00:25:46,530 --> 00:25:50,073
Örneğin, bir metni beslediğinizde, tüm bu kelime katıştırmaları ağ 

412
00:25:50,073 --> 00:25:53,774
üzerinden akar ve bu son çarpımı katıştırılmamış matrisle yaparsınız, 

413
00:25:53,774 --> 00:25:57,265
makine öğrenimi çalışanları bu ham, normalleştirilmemiş çıktıdaki 

414
00:25:57,265 --> 00:26:01,390
bileşenlere bir sonraki kelime tahmini için logitler olarak atıfta bulunurlar.

415
00:26:03,330 --> 00:26:08,340
Bu bölümdeki amacın çoğu, dikkat mekanizmasını anlamak için temelleri atmaktı, 

416
00:26:08,340 --> 00:26:10,370
Karate Kid wax-on-wax-off tarzı.

417
00:26:10,850 --> 00:26:16,265
Gördüğünüz gibi, kelime katıştırma, softmax, nokta çarpımlarının benzerliği nasıl ölçtüğü 

418
00:26:16,265 --> 00:26:21,439
ve ayrıca hesaplamaların çoğunun ayarlanabilir parametrelerle dolu matrislerle matris 

419
00:26:21,439 --> 00:26:25,591
çarpımı gibi görünmesi gerektiği konusunda güçlü bir sezginiz varsa, 

420
00:26:25,591 --> 00:26:30,705
yapay zekadaki tüm modern patlamanın bu temel taşı olan dikkat mekanizmasını anlamak 

421
00:26:30,705 --> 00:26:32,210
nispeten kolay olacaktır.

422
00:26:32,650 --> 00:26:34,510
Bunun için bir sonraki bölümde bana katılın.

423
00:26:36,390 --> 00:26:38,800
Bunu yayınladığım sırada, bir sonraki bölümün taslağı 

424
00:26:38,800 --> 00:26:41,210
Patreon destekçileri tarafından incelenebilir durumda.

425
00:26:41,770 --> 00:26:44,383
Son versiyon bir ya da iki hafta içinde kamuoyuna açıklanacak, 

426
00:26:44,383 --> 00:26:47,370
genellikle bu incelemeye dayanarak ne kadar değişiklik yapacağıma bağlı.

427
00:26:47,810 --> 00:26:52,410
Bu arada, dikkat çekmek ve kanala biraz yardımcı olmak isterseniz, orada bekliyor.

