[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "GPT'nin baş harfleri Generative Pretrained Transformer'ın kısaltmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Yani ilk kelime yeterince açık, bunlar yeni metin üreten botlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Önceden eğitilmiş, modelin büyük miktarda veriden nasıl bir öğrenme sürecinden geçtiğini ifade eder ve önek, ek eğitimle belirli görevlerde ince ayar yapmak için daha fazla alan olduğunu ima eder.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Ama son söz, asıl anahtar parça budur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Transformatör, özel bir tür sinir ağı, bir makine öğrenimi modelidir ve yapay zekadaki mevcut patlamanın altında yatan temel buluştur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Bu video ve sonraki bölümlerde yapmak istediğim şey, bir transformatörün içinde gerçekte neler olduğunu görsel olarak açıklamak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "İçinden akan verileri takip edeceğiz ve adım adım ilerleyeceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Transformatörleri kullanarak inşa edebileceğiniz birçok farklı türde model vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Bazı modeller ses alır ve bir transkript üretir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Bu cümle, tam tersi yönde ilerleyen ve sadece metinden sentetik konuşma üreten bir modelden geliyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "2022'de dünyayı kasıp kavuran Dolly ve Midjourney gibi bir metin açıklaması alıp bir görüntü üreten tüm o araçlar transformatörlere dayanıyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Bir turta yaratığının ne olması gerektiğini tam olarak anlayamasam bile, bu tür bir şeyin uzaktan bile mümkün olması beni hala şaşırtıyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "Ve 2017 yılında Google tarafından tanıtılan orijinal transformatör, metni bir dilden diğerine çevirme özel kullanım durumu için icat edildi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Ancak, ChatGPT gibi araçların temelini oluşturan, sizin ve benim odaklanacağımız varyant, bir metin parçasını, belki de ona eşlik eden bazı çevre görüntüleri veya seslerle birlikte almak ve pasajda bir sonraki adımda ne olacağına dair bir tahmin üretmek için eğitilmiş bir model olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Bu tahmin, takip edebilecek birçok farklı metin parçası üzerinde bir olasılık dağılımı şeklini alır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "İlk bakışta, bir sonraki kelimeyi tahmin etmenin yeni metin oluşturmaktan çok farklı bir amaç olduğunu düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Ancak bunun gibi bir tahmin modeline sahip olduğunuzda, daha uzun bir metin parçası oluşturmanın basit bir yolu, ona üzerinde çalışması için bir başlangıç parçacığı vermek, yeni oluşturduğu dağılımdan rastgele bir örnek almasını sağlamak, bu örneği metne eklemek ve ardından yeni ekledikleri de dahil olmak üzere tüm yeni metne dayalı yeni bir tahmin yapmak için tüm süreci yeniden çalıştırmaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Sizi bilmem ama bunun gerçekten işe yaraması gerekiyormuş gibi gelmiyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Örneğin bu animasyonda, GPT-2'yi dizüstü bilgisayarımda çalıştırıyorum ve tohum metne dayalı bir hikaye oluşturmak için bir sonraki metin parçasını tekrar tekrar tahmin etmesini ve örneklemesini sağlıyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "Hikaye o kadar da mantıklı değil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Ancak bunun yerine aynı temel model olan GPT-3'e API çağrıları yaparsam, sadece çok daha büyük, aniden neredeyse sihirli bir şekilde mantıklı bir hikaye elde ederiz, hatta bir pi yaratığının matematik ve hesaplama diyarında yaşayacağı sonucunu çıkarır gibi görünen bir hikaye.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Buradaki tekrarlanan tahmin ve örnekleme süreci, ChatGPT veya diğer büyük dil modellerinden herhangi biriyle etkileşime girdiğinizde ve her seferinde bir kelime ürettiklerini gördüğünüzde esasen olan şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Aslında, çok hoşuma gidecek bir özellik, seçtiği her yeni kelime için altta yatan dağılımı görebilmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Verilerin bir dönüştürücüden nasıl geçtiğine dair çok üst düzey bir önizleme ile başlayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Her bir adımın ayrıntılarını motive etmek, yorumlamak ve genişletmek için çok daha fazla zaman harcayacağız, ancak genel hatlarıyla, bu sohbet robotlarından biri belirli bir kelime ürettiğinde, kaputun altında neler olup bittiğini burada bulabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "İlk olarak, girdi bir grup küçük parçaya ayrılır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Bu parçalara belirteç adı verilir ve metin söz konusu olduğunda bunlar genellikle kelimeler veya kelimelerin küçük parçaları ya da diğer yaygın karakter kombinasyonlarıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Görüntüler veya sesler söz konusuysa, belirteçler bu görüntünün küçük parçaları veya bu sesin küçük parçaları olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Bu belirteçlerin her biri daha sonra bir vektörle ilişkilendirilir, yani o parçanın anlamını bir şekilde kodlamak için bazı sayılar listesi anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Bu vektörleri çok yüksek boyutlu bir uzayda koordinatlar veriyor gibi düşünürseniz, benzer anlamlara sahip kelimeler bu uzayda birbirine yakın vektörlere düşme eğilimindedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Bu vektör dizisi daha sonra dikkat bloğu olarak bilinen bir işlemden geçer ve bu, vektörlerin birbirleriyle konuşmasına ve değerlerini güncellemek için ileri geri bilgi aktarmasına olanak tanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Örneğin, model kelimesinin bir makine öğrenimi modeli ifadesindeki anlamı, bir moda modeli ifadesindeki anlamından farklıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Dikkat bloğu, bağlamdaki hangi kelimelerin hangi diğer kelimelerin anlamlarını güncellemekle ilgili olduğunu ve bu anlamların tam olarak nasıl güncellenmesi gerektiğini bulmaktan sorumlu olan şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "Ve yine, ne zaman anlam kelimesini kullansam, bu bir şekilde tamamen bu vektörlerin girdilerinde kodlanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Bundan sonra, bu vektörler farklı bir işlemden geçer ve okuduğunuz kaynağa bağlı olarak bu, çok katmanlı bir algılayıcı veya belki de ileri beslemeli bir katman olarak adlandırılacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "Ve burada vektörler birbirleriyle konuşmazlar, hepsi paralel olarak aynı işlemden geçerler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Bu bloğu yorumlamak biraz daha zor olsa da, daha sonra bu adımın nasıl her bir vektör hakkında uzun bir soru listesi sormak ve ardından bu soruların yanıtlarına göre onları güncellemek gibi bir şey olduğundan bahsedeceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Bu iki bloktaki tüm işlemler dev bir matris çarpımı yığınına benziyor ve bizim öncelikli işimiz altta yatan matrislerin nasıl okunacağını anlamak olacak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Arada gerçekleşen bazı normalleştirme adımlarıyla ilgili bazı ayrıntıları atlıyorum, ancak bu sonuçta üst düzey bir önizleme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Bundan sonra, süreç esasen tekrarlanır, dikkat blokları ve çok katmanlı algılayıcı blokları arasında ileri geri gidersiniz, ta ki en sonunda pasajın tüm temel anlamının bir şekilde dizideki en son vektörde pişirilmiş olması umulur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Daha sonra bu son vektör üzerinde belirli bir işlem gerçekleştirerek olası tüm belirteçler, bir sonraki adımda gelebilecek tüm olası küçük metin parçaları üzerinde bir olasılık dağılımı üretiriz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "Ve dediğim gibi, bir metin parçacığı verildiğinde bir sonraki adımın ne olacağını tahmin eden bir araca sahip olduğunuzda, onu biraz tohum metinle besleyebilir ve bir sonraki adımın ne olacağını tahmin etme, dağıtımdan örnekleme, ekleme ve sonra tekrar tekrar tekrarlama oyununu tekrar tekrar oynatabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Bazılarınız ChatGPT'nin sahneye çıkmasından ne kadar önce GPT-3'ün ilk demolarının böyle göründüğünü hatırlayabilir, başlangıçtaki bir parçacığa dayalı olarak hikayeleri ve denemeleri otomatik olarak tamamlamasını sağlardınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Bunun gibi bir aracı bir sohbet robotuna dönüştürmek için en kolay başlangıç noktası, yardımcı bir yapay zeka asistanıyla etkileşime giren bir kullanıcının ortamını belirleyen, sistem istemi olarak adlandırabileceğiniz küçük bir metne sahip olmak ve ardından kullanıcının ilk sorusunu veya istemini diyaloğun ilk parçası olarak kullanmak ve ardından böyle yardımcı bir yapay zeka asistanının yanıt olarak ne söyleyeceğini tahmin etmeye başlamaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Bunun iyi işlemesi için gerekli olan bir eğitim adımı hakkında söylenecek daha çok şey var, ancak yüksek düzeyde fikir budur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "Bu bölümde, siz ve ben ağın en başında ve en sonunda neler olduğuna dair ayrıntıları genişleteceğiz ve ayrıca transformatörler ortaya çıktığında herhangi bir makine öğrenimi mühendisi için ikinci doğa olacak bazı önemli arka plan bilgilerini gözden geçirmek için çok zaman harcamak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Bu arka plan bilgisine sahipseniz ve biraz sabırsızsanız, genellikle transformatörün kalbi olarak kabul edilen dikkat bloklarına odaklanacak olan bir sonraki bölüme geçmekte özgürsünüz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Bundan sonra, bu çok katmanlı algılayıcı bloklar, eğitimin nasıl çalıştığı ve bu noktaya kadar atlanmış olan bir dizi diğer ayrıntı hakkında daha fazla konuşmak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Daha geniş bir bağlam için, bu videolar derin öğrenmeyle ilgili bir mini dizinin ekleridir ve öncekileri izlemediyseniz sorun değil, bence bunu sırasız yapabilirsiniz, ancak özellikle transformatörlere dalmadan önce, derin öğrenmenin temel öncülü ve yapısı hakkında aynı sayfada olduğumuzdan emin olmaya değer olduğunu düşünüyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Bariz olanı belirtme riskini göze alarak, bu, bir modelin nasıl davrandığını bir şekilde belirlemek için verileri kullandığınız herhangi bir modeli tanımlayan makine öğrenimine yönelik bir yaklaşımdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Bununla kastettiğim şey, diyelim ki bir görüntüyü alan ve onu tanımlayan bir etiket üreten bir işlev istiyorsunuz ya da bir metin parçasına verilen bir sonraki kelimeyi tahmin etme örneğimiz ya da bazı sezgi ve örüntü tanıma unsurları gerektiren başka herhangi bir görev.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Bugünlerde bunu neredeyse doğal karşılıyoruz, ancak makine öğrenimi ile ilgili fikir, bu görevin kodda nasıl yapılacağına dair bir prosedürü açıkça tanımlamaya çalışmak yerine, ki bu insanların yapay zekanın ilk günlerinde yaptıkları şeydi, bunun yerine bir grup düğme ve kadran gibi ayarlanabilir parametrelerle çok esnek bir yapı kurarsınız ve daha sonra bir şekilde bu davranışı taklit etmek için bu parametrelerin değerlerini değiştirmek ve ayarlamak için belirli bir girdi için çıktının nasıl görünmesi gerektiğine dair birçok örnek kullanırsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Örneğin, makine öğreniminin belki de en basit biçimi doğrusal regresyondur; burada girdileriniz ve çıktılarınızın her biri tek bir sayıdır, bir evin metrekaresi ve fiyatı gibi bir şeydir ve istediğiniz şey, gelecekteki ev fiyatlarını tahmin etmek için bu veriler üzerinden en iyi uyum çizgisini bulmaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Bu çizgi, eğim ve y-kesişimi gibi iki sürekli parametre ile tanımlanır ve doğrusal regresyonun amacı, bu parametreleri veriyle yakından eşleşecek şekilde belirlemektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Söylemeye gerek yok, derin öğrenme modelleri çok daha karmaşık hale geliyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "Örneğin GPT-3'ün iki değil 175 milyar parametresi vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Ancak şu da var ki, eğitim verilerine büyük ölçüde aşırı uyum sağlamadan ya da eğitilmesi tamamen zor olmadan çok sayıda parametreye sahip dev bir model oluşturabileceğiniz kesin değildir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "Derin öğrenme, son birkaç on yılda oldukça iyi ölçeklendirildiği kanıtlanmış bir model sınıfını tanımlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Bunları birleştiren şey, geriye yayılma adı verilen aynı eğitim algoritmasıdır ve içeri girerken sahip olmanızı istediğim bağlam, bu eğitim algoritmasının ölçekte iyi çalışması için bu modellerin belirli bir formatı takip etmesi gerektiğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Eğer bu formatı biliyorsanız, bir transformatörün dili nasıl işlediğine dair, aksi takdirde keyfi hissettirme riski taşıyan birçok seçeneği açıklamaya yardımcı olur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "İlk olarak, yaptığınız model ne olursa olsun, girdinin bir gerçek sayı dizisi olarak biçimlendirilmesi gerekir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Bu, bir sayı listesi anlamına gelebilir, iki boyutlu bir dizi olabilir veya genel terimin tensör olduğu daha yüksek boyutlu dizilerle çok sık uğraşırsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Genellikle bu girdi verilerinin aşamalı olarak birçok farklı katmana dönüştürüldüğünü düşünürsünüz; burada her katman, çıktıyı düşündüğünüz son katmana ulaşana kadar her zaman bir tür gerçek sayı dizisi olarak yapılandırılır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Örneğin, metin işleme modelimizdeki son katman, olası tüm sonraki belirteçler için olasılık dağılımını temsil eden bir sayı listesidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "Derin öğrenmede, bu model parametreleri neredeyse her zaman ağırlık olarak adlandırılır ve bunun nedeni, bu modellerin temel bir özelliğinin, bu parametrelerin işlenen verilerle etkileşime girmesinin tek yolunun ağırlıklı toplamlar olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Ayrıca bazı doğrusal olmayan fonksiyonları da serpiştirirsiniz, ancak bunlar parametrelere bağlı olmayacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Ancak tipik olarak, ağırlıklı toplamları çıplak olarak görmek ve bu şekilde açıkça yazmak yerine, bunları bir matris vektör çarpımında çeşitli bileşenler olarak bir araya getirilmiş olarak görürsünüz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Matris vektör çarpımının nasıl çalıştığını düşünürseniz, çıktıdaki her bileşenin ağırlıklı bir toplam gibi göründüğü aynı şeyi söylemek anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "İşlenen verilerden alınan vektörleri dönüştüren ayarlanabilir parametrelerle dolu matrisler hakkında düşünmek sizin ve benim için genellikle kavramsal olarak daha temizdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Örneğin, GPT-3'teki 175 milyar ağırlık 28.000'den biraz az farklı matris halinde düzenlenmiştir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Bu matrisler sırayla sekiz farklı kategoriye ayrılır ve sizin ve benim yapacağımız şey, bu türün ne yaptığını anlamak için bu kategorilerin her birini adım adım incelemektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "İlerledikçe, 175 milyarın tam olarak nereden geldiğini saymak için GPT-3'teki belirli sayılara başvurmanın eğlenceli olduğunu düşünüyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Günümüzde daha büyük ve daha iyi modeller olsa bile, bu model ML toplulukları dışında dünyanın dikkatini çeken geniş dilli model olarak belirli bir çekiciliğe sahiptir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Ayrıca, pratik olarak konuşmak gerekirse, şirketler daha modern ağlar için belirli sayıları çok daha sıkı tutma eğilimindedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Sadece ChatGPT gibi bir aracın içinde neler olduğunu görmek için kaputun altına baktığınızda, gerçek hesaplamanın neredeyse tamamının matris vektör çarpımı gibi göründüğünü belirtmek istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Milyarlarca sayı denizinde kaybolma riski biraz var, ancak her zaman mavi veya kırmızıyla renklendireceğim modelin ağırlıkları ile her zaman griyle renklendireceğim işlenmekte olan veriler arasında zihninizde çok keskin bir ayrım yapmalısınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Ağırlıklar gerçek beyinlerdir, eğitim sırasında öğrenilen şeylerdir ve nasıl davranacağını belirlerler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "İşlenen veriler, örnek bir metin parçacığı gibi, belirli bir çalıştırma için modele beslenen belirli girdileri kodlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Tüm bunları temel alarak, girdiyi küçük parçalara ayırmak ve bu parçaları vektörlere dönüştürmek olan bu metin işleme örneğinin ilk adımına geçelim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Bu parçalara nasıl jeton denildiğinden bahsetmiştim, bunlar kelime parçaları veya noktalama işaretleri olabilir, ancak bu bölümde ve özellikle bir sonrakinde ara sıra, kelimelere daha temiz bir şekilde bölünmüş gibi davranmak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Biz insanlar kelimelerle düşündüğümüz için, bu sadece küçük örneklere başvurmayı ve her adımı netleştirmeyi çok daha kolay hale getirecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Modelin önceden tanımlanmış bir kelime dağarcığı vardır, tüm olası kelimelerin bir listesi, diyelim ki 50.000 tanesi, ve karşılaşacağımız ilk matris, gömme matrisi olarak bilinir, bu kelimelerin her biri için tek bir sütuna sahiptir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Bu sütunlar, her bir kelimenin ilk adımda hangi vektöre dönüşeceğini belirler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Bunu We olarak etiketliyoruz ve gördüğümüz tüm matrisler gibi, değerleri rastgele başlıyor, ancak verilere dayalı olarak öğrenilecekler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Kelimeleri vektörlere dönüştürmek, transformatörlerden çok önce makine öğreniminde yaygın bir uygulamaydı, ancak daha önce hiç görmediyseniz biraz gariptir ve bundan sonraki her şeyin temelini oluşturur, bu yüzden bir dakikanızı ayırıp buna aşina olalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Bu gömme işlemini genellikle bir kelime olarak adlandırırız, bu da sizi bu vektörleri geometrik olarak yüksek boyutlu bir uzaydaki noktalar olarak düşünmeye davet eder.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Üç sayıdan oluşan bir listeyi 3B uzaydaki noktaların koordinatları olarak görselleştirmek sorun olmayacaktır, ancak sözcük katıştırmaları çok daha yüksek boyutlu olma eğilimindedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "GPT-3'te 12.288 boyut vardır ve göreceğiniz gibi, çok sayıda farklı yönü olan bir alanda çalışmak önemlidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Tıpkı 3 boyutlu bir uzayda iki boyutlu bir kesit alıp tüm noktaları bu kesit üzerine yansıtabileceğiniz gibi, basit bir modelin bana verdiği kelime yerleştirmelerini canlandırmak için, bu çok yüksek boyutlu uzayda üç boyutlu bir kesit seçerek ve kelime vektörlerini bunun üzerine yansıtarak ve sonuçları görüntüleyerek benzer bir şey yapacağım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "Buradaki ana fikir, bir modelin eğitim sırasında kelimelerin vektörler olarak tam olarak nasıl gömüleceğini belirlemek için ağırlıklarını değiştirip ayarladıkça, uzaydaki yönlerin bir tür anlamsal anlama sahip olduğu bir dizi gömme üzerinde karar kılma eğiliminde olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Burada çalıştırdığım basit kelime-vektör modeli için, gömülmeleri kuleninkine en yakın olan tüm kelimeler için bir arama yaparsam, hepsinin nasıl çok benzer kule-imsi hisler verdiğini fark edeceksiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Eğer Python'u açıp evde oynamak isterseniz, animasyonları yapmak için kullandığım özel model bu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Bu bir transformatör değil, ancak uzaydaki yönlerin semantik anlam taşıyabileceği fikrini göstermek için yeterli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Bunun çok klasik bir örneği, kadın ve erkek vektörleri arasındaki farkı ele alırsanız, birinin ucunu diğerinin ucuna bağlayan küçük bir vektör olarak görselleştireceğiniz bir şey, kral ve kraliçe arasındaki farka çok benzer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Diyelim ki bir kadın hükümdar için kullanılan kelimeyi bilmiyordunuz, king'i alıp bu kadın-erkek yönünü ekleyerek ve bu noktaya en yakın yerleştirmeleri arayarak bulabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "En azından öyle sayılır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Bu, üzerinde çalıştığım model için klasik bir örnek olmasına rağmen, kraliçenin gerçek gömülmesi aslında bunun gösterdiğinden biraz daha uzaktır, çünkü muhtemelen kraliçenin eğitim verilerinde kullanılma şekli sadece kralın dişil bir versiyonu değildir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Biraz oynadığımda, aile ilişkileri bu fikri çok daha iyi açıklıyor gibiydi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Mesele şu ki, eğitim sırasında model, bu uzaydaki bir yönün cinsiyet bilgisini kodlayacağı şekilde katıştırmaları seçmeyi avantajlı bulmuş gibi görünüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Bir başka örnek de, İtalya'nın gömülmesini alıp Almanya'nın gömülmesini çıkarırsanız ve bunu Hitler'in gömülmesine eklerseniz, Mussolini'nin gömülmesine çok yakın bir şey elde edersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Sanki model bazı yönleri İtalyanlıkla, bazılarını ise İkinci Dünya Savaşı eksen liderleriyle ilişkilendirmeyi öğrenmiş gibi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Belki de bu konudaki en sevdiğim örnek, bazı modellerde Almanya ve Japonya arasındaki farkı alıp suşiye eklediğinizde sucuğa çok yakın bir sonuç elde etmenizdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Ayrıca bu en yakın komşuları bulma oyununu oynarken, Kat'in hem canavara hem de canavara ne kadar yakın olduğunu görmekten memnun oldum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Özellikle bir sonraki bölüm için akılda tutulması yararlı olan bir matematiksel sezgi, iki vektörün nokta çarpımının ne kadar iyi hizalandıklarını ölçmenin bir yolu olarak nasıl düşünülebileceğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Hesaplama açısından nokta çarpımlar, karşılık gelen tüm bileşenlerin çarpılmasını ve ardından sonuçların toplanmasını içerir; bu da iyi bir şeydir, çünkü hesaplamalarımızın çoğu ağırlıklı toplamlar gibi görünmelidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geometrik olarak, vektörler benzer yönleri gösterdiğinde nokta çarpımı pozitiftir, dik olduklarında sıfırdır ve zıt yönleri gösterdiklerinde negatiftir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Örneğin, bu modelle oynadığınızı ve kedi eksi kedi gömülmesinin bu uzayda bir tür çoğulluk yönünü temsil edebileceğini varsaydığınızı varsayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Bunu test etmek için, bu vektörü alacağım ve belirli tekil isimlerin gömülerine karşı nokta çarpımını hesaplayacağım ve bunu karşılık gelen çoğul isimlerle nokta çarpımlarıyla karşılaştıracağım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Bununla oynarsanız, çoğul olanların gerçekten de tekil olanlardan sürekli olarak daha yüksek değerler verdiğini fark edeceksiniz, bu da bu yöne daha fazla uyum sağladıklarını gösterir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Ayrıca, bu nokta çarpımını 1, 2, 3 ve benzeri kelimelerin gömülmeleriyle alırsanız, artan değerler vermeleri de eğlencelidir, bu nedenle modelin belirli bir kelimeyi ne kadar çoğul bulduğunu nicel olarak ölçebiliriz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Yine, kelimelerin nasıl gömüldüğüne ilişkin ayrıntılar veriler kullanılarak öğrenilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Sütunları bize her kelimeye ne olduğunu söyleyen bu gömme matrisi, modelimizdeki ilk ağırlık yığınıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "GPT-3 sayılarını kullanarak, kelime haznesi boyutu özellikle 50.257'dir ve yine teknik olarak bu, kendi başına kelimelerden değil, belirteçlerden oluşur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "Gömme boyutu 12.288'dir ve bunları çarpmak bize bunun yaklaşık 617 milyon ağırlıktan oluştuğunu söyler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Devam edelim ve bunu bir çeteleye ekleyelim, sonunda 175 milyara kadar saymamız gerektiğini hatırlayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "Dönüştürücüler söz konusu olduğunda, bu gömme uzayındaki vektörlerin yalnızca tek tek kelimeleri temsil etmediğini düşünmek istersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Birincisi, daha sonra bahsedeceğimiz gibi, o kelimenin konumu hakkında da bilgi kodlarlar, ancak daha da önemlisi, onları bağlam içinde ıslatma kapasitesine sahip olarak düşünmelisiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Örneğin, hayatına kral kelimesinin gömülmesi olarak başlayan bir vektör, bu ağdaki çeşitli bloklar tarafından aşamalı olarak çekilip çekilebilir, böylece sonunda bir şekilde İskoçya'da yaşayan ve bir önceki kralı öldürdükten sonra makamına ulaşan ve Shakespeare dilinde tanımlanan bir kral olduğunu kodlayan çok daha spesifik ve incelikli bir yöne işaret eder.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Belirli bir kelime hakkında kendi anlayışınızı düşünün.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Bu kelimenin anlamı açıkça çevreden öğrenilir ve bazen bu çok uzak mesafeden gelen bağlamı da içerir, bu nedenle bir sonraki kelimenin ne olduğunu tahmin etme yeteneğine sahip bir modeli bir araya getirirken amaç, bir şekilde bağlamı verimli bir şekilde dahil etmesi için onu güçlendirmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Açık olmak gerekirse, bu ilk adımda, girdi metnine dayalı vektör dizisini oluşturduğunuzda, bunların her biri basitçe gömme matrisinden koparılır, bu nedenle başlangıçta her biri çevresinden herhangi bir girdi olmadan yalnızca tek bir kelimenin anlamını kodlayabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Ancak bu ağın birincil amacının, bu vektörlerin her birinin, tek tek kelimelerin temsil edebileceğinden çok daha zengin ve spesifik bir anlamı emmesini sağlamak olduğunu düşünmelisiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Ağ, bir seferde yalnızca bağlam boyutu olarak bilinen sabit sayıda vektörü işleyebilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "GPT-3 için 2048 bağlam boyutu ile eğitilmiştir, bu nedenle ağdan akan veriler her zaman her biri 12.000 boyuta sahip 2048 sütundan oluşan bu dizi gibi görünür.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Bu bağlam boyutu, dönüştürücünün bir sonraki kelimenin tahminini yaparken ne kadar metni dahil edebileceğini sınırlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Bu nedenle ChatGPT'nin ilk sürümleri gibi bazı sohbet botlarıyla yapılan uzun sohbetler, siz çok uzun süre devam ettikçe botun sohbetin akışını kaybettiği hissini veriyordu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Zamanı geldiğinde dikkatin ayrıntılarına gireceğiz, ancak ileriye atlayarak en sonunda ne olduğu hakkında bir dakika konuşmak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Unutmayın, istenen çıktı, daha sonra gelebilecek tüm belirteçler üzerinde bir olasılık dağılımıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Örneğin, en son kelime Profesör ise ve bağlam Harry Potter gibi kelimeleri içeriyorsa ve hemen öncesinde en az sevilen öğretmeni görüyorsak ve ayrıca belirteçlerin sadece tam kelimeler gibi göründüğünü varsaymama izin vererek bana biraz serbestlik tanırsanız, Harry Potter hakkında bilgi edinmiş iyi eğitilmiş bir ağ muhtemelen Snape kelimesine yüksek bir sayı atayacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Bu iki farklı adımı içerir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Birincisi, bu bağlamdaki en son vektörü, kelime dağarcığındaki her bir belirteç için bir tane olmak üzere 50.000 değerden oluşan bir listeye eşleyen başka bir matris kullanmaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Daha sonra bunu bir olasılık dağılımına normalleştiren bir fonksiyon var, buna Softmax deniyor ve birazdan bundan daha fazla bahsedeceğiz, ancak bundan önce bir tahmin yapmak için sadece bu son katıştırmayı kullanmak biraz garip görünebilir, sonuçta bu son adımda katmanda kendi bağlam açısından zengin anlamlarıyla orada duran binlerce başka vektör var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Bunun nedeni, eğitim sürecinde, son katmandaki vektörlerin her birini aynı anda hemen ardından gelecek olan için bir tahmin yapmak üzere kullanırsanız çok daha verimli olacağı gerçeğiyle ilgilidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Eğitim hakkında daha sonra söylenecek çok şey var, ancak şu anda sadece bunu belirtmek istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Bu matris Unembedding matrisi olarak adlandırılır ve ona WU etiketi verilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Yine, gördüğümüz tüm ağırlık matrisleri gibi, girişleri rastgele başlar, ancak eğitim sürecinde öğrenilirler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Toplam parametre sayımızı koruyarak, bu Gömme matrisi kelime dağarcığındaki her kelime için bir satıra sahiptir ve her satır gömme boyutuyla aynı sayıda elemana sahiptir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Gömme matrisine çok benzer, sadece sırası değiştirilmiştir, bu yüzden ağa 617 milyon parametre daha ekler, yani şu ana kadarki sayımız bir milyardan biraz fazla, toplamda elde edeceğimiz 175 milyarın küçük ama tamamen önemsiz olmayan bir kısmı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "Bu bölümün son mini dersi olarak bu softmax fonksiyonu hakkında daha fazla konuşmak istiyorum, çünkü dikkat bloklarına daldığımızda karşımıza bir kez daha çıkacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Buradaki fikir, bir sayı dizisinin bir olasılık dağılımı olarak hareket etmesini istiyorsanız, örneğin tüm olası sonraki kelimeler üzerinde bir dağılım, o zaman her değerin 0 ile 1 arasında olması ve ayrıca hepsinin toplamının 1 olması gerekir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Ancak, yaptığınız her şeyin matris-vektör çarpımına benzediği öğrenme oyununu oynuyorsanız, varsayılan olarak elde ettiğiniz çıktılar buna hiç uymaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Değerler genellikle negatiftir veya 1'den çok daha büyüktür ve neredeyse kesinlikle 1'e eşit değildir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax, rastgele bir sayı listesini, en büyük değerler 1'e en yakın olacak ve daha küçük değerler 0'a çok yakın olacak şekilde geçerli bir dağılıma dönüştürmenin standart yoludur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Gerçekten bilmeniz gereken tek şey bu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Ancak merak ediyorsanız, bunun çalışma şekli önce e'yi sayıların her birinin gücüne yükseltmektir, bu da artık pozitif değerlerin bir listesine sahip olduğunuz anlamına gelir ve daha sonra tüm bu pozitif değerlerin toplamını alabilir ve her terimi bu toplama bölebilirsiniz, bu da onu 1'e kadar olan bir listeye normalleştirir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Girdideki sayılardan biri diğerlerinden anlamlı bir şekilde büyükse, çıktıda ilgili terimin dağılıma hakim olduğunu fark edeceksiniz, bu nedenle örnekleme yapıyorsanız neredeyse kesinlikle sadece maksimize eden girdiyi seçeceksiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Ancak, diğer değerler de benzer şekilde büyük olduğunda, dağılımda anlamlı bir ağırlık kazanmaları ve girdileri sürekli olarak değiştirdikçe her şeyin sürekli olarak değişmesi anlamında sadece maksimum değeri seçmekten daha yumuşaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "ChatGPT'nin bu dağılımı bir sonraki kelimeyi oluşturmak için kullanması gibi bazı durumlarda, bu fonksiyona biraz daha fazla baharat ekleyerek, bu üslerin paydasına atılan bir t sabiti ile biraz daha fazla eğlence için yer vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Buna sıcaklık diyoruz, çünkü belli termodinamik denklemlerinde sıcaklığın rolüne belli belirsiz benziyor ve etkisi şu ki, t daha büyük olduğunda, daha düşük değerlere daha fazla ağırlık verirsiniz, bu da dağılımın biraz daha düzgün olduğu anlamına gelir ve t daha küçükse, daha büyük değerler daha agresif bir şekilde baskın olacaktır, burada t'yi sıfıra eşitlemek, tüm ağırlığın maksimum değere gitmesi anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Örneğin, GPT-3'ün bir zamanlar A varmış şeklindeki tohum metniyle bir hikaye oluşturmasını sağlayacağım, ancak her durumda farklı sıcaklıklar kullanacağım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Sıfır sıcaklık, her zaman en öngörülebilir kelimeyi seçtiği anlamına gelir ve elde ettiğiniz şey Goldilocks'un basmakalıp bir türevi olur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Daha yüksek bir sıcaklık, daha az olası kelimeleri seçme şansı verir, ancak bir riskle birlikte gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "Bu durumda, hikaye Güney Kore'den genç bir web sanatçısı hakkında daha orijinal bir şekilde başlıyor, ancak hızla saçmalığa dönüşüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Teknik olarak konuşmak gerekirse, API aslında 2'den büyük bir sıcaklık seçmenize izin vermez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Bunun matematiksel bir nedeni yok, sadece araçlarının çok saçma şeyler ürettiğinin görülmesini engellemek için konulmuş keyfi bir kısıtlama.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Merak ediyorsanız, bu animasyonun çalışma şekli aslında GPT-3'ün ürettiği en olası 20 sonraki jetonu alıyorum, ki bu bana verecekleri maksimum değer gibi görünüyor ve sonra olasılıkları 15'lik bir üsse göre değiştiriyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Bir başka jargon olarak, bu fonksiyonun çıktısının bileşenlerini olasılık olarak adlandırabileceğiniz gibi, insanlar genellikle girdileri logit olarak adlandırır veya bazıları logit, bazıları logit, ben logit diyeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Örneğin, bir metni beslediğinizde, tüm bu kelime katıştırmaları ağ üzerinden akar ve bu son çarpımı katıştırılmamış matrisle yaparsınız, makine öğrenimi çalışanları bu ham, normalleştirilmemiş çıktıdaki bileşenlere bir sonraki kelime tahmini için logitler olarak atıfta bulunurlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Bu bölümdeki amacın çoğu, dikkat mekanizmasını anlamak için temelleri atmaktı, Karate Kid wax-on-wax-off tarzı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Gördüğünüz gibi, kelime katıştırma, softmax, nokta çarpımlarının benzerliği nasıl ölçtüğü ve ayrıca hesaplamaların çoğunun ayarlanabilir parametrelerle dolu matrislerle matris çarpımı gibi görünmesi gerektiği konusunda güçlü bir sezginiz varsa, yapay zekadaki tüm modern patlamanın bu temel taşı olan dikkat mekanizmasını anlamak nispeten kolay olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Bunun için bir sonraki bölümde bana katılın.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Bunu yayınladığım sırada, bir sonraki bölümün taslağı Patreon destekçileri tarafından incelenebilir durumda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Son versiyon bir ya da iki hafta içinde kamuoyuna açıklanacak, genellikle bu incelemeye dayanarak ne kadar değişiklik yapacağıma bağlı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Bu arada, dikkat çekmek ve kanala biraz yardımcı olmak isterseniz, orada bekliyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]