1
00:00:00,000 --> 00:00:01,761
GPT 是 Generative 

2
00:00:01,761 --> 00:00:04,560
Pretrained Transformer 的缩写。

3
00:00:05,220 --> 00:00:09,020
所以，第一个词很简单，这些都是生成新文本的机器人。

4
00:00:09,800 --> 00:00:13,664
预训练指的是模型从海量数据中学习的过程，

5
00:00:13,664 --> 00:00:18,687
而前缀则暗示着通过额外的训练，模型在特定任务上还有更

6
00:00:18,687 --> 00:00:20,040
多的微调空间。

7
00:00:20,720 --> 00:00:22,900
但最后一句话，才是真正的关键。

8
00:00:23,380 --> 00:00:27,840
变压器是一种特殊的神经网络，是一种机器学习模型，

9
00:00:27,840 --> 00:00:31,000
是当前人工智能蓬勃发展的核心发明。

10
00:00:31,740 --> 00:00:35,430
我想通过这段视频和接下来的章节，

11
00:00:35,430 --> 00:00:39,120
直观地讲解变压器内部的实际情况。

12
00:00:39,700 --> 00:00:42,820
我们将跟随数据流，一步一步地进行操作。

13
00:00:43,440 --> 00:00:47,380
您可以使用变压器制作多种不同的模型。

14
00:00:47,800 --> 00:00:50,800
有些模式可以接收音频并生成文字记录。

15
00:00:51,340 --> 00:00:56,220
这个句子来自一个反向模型，该模型只根据文本生成合成语音。

16
00:00:56,660 --> 00:00:58,794
所有那些在 2022 年风靡全球的工具，

17
00:00:58,794 --> 00:01:01,677
如 "多莉"（Dolly）和 "Midjourney"

18
00:01:01,677 --> 00:01:03,918
（Midjourney），都是基于变压器，

19
00:01:03,918 --> 00:01:05,519
它们能接收文字描述并生成图像。

20
00:01:06,000 --> 00:01:09,645
即使我无法让它理解馅饼生物应该是什么，

21
00:01:09,645 --> 00:01:13,100
但我仍然对这种事情的可能性感到震惊。

22
00:01:13,900 --> 00:01:17,053
而谷歌在 2017 年推出的原始转换器，

23
00:01:17,053 --> 00:01:21,153
就是针对将文本从一种语言翻译成另一种语言的特定使用情

24
00:01:21,153 --> 00:01:22,100
况而发明的。

25
00:01:22,660 --> 00:01:27,623
但是，你我将重点关注的变体，也就是支持 ChatGPT 

26
00:01:27,623 --> 00:01:32,764
等工具的类型，将是一个经过训练的模型，它可以接收一段文字，

27
00:01:32,764 --> 00:01:37,905
甚至可能还有周围的图像或声音，并对该段落接下来的内容做出预

28
00:01:37,905 --> 00:01:38,260
测。

29
00:01:38,600 --> 00:01:41,199
这种预测以概率分布的形式出现在随

30
00:01:41,199 --> 00:01:43,800
后可能出现的许多不同的文本块中。

31
00:01:45,040 --> 00:01:47,410
乍一看，您可能会认为预测下一个

32
00:01:47,410 --> 00:01:49,940
单词与生成新文本的目标截然不同。

33
00:01:50,180 --> 00:01:53,227
但是，一旦你有了这样一个预测模型，

34
00:01:53,227 --> 00:01:57,529
生成一段较长文本的简单方法就是给它一个初始片段，

35
00:01:57,529 --> 00:02:01,114
让它从刚刚生成的分布中随机抽取一个样本，

36
00:02:01,114 --> 00:02:05,058
将该样本添加到文本中，然后再次运行整个过程，

37
00:02:05,058 --> 00:02:09,539
根据所有新文本（包括刚刚添加的内容）做出新的预测。

38
00:02:10,100 --> 00:02:13,000
我不知道你是怎么想的，但我真的不觉得这应该真的有用。

39
00:02:13,420 --> 00:02:17,759
例如，在这个动画中，我在笔记本电脑上运行 GPT-2，

40
00:02:17,759 --> 00:02:22,420
让它反复预测和采样下一个文本块，根据种子文本生成一个故事。

41
00:02:22,420 --> 00:02:26,120
这个故事其实并没有什么意义。

42
00:02:26,500 --> 00:02:29,786
但是，如果我把它换成对 GPT-3 的 API 

43
00:02:29,786 --> 00:02:33,347
调用（GPT-3 是相同的基本模型，只是要大得多），

44
00:02:33,347 --> 00:02:36,360
突然间，我们几乎神奇地得到了一个合理的故事，

45
00:02:36,360 --> 00:02:38,688
这个故事甚至似乎可以推断出，Pi 

46
00:02:38,688 --> 00:02:40,880
生物会生活在数学和计算的国度里。

47
00:02:41,580 --> 00:02:46,348
当你与 ChatGPT 或其他大型语言模型交互时，

48
00:02:46,348 --> 00:02:51,880
你会看到它们一次生成一个单词，这就是重复预测和采样的过程。

49
00:02:52,480 --> 00:02:55,475
事实上，我非常喜欢的一个功能是，

50
00:02:55,475 --> 00:02:59,220
可以查看它选择的每个新词的基本分布情况。

51
00:03:03,820 --> 00:03:08,180
让我们先从高层预览一下数据是如何流经转换器的。

52
00:03:08,640 --> 00:03:12,934
我们会花更多时间对每个步骤的细节进行激励、解释和扩展，

53
00:03:12,934 --> 00:03:16,910
但概括地说，当这些聊天机器人生成一个给定的单词时，

54
00:03:16,910 --> 00:03:18,660
引擎盖下面会发生什么。

55
00:03:19,080 --> 00:03:22,040
首先，输入被分解成许多小块。

56
00:03:22,620 --> 00:03:25,429
这些片段被称为标记，就文本而言，

57
00:03:25,429 --> 00:03:29,820
它们往往是单词、单词的小片段或其他常见的字符组合。

58
00:03:30,740 --> 00:03:37,080
如果涉及图像或声音，那么代币可以是图像的小块或声音的小块。

59
00:03:37,580 --> 00:03:41,396
然后，这些标记中的每一个都与一个向量（即一些数字列表

60
00:03:41,396 --> 00:03:45,360
）相关联，该向量旨在以某种方式对该作品的含义进行编码。

61
00:03:45,880 --> 00:03:49,730
如果把这些向量看作是某个高维空间中的坐标，

62
00:03:49,730 --> 00:03:54,680
那么意思相近的词往往会出现在该空间中相互接近的向量上。

63
00:03:55,280 --> 00:03:59,806
然后，这一连串的向量会经过一个被称为注意力区块的操作，

64
00:03:59,806 --> 00:04:04,500
这使得向量之间可以相互对话，来回传递信息，更新它们的值。

65
00:04:04,880 --> 00:04:07,888
例如，"模型 "一词在 "机器学习模型 

66
00:04:07,888 --> 00:04:11,800
"短语中的含义不同于在 "时尚模型 "短语中的含义。

67
00:04:12,260 --> 00:04:18,884
注意力模块负责找出上下文中哪些词与更新哪些词的含义相关，

68
00:04:18,884 --> 00:04:21,959
以及应该如何更新这些含义。

69
00:04:22,500 --> 00:04:25,205
再说一遍，每当我使用 "意义 "这个词时，

70
00:04:25,205 --> 00:04:28,040
它都会以某种方式完全编码在这些向量的条目中。

71
00:04:29,180 --> 00:04:32,788
之后，这些向量会经过不同的运算，

72
00:04:32,788 --> 00:04:38,200
根据你阅读的资料，这将被称为多层感知器或前馈层。

73
00:04:38,580 --> 00:04:42,660
在这里，矢量之间并不对话，它们都并行地进行相同的运算。

74
00:04:43,060 --> 00:04:47,090
虽然这个区块比较难解释，但稍后我们会讲到，

75
00:04:47,090 --> 00:04:50,929
这个步骤有点像对每个矢量提出一长串问题，

76
00:04:50,929 --> 00:04:54,000
然后根据这些问题的答案更新它们。

77
00:04:54,900 --> 00:05:00,820
这两个区块中的所有操作看起来都像是一大堆矩阵乘法，

78
00:05:00,820 --> 00:05:05,320
我们的主要工作是了解如何读取底层矩阵。

79
00:05:06,980 --> 00:05:12,980
我略去了中间一些规范化步骤的细节，但这毕竟是一个高级预览。

80
00:05:13,680 --> 00:05:18,491
在此之后，这个过程会不断重复，你会在注意力区块和多

81
00:05:18,491 --> 00:05:21,956
层感知器区块之间来回切换，直到最后，

82
00:05:21,956 --> 00:05:26,767
希望这段话的所有基本含义都能以某种方式融入到序列中

83
00:05:26,767 --> 00:05:28,500
的最后一个向量中。

84
00:05:28,920 --> 00:05:32,438
然后，我们对最后一个向量进行一定的运算，

85
00:05:32,438 --> 00:05:37,188
得出所有可能的标记的概率分布，即接下来可能出现的所有可

86
00:05:37,188 --> 00:05:38,420
能的小块文本。

87
00:05:38,980 --> 00:05:41,735
就像我说过的，一旦你有了一个工具，

88
00:05:41,735 --> 00:05:46,597
它能根据一段文字预测下一步的内容，你就可以给它一点种子文字，

89
00:05:46,597 --> 00:05:51,297
让它反复玩这个游戏，预测下一步的内容，从分布中采样，添加，

90
00:05:51,297 --> 00:05:53,080
然后一遍又一遍地重复。

91
00:05:53,640 --> 00:05:58,197
一些了解情况的人可能还记得，在 ChatGPT 出现之前，

92
00:05:58,197 --> 00:06:01,182
GPT-3 的早期演示版就是这个样子，

93
00:06:01,182 --> 00:06:04,640
你可以让它根据最初的片段自动完成故事和文章。

94
00:06:05,580 --> 00:06:10,867
要把这样的工具做成聊天机器人，最简单的起点就是用一

95
00:06:10,867 --> 00:06:15,096
段文字设定用户与人工智能助手互动的场景，

96
00:06:15,096 --> 00:06:20,383
也就是所谓的系统提示，然后用用户的初始问题或提示作

97
00:06:20,383 --> 00:06:25,671
为第一段对话，然后让它开始预测人工智能助手会说些什

98
00:06:25,671 --> 00:06:26,940
么作为回应。

99
00:06:27,720 --> 00:06:30,930
要做好这一步，还需要更多的培训，

100
00:06:30,930 --> 00:06:33,940
但总体而言，这就是我们的想法。

101
00:06:35,720 --> 00:06:41,412
在本章中，你和我将详细介绍在网络的最开始和最后会发生什么，

102
00:06:41,412 --> 00:06:45,337
我还想花大量时间回顾一些重要的背景知识，

103
00:06:45,337 --> 00:06:48,870
这些知识对于任何机器学习工程师来说，

104
00:06:48,870 --> 00:06:52,600
在变形金刚问世之前都是第二天性的东西。

105
00:06:53,060 --> 00:06:56,878
如果您对这些背景知识比较熟悉，又有点不耐烦，

106
00:06:56,878 --> 00:07:00,523
可以随意跳到下一章，这一章的重点是注意块，

107
00:07:00,523 --> 00:07:02,780
一般认为它是变压器的核心。

108
00:07:03,360 --> 00:07:08,493
在此之后，我想进一步谈谈这些多层感知器模块、训练工作原理，

109
00:07:08,493 --> 00:07:11,680
以及其他一些在此之前已经略过的细节。

110
00:07:12,180 --> 00:07:16,893
从更广泛的背景来看，这些视频是关于深度学习的迷你系列的补充，

111
00:07:16,893 --> 00:07:21,606
如果你没有看过前面的视频也没关系，我认为你可以不按顺序观看，

112
00:07:21,606 --> 00:07:25,691
但在具体深入研究变形金刚之前，我认为值得确保我们对深

113
00:07:25,691 --> 00:07:28,520
度学习的基本前提和结构有一致的认识。

114
00:07:29,020 --> 00:07:33,466
冒着说得太明显的风险，这是机器学习的一种方法，

115
00:07:33,466 --> 00:07:38,300
它描述了使用数据以某种方式确定模型行为的任何模型。

116
00:07:39,140 --> 00:07:43,631
我的意思是，比方说，你需要一个接收图像并生成描述该图像

117
00:07:43,631 --> 00:07:48,621
的标签的函数，或者我们的例子，即根据一段文字预测下一个单词，

118
00:07:48,621 --> 00:07:52,780
或者任何其他似乎需要一些直觉和模式识别元素的任务。

119
00:07:53,200 --> 00:07:58,034
如今，我们几乎认为这是理所当然的，但机器学习的理念是，

120
00:07:58,034 --> 00:08:03,227
不要试图用代码明确定义如何完成任务的程序（这是人工智能发展

121
00:08:03,227 --> 00:08:07,882
初期人们会做的事情），而是要建立一个非常灵活的结构，

122
00:08:07,882 --> 00:08:11,821
其中包含可调整的参数，就像一堆旋钮和刻度盘，

123
00:08:11,821 --> 00:08:17,014
然后使用许多给定输入时输出应该是什么样子的示例来调整和调整

124
00:08:17,014 --> 00:08:19,700
这些参数的值，以模仿这种行为。

125
00:08:19,700 --> 00:08:24,303
例如，最简单的机器学习形式可能是线性回归，

126
00:08:24,303 --> 00:08:29,346
输入和输出都是单个数字，比如房子的面积和价格，

127
00:08:29,346 --> 00:08:35,046
而你想要的是通过这些数据找到一条最佳拟合线，你知道，

128
00:08:35,046 --> 00:08:36,799
预测未来的房价。

129
00:08:37,440 --> 00:08:42,690
该直线由两个连续参数（如斜率和 y-截距）描述，

130
00:08:42,690 --> 00:08:48,160
线性回归的目标是确定这些参数，使其与数据紧密匹配。

131
00:08:48,880 --> 00:08:52,100
不用说，深度学习模型会变得更加复杂。

132
00:08:52,620 --> 00:08:57,660
例如，GPT-3 的参数不是两个，而是 1 750 亿个。

133
00:08:58,120 --> 00:09:04,423
但问题是，并不是说你创建了一个拥有大量参数的巨型模型，

134
00:09:04,423 --> 00:09:09,560
它就不会严重过度拟合训练数据或完全难以训练。

135
00:09:10,260 --> 00:09:13,447
深度学习描述了一类模型，在过去的几十年中，

136
00:09:13,447 --> 00:09:16,180
这类模型已被证明具有出色的扩展能力。

137
00:09:16,480 --> 00:09:19,473
它们的共同点是采用了相同的训练算法，

138
00:09:19,473 --> 00:09:23,297
即反向传播（backpropagation），

139
00:09:23,297 --> 00:09:28,120
而我想让大家了解的是，为了让这种训练算法在大规模应用中运行

140
00:09:28,120 --> 00:09:31,280
良好，这些模型必须遵循某种特定的格式。

141
00:09:31,800 --> 00:09:38,035
如果你知道这种格式，就能解释转换器如何处理语言的许多选择，

142
00:09:38,035 --> 00:09:40,400
否则就有可能感觉随意。

143
00:09:41,440 --> 00:09:46,740
首先，无论您要制作什么模型，输入都必须格式化为实数数组。

144
00:09:46,740 --> 00:09:51,086
这可能是指一个数字列表，也可能是一个二维数组，

145
00:09:51,086 --> 00:09:56,000
或者经常是更高维度的数组，这里使用的一般术语是张量。

146
00:09:56,560 --> 00:10:01,875
你通常会认为，输入数据会被逐步转换成许多不同的层，

147
00:10:01,875 --> 00:10:06,341
而每一层的结构都是实数数组，直到最后一层，

148
00:10:06,341 --> 00:10:08,680
也就是你认为的输出层。

149
00:10:09,280 --> 00:10:13,725
例如，我们文本处理模型的最后一层是一个数字列表，

150
00:10:13,725 --> 00:10:17,060
代表所有可能的下一个标记的概率分布。

151
00:10:17,820 --> 00:10:22,029
在深度学习中，这些模型参数几乎总是被称为权重，

152
00:10:22,029 --> 00:10:25,141
这是因为这些模型的一个主要特点是，

153
00:10:25,141 --> 00:10:29,900
这些参数与所处理数据的唯一交互方式就是通过加权求和。

154
00:10:30,340 --> 00:10:32,933
你还可以在整个过程中使用一些非线性函数，

155
00:10:32,933 --> 00:10:34,360
但它们并不依赖于参数。

156
00:10:35,200 --> 00:10:40,826
不过，通常情况下，你不会看到像这样赤裸裸地写出加权和，

157
00:10:40,826 --> 00:10:45,620
而是会发现它们被打包成矩阵向量积中的各种成分。

158
00:10:46,740 --> 00:10:49,648
如果你回想一下矩阵向量乘法的工作原理，

159
00:10:49,648 --> 00:10:54,240
输出中的每个分量看起来都像是加权和，这就等于在说同样的事情。

160
00:10:54,780 --> 00:10:58,579
对于你我来说，矩阵中充满了可调整的参数，

161
00:10:58,579 --> 00:11:02,569
这些参数可以转换从被处理数据中提取的矢量，

162
00:11:02,569 --> 00:11:05,420
这样的矩阵往往在概念上更简洁。

163
00:11:06,340 --> 00:11:09,613
例如，GPT-3 中的 1 750 

164
00:11:09,613 --> 00:11:14,160
亿个权重被组织成不到 28 000 个不同的矩阵。

165
00:11:14,660 --> 00:11:18,680
这些矩阵又分为八个不同的类别，我们

166
00:11:18,680 --> 00:11:22,700
要做的就是逐一了解这些类别的作用。

167
00:11:23,160 --> 00:11:27,185
在我们讨论的过程中，我觉得参考《GPT-3》中的具体数

168
00:11:27,185 --> 00:11:31,360
字来计算这 1,750 亿的具体来源是一件很有趣的事情。

169
00:11:31,880 --> 00:11:35,969
即使如今有了更大更好的模型，但作为真正在 ML 

170
00:11:35,969 --> 00:11:40,740
社区之外吸引世界目光的大型语言模型，它仍具有一定的魅力。

171
00:11:41,440 --> 00:11:44,255
此外，实际上，对于更现代化的网络，

172
00:11:44,255 --> 00:11:46,740
公司往往会对具体数字守口如瓶。

173
00:11:47,360 --> 00:11:53,004
我只想说明一点，当你窥视 ChatGPT 这样的工具时，

174
00:11:53,004 --> 00:11:57,440
几乎所有的实际计算看起来都像是矩阵向量乘法。

175
00:11:57,900 --> 00:12:02,292
在数以亿计的数字海洋中迷失方向是有一点风险的，

176
00:12:02,292 --> 00:12:06,875
但你应该在头脑中将模型的权重（我总是用蓝色或红色

177
00:12:06,875 --> 00:12:11,840
表示）和正在处理的数据（我总是用灰色表示）区分开来。

178
00:12:12,180 --> 00:12:17,920
权重是实际的大脑，是在训练中学到的东西，决定了它的行为方式。

179
00:12:18,280 --> 00:12:24,294
被处理的数据只是对特定运行中输入模型的任何特定输入进行编码，

180
00:12:24,294 --> 00:12:26,500
比如一个文本示例片段。

181
00:12:27,480 --> 00:12:32,371
有了所有这些作为基础，让我们进入这个文本处理示例的第一步，

182
00:12:32,371 --> 00:12:36,420
即把输入内容分割成小块，并把这些小块转化为矢量。

183
00:12:37,020 --> 00:12:41,784
我提到过这些语块被称为标记，可能是单词或标点符号的片段，

184
00:12:41,784 --> 00:12:45,868
但在这一章，尤其是下一章，我想时不时地假装一下，

185
00:12:45,868 --> 00:12:48,080
把它们更清晰地分解成单词。

186
00:12:48,600 --> 00:12:51,273
因为我们人类是用语言来思考的，这只会让我

187
00:12:51,273 --> 00:12:54,080
们更容易参考一些小例子，并澄清每一个步骤。

188
00:12:55,260 --> 00:13:00,054
该模型有一个预定义的词汇表，包含所有可能出现的单词，

189
00:13:00,054 --> 00:13:05,587
比如说 50000 个，我们将遇到的第一个矩阵称为嵌入矩阵，

190
00:13:05,587 --> 00:13:07,800
其中每一个单词都有一列。

191
00:13:08,940 --> 00:13:13,760
这些列决定了每个单词在第一步中变成什么向量。

192
00:13:15,100 --> 00:13:19,165
我们给它贴上 We 的标签，就像我们看到的所有矩阵一样，

193
00:13:19,165 --> 00:13:22,360
它的值一开始是随机的，但会根据数据进行学习。

194
00:13:23,620 --> 00:13:27,619
早在变形金刚出现之前，将单词转化为向量就已经是机器学习中

195
00:13:27,619 --> 00:13:31,332
的常见做法了，但如果你从未见过它，就会觉得有点奇怪，

196
00:13:31,332 --> 00:13:35,331
而且它为后面的一切奠定了基础，所以让我们花点时间来熟悉一

197
00:13:35,331 --> 00:13:35,760
下它。

198
00:13:36,040 --> 00:13:38,796
我们通常把这种嵌入称为 "词"，

199
00:13:38,796 --> 00:13:43,620
这就需要把这些向量非常几何化地看成是某个高维空间中的点。

200
00:13:44,180 --> 00:13:48,999
将三个数字的列表可视化为三维空间中点的坐标不成问题，

201
00:13:48,999 --> 00:13:51,780
但单词嵌入的维度往往要高得多。

202
00:13:52,280 --> 00:13:55,989
在 GPT-3 中，它们有 12 288 个维度，

203
00:13:55,989 --> 00:14:00,440
正如你所看到的，在一个有很多不同方向的空间中工作是很重要的。

204
00:14:01,180 --> 00:14:05,913
就像你可以在三维空间中选择一个二维切片并将所有点投射

205
00:14:05,913 --> 00:14:11,194
到该切片上一样，为了让简单模型给出的单词嵌入动画更加生动，

206
00:14:11,194 --> 00:14:16,656
我将做一个类似的事情，在这个非常高的空间中选择一个三维切片，

207
00:14:16,656 --> 00:14:20,480
并将单词向量投射到该切片上，然后显示结果。

208
00:14:21,280 --> 00:14:25,666
这里的大意是，当一个模型在训练过程中调整权重

209
00:14:25,666 --> 00:14:31,050
以确定如何将单词嵌入向量时，它往往会确定一组嵌入向量，

210
00:14:31,050 --> 00:14:34,440
其中空间中的方向具有某种语义意义。

211
00:14:34,980 --> 00:14:38,620
对于我在这里运行的简单的词到向量模型，

212
00:14:38,620 --> 00:14:41,876
如果我搜索所有嵌入式最接近塔的词，

213
00:14:41,876 --> 00:14:45,900
你会发现它们似乎都给人非常相似的塔的感觉。

214
00:14:46,340 --> 00:14:49,140
如果你想在家里调出 Python 演示，

215
00:14:49,140 --> 00:14:51,380
这就是我用来制作动画的特定模型。

216
00:14:51,620 --> 00:14:57,600
这不是一个变压器，但足以说明空间中的方向可能具有语义意义。

217
00:14:58,300 --> 00:15:03,195
一个非常典型的例子是，如果你把女人和男人的矢量

218
00:15:03,195 --> 00:15:08,091
之间的差异（你可以把它想象成一个连接一个尖端和

219
00:15:08,091 --> 00:15:13,200
另一个尖端的小矢量）看成是国王和王后之间的差异。

220
00:15:15,080 --> 00:15:19,632
因此，假设您不知道女性君主的单词，您可以通过国王，

221
00:15:19,632 --> 00:15:24,731
加上这个女人-男人的方向，然后搜索最接近这个点的嵌入词来

222
00:15:24,731 --> 00:15:25,460
找到它。

223
00:15:27,000 --> 00:15:28,200
至少，有点像。

224
00:15:28,480 --> 00:15:32,154
尽管这对我正在使用的模型来说是一个典型的例子，

225
00:15:32,154 --> 00:15:35,668
但女王的真实嵌入实际上比这一结果要偏离一些，

226
00:15:35,668 --> 00:15:39,661
这大概是因为女王在训练数据中的使用方式不仅仅是国王

227
00:15:39,661 --> 00:15:40,780
的女性化版本。

228
00:15:41,620 --> 00:15:45,260
当我玩了一圈之后，家庭关系似乎更能说明问题。

229
00:15:46,340 --> 00:15:50,962
问题是，在训练过程中，模型似乎发现选择嵌入式是有利的，

230
00:15:50,962 --> 00:15:54,900
因为在这个空间中，有一个方向可以编码性别信息。

231
00:15:56,800 --> 00:16:02,235
另一个例子是，如果把意大利的内嵌值减去德国的内嵌值，

232
00:16:02,235 --> 00:16:08,090
再加上希特勒的内嵌值，就会得到非常接近墨索里尼的内嵌值。

233
00:16:08,570 --> 00:16:12,281
就好像模型学会了将某些方向与意大利性联系起来，

234
00:16:12,281 --> 00:16:15,670
而将另一些方向与二战轴心国领导人联系起来。

235
00:16:16,470 --> 00:16:20,627
在这方面，我最喜欢的例子可能是，在某些模型中，

236
00:16:20,627 --> 00:16:24,061
如果把德国和日本之间的差异加到寿司上，

237
00:16:24,061 --> 00:16:26,230
结果就会非常接近于香肠。

238
00:16:27,350 --> 00:16:30,350
此外，在玩这个寻找最近邻居的游戏时，

239
00:16:30,350 --> 00:16:33,850
我很高兴地看到吉与野兽和怪物的距离都很近。

240
00:16:34,690 --> 00:16:38,354
有一点数学直觉很有帮助，尤其是在下一章，

241
00:16:38,354 --> 00:16:43,850
那就是两个向量的点积可以被看作是衡量它们对齐程度的一种方法。

242
00:16:44,870 --> 00:16:50,063
在计算上，点乘需要将所有相应的分量相乘，然后将结果相加，

243
00:16:50,063 --> 00:16:54,330
这很好，因为我们的很多计算看起来都像是加权和。

244
00:16:55,190 --> 00:16:59,049
从几何学角度看，当矢量指向相似的方向时，

245
00:16:59,049 --> 00:17:04,645
点积为正；当矢量垂直时，点积为零；当矢量指向相反的方向时，

246
00:17:04,645 --> 00:17:05,609
点积为负。

247
00:17:06,550 --> 00:17:11,652
例如，假设你正在玩这个模型，你假设猫减去

248
00:17:11,652 --> 00:17:17,010
猫的嵌入可能代表了这个空间的一种多元方向。

249
00:17:17,430 --> 00:17:22,145
为了验证这一点，我将利用这个向量计算它与某些单数名

250
00:17:22,145 --> 00:17:27,050
词嵌入的点积，并将其与相应的复数名词的点积进行比较。

251
00:17:27,270 --> 00:17:31,670
如果你仔细研究一下，就会发现复数的数值似乎确

252
00:17:31,670 --> 00:17:36,070
实一直比单数的高，这说明它们更符合这个方向。

253
00:17:37,070 --> 00:17:41,230
同样有趣的是，如果将这个点积与单词 1、2、3 

254
00:17:41,230 --> 00:17:44,350
等的嵌入值相乘，它们的值会不断增加，

255
00:17:44,350 --> 00:17:49,030
因此我们仿佛可以定量地衡量模型发现某个单词的复数程度。

256
00:17:50,250 --> 00:17:53,570
同样，如何嵌入单词的具体方法也是通过数据得知的。

257
00:17:54,050 --> 00:17:57,216
这个嵌入矩阵的列告诉我们每个词的情况，

258
00:17:57,216 --> 00:17:59,550
它是我们模型中的第一堆权重。

259
00:18:00,030 --> 00:18:04,732
使用 GPT-3 数字，词汇量具体为 50 257 个，

260
00:18:04,732 --> 00:18:09,770
同样，从技术上讲，这不是由单词本身组成的，而是由标记组成的。

261
00:18:10,630 --> 00:18:14,122
嵌入维度为 12 288，乘以这些维度，

262
00:18:14,122 --> 00:18:17,790
我们可以得出这包括约 6.17 亿个权值。

263
00:18:18,250 --> 00:18:20,950
让我们继续把这个数字加到流水账上，

264
00:18:20,950 --> 00:18:23,810
记住最后我们应该数到 1750 亿。

265
00:18:25,430 --> 00:18:28,678
就转换器而言，你确实需要把嵌入空

266
00:18:28,678 --> 00:18:32,130
间中的向量看作不仅仅代表单个单词。

267
00:18:32,550 --> 00:18:38,124
首先，它们还编码有关该单词位置的信息，这一点我们稍后会讲到，

268
00:18:38,124 --> 00:18:42,770
但更重要的是，你应该认为它们具有吸收上下文的能力。

269
00:18:43,350 --> 00:18:48,423
例如，一个向量一开始嵌入的是 "king"（国王）一词，

270
00:18:48,423 --> 00:18:51,865
可能会逐渐被这个网络中的各种区块拉扯，

271
00:18:51,865 --> 00:18:54,945
最后指向一个更具体、更细微的方向，

272
00:18:54,945 --> 00:18:58,750
以某种方式编码出这是一个住在苏格兰的国王，

273
00:18:58,750 --> 00:19:01,649
他在谋杀了前任国王后登上了王位，

274
00:19:01,649 --> 00:19:04,730
并用莎士比亚的语言对他进行了描述。

275
00:19:05,210 --> 00:19:07,790
想想你自己对某个单词的理解。

276
00:19:08,250 --> 00:19:11,656
这个词的含义显然是由周围环境决定的，

277
00:19:11,656 --> 00:19:15,063
有时还包括来自很远地方的语境，因此，

278
00:19:15,063 --> 00:19:19,037
在建立一个能够预测下一个词是什么的模型时，

279
00:19:19,037 --> 00:19:23,390
我们的目标是以某种方式使其能够有效地结合语境。

280
00:19:24,050 --> 00:19:27,940
说白了，在第一步中，当你根据输入文本创建向量数组时，

281
00:19:27,940 --> 00:19:31,083
每一个向量都是从嵌入矩阵中简单提取出来的，

282
00:19:31,083 --> 00:19:34,226
因此最初每一个向量只能编码一个单词的含义，

283
00:19:34,226 --> 00:19:36,770
而不会从其周围环境中输入任何信息。

284
00:19:37,710 --> 00:19:43,340
但你应该认为，这个网络的主要目标是让每个矢

285
00:19:43,340 --> 00:19:48,970
量都能吸收比单个词语更丰富、更具体的意义。

286
00:19:49,510 --> 00:19:54,170
网络一次只能处理固定数量的向量，即上下文大小。

287
00:19:54,510 --> 00:19:58,169
对于 GPT-3，它的上下文大小为 2048，

288
00:19:58,169 --> 00:20:02,782
因此流经网络的数据看起来总是像由 2048 列组成的数组，

289
00:20:02,782 --> 00:20:05,010
每列有 12000 个维度。

290
00:20:05,590 --> 00:20:11,830
这种上下文大小限制了转换器在预测下一个单词时能包含多少文本。

291
00:20:12,370 --> 00:20:15,067
这就是为什么与某些聊天机器人（如 

292
00:20:15,067 --> 00:20:18,717
ChatGPT 的早期版本）进行长时间对话时，

293
00:20:18,717 --> 00:20:22,050
机器人常常会感觉对话时间过长而失去了主线。

294
00:20:23,030 --> 00:20:26,430
我们将在适当的时候详细讨论注意力的问题，

295
00:20:26,430 --> 00:20:28,810
但我想先谈谈最后发生的事情。

296
00:20:29,450 --> 00:20:34,870
请记住，所需的输出是所有接下来可能出现的标记的概率分布。

297
00:20:35,170 --> 00:20:37,575
例如，如果最后一个词是 "教授"，

298
00:20:37,575 --> 00:20:40,547
而上下文中包括 "哈利-波特 "这样的词，

299
00:20:40,547 --> 00:20:43,377
紧接着我们又看到了 "最不喜欢的老师"，

300
00:20:43,377 --> 00:20:47,481
而且如果你给我一些回旋余地，让我假装标记看起来只是完整的单

301
00:20:47,481 --> 00:20:50,877
词，那么一个训练有素的网络，如果已经积累了关于 

302
00:20:50,877 --> 00:20:53,848
"哈利-波特 "的知识，就会给 "斯内普 

303
00:20:53,848 --> 00:20:55,830
"这个词分配一个较高的数字。

304
00:20:56,510 --> 00:20:57,970
这涉及两个不同的步骤。

305
00:20:58,310 --> 00:21:01,361
第一个方法是使用另一个矩阵，将该上下文中的

306
00:21:01,361 --> 00:21:05,430
最后一个向量映射到一个包含 50,000 个值的列表中，

307
00:21:05,430 --> 00:21:07,610
词汇表中的每个标记都有一个值。

308
00:21:08,170 --> 00:21:16,527
但在此之前，只使用最后一个嵌入来进行预测似乎有点奇怪，

309
00:21:16,527 --> 00:21:23,646
毕竟在最后一步中，层中还有成千上万的其他向量，

310
00:21:23,646 --> 00:21:28,290
它们都有自己丰富的上下文含义。

311
00:21:28,930 --> 00:21:34,479
这是因为在训练过程中，如果使用最后一层中的每个

312
00:21:34,479 --> 00:21:40,270
向量同时对紧随其后的内容进行预测，就会事半功倍。

313
00:21:40,970 --> 00:21:45,090
关于训练，以后还有很多话要说，但我现在只想说说这个。

314
00:21:45,730 --> 00:21:49,690
这个矩阵被称为解嵌入矩阵，我们给它取名为 WU。

315
00:21:50,210 --> 00:21:52,652
同样，与我们看到的所有权重矩阵一样，

316
00:21:52,652 --> 00:21:55,910
它的条目一开始是随机的，但在训练过程中会被学习。

317
00:21:56,470 --> 00:22:00,962
根据我们的参数总数，这个解嵌入矩阵为词汇表中的

318
00:22:00,962 --> 00:22:05,650
每个单词设置了一行，每行的元素数与嵌入维数相同。

319
00:22:06,410 --> 00:22:09,552
它与嵌入矩阵非常相似，只是交换了阶次，

320
00:22:09,552 --> 00:22:12,859
因此又为网络增加了 6.17 亿个参数，

321
00:22:12,859 --> 00:22:16,497
这意味着我们目前的计算结果略高于 10 亿，

322
00:22:16,497 --> 00:22:20,301
虽然在最终的 1 750 亿个参数中占比很小，

323
00:22:20,301 --> 00:22:21,790
但也并非微不足道。

324
00:22:22,550 --> 00:22:25,409
作为本章的最后一堂小课，我想更多地谈谈这个 

325
00:22:25,409 --> 00:22:28,920
softmax 函数，因为一旦我们深入研究注意力模块，

326
00:22:28,920 --> 00:22:30,610
它就会再次出现在我们面前。

327
00:22:31,430 --> 00:22:35,576
其原理是，如果你想让一个数字序列作为概率分布，

328
00:22:35,576 --> 00:22:38,460
比如所有可能的下一个单词的分布，

329
00:22:38,460 --> 00:22:42,066
那么每个值都必须介于 0 和 1 之间，

330
00:22:42,066 --> 00:22:44,590
而且所有值加起来必须是 1。

331
00:22:45,250 --> 00:22:50,030
不过，如果你玩的是学习游戏，你所做的一切看起来都像是矩阵与

332
00:22:50,030 --> 00:22:54,810
向量相乘，那么默认情况下得到的输出结果就完全不符合这一点。

333
00:22:55,330 --> 00:22:57,924
这些数值通常是负数，或者比 1 大得多，

334
00:22:57,924 --> 00:22:59,870
而且几乎肯定加起来不等于 1。

335
00:23:00,510 --> 00:23:06,889
Softmax 是将任意数字列表转化为有效分布的标准方法，

336
00:23:06,889 --> 00:23:11,290
其最大值最接近 1，而较小值最接近 0。

337
00:23:11,830 --> 00:23:13,070
这就是你真正需要知道的。

338
00:23:13,090 --> 00:23:16,730
但如果你好奇的话，它的工作原理是首先将 e 

339
00:23:16,730 --> 00:23:21,031
提升到每个数字的幂，这意味着你现在有了一个正值列表，

340
00:23:21,031 --> 00:23:25,664
然后你可以求出所有这些正值的总和，再将每个项除以该总和，

341
00:23:25,664 --> 00:23:29,470
这样就将其归一化为一个加起来等于 1 的列表。

342
00:23:30,170 --> 00:23:36,320
你会注意到，如果输入中的某个数字比其他数字大，

343
00:23:36,320 --> 00:23:42,470
那么在输出中，相应的项就会在分布中占主导地位。

344
00:23:42,990 --> 00:23:46,076
但是，这比仅仅选取最大值要柔和得多，

345
00:23:46,076 --> 00:23:51,049
因为当其他值同样很大时，它们也会在分布中获得有意义的权重，

346
00:23:51,049 --> 00:23:54,650
而且随着输入的不断变化，一切都会不断变化。

347
00:23:55,130 --> 00:23:58,322
在某些情况下，比如当 ChatGPT 

348
00:23:58,322 --> 00:24:01,011
使用这种分布来创建下一个单词时，

349
00:24:01,011 --> 00:24:04,204
可以在这个函数中加入一些额外的调味料，

350
00:24:04,204 --> 00:24:08,910
在这些指数的分母中加入常数 t，从而带来一些额外的乐趣。

351
00:24:09,550 --> 00:24:15,719
我们称其为温度，因为它与某些热力学方程中温度的作用有些相似。

352
00:24:15,719 --> 00:24:20,244
其效果是，当 t 越大时，低值的权重就越大，

353
00:24:20,244 --> 00:24:24,563
这意味着分布会更均匀一些；如果 t 越小，

354
00:24:24,563 --> 00:24:28,471
那么大值的权重就会越大，在极端情况下，

355
00:24:28,471 --> 00:24:32,790
将 t 设为零意味着所有权重都归于最大值。

356
00:24:33,470 --> 00:24:37,165
例如，我会让 GPT-3 生成一个种子文本为 

357
00:24:37,165 --> 00:24:40,218
"很久很久以前，有一个 A "的故事，

358
00:24:40,218 --> 00:24:42,950
但我会在每种情况下使用不同的温度。

359
00:24:43,630 --> 00:24:48,000
温度为零意味着它总是用最容易预测的词，

360
00:24:48,000 --> 00:24:52,370
而你最终得到的是金发女郎的老套衍生词。

361
00:24:53,010 --> 00:24:57,910
温度越高，它就越有机会选择可能性较小的词语，但这也有风险。

362
00:24:58,230 --> 00:25:00,773
在这部影片中，故事的开头比较新颖，

363
00:25:00,773 --> 00:25:03,915
讲述了一位来自韩国的年轻网络艺术家的故事，

364
00:25:03,915 --> 00:25:06,010
但很快就陷入了无厘头的境地。

365
00:25:06,950 --> 00:25:10,830
从技术上讲，应用程序接口实际上不允许您选择大于 2 的温度。

366
00:25:11,170 --> 00:25:15,260
这并没有什么数学上的原因，只是为了防止他们

367
00:25:15,260 --> 00:25:19,350
的工具产生太无厘头的东西而任意施加的限制。

368
00:25:19,870 --> 00:25:23,324
因此，如果你好奇的话，这个动画的实际工作方式是，

369
00:25:23,324 --> 00:25:25,628
我从 GPT-3 生成的 20 

370
00:25:25,628 --> 00:25:29,802
个最有可能的下一个代币（这似乎是他们给我的最大值）中选取 

371
00:25:29,802 --> 00:25:32,970
20 个，然后根据 1/5 的指数调整概率。

372
00:25:33,130 --> 00:25:37,470
还有一点行话，就像你可能会把这个函数输出的组

373
00:25:37,470 --> 00:25:41,810
成部分称为概率一样，人们通常把输入称为对数，

374
00:25:41,810 --> 00:25:46,150
或者有些人说对数，有些人说对数，我会说对数。

375
00:25:46,530 --> 00:25:52,102
因此，举例来说，当你输入一些文本，让所有这些词嵌入流经网络，

376
00:25:52,102 --> 00:25:55,260
并与未嵌入矩阵做最后的乘法运算时，

377
00:25:55,260 --> 00:26:00,089
机器学习人员会将原始、未规范化输出中的成分称为下一个

378
00:26:00,089 --> 00:26:01,390
词预测的对数。

379
00:26:03,330 --> 00:26:07,025
本章的主要目的是为理解注意力机制奠定基础，

380
00:26:07,025 --> 00:26:10,370
即 "空手道小子 "式的 "蜡上蜡"。

381
00:26:10,850 --> 00:26:16,040
你看，如果你对词嵌入、softmax、点积如何衡量相

382
00:26:16,040 --> 00:26:21,829
似性有很强的直觉，并且知道大部分计算看起来都像是矩阵乘法，

383
00:26:21,829 --> 00:26:27,019
其中的矩阵充满了可调整的参数，那么理解注意力机制--

384
00:26:27,019 --> 00:26:32,210
整个现代人工智能蓬勃发展的基石--就会相对容易一些。

385
00:26:32,650 --> 00:26:34,510
为此，请与我一起进入下一章。

386
00:26:36,390 --> 00:26:39,356
在我发表这篇文章的时候，下一章的草稿已经可以供 

387
00:26:39,356 --> 00:26:41,210
Patreon 支持者审阅了。

388
00:26:41,770 --> 00:26:44,489
最终版本应该会在一两周内公开，这通

389
00:26:44,489 --> 00:26:47,370
常取决于我在审查基础上做了多少改动。

390
00:26:47,810 --> 00:26:51,192
与此同时，如果你想深入关注，如果你想为频道出点力，

391
00:26:51,192 --> 00:26:52,410
它就在那里等着你。

