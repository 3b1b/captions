1
00:00:00,160 --> 00:00:02,064
GPT 是 Generative 

2
00:00:02,064 --> 00:00:05,200
Pre-trained Transformer 的缩写。

3
00:00:05,200 --> 00:00:07,140
首个单词较为直接，

4
00:00:07,140 --> 00:00:09,290
它们是用来生成新文本的机器人。

5
00:00:09,290 --> 00:00:11,810
"Pre-trained" 指的是

6
00:00:11,810 --> 00:00:14,480
模型经历了从大量数据中学习的过程， 

7
00:00:14,480 --> 00:00:17,151
这个词暗示了该模型还有进一步在特

8
00:00:17,151 --> 00:00:19,990
定任务中进行额外训练和微调的可能。

9
00:00:19,990 --> 00:00:23,030
然而，最后一个词，才是真正重要的部分。

10
00:00:23,030 --> 00:00:26,625
Transformer 是一种特定类型的神经网络，

11
00:00:26,625 --> 00:00:27,920
一个机器学习模型，

12
00:00:27,920 --> 00:00:31,740
它是现今 AI 高速发展的核心创新。

13
00:00:31,740 --> 00:00:34,500
我希望通过这个视频和接下来的章节，

14
00:00:34,500 --> 00:00:36,520
以一种便于理解的方式，

15
00:00:36,520 --> 00:00:39,150
阐述 Transformer 内部实际发生的过程。

16
00:00:39,150 --> 00:00:42,640
我们将逐步探索流经它的数据。

17
00:00:42,640 --> 00:00:47,750
你可以使用 Transformer 构建许多不同类型的模型。

18
00:00:47,750 --> 00:00:50,650
有些模型接受音频输入并生成文字。

19
00:00:50,650 --> 00:00:53,760
这句话来自一个反向工作的模型，

20
00:00:54,000 --> 00:00:56,120
只需要文本输入就能生成人工语音。

21
00:00:56,120 --> 00:00:59,360
所有那些在 2022 年风靡全球的工具，

22
00:00:59,360 --> 00:01:02,427
如 DALL-E 和 MidJourney，

23
00:01:02,427 --> 00:01:04,239
能够将文本描述转化为图像，

24
00:01:04,239 --> 00:01:05,519
都是基于 Transformer 的。

25
00:01:05,519 --> 00:01:06,830
即使我无法让它完全理解

26
00:01:06,830 --> 00:01:09,520
"π 生物"到底是什么，

27
00:01:09,520 --> 00:01:13,040
我仍对这样的事情有可能发生感到惊讶。

28
00:01:13,040 --> 00:01:15,190
最初的 Transformer 是 

29
00:01:15,190 --> 00:01:17,460
Google 在 2017 年推出的，

30
00:01:17,460 --> 00:01:22,440
主要用于将一种语言的文本翻译成另一种语言。

31
00:01:22,440 --> 00:01:24,880
但我们将关注的版本，

32
00:01:24,880 --> 00:01:28,030
也就是像 ChatGPT 这样的工具所依赖的类型，

33
00:01:28,030 --> 00:01:35,070
会是一个接受一段文本（可能伴随一些图像或声音）的模型，

34
00:01:35,070 --> 00:01:38,050
然后预测文章接下来的内容。

35
00:01:38,050 --> 00:01:41,130
这种预测呈现为概率分布形式

36
00:01:41,130 --> 00:01:43,770
涵盖了很多可能接下来出现的文字片段。

37
00:01:43,770 --> 00:01:45,520
乍一看，

38
00:01:45,520 --> 00:01:47,410
你可能觉得预测下一个词

39
00:01:47,410 --> 00:01:50,400
似乎与生成新的文字有着天壤之别。

40
00:01:50,400 --> 00:01:52,600
但当你有了像这样的预测模型后，

41
00:01:52,600 --> 00:01:55,940
你可以试着让它生成一段更长的文字，

42
00:01:55,940 --> 00:01:58,750
方法就是给它一个初始的片段，

43
00:01:58,750 --> 00:02:02,290
然后随机从刚生成的概率分布中选取一个样本，

44
00:02:02,290 --> 00:02:03,690
将这个样本追加到文字中，

45
00:02:03,690 --> 00:02:05,440
接着再进行一轮预测，

46
00:02:05,440 --> 00:02:08,190
这次的预测需要基于所有新生成的文字，

47
00:02:08,190 --> 00:02:09,919
包括刚刚添加的那部分。

48
00:02:09,919 --> 00:02:10,780
我不知道你怎么看，

49
00:02:10,780 --> 00:02:13,210
但我真的觉得这个方法的效果可能并不理想。

50
00:02:13,210 --> 00:02:14,740
举个例子，在这个动画中，

51
00:02:14,740 --> 00:02:16,470
我在我的笔记本电脑上运行 GPT-2，

52
00:02:16,470 --> 00:02:19,960
并让它不断地预测与取样下一个文字块，

53
00:02:19,960 --> 00:02:22,790
尝试基于一段起始文本生成一个故事。

54
00:02:22,790 --> 00:02:26,710
结果呢，这个故事基本上没什么逻辑可言。  

55
00:02:26,710 --> 00:02:30,090
但是，如果我换成 GPT-3 的 API 调用，

56
00:02:30,090 --> 00:02:34,930
这是同样的基本模型，只是规模更大，突然间就像变魔法一样，

57
00:02:34,930 --> 00:02:36,490
我们不仅得到了一个合乎逻辑的故事，

58
00:02:36,490 --> 00:02:38,810
这个故事甚至能暗示出一个 π 生物

59
00:02:38,810 --> 00:02:41,550
可能居住在一个充满数学和计算的世界里。

60
00:02:41,550 --> 00:02:45,390
这个过程，就是通过重复的预测和选取来生成文本，

61
00:02:45,390 --> 00:02:50,056
正是你在使用ChatGPT或其他大型语言模型时所经历的，

62
00:02:50,056 --> 00:02:51,890
模型会逐字地生成文本。

63
00:02:51,890 --> 00:02:52,490
其实，

64
00:02:52,490 --> 00:02:54,700
我特别希望能有一种功能，即

65
00:02:54,700 --> 00:02:59,180
能看到它在选择每个新词时的底层概率分布。

66
00:03:03,921 --> 00:03:06,320
我们先从宏观角度看看

67
00:03:06,320 --> 00:03:08,480
数据是如何在 Transformer 模型中流转的。 

68
00:03:08,480 --> 00:03:13,480
接下来，我们会详细探讨、解释每一个步骤，并对其进行扩展。 

69
00:03:13,480 --> 00:03:16,730
但是大体来说，当聊天机器人生成某个特定词汇时，

70
00:03:16,730 --> 00:03:18,560
下面就是它底层的运行机制。

71
00:03:18,560 --> 00:03:22,520
首先，输入内容会被拆分成许多小片段。

72
00:03:22,520 --> 00:03:23,960
这些小片段被称为词元 (Tokens)。

73
00:03:23,960 --> 00:03:27,600
对于文本来说，这些 Token 通常是单词、单词的一小部分，

74
00:03:27,600 --> 00:03:29,840
或者其他常见的字符组合。

75
00:03:29,840 --> 00:03:32,040
如果是图像或声音，

76
00:03:32,040 --> 00:03:35,550
Token 则可能代表图像的一小块区域

77
00:03:35,550 --> 00:03:37,450
或声音的一段小片段。

78
00:03:37,450 --> 00:03:40,490
然后，每个 Token 会对应到一个向量上，

79
00:03:40,490 --> 00:03:42,310
也就是一串数字，

80
00:03:42,310 --> 00:03:45,600
这串数字的目的是以某种方式来表达该片段的含义。

81
00:03:45,600 --> 00:03:46,900
如果你把这些向量看作是

82
00:03:46,900 --> 00:03:50,100
在一个高维空间中的坐标，

83
00:03:50,100 --> 00:03:51,730
那么含义相似的词汇倾向于

84
00:03:51,730 --> 00:03:54,560
彼此接近的向量上。

85
00:03:54,560 --> 00:03:56,950
这些向量序列接下来

86
00:03:56,950 --> 00:03:59,480
会经过一个称为“注意力块”的处理过程，

87
00:03:59,480 --> 00:04:01,840
使得向量能够相互“交流” 

88
00:04:01,840 --> 00:04:04,880
并根据彼此信息更新自身的值。

89
00:04:04,880 --> 00:04:05,360
例如，

90
00:04:05,360 --> 00:04:06,950
“model”这个单词

91
00:04:06,950 --> 00:04:09,270
在“机器学习模型（model）”中的意思

92
00:04:09,270 --> 00:04:12,040
和在“时尚模特（model）”中的意思是不同的。

93
00:04:12,040 --> 00:04:13,840
注意力模块的作用

94
00:04:13,840 --> 00:04:19,200
就是要确定上下文中哪些词对更新其他词的意义有关，

95
00:04:19,200 --> 00:04:22,390
以及应该如何准确地更新这些含义。

96
00:04:22,390 --> 00:04:24,560
每当我说到“含义”这个词时，

97
00:04:24,560 --> 00:04:28,000
完全通过向量中的数字来表达。

98
00:04:28,000 --> 00:04:32,020
之后，这些向量会经过另一种处理，

99
00:04:32,020 --> 00:04:33,920
这个过程根据资料的不同，

100
00:04:33,920 --> 00:04:36,680
可能被称作多层感知机

101
00:04:36,680 --> 00:04:38,400
或者前馈层。

102
00:04:38,400 --> 00:04:40,270
这个阶段，向量不再互相“交流”，

103
00:04:40,270 --> 00:04:42,960
而是并行地经历同一处理。

104
00:04:42,960 --> 00:04:45,680
虽然这个步骤比较难以理解，

105
00:04:45,680 --> 00:04:46,560
但我们会在后面讨论，

106
00:04:46,560 --> 00:04:48,190
这个步骤有点像

107
00:04:48,190 --> 00:04:51,290
对每个向量提出一系列的问题，

108
00:04:51,290 --> 00:04:54,930
然后根据这些问题的答案来更新向量。

109
00:04:54,930 --> 00:04:56,500
这两个处理阶段的操作

110
00:04:56,500 --> 00:05:00,750
本质上都是大量的矩阵乘法，

111
00:05:00,750 --> 00:05:03,280
我们要学习的主要是 

112
00:05:03,470 --> 00:05:05,440
如何解读这些背后的矩阵。

113
00:05:05,440 --> 00:05:11,090
在讲解中，我省略了一些中间步骤的归一化细节，

114
00:05:11,090 --> 00:05:13,560
毕竟这只是宏观概览。

115
00:05:13,560 --> 00:05:15,690
接下来，过程基本上是重复的。 

116
00:05:15,690 --> 00:05:19,440
你需要在注意力模块和多层感知机（MLP）模块之间不断切换，

117
00:05:19,440 --> 00:05:20,940
直到最后，

118
00:05:20,940 --> 00:05:22,220
我们期望通过某种方式，

119
00:05:22,220 --> 00:05:28,840
文章的核心意义已经被完全融入到序列的最后一个向量中。

120
00:05:28,840 --> 00:05:31,800
然后对这最后一个向量进行特定操作，

121
00:05:31,800 --> 00:05:35,440
产生一个覆盖所有可能 Token 的概率分布，

122
00:05:35,440 --> 00:05:38,990
这些 Token 代表的是可能接下来出现的任何小段文本。

123
00:05:38,990 --> 00:05:39,820
就像我说的，

124
00:05:39,820 --> 00:05:43,620
一旦拥有了这样一个工具，它可以根据一小段文本预测下一步，

125
00:05:43,620 --> 00:05:44,700
你就可以给它输入一些初始文本，

126
00:05:44,700 --> 00:05:49,220
让它不断地进行预测下一步，

127
00:05:49,220 --> 00:05:51,440
从概率分布中抽样，添加到现有文本，

128
00:05:51,440 --> 00:05:53,590
然后不断重复这个过程。

129
00:05:53,590 --> 00:05:56,070
了解这一点的人可能还记得， 早在

130
00:05:56,070 --> 00:05:58,140
ChatGPT 出现之前，

131
00:05:58,140 --> 00:06:00,880
GPT-3 的早期演示就是这样的，

132
00:06:00,880 --> 00:06:04,560
根据一段起始文本自动补全故事和文章。 

133
00:06:04,560 --> 00:06:08,170
将这样的工具转化为聊天机器人的一个简单方法是，

134
00:06:08,170 --> 00:06:10,600
就是准备一段文本，

135
00:06:10,600 --> 00:06:15,040
设定出用户与一个有帮助的 AI 助手交互的场景，

136
00:06:15,040 --> 00:06:16,960
这就是所谓的系统提示。

137
00:06:16,960 --> 00:06:19,550
然后，你可以利用用户的初始问题

138
00:06:19,550 --> 00:06:21,640
或提示词作为对话的开头，

139
00:06:21,640 --> 00:06:23,580
接着让 AI 开始预测

140
00:06:23,580 --> 00:06:26,730
这个有用的 AI 助手会如何进行回应。

141
00:06:26,730 --> 00:06:31,810
为了使这个过程运行得更好，还需要额外的训练步骤，

142
00:06:31,810 --> 00:06:33,990
不过总的来说，这就是基本的思路。

143
00:06:33,990 --> 00:06:42,670
在这一章节中，我们将深入讨论网络开始和结束时发生的事情，

144
00:06:42,670 --> 00:06:44,610
同时，我也会花大量的时间

145
00:06:44,610 --> 00:06:46,900
回顾一些重要的背景知识，

146
00:06:46,900 --> 00:06:49,459
这些知识对于熟悉 Transformer 

147
00:06:49,459 --> 00:06:50,800
的机器学习工程师来说，

148
00:06:50,800 --> 00:06:52,400
都是基础常识。

149
00:06:52,400 --> 00:06:55,710
如果你已经对背景知识比较熟悉，而且迫不及待想要了解更多，

150
00:06:55,710 --> 00:06:57,910
那么你可以选择直接跳到下一章节，

151
00:06:57,910 --> 00:07:02,341
这一章节将会关注 Transformer 的核心部分，

152
00:07:02,341 --> 00:07:03,490
即注意力模块。

153
00:07:03,490 --> 00:07:07,040
在这之后，我还会详细讨论多层感知机模块，

154
00:07:07,040 --> 00:07:07,910
训练过程，

155
00:07:07,910 --> 00:07:12,050
以及之前被省略的其他一些细节。

156
00:07:12,050 --> 00:07:13,190
作为背景信息，

157
00:07:13,190 --> 00:07:16,260
这些视频是我们的深度学习系列课程的补充部分，

158
00:07:16,260 --> 00:07:20,730
你不一定非得按照顺序来看，

159
00:07:20,730 --> 00:07:23,460
但在深入研究 Transformer 之前，

160
00:07:23,460 --> 00:07:24,830
我认为有必要确保

161
00:07:24,830 --> 00:07:28,890
我们对深度学习的基本概念和架构有共同的理解。

162
00:07:28,890 --> 00:07:33,190
这里需要明确的是，机器学习是一种方法论，

163
00:07:33,190 --> 00:07:38,150
它涉及到使用数据来指导模型的行为模式。

164
00:07:38,150 --> 00:07:39,990
具体来说，

165
00:07:39,990 --> 00:07:42,360
你可能需要一个函数，输入一张图片， 

166
00:07:42,360 --> 00:07:44,470
输出对应的标签描述，

167
00:07:44,470 --> 00:07:48,280
或者预测给定文本片段的下一个单词，

168
00:07:48,280 --> 00:07:52,970
或者其他需要直觉和模式识别的任务，

169
00:07:52,970 --> 00:07:55,040
虽然我们现在已经习以为常，

170
00:07:55,040 --> 00:07:57,480
机器学习的核心思想在于，我们不再尝试

171
00:07:57,480 --> 00:08:00,670
去编写固定的程序来完成这些任务，

172
00:08:00,670 --> 00:08:04,020
这是在 AI 的最早阶段人们会做的事情。

173
00:08:04,020 --> 00:08:08,080
而是构建一个具有可调节参数的灵活结构，

174
00:08:08,080 --> 00:08:10,160
就像一系列的旋钮和调节器，

175
00:08:10,160 --> 00:08:15,400
然后通过大量实例输入和期望输出的学习，

176
00:08:15,400 --> 00:08:18,250
调整和微调参数的值，

177
00:08:18,250 --> 00:08:20,160
以此来模拟这种直觉行为。

178
00:08:20,160 --> 00:08:24,900
比如，可能最直观的机器学习入门模型就是线性回归了，

179
00:08:24,900 --> 00:08:27,570
这里你把输入和输出都视为单个数字，

180
00:08:27,570 --> 00:08:30,730
如房子的面积和价格，

181
00:08:30,730 --> 00:08:35,120
你要做的，就是找出一条最拟合这些数据的直线，

182
00:08:35,120 --> 00:08:37,460
以此来预测将来的房价。

183
00:08:37,460 --> 00:08:40,400
这条线由两个连续的参数，

184
00:08:40,400 --> 00:08:42,390
即斜率和 y 轴截距。

185
00:08:42,390 --> 00:08:44,760
线性回归的目标就是

186
00:08:44,760 --> 00:08:48,910
确定这些参数以尽可能匹配数据。

187
00:08:48,910 --> 00:08:52,400
不用说，深度学习模型会更复杂。

188
00:08:52,400 --> 00:08:56,643
比如，GPT-3 就拥有 1750 亿个参数，

189
00:08:56,643 --> 00:08:58,120
而不仅仅是两个。

190
00:08:58,120 --> 00:08:59,200
但值得注意的是，

191
00:08:59,200 --> 00:09:03,760
并不是简单地构建一个参数众多的庞大模型就能有效工作，

192
00:09:04,010 --> 00:09:07,120
这样做可能会导致模型严重过拟合训练数据，

193
00:09:07,120 --> 00:09:09,440
或者训练起来极其困难。

194
00:09:09,440 --> 00:09:12,120
深度学习涵盖了一系列

195
00:09:12,120 --> 00:09:16,200
在过去几十年里证明了具有出色扩展能力的模型类别。

196
00:09:16,200 --> 00:09:19,520
它们之所以能够成功，关键在于都采用了相同的训练算法：

197
00:09:19,520 --> 00:09:23,140
即反向传播，我们在前面的章节已经介绍过这一点。

198
00:09:23,140 --> 00:09:25,230
你需要理解的是，

199
00:09:25,230 --> 00:09:28,560
要让这种训练算法能在大规模应用中顺利进行，

200
00:09:28,800 --> 00:09:31,830
模型必须遵循一种特定的结构。

201
00:09:31,830 --> 00:09:33,760
如果你对这种结构有所了解，

202
00:09:33,760 --> 00:09:36,045
就能更好地理解 Transformer 

203
00:09:36,045 --> 00:09:37,760
处理语言的方式及其背后的逻辑，

204
00:09:37,760 --> 00:09:40,560
否则某些设计选择可能显得有些随意。

205
00:09:40,560 --> 00:09:43,600
首先，不管你构建的是哪种模型，

206
00:09:43,600 --> 00:09:46,690
输入都必须是一个实数数组。

207
00:09:46,690 --> 00:09:51,040
这可能只是一个数字列表，也可能是一个二维数组，

208
00:09:51,040 --> 00:09:53,760
或者更常见的是更高维度的数组，

209
00:09:53,760 --> 00:09:56,320
这种通用的术语我们称之为张量。

210
00:09:56,510 --> 00:10:01,810
这些输入数据通常会被逐步转换成多个不同的层，

211
00:10:01,810 --> 00:10:06,000
每一层都构成了实数数组，

212
00:10:06,000 --> 00:10:07,450
直到最后一层，

213
00:10:07,450 --> 00:10:09,260
你可以将其视为输出层。

214
00:10:09,260 --> 00:10:09,680
例如，

215
00:10:09,680 --> 00:10:13,700
我们的文本处理模型的最终输出层是一个数字列表，

216
00:10:13,700 --> 00:10:17,100
这些数字代表了所有可能的下一词汇的概率分布。

217
00:10:17,100 --> 00:10:18,170
在深度学习领域，

218
00:10:18,170 --> 00:10:21,600
这些模型的参数通常被称作权重。

219
00:10:21,600 --> 00:10:24,130
这样称呼的原因是，这些模型的一个核心特点是

220
00:10:24,130 --> 00:10:26,720
这些参数与正在处理的数据之间的唯一交互方式

221
00:10:26,720 --> 00:10:29,960
就是通过权重和。

222
00:10:29,960 --> 00:10:32,840
虽然模型中也会穿插一些非线性函数，

223
00:10:32,840 --> 00:10:35,140
但它们并不依赖于这些参数。

224
00:10:35,140 --> 00:10:36,200
通常来说，

225
00:10:36,200 --> 00:10:40,060
我们不会直接看到这些权重和的裸露形式，

226
00:10:40,060 --> 00:10:45,510
而是会发现它们被作为矩阵向量乘积的不同部分封装起来。

227
00:10:45,510 --> 00:10:48,180
这其实是在表达同一种概念。

228
00:10:48,180 --> 00:10:51,030
如果你回想一下矩阵向量乘法是如何运作的，

229
00:10:51,030 --> 00:10:54,500
输出中的每一部分都像是一个权重和。

230
00:10:54,500 --> 00:10:57,060
更直观的方式是，

231
00:10:57,060 --> 00:11:01,340
将这些可调参数填充的矩阵想象成

232
00:11:01,340 --> 00:11:06,240
对处理中数据进行向量转换的工具。

233
00:11:06,240 --> 00:11:10,510
例如，GPT-3 中的那 1750 亿个权重

234
00:11:10,510 --> 00:11:14,330
就被组织在大约 28,000 个不同的矩阵中。

235
00:11:14,330 --> 00:11:17,520
这些矩阵又被分为八个不同的类别，

236
00:11:17,520 --> 00:11:18,900
你和我将要做的就是

237
00:11:18,900 --> 00:11:20,800
逐一理解这些类别，

238
00:11:20,800 --> 00:11:22,540
了解每种类型的功能。

239
00:11:22,540 --> 00:11:24,470
接下来的过程会非常有趣，

240
00:11:24,470 --> 00:11:27,120
我们将参考 GPT-3 的具体数据

241
00:11:27,120 --> 00:11:31,680
来统计这 1750 亿是如何分配的。

242
00:11:31,680 --> 00:11:34,720
即使现在有更大更好的模型，

243
00:11:34,720 --> 00:11:35,780
GPT-3 模型仍具有独特的魅力，

244
00:11:35,780 --> 00:11:39,350
作为第一个引发全球关注的大语言模型，

245
00:11:39,350 --> 00:11:40,860
影响力并未局限于机器学习社区。

246
00:11:40,860 --> 00:11:42,480
实际上，

247
00:11:42,480 --> 00:11:47,120
对于更现代的模型，公司往往对具体的数据保持更严格的保密。

248
00:11:47,120 --> 00:11:48,820
在这里，我想说明的是，

249
00:11:48,820 --> 00:11:52,960
当你深入探索像 ChatGPT 这样的工具的内部机制时，

250
00:11:52,960 --> 00:11:57,970
你会发现几乎所有的计算过程都体现为矩阵和向量的乘积。

251
00:11:57,970 --> 00:12:01,440
在这海量的数字中，很容易迷失方向，

252
00:12:01,440 --> 00:12:06,400
但你需要在心中清楚地区分两个概念：模型的权重 

253
00:12:06,400 --> 00:12:10,360
（我将用蓝色或红色表示）和正在处理的数据

254
00:12:10,360 --> 00:12:11,880
（我将用灰色表示）。

255
00:12:11,880 --> 00:12:13,880
权重就是模型的“大脑”。

256
00:12:13,880 --> 00:12:18,400
这些是在训练过程中学习到的，它们决定了模型的行为模式。 

257
00:12:18,400 --> 00:12:20,640
正在处理的数据仅仅是编码了

258
00:12:20,640 --> 00:12:24,340
某次操作中模型接收的具体输入， 

259
00:12:24,340 --> 00:12:26,400
比如一段文本示例。

260
00:12:26,400 --> 00:12:29,040
理解了上述基础知识后，

261
00:12:29,040 --> 00:12:32,260
让我们探讨文本处理示例的第一步：

262
00:12:32,260 --> 00:12:34,780
将输入分割成小片段，

263
00:12:34,780 --> 00:12:36,620
并将这些片段转换成向量。

264
00:12:36,620 --> 00:12:38,960
我之前提到过，这些小片段被称为 Tokens，

265
00:12:38,960 --> 00:12:41,260
它们可能是单词的一部分或是标点符号，

266
00:12:41,260 --> 00:12:44,750
但在本章，特别是在下一章中，

267
00:12:44,750 --> 00:12:48,650
我倾向于简化理解，假设它们完整地对应于单词。

268
00:12:48,650 --> 00:12:49,980
因为我们人类用单词来思考，

269
00:12:49,980 --> 00:12:54,180
通过参考小例子和解释每一步可以使这个过程更加容易理解。

270
00:12:54,180 --> 00:12:58,900
模型拥有一个预设的词汇库，包含所有可能的单词，

271
00:12:58,900 --> 00:13:03,340
比如说有 50,000 个。我们将首先遇到的一个矩阵

272
00:13:03,340 --> 00:13:05,190
叫做嵌入矩阵，

273
00:13:05,190 --> 00:13:08,450
它为每个单词都分配了一个独立的列。

274
00:13:08,450 --> 00:13:14,390
这些列定义了第一步中每个单词转换成的向量。

275
00:13:14,390 --> 00:13:18,650
我们将其称为 we，就像我们看到的所有其他矩阵一样，

276
00:13:18,650 --> 00:13:23,530
它的初始值是随机的，但基于数据进行学习和调整。

277
00:13:23,530 --> 00:13:25,752
在 Transformers 出现之前，

278
00:13:25,752 --> 00:13:28,530
机器学习中就已经普遍采用了将单词转换为向量的做法，

279
00:13:28,530 --> 00:13:31,120
虽然这对于初次接触的人来说可能有些奇怪，

280
00:13:31,120 --> 00:13:33,550
但它为接下来的一切建立了基础，

281
00:13:33,550 --> 00:13:35,770
因此，我们需要花一些时间来熟悉它。

282
00:13:35,770 --> 00:13:37,560
我们通常称这种转换为词嵌入，

283
00:13:37,560 --> 00:13:40,980
这种表述让你可以从几何的角度去理解这些向量，

284
00:13:40,980 --> 00:13:43,350
把它们想象成高维空间中的点。

285
00:13:43,350 --> 00:13:48,770
将三个数字看作是三维空间中的坐标点很简单，

286
00:13:48,770 --> 00:13:52,400
但词向量的维度远远超出这个范畴。

287
00:13:52,400 --> 00:13:56,960
在 GPT-3 中，它们的维度高达 12,288，如你所见，

288
00:13:56,960 --> 00:14:00,400
选择一个有很多不同方向的空间进行工作是很重要的。

289
00:14:00,400 --> 00:14:04,810
就像你可以在三维空间中选择一个二维切片，

290
00:14:04,810 --> 00:14:07,600
并将所有点投影到这个切片上一样，

291
00:14:07,760 --> 00:14:11,120
为了让简单模型输出的词向量能够被动态展示，

292
00:14:11,120 --> 00:14:16,800
我采取了相似的方法，选择了一个高维空间中的三维“切片”，

293
00:14:16,800 --> 00:14:20,420
并将词向量映射到这个切片上来展示它们。

294
00:14:20,420 --> 00:14:24,850
这里的关键思想是，模型在训练过程中调整和微调权重，

295
00:14:24,850 --> 00:14:29,080
 以确定词具体如何被嵌入为向量，

296
00:14:29,080 --> 00:14:31,000
它会倾向于找到一组嵌入，

297
00:14:31,000 --> 00:14:34,610
使得这个空间中的方向含有特定的语义意义。

298
00:14:34,610 --> 00:14:37,520
对于我目前运行的这个简单的词向量模型来说，

299
00:14:37,520 --> 00:14:42,080
如果进行搜索，找到所有与“塔楼”最相似的词向量，

300
00:14:42,080 --> 00:14:46,220
你会发现这些词都有着类似的“塔楼感”。

301
00:14:46,220 --> 00:14:48,580
如果你想在家里用 Python 试一试，

302
00:14:48,580 --> 00:14:51,570
这就是我用来制作动画的模型。

303
00:14:51,570 --> 00:14:52,790
虽然它不是一个 Transformer 模型，

304
00:14:52,790 --> 00:14:54,680
但足以说明一个观点：

305
00:14:54,680 --> 00:14:58,190
空间中的方向能够传达特定的语义。

306
00:14:58,190 --> 00:15:00,060
一个经典的例子是，

307
00:15:00,060 --> 00:15:03,850
如果你计算“女人”和“男人”向量之间的差值，

308
00:15:03,850 --> 00:15:06,820
你会发现这个差异可以被可视化为空间中的一个小向量，

309
00:15:06,820 --> 00:15:09,600
连接一个词的尖端和另一个词的尖端，

310
00:15:09,600 --> 00:15:13,590
这个差异与“国王”和“女王”之间的差值非常相似。

311
00:15:13,590 --> 00:15:18,020
所以假设你不知道表示“女性君主”的词，

312
00:15:18,020 --> 00:15:22,080
你可以通过向“国王”向量添加“女人减男人”的方向，

313
00:15:22,080 --> 00:15:25,420
并搜索最接近这个点的词向量来找到它。

314
00:15:25,420 --> 00:15:26,980
至少在理论上是这样。

315
00:15:26,980 --> 00:15:31,200
虽然这是我正在使用的模型的一个经典例子，

316
00:15:31,240 --> 00:15:35,300
但实际上，真正的“女王”嵌入实际上比这种方法预想的要远一些，

317
00:15:35,300 --> 00:15:38,390
这可能是因为在训练数据中，

318
00:15:38,390 --> 00:15:40,720
“女王”并不仅仅是“国王”的女性版本。

319
00:15:40,720 --> 00:15:42,580
深入挖掘时，

320
00:15:42,580 --> 00:15:46,200
我发现通过家族关系来解释这一现象似乎更为恰当。

321
00:15:46,200 --> 00:15:48,260
关键在于，在训练过程中，

322
00:15:48,260 --> 00:15:50,770
模型发现采用这样的嵌入方式更为有利，

323
00:15:50,770 --> 00:15:55,410
即这个空间中的一个方向能够编码性别信息。

324
00:15:55,410 --> 00:15:59,210
另一个例子是，如果你从“意大利”的向量表示中

325
00:15:59,210 --> 00:16:01,660
减去“德国”的向量表示，

326
00:16:01,660 --> 00:16:04,800
再加上“希特勒”的向量表示，

327
00:16:04,800 --> 00:16:08,500
结果非常接近于“墨索里尼”的向量表示。

328
00:16:08,500 --> 00:16:13,100
这就好像模型学会了将某些方向与“意大利”特性相关联，

329
00:16:13,100 --> 00:16:15,590
而将其他方向与二战轴心国的领导人相关联。

330
00:16:15,590 --> 00:16:19,410
我个人最喜欢的一个例子是，在某些模型中，

331
00:16:19,410 --> 00:16:22,234
如果你计算“德国”和“日本”的向量差值，

332
00:16:22,234 --> 00:16:23,930
然后加上“寿司”的向量，

333
00:16:23,930 --> 00:16:26,290
你得到的结果会非常接近“德国香肠”。

334
00:16:26,290 --> 00:16:29,980
此外，在寻找最近邻居的过程中，

335
00:16:29,980 --> 00:16:33,840
我还惊喜地发现“猫”离“野兽”和“怪物”都很近。

336
00:16:33,840 --> 00:16:37,380
有一个有用的数学概念，

337
00:16:37,380 --> 00:16:38,960
尤其对于接下来的章节非常重要，

338
00:16:38,960 --> 00:16:40,780
那就是两个向量的点积

339
00:16:40,780 --> 00:16:43,680
可以被视为一种衡量它们是否对齐的方法。

340
00:16:43,680 --> 00:16:44,920
从计算角度看，

341
00:16:44,920 --> 00:16:49,120
点积涉及到逐一乘以对应元素，

342
00:16:49,120 --> 00:16:50,440
然后进行求和，

343
00:16:50,440 --> 00:16:54,240
这很好，因为我们的很多计算看起来就像是权重求和。

344
00:16:54,240 --> 00:16:55,400
从几何角度来看，

345
00:16:55,400 --> 00:17:00,240
当两个向量指向相似方向时，点积为正；

346
00:17:00,240 --> 00:17:02,230
如果它们垂直，点积为零；

347
00:17:02,230 --> 00:17:05,520
当它们指向相反方向时，点积为负。

348
00:17:05,520 --> 00:17:06,589
例如，

349
00:17:06,589 --> 00:17:09,230
假设你在测试这个模型，

350
00:17:09,230 --> 00:17:11,043
从“cats”（复数）的向量表示

351
00:17:11,043 --> 00:17:12,970
中减去“cat”（单数）的向量表示

352
00:17:12,970 --> 00:17:17,030
可能会在这个空间中找到表示复数概念的方向。

353
00:17:17,030 --> 00:17:17,760
为了验证这个观点，

354
00:17:17,760 --> 00:17:19,210
我将计算某个向量

355
00:17:19,210 --> 00:17:23,020
与一些特定的单数名词嵌入的点积，

356
00:17:23,020 --> 00:17:26,560
并将其与相应的复数名词的点积进行比较。

357
00:17:26,560 --> 00:17:28,030
如果你试一试，

358
00:17:28,030 --> 00:17:29,650
你会发现复数名词的点积值

359
00:17:29,650 --> 00:17:33,670
通常比单数的更高，

360
00:17:33,670 --> 00:17:36,370
这表明它们在某种方向上的对齐更为紧密。

361
00:17:36,370 --> 00:17:38,480
更有趣的是，如果你将这个点积操作

362
00:17:38,480 --> 00:17:41,440
应用到“一”、“二”、“三”等词汇的嵌入上，

363
00:17:41,440 --> 00:17:43,940
会发现得到的数值是逐渐增加的，

364
00:17:43,940 --> 00:17:46,720
就像我们能够量化地衡量

365
00:17:46,720 --> 00:17:48,950
模型认为一个词的"复数程度"。

366
00:17:48,950 --> 00:17:53,640
再次说明，单词的嵌入方式是通过数据学习得到的。

367
00:17:53,640 --> 00:17:57,600
这种嵌入矩阵揭示了每个词汇的变化过程，

368
00:17:57,600 --> 00:18:01,430
它是我们模型中的第一批权重，根据 GPT-3 的数据，

369
00:18:01,430 --> 00:18:05,810
其词汇量具体为 50,257，但要注意，

370
00:18:05,810 --> 00:18:09,760
实际上它指的不是单词本身，而是 Tokens。

371
00:18:10,400 --> 00:18:13,050
嵌入的维度是 12,288。

372
00:18:13,050 --> 00:18:17,910
将这两者相乘，我们得到大约有 6.17 亿个权重。

373
00:18:17,910 --> 00:18:20,050
我们将这个数字加入到我们的累计计数中，

374
00:18:20,050 --> 00:18:23,790
最后，我们应该得到 1750 亿个权重。

375
00:18:23,790 --> 00:18:25,580
在谈到 transformer 时，

376
00:18:25,580 --> 00:18:29,370
你会想到这些嵌入空间中的向量

377
00:18:29,370 --> 00:18:31,960
不仅仅代表着单个词汇。

378
00:18:31,960 --> 00:18:36,170
它们还携带了词汇位置的信息，

379
00:18:36,170 --> 00:18:37,970
这一点我们稍后会详细说明。

380
00:18:37,970 --> 00:18:39,230
但更关键的是，

381
00:18:39,230 --> 00:18:43,340
这些向量能够吸纳并反映语境。

382
00:18:43,340 --> 00:18:47,240
举个例子，一个最初代表“国王”的向量，

383
00:18:47,240 --> 00:18:51,530
在网络中的各个环节的作用下，可能会逐渐变化，

384
00:18:51,530 --> 00:18:55,480
因此最后，它指向的方向会更具体、更微妙，

385
00:18:55,480 --> 00:18:59,200
以某种方式编码了一位生活在苏格兰的国王，

386
00:18:59,200 --> 00:19:02,540
在杀死前任国王后取得其职位的国王，

387
00:19:02,540 --> 00:19:05,360
此人的描绘方式充满了莎士比亚式的语言。

388
00:19:05,360 --> 00:19:07,910
想想你对某个词汇的理解通常是怎样形成的。

389
00:19:07,910 --> 00:19:11,760
那个词的意义很大程度上是由其所处的环境决定的，

390
00:19:11,760 --> 00:19:15,110
有时这甚至包括来自很远的上下文。

391
00:19:15,110 --> 00:19:19,600
因此，在构建一个能预测下一个词汇的模型时，

392
00:19:19,600 --> 00:19:23,970
关键目标就是让它能够高效地融合上下文信息。

393
00:19:23,970 --> 00:19:25,710
明确一点，在第一步，

394
00:19:25,710 --> 00:19:28,480
当我们根据输入文本创建向量数组时，

395
00:19:28,480 --> 00:19:31,810
每个向量都是直接从嵌入矩阵中选取的。

396
00:19:31,810 --> 00:19:34,870
这意味着，起初，每个向量仅能代表一个单词的含义，

397
00:19:34,870 --> 00:19:36,870
而不涉及其周边环境的信息。

398
00:19:36,870 --> 00:19:41,580
但我们的主要目标是让这些向量通过网络传递，

399
00:19:41,580 --> 00:19:49,360
使每一个向量都能获得比单个词更丰富、更具体的含义。

400
00:19:49,360 --> 00:19:52,580
这个网络每次只能处理一定数量的向量，

401
00:19:52,580 --> 00:19:54,230
这就是所谓的上下文大小。

402
00:19:54,230 --> 00:19:57,950
对于 GPT-3，它的训练上下文大小为 2048，

403
00:19:57,950 --> 00:20:00,240
意味着数据在网络中流动时，

404
00:20:00,350 --> 00:20:02,870
总是看起来像一串 2048 列的数组， 

405
00:20:02,870 --> 00:20:05,630
每一列都有 12000 个维度。

406
00:20:05,630 --> 00:20:08,983
这个上下文大小限制了 Transformer 

407
00:20:08,983 --> 00:20:11,900
在预测下一个词的过程中可以纳入的文本量。

408
00:20:11,900 --> 00:20:14,880
这就解释了为什么如果和一些聊天机器人

409
00:20:14,880 --> 00:20:16,520
比如 ChatGPT 的早期版本

410
00:20:16,520 --> 00:20:20,710
进行长对话时，你可能会感觉到机器人似乎在对话中迷失了方向，

411
00:20:20,710 --> 00:20:22,070
尤其是当对话持续过长时。

412
00:20:22,070 --> 00:20:26,000
我们会在适当的时候详细讨论注意力机制的细节，

413
00:20:26,000 --> 00:20:29,440
但先让我们简单了解一下最终阶段的处理过程。

414
00:20:29,440 --> 00:20:29,770
请记住，

415
00:20:29,770 --> 00:20:34,190
最终的目标是产生一个概率分布，预测下一个可能出现的 

416
00:20:34,190 --> 00:20:35,210
Token。

417
00:20:35,210 --> 00:20:38,320
举例来说，如果最后一个词是“教授”，

418
00:20:38,320 --> 00:20:41,290
上下文中包含了“哈利·波特”等词语，

419
00:20:41,290 --> 00:20:44,800
紧接着出现的是“最不喜欢的老师”，

420
00:20:44,800 --> 00:20:46,050
如果允许我稍微发挥一下，

421
00:20:46,050 --> 00:20:48,970
假设 Tokens 就是完整的单词。

422
00:20:48,970 --> 00:20:52,400
那么，一个经过良好训练并对哈利·波特世界有所了解的网络，

423
00:20:52,400 --> 00:20:55,810
会很可能给“斯内普”这个词赋予一个较高的权重。

424
00:20:55,810 --> 00:20:58,360
此过程涉及两个不同的步骤。

425
00:20:58,360 --> 00:21:00,760
首先，使用另一个矩阵，

426
00:21:00,760 --> 00:21:03,440
将上下文中的最后一个向量映射到

427
00:21:03,440 --> 00:21:05,510
一个包含 50,000 个值的列表，

428
00:21:05,510 --> 00:21:07,570
词汇表中的每个 token 都对应一个值。

429
00:21:07,570 --> 00:21:12,080
接着，通过一个函数，把这些值转换成概率分布。

430
00:21:12,080 --> 00:21:15,410
这个函数叫 softmax，我们稍后会详细讨论。

431
00:21:15,410 --> 00:21:17,910
但在此之前，你可能会觉得，

432
00:21:17,910 --> 00:21:21,070
仅仅基于最后一个嵌入来做出预测似乎有些奇怪，

433
00:21:21,070 --> 00:21:25,340
毕竟在最后一层中还有成千上万的其他向量，

434
00:21:25,340 --> 00:21:28,840
每个向量都蕴含着丰富的上下文意义。

435
00:21:28,840 --> 00:21:31,380
这是因为在训练过程中，

436
00:21:31,380 --> 00:21:37,683
如果我们利用最终层的每一个向量来预测其后可能出现的内容，

437
00:21:37,683 --> 00:21:40,160
被证明是更高效的方法。

438
00:21:40,160 --> 00:21:42,740
关于训练的更多细节我们稍后还会提到，

439
00:21:42,740 --> 00:21:45,430
现在先简单指出这一点。

440
00:21:45,430 --> 00:21:48,320
这个矩阵被称为 unembedding 矩阵，

441
00:21:48,320 --> 00:21:50,080
我们用标签 Wu 来标记它。

442
00:21:50,080 --> 00:21:53,830
就像我们见过的所有权重矩阵一样，这个矩阵的初始值是随机的，

443
00:21:53,830 --> 00:21:56,430
但在训练过程中，这些值会被更新。

444
00:21:56,430 --> 00:21:58,150
关于参数总数的统计，

445
00:21:58,150 --> 00:22:00,050
这个 unembedding 矩阵

446
00:22:00,050 --> 00:22:01,950
为词汇表中的每个单词都分配了一行，

447
00:22:01,950 --> 00:22:05,940
每一行包含与嵌入维度相同数量的元素。

448
00:22:05,940 --> 00:22:08,204
这与嵌入（embedding）矩阵非常相似，

449
00:22:08,204 --> 00:22:09,440
只不过是把顺序倒过来了，

450
00:22:09,440 --> 00:22:13,130
因此它为网络增加了另外 6.17 亿个参数。

451
00:22:13,130 --> 00:22:15,920
到目前为止，我们的参数总数已经超过了 10 亿，

452
00:22:15,920 --> 00:22:21,970
这只是我们最终要达到的 1750 亿的一小部分。

453
00:22:21,970 --> 00:22:24,430
在这一章的最后一个小课中，

454
00:22:24,430 --> 00:22:26,920
我想更详细地讨论一下 softmax 函数，

455
00:22:26,920 --> 00:22:30,560
因为它在我们探索注意力机制时会再次成为焦点。

456
00:22:30,560 --> 00:22:36,160
如果你想让一串数字成为概率分布，

457
00:22:36,310 --> 00:22:38,980
比如预测下一个可能出现的词的概率，

458
00:22:38,980 --> 00:22:41,600
那么这些数字每个都得在 0 到 1 之间，

459
00:22:41,600 --> 00:22:44,560
并且加起来总和为 1。

460
00:22:44,560 --> 00:22:45,920
但是，

461
00:22:45,920 --> 00:22:47,680
如果你正在进行深度学习的实践，

462
00:22:47,680 --> 00:22:50,640
你所做的每一步操作都可能看起来像是矩阵和向量的乘法，

463
00:22:50,640 --> 00:22:54,650
那么你得到的结果可能并不符合这个条件。

464
00:22:54,650 --> 00:22:57,920
这些值可能是负的，也可能远大于 1，

465
00:22:57,920 --> 00:23:00,460
而且加起来的和几乎确定不会是 1。

466
00:23:00,460 --> 00:23:01,690
Softmax 就是一种标准方法，

467
00:23:01,690 --> 00:23:05,330
可以把任何一组数字转换成一个有效的分布，

468
00:23:05,330 --> 00:23:09,120
使得最大的数值非常接近 1，

469
00:23:09,120 --> 00:23:11,240
而较小的值将会非常接近 0。

470
00:23:11,240 --> 00:23:13,210
了解这一点就足够了。

471
00:23:13,210 --> 00:23:14,080
但如果你感到好奇，

472
00:23:14,080 --> 00:23:18,720
具体的工作原理是：首先对每个数值进行 e 的指数运算，

473
00:23:18,720 --> 00:23:21,280
这样就得到了一组正数；

474
00:23:21,280 --> 00:23:24,470
然后取所有正数的和，

475
00:23:24,470 --> 00:23:26,710
然后用每个数除以这个总和，

476
00:23:26,710 --> 00:23:29,620
这样就把它们标准化为一个总和为 1 的列表。

477
00:23:29,620 --> 00:23:32,450
你会注意到，如果输入中的某个数值

478
00:23:32,450 --> 00:23:34,490
明显大于其他数值，

479
00:23:34,490 --> 00:23:38,000
那么在输出中，这个数值对应的项就会在分布中占主导地位，

480
00:23:38,000 --> 00:23:42,990
几乎确定你在采样时会选择这个最大的输入值。

481
00:23:42,990 --> 00:23:45,210
但这种方法比直接挑选最大值更为细腻，

482
00:23:45,210 --> 00:23:48,100
因为当其他数值也接近最大值时，

483
00:23:48,100 --> 00:23:51,120
它们在整体分布中同样能获得重要的比重，

484
00:23:51,120 --> 00:23:52,870
而且随着你不断改变输入，

485
00:23:52,870 --> 00:23:54,980
一切都会连续地变化。

486
00:23:54,980 --> 00:23:55,940
在一些情况下，

487
00:23:55,940 --> 00:23:59,690
比如当 ChatGPT 利用这个分布生成下一个单词时，

488
00:23:59,690 --> 00:24:01,980
可以为这个函数增加一些趣味性，

489
00:24:01,980 --> 00:24:09,180
可以通过在指数的分母里添加一个常量 t 来实现。 

490
00:24:09,180 --> 00:24:10,550
我们称之为“温度”，

491
00:24:10,550 --> 00:24:15,300
因其在某种程度上与热力学方程中温度的作用相似。

492
00:24:15,300 --> 00:24:17,760
它的效果是，当 t 值较大时，

493
00:24:17,760 --> 00:24:19,770
会使较小的数值获得更多的权重，

494
00:24:19,770 --> 00:24:24,250
使得分布稍微均匀一些。而如果 t 值较小，

495
00:24:24,250 --> 00:24:28,330
则较大的数值则会更加明显地占据主导，极端情况下，

496
00:24:28,330 --> 00:24:32,640
如果把 t 设为 0，那么所有的权重都会集中在最大的值上。

497
00:24:32,640 --> 00:24:33,690
例如，

498
00:24:33,690 --> 00:24:36,200
我将用 GPT-3 生成一个故事

499
00:24:36,200 --> 00:24:39,600
种子文本是"从前，有一个 A"

500
00:24:39,600 --> 00:24:42,750
但我会在每次测试中使用不同的温度。

501
00:24:42,750 --> 00:24:48,400
温度为 0 意味着它总是选择最可预测的词，

502
00:24:48,400 --> 00:24:52,320
而你所得到的结果就变成了一个陈词滥调的金发姑娘故事。

503
00:24:52,940 --> 00:24:56,640
较高的温度给它提供了选择不太可能出现的词的机会，

504
00:24:56,640 --> 00:24:57,920
但这也伴随着风险。

505
00:24:57,920 --> 00:25:00,800
在这个例子中，故事的开始部分较为原创，

506
00:25:00,800 --> 00:25:03,530
讲述的是韩国的一位年轻网页艺术家，

507
00:25:03,530 --> 00:25:06,000
但很快就变得毫无意义。

508
00:25:06,000 --> 00:25:07,310
严格地说，

509
00:25:07,310 --> 00:25:11,040
API 实际上并不允许你选择大于 2 的温度。

510
00:25:11,040 --> 00:25:12,950
这个限制并没有数学上的根据，

511
00:25:12,950 --> 00:25:15,710
只是一个人为的限制，我猜，

512
00:25:15,710 --> 00:25:19,680
目的是为了防止他们的工具产生太过荒诞的结果。

513
00:25:19,780 --> 00:25:20,770
所以，如果你感到好奇，

514
00:25:20,770 --> 00:25:22,800
这个动画的工作原理是这样的：

515
00:25:22,800 --> 00:25:25,273
我选择了 GPT-3 生成的可能性最高的前 

516
00:25:25,273 --> 00:25:26,510
20 个 Token，

517
00:25:26,510 --> 00:25:29,360
这看起来是他们能给我的最多的数量。

518
00:25:29,360 --> 00:25:32,480
然后，我会根据 1/5 的指数来调整这些概率。

519
00:25:32,480 --> 00:25:33,850
再给你介绍一个专业术语，

520
00:25:33,850 --> 00:25:39,180
在这个上下文中，我们通常把这个函数的输出成分称作概率，

521
00:25:39,180 --> 00:25:43,630
人们通常将输入称为 logits，有的人说 logits，

522
00:25:43,630 --> 00:25:46,320
有的人说 logits，我选择说 logits。

523
00:25:46,410 --> 00:25:48,030
举个例子，当你输入一段文本时，

524
00:25:48,030 --> 00:25:50,420
所有这些词向量就会通过网络流动，

525
00:25:50,420 --> 00:25:53,990
并与 unembedding 矩阵进行最终的乘法运算。

526
00:25:53,990 --> 00:25:57,520
机器学习专家会把这种原始、

527
00:25:57,520 --> 00:26:01,360
未标准化的输出成分称为下一个词预测的 logits。

528
00:26:01,360 --> 00:26:04,880
这一章的主要目标是

529
00:26:04,880 --> 00:26:08,480
为理解注意力机制打下基础，

530
00:26:08,480 --> 00:26:10,850
就像电影《龙威小子》里的基本功训练“上蜡刮蜡”。

531
00:26:10,850 --> 00:26:14,890
你看，如果你对词嵌入、softmax 有深入的理解，

532
00:26:14,890 --> 00:26:17,530
点积如何衡量相似度，

533
00:26:17,530 --> 00:26:21,335
以及大部分计算看起来都像是填满可

534
00:26:21,335 --> 00:26:25,140
调参数的矩阵乘法有着深刻的理解，

535
00:26:25,140 --> 00:26:27,690
那么掌握注意力机制——

536
00:26:27,690 --> 00:26:30,730
现代 AI 浪潮中的关键技术，

537
00:26:30,730 --> 00:26:32,640
对你来说应该会比较容易。

538
00:26:32,640 --> 00:26:34,400
为此，欢迎在下一章中加入我。

539
00:26:34,400 --> 00:26:36,570
发布这篇文章时，

540
00:26:36,570 --> 00:26:41,540
下一章的草稿已经可以供赞助者审阅。

541
00:26:41,540 --> 00:26:44,240
最终版应该在一两周内公开。

542
00:26:44,240 --> 00:26:47,610
这通常取决于我根据审阅结果做出的修改有多大。

543
00:26:47,610 --> 00:26:50,080
与此同时，如果你想深入研究注意力机制，

544
00:26:50,080 --> 00:26:52,720
如果你想帮助这个频道，那么它就在那里等你。

