[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "GPT 是 Generative Pretrained Transformer 的缩写。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "所以，第一个词很简单，这些都是生成新文本的机器人。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "预训练指的是模型从海量数据中学习的过程，而前缀则暗示着通过额外的训练，模型在特定任务上还有更多的微调空间。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "然而，最后一个词，才是真正重要的部分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "变压器是一种特殊的神经网络，是一种机器学习模型，是当前人工智能蓬勃发展的核心发明。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "我想通过这段视频和接下来的章节，直观地讲解变压器内部的实际情况。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "我们将逐步探索流经它的数据。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "你可以使用 Transformer 构建许多不同类型的模型。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "有些模型接受音频输入并生成文字。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "这个句子来自一个反向模型，该模型只根据文本生成合成语音。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "所有那些在 2022 年风靡全球的工具，如 \"多莉\"（Dolly）和 \"Midjourney\"（Midjourney），都是基于变压器，它们能接收文字描述并生成图像。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "即使我无法让它理解馅饼生物应该是什么，但我仍然对这种事情的可能性感到震惊。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "而谷歌在 2017 年推出的原始转换器，就是针对将文本从一种语言翻译成另一种语言的特定使用情况而发明的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "但是，你我将重点关注的变体，也就是支持 ChatGPT 等工具的类型，将是一个经过训练的模型，它可以接收一段文字，甚至可能还有周围的图像或声音，并对该段落接下来的内容做出预测。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "这种预测以概率分布的形式出现在随后可能出现的许多不同的文本块中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "乍一看，您可能会认为预测下一个单词与生成新文本的目标截然不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "但是，一旦你有了这样一个预测模型，生成一段较长文本的简单方法就是给它一个初始片段，让它从刚刚生成的分布中随机抽取一个样本，将该样本添加到文本中，然后再次运行整个过程，根据所有新文本（包括刚刚添加的内容）做出新的预测。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "我不知道你是怎么想的，但我真的不觉得这应该真的有用。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "例如，在这个动画中，我在笔记本电脑上运行 GPT-2，让它反复预测和采样下一个文本块，根据种子文本生成一个故事。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "这个故事其实并没有什么意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "但是，如果我把它换成对 GPT-3 的 API 调用（GPT-3 是相同的基本模型，只是要大得多），突然间，我们几乎神奇地得到了一个合情合理的故事，这个故事甚至似乎可以推断出，Pi 生物会生活在数学和计算的国度里。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "当你与 ChatGPT 或其他大型语言模型交互时，你会看到它们一次生成一个单词，这就是重复预测和采样的过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "事实上，我非常喜欢的一个功能是，可以查看它选择的每个新词的基本分布情况。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "让我们先从高层预览一下数据是如何流经转换器的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "我们会花更多时间对每个步骤的细节进行激励、解释和扩展，但概括地说，当这些聊天机器人生成一个给定的单词时，引擎盖下面会发生什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "首先，输入内容会被拆分成许多小片段。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "这些片段被称为标记，就文本而言，它们往往是单词、单词的小片段或其他常见的字符组合。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "如果涉及图像或声音，那么代币可以是图像的小块或声音的小块。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "然后，这些标记中的每一个都与一个向量（即一些数字列表）相关联，该向量旨在以某种方式对该作品的含义进行编码。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "如果把这些向量看作是某个高维空间中的坐标，那么意思相近的词往往会出现在该空间中相互接近的向量上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "然后，这一连串的向量会经过一个被称为注意力区块的操作，这使得向量之间可以相互对话，来回传递信息，更新它们的值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "例如，\"模型 \"一词在 \"机器学习模型 \"短语中的含义不同于在 \"时尚模型 \"短语中的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "注意力模块负责找出上下文中哪些词与更新哪些词的含义相关，以及应该如何更新这些含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "再说一遍，每当我使用 \"意义 \"这个词时，它都会以某种方式完全编码在这些向量的条目中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "之后，这些向量会经过不同的运算，根据你阅读的资料，这将被称为多层感知器或前馈层。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "在这里，矢量之间并不对话，它们都并行地进行相同的运算。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "虽然这个区块比较难解释，但稍后我们会讲到，这个步骤有点像对每个矢量提出一长串问题，然后根据这些问题的答案更新它们。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "这两个区块中的所有操作看起来都像是一大堆矩阵乘法，我们的主要工作是了解如何读取底层矩阵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "我略去了中间一些规范化步骤的细节，但这毕竟是一个高级预览。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "在此之后，这个过程会不断重复，你会在注意力区块和多层感知器区块之间来回切换，直到最后，希望这段话的所有基本含义都能以某种方式融入到序列中的最后一个向量中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "然后，我们对最后一个向量进行一定的运算，得出所有可能的标记的概率分布，即接下来可能出现的所有可能的小块文本。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "就像我说过的，一旦你有了一个工具，它能根据一段文字预测下一步的内容，你就可以给它一点种子文字，让它反复玩这个游戏，预测下一步的内容，从分布中采样，添加，然后一遍又一遍地重复。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "一些了解情况的人可能还记得，在 ChatGPT 出现之前，GPT-3 的早期演示版就是这个样子，你可以让它根据最初的片段自动完成故事和文章。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "要把这样的工具做成聊天机器人，最简单的起点就是用一段文字设定用户与人工智能助手互动的场景，也就是所谓的系统提示，然后用用户的初始问题或提示作为第一段对话，然后让它开始预测人工智能助手会说些什么作为回应。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "要做好这一步，还需要更多的培训，但总体而言，这就是我们的想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "在本章中，你和我将详细介绍在网络的最开始和最后会发生什么，我还想花大量时间回顾一些重要的背景知识，这些知识对于任何机器学习工程师来说，在变形金刚问世之前都是第二天性的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "如果您对这些背景知识比较熟悉，又有点不耐烦，可以随意跳到下一章，这一章将重点介绍注意块，一般认为它是变压器的核心部件。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "在此之后，我想进一步谈谈这些多层感知器模块、训练工作原理，以及其他一些在此之前已经略过的细节。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "从更广泛的背景来看，这些视频是关于深度学习的迷你系列的补充，如果你没有看过前面的视频也没关系，我认为你可以不按顺序观看，但在具体深入研究变形金刚之前，我认为值得确保我们对深度学习的基本前提和结构有一致的认识。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "冒着说得太明显的风险，这是机器学习的一种方法，它描述了使用数据以某种方式确定模型行为的任何模型。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "我的意思是，比方说，你需要一个接收图像并生成描述该图像的标签的函数，或者我们的例子，即根据一段文字预测下一个单词，或者任何其他似乎需要一些直觉和模式识别元素的任务。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "如今，我们几乎认为这是理所当然的，但机器学习的理念是，不要试图用代码明确定义如何完成任务的程序（这是人工智能发展初期人们会做的事情），而是要建立一个非常灵活的结构，其中包含可调整的参数，就像一堆旋钮和刻度盘，然后使用许多给定输入时输出应该是什么样子的示例来调整和调整这些参数的值，以模仿这种行为。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "例如，最简单的机器学习形式可能是线性回归，输入和输出都是单个数字，比如房子的面积和价格，而你想要的是通过这些数据找到一条最佳拟合线，你知道，预测未来的房价。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "该直线由两个连续参数（如斜率和 y-截距）描述，线性回归的目标是确定这些参数，使其与数据紧密匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "不用说，深度学习模型会更复杂。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "比如，GPT-3 就拥有 1750 亿个参数，而不仅仅是两个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "但问题是，并不是说你创建了一个拥有大量参数的巨型模型，它就不会严重过拟合训练数据或完全难以训练。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "深度学习描述了一类模型，在过去的几十年中，这类模型已被证明具有出色的扩展能力。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "它们的共同点是采用了相同的训练算法，即反向传播（backpropagation），而我想让大家知道的是，为了让这种训练算法在大规模应用中运行良好，这些模型必须遵循某种特定的格式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "如果你知道这种格式，就能解释转换器如何处理语言的许多选择，否则就有可能感觉随意。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "首先，无论您要制作什么模型，输入都必须格式化为实数数组。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "这可能是指一个数字列表，也可能是一个二维数组，或者经常是更高维度的数组，这里使用的一般术语是张量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "你通常会认为，输入数据会被逐步转换成许多不同的层，而每一层的结构都是实数数组，直到最后一层，也就是你认为的输出层。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "例如，我们文本处理模型的最后一层是一个数字列表，代表所有可能的下一个标记的概率分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "在深度学习中，这些模型参数几乎总是被称为权重，这是因为这些模型的一个主要特点是，这些参数与所处理数据的唯一交互方式就是通过加权求和。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "你还可以在整个过程中使用一些非线性函数，但它们并不依赖于参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "不过，通常情况下，你不会看到像这样赤裸裸地写出加权和，而是会发现它们被打包成矩阵向量积中的各种成分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "如果你回想一下矩阵向量乘法的工作原理，输出中的每个分量看起来都像是加权和，这就等于在说同样的事情。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "对于你我来说，矩阵中充满了可调整的参数，这些参数可以转换从被处理数据中提取的矢量，这样的矩阵往往在概念上更简洁。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "例如，GPT-3 中的 1 750 亿个权重被组织成不到 28 000 个不同的矩阵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "这些矩阵又分为八个不同的类别，我们要做的就是逐一了解这些类别的作用。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "在我们讨论的过程中，我觉得参考《GPT-3》中的具体数字来计算这 1,750 亿的具体来源是一件很有趣的事情。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "即使如今有了更大更好的模型，但作为真正吸引 ML 社区以外世界注意力的大型语言模型，它仍具有一定的魅力。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "此外，实际上，对于更现代化的网络，公司往往会对具体数字守口如瓶。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "我只想说明一点，当你窥视 ChatGPT 这样的工具时，几乎所有的实际计算看起来都像是矩阵向量乘法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "在数以亿计的数字海洋中迷失方向是有一点风险的，但你应该在头脑中将模型的权重（我总是用蓝色或红色表示）和正在处理的数据（我总是用灰色表示）区分开来。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "权重是实际的大脑，是在训练中学到的东西，决定了它的行为方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "被处理的数据只是对特定运行中输入模型的任何特定输入进行编码，例如文本示例片段。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "有了所有这些作为基础，让我们进入这个文本处理示例的第一步，即把输入分割成小块，并把这些小块转化为矢量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "我提到过这些语块被称为标记，可能是单词或标点符号的片段，但在这一章，尤其是下一章，我想时不时地假装一下，把它们更清晰地分解成单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "因为我们人类是用语言来思考的，这只会让我们更容易参考一些小例子，并澄清每一个步骤。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "该模型有一个预定义的词汇表，包含所有可能出现的单词，比如说 50000 个，我们将遇到的第一个矩阵被称为嵌入矩阵，其中每一个单词都有一列。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "这些列定义了第一步中每个单词转换成的向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "我们给它贴上 We 的标签，就像我们看到的所有矩阵一样，它的值一开始是随机的，但会根据数据进行学习。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "早在变形金刚出现之前，将单词转化为向量就已经是机器学习中的常见做法了，但如果你从未见过它，就会觉得有点奇怪，而且它为后面的一切奠定了基础，所以让我们花点时间来熟悉一下它。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "我们通常把这种嵌入称为 \"词\"，这就需要把这些向量非常几何化地看成是某个高维空间中的点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "将三个数字的列表可视化为三维空间中点的坐标不成问题，但单词嵌入的维度往往要高得多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "在 GPT-3 中，它们有 12 288 个维度，正如你将看到的，在一个有很多不同方向的空间中工作是很重要的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "就像你可以在三维空间中选择一个二维切片并将所有点投射到该切片上一样，为了让简单模型给出的单词嵌入动画更加生动，我将做一个类似的事情，在这个非常高的空间中选择一个三维切片，并将单词向量投射到该切片上，然后显示结果。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "这里的大意是，当一个模型在训练过程中调整权重以确定如何将单词嵌入向量时，它往往会确定一组嵌入向量，其中空间中的方向具有某种语义意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "对于我在这里运行的简单的词到向量模型，如果我搜索所有嵌入式最接近塔的词，你会发现它们似乎都给人非常相似的塔的感觉。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "如果你想在家里调出 Python 演示，这就是我用来制作动画的特定模型。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "这不是一个变压器，但足以说明空间中的方向可能具有语义意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "一个非常典型的例子是，如果你把女人和男人的矢量之间的差异（你可以把它想象成一个连接一个尖端和另一个尖端的小矢量）看成是国王和王后之间的差异。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "因此，假设您不知道女性君主的单词，您可以通过国王，加上这个女人-男人的方向，然后搜索最接近这个点的嵌入词来找到它。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "至少在理论上是这样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "尽管这对我正在使用的模型来说是一个典型的例子，但女王的真实嵌入实际上比这一结果要偏离一些，这大概是因为女王在训练数据中的使用方式不仅仅是国王的女性化版本。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "当我玩了一圈之后，家庭关系似乎更能说明问题。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "问题是，在训练过程中，模型似乎发现选择嵌入式是有利的，因为在这个空间中，有一个方向可以编码性别信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "另一个例子是，如果把意大利的内嵌值减去德国的内嵌值，再加上希特勒的内嵌值，就会得到非常接近墨索里尼的内嵌值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "就好像模型学会了将某些方向与意大利性联系起来，而将另一些方向与二战轴心国领导人联系起来。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "在这方面，我最喜欢的例子可能是，在某些模型中，如果把德国和日本之间的差异加到寿司上，结果就会非常接近于香肠。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "此外，在玩这个寻找最近邻居的游戏时，我很高兴地看到吉与野兽和怪物的距离都很近。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "有一点数学直觉很有帮助，尤其是在下一章，那就是两个向量的点积可以被看作是衡量它们对齐程度的一种方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "在计算上，点乘需要将所有相应的分量相乘，然后将结果相加，这很好，因为我们的很多计算看起来都像是加权和。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "从几何学角度看，当矢量指向相似的方向时，点积为正；当矢量垂直时，点积为零；当矢量指向相反的方向时，点积为负。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "例如，假设你正在玩这个模型，你假设猫减去猫的嵌入可能代表了这个空间的一种多元方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "为了验证这一点，我将利用这个向量计算它与某些单数名词嵌入的点积，并与相应的复数名词的点积进行比较。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "如果你仔细研究一下，就会发现复数的数值似乎确实一直比单数的高，这说明它们更符合这个方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "同样有趣的是，如果将这个点积与单词 1、2、3 等的嵌入值相乘，它们的值会不断增加，因此我们仿佛可以定量地衡量模型发现某个单词的复数程度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "再次说明，单词的嵌入方式是通过数据学习得到的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "这个嵌入矩阵的列告诉我们每个词的情况，它是我们模型中的第一堆权重。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "使用 GPT-3 数字，词汇量具体为 50 257 个，同样，从技术上讲，这不是由单词本身组成的，而是由标记组成的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "嵌入维度为 12 288，乘以这些维度，我们可以得出这包括约 6.17 亿个权值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "让我们继续把这个数字加到流水账上，记住最后我们应该数到 1750 亿。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "就转换器而言，你确实需要把嵌入空间中的向量看作不仅仅代表单个单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "首先，它们还编码有关该单词位置的信息，这一点我们稍后会讲到，但更重要的是，你应该认为它们具有吸收上下文的能力。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "例如，一个向量一开始嵌入的是 \"king\"（国王）一词，可能会逐渐被这个网络中的各种区块拉扯，最后指向一个更具体、更细微的方向，以某种方式编码出这是一个住在苏格兰的国王，他在谋杀了前任国王后登上了王位，并用莎士比亚的语言对他进行了描述。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "想想你对某个词汇的理解通常是怎样形成的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "这个词的含义显然是由周围环境决定的，有时还包括来自很远地方的语境，因此，在建立一个能够预测下一个词是什么的模型时，我们的目标是以某种方式使其能够有效地结合语境。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "说白了，在第一步中，当你根据输入文本创建向量数组时，每一个向量都是从嵌入矩阵中简单提取出来的，因此最初每一个向量只能编码一个单词的含义，而不会从其周围环境中输入任何信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "但你应该认为，这个网络的主要目标是让每个矢量都能吸收比单个词语更丰富、更具体的意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "网络一次只能处理固定数量的向量，即上下文大小。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "对于 GPT-3，它的上下文大小为 2048，因此流经网络的数据看起来总是像由 2048 列组成的数组，每列有 12000 个维度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "这个上下文大小限制了 Transformer 在预测下一个词的过程中可以纳入的文本量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "这就是为什么与某些聊天机器人（如 ChatGPT 的早期版本）进行长时间对话时，机器人常常会感觉对话时间过长而失去了主线。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "我们将在适当的时候详细讨论注意力的问题，但我想先谈谈最后发生的事情。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "请记住，所需的输出是所有接下来可能出现的标记的概率分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "例如，如果最后一个词是 \"教授\"，而上下文中包括 \"哈利-波特 \"这样的词，紧接着我们又看到了 \"最不喜欢的老师\"，而且如果你给我一些回旋余地，让我假装标记看起来只是完整的单词，那么一个训练有素的网络，如果已经积累了关于 \"哈利-波特 \"的知识，就会给 \"斯内普 \"这个词分配一个较高的数字。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "此过程涉及两个不同的步骤。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "第一个方法是使用另一个矩阵，将该上下文中的最后一个向量映射到一个包含 50,000 个值的列表中，词汇表中的每个标记都有一个值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "但在此之前，只使用最后一个嵌入来进行预测似乎有点奇怪，毕竟在最后一步中，层中还有成千上万的其他向量，它们都有自己丰富的上下文含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "这是因为在训练过程中，如果使用最后一层中的每个向量同时对紧随其后的内容进行预测，效率会更高。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "关于训练，以后还有很多话要说，但我现在只想说说这个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "这个矩阵被称为解嵌入矩阵，我们给它取名为 WU。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "同样，与我们看到的所有权重矩阵一样，它的条目一开始是随机的，但在训练过程中会被学习。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "根据我们的参数总数，这个解嵌入矩阵为词汇表中的每个单词设置了一行，每行的元素数与嵌入维数相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "它与嵌入矩阵非常相似，只是交换了阶数，因此又为网络增加了 6.17 亿个参数，也就是说，到目前为止，我们的计算结果略高于十亿，虽然这只是我们最终将得到的 1 750 亿个参数中的一小部分，但也并非完全无关紧要。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "作为本章的最后一堂小课，我想更多地谈谈这个 softmax 函数，因为一旦我们深入研究注意力模块，它就会再次出现在我们面前。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "其原理是，如果你想让一个数字序列作为概率分布，比如所有可能的下一个单词的分布，那么每个值都必须介于 0 和 1 之间，而且所有值加起来必须是 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "不过，如果你玩的是学习游戏，你所做的一切看起来都像是矩阵与向量相乘，那么默认情况下得到的输出结果就完全不符合这一点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "这些数值通常是负数，或者比 1 大得多，而且几乎肯定加起来不等于 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax 是将任意数字列表转化为有效分布的标准方法，其最大值最接近 1，而较小值最接近 0。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "了解这一点就足够了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "但如果你好奇的话，它的工作原理是首先将 e 提升到每个数字的幂，这意味着你现在有了一个正值列表，然后你可以求出所有这些正值的总和，再将每个项除以该总和，这样就将其归一化为一个加起来等于 1 的列表。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "你会注意到，如果输入中的某个数字比其他数字大，那么在输出中，相应的项就会在分布中占主导地位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "但是，这比仅仅选取最大值要柔和得多，因为当其他值同样很大时，它们也会在分布中获得有意义的权重，而且随着输入的不断变化，一切都会不断变化。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "在某些情况下，比如当 ChatGPT 使用这种分布来创建下一个单词时，可以在这个函数中加入一些额外的调味料，在这些指数的分母中加入常数 t，从而增加一些额外的乐趣。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "我们称其为温度，因为它与某些热力学方程中温度的作用有些相似。其效果是，当 t 越大时，低值的权重就越大，这意味着分布会更均匀一些；如果 t 越小，那么大值的权重就会越大，在极端情况下，将 t 设为零意味着所有权重都归于最大值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "例如，我会让 GPT-3 生成一个种子文本为 \"很久很久以前，有一个 A \"的故事，但我会在每种情况下使用不同的温度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "温度为零意味着它总是用最容易预测的词，而你最终得到的是金发女郎的老套衍生词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "温度越高，它就越有机会选择可能性较小的词语，但这也有风险。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "在这部影片中，故事的开头比较新颖，讲述了一位来自韩国的年轻网络艺术家的故事，但很快就陷入了无厘头的境地。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "从技术上讲，应用程序接口实际上不允许您选择大于 2 的温度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "这并没有什么数学上的原因，只是为了防止他们的工具产生太无厘头的东西而任意施加的限制。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "因此，如果你好奇的话，这个动画的实际工作方式是，我从 GPT-3 生成的 20 个最有可能的下一个代币（这似乎是他们给我的最大值）中选取 20 个，然后根据 1/5 的指数调整概率。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "还有一点行话，就像你可能会把这个函数输出的组成部分称为概率一样，人们通常把输入称为对数，或者有些人说对数，有些人说对数，我会说对数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "因此，举例来说，当你输入一些文本时，所有这些单词嵌入都会流经网络，最后与未嵌入矩阵相乘，机器学习人员会将原始、未规范化输出中的成分称为下一个单词预测的对数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "本章的主要目的是为理解注意力机制奠定基础，即 \"空手道小子 \"式的 \"上蜡-脱蜡\"。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "你看，如果你对词嵌入、softmax、点积如何度量相似性有很强的直觉，并且知道大部分计算看起来都像是矩阵乘法，其中的矩阵充满了可调整的参数，那么理解注意力机制--整个现代人工智能蓬勃发展的基石--就会相对容易一些。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "为此，欢迎在下一章中加入我。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "在我发表这篇文章的同时，下一章的草稿也已提供给 Patreon 支持者审阅。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "最终版本应该会在一两周内公开，这通常取决于我在审查基础上做了多少改动。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "与此同时，如果你想深入关注，如果你想为频道出点力，它就在那里等着你。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]