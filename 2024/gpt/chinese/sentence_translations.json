[
  {
    "input": " The initials GPT stand for Generative Pre-trained Transformer.",
    "translatedText": "GPT 是 Generative Pre-trained Transformer 的缩写。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 0.16,
    "end": 5.2
  },
  {
    "input": " That first word is straightforward enough, ",
    "translatedText": "首个单词较为直接，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 5.2,
    "end": 7.14
  },
  {
    "input": " these are bots that generate new text.",
    "translatedText": "它们是用来生成新文本的机器人。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 7.14,
    "end": 9.29
  },
  {
    "input": " \"Pre-trained\" refers to how the model went through a process of learning from a massive amount of data, ",
    "translatedText": "\"Pre-trained\" 指的是模型经历了从大量数据中学习的过程， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 9.29,
    "end": 14.48
  },
  {
    "input": " and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
    "translatedText": "这个词暗示了该模型还有进一步在特定任务中进行额外训练和微调的可能。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 14.48,
    "end": 19.99
  },
  {
    "input": " But the last word, that's the real key piece.",
    "translatedText": "然而，最后一个词，才是真正重要的部分。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 19.99,
    "end": 23.03
  },
  {
    "input": " A transformer is a specific kind of neural network, a machine learning model, ",
    "translatedText": "Transformer 是一种特定类型的神经网络，一个机器学习模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 23.03,
    "end": 27.92
  },
  {
    "input": " and it's the core invention underlying the current boom in AI.",
    "translatedText": "它是现今 AI 高速发展的核心创新。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 27.92,
    "end": 31.74
  },
  {
    "input": " What I want to do with this video and the following chapters is ",
    "translatedText": "我希望通过这个视频和接下来的章节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 31.74,
    "end": 34.5
  },
  {
    "input": " go through a visually driven explanation ",
    "translatedText": "以一种便于理解的方式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 34.5,
    "end": 36.52
  },
  {
    "input": " for what actually happens inside a transformer.",
    "translatedText": "阐述 Transformer 内部实际发生的过程。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 36.52,
    "end": 39.15
  },
  {
    "input": " We're going to follow the data that flows through it and go step by step.",
    "translatedText": "我们将逐步探索流经它的数据。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 39.15,
    "end": 42.64
  },
  {
    "input": " There are many different kinds of models that you can build using transformers.",
    "translatedText": "你可以使用 Transformer 构建许多不同类型的模型。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 42.64,
    "end": 47.75
  },
  {
    "input": " Some models take in audio and produce a transcript.",
    "translatedText": "有些模型接受音频输入并生成文字。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 47.75,
    "end": 50.65
  },
  {
    "input": " This sentence comes from a model going the other way around, ",
    "translatedText": "这句话来自一个反向工作的模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 50.65,
    "end": 53.76
  },
  {
    "input": " producing synthetic speech just from text.",
    "translatedText": "只需要文本输入就能生成人工语音。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 54,
    "end": 56.12
  },
  {
    "input": " All those tools that took the world by storm in 2022, ",
    "translatedText": "所有那些在 2022 年风靡全球的工具，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 56.12,
    "end": 59.36
  },
  {
    "input": " like DALL-E and MidJourney that take in a text description and produce an image, ",
    "translatedText": "如 DALL-E 和 MidJourney，能够将文本描述转化为图像，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 59.36,
    "end": 64.24
  },
  {
    "input": " are based on transformers.",
    "translatedText": "都是基于 Transformer 的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 64.24,
    "end": 65.52
  },
  {
    "input": " And even if I can't quite get it ",
    "translatedText": "即使我无法让它完全理解",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 65.52,
    "end": 66.83
  },
  {
    "input": " to understand what a Pi creature is supposed to be, ",
    "translatedText": "\"π 生物\"到底是什么，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 66.83,
    "end": 69.52
  },
  {
    "input": " I'm still blown away that this kind of thing is even remotely possible.",
    "translatedText": "我仍对这样的事情有可能发生感到惊讶。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 69.52,
    "end": 73.04
  },
  {
    "input": " And the original transformer, introduced in 2017 by Google, ",
    "translatedText": "最初的 Transformer 是 Google 在 2017 年推出的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 73.04,
    "end": 77.46
  },
  {
    "input": " was invented for the specific use case of translating text from one language into another.",
    "translatedText": "主要用于将一种语言的文本翻译成另一种语言。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 77.46,
    "end": 82.44
  },
  {
    "input": " But the variant that you and I will focus on, ",
    "translatedText": "但我们将关注的版本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 82.44,
    "end": 84.88
  },
  {
    "input": " which is the type that underlies tools like ChatGPT, ",
    "translatedText": "也就是像 ChatGPT 这样的工具所依赖的类型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 84.88,
    "end": 88.03
  },
  {
    "input": " will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, ",
    "translatedText": "会是一个接受一段文本（可能伴随一些图像或声音）的模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 88.03,
    "end": 95.07
  },
  {
    "input": " and produce a prediction for what comes next in the passage.",
    "translatedText": "然后预测文章接下来的内容。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 95.07,
    "end": 98.05
  },
  {
    "input": " That prediction takes the form of a probability distribution ",
    "translatedText": "这种预测呈现为概率分布形式",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 98.05,
    "end": 101.13
  },
  {
    "input": " over many different chunks of text that might follow.",
    "translatedText": "涵盖了很多可能接下来出现的文字片段。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 101.13,
    "end": 103.77
  },
  {
    "input": " At first glance, ",
    "translatedText": "乍一看，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 103.77,
    "end": 105.52
  },
  {
    "input": " you might think that predicting the next word ",
    "translatedText": "你可能觉得预测下一个词",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 105.52,
    "end": 107.41
  },
  {
    "input": " feels like a very different goal from generating new text.",
    "translatedText": "似乎与生成新的文字有着天壤之别。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 107.41,
    "end": 110.4
  },
  {
    "input": " But once you have a prediction model like this, ",
    "translatedText": "但当你有了像这样的预测模型后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 110.4,
    "end": 112.6
  },
  {
    "input": " a simple thing you could try to make it generate a longer piece of text ",
    "translatedText": "你可以试着让它生成一段更长的文字，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 112.6,
    "end": 115.94
  },
  {
    "input": " is to give it an initial snippet to work with, ",
    "translatedText": "方法就是给它一个初始的片段，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 115.94,
    "end": 118.75
  },
  {
    "input": " have it take a random sample from the distribution it just generated, ",
    "translatedText": "然后随机从刚生成的概率分布中选取一个样本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 118.75,
    "end": 122.29
  },
  {
    "input": " append that sample to the text, ",
    "translatedText": "将这个样本追加到文字中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 122.29,
    "end": 123.69
  },
  {
    "input": " and then run the whole process again ",
    "translatedText": "接着再进行一轮预测，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 123.69,
    "end": 125.44
  },
  {
    "input": " to make a new prediction based on all the new text, ",
    "translatedText": "这次的预测需要基于所有新生成的文字，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 125.44,
    "end": 128.19
  },
  {
    "input": " including what it just added.",
    "translatedText": "包括刚刚添加的那部分。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 128.19,
    "end": 129.92
  },
  {
    "input": " I don't know about you, ",
    "translatedText": "我不知道你怎么看，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 129.92,
    "end": 130.78
  },
  {
    "input": " but it really doesn't feel like this should actually work.",
    "translatedText": "但我真的觉得这个方法的效果可能并不理想。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 130.78,
    "end": 133.21
  },
  {
    "input": " In this animation, for example, ",
    "translatedText": "举个例子，在这个动画中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 133.21,
    "end": 134.74
  },
  {
    "input": " I'm running GPT-2 on my laptop ",
    "translatedText": "我在我的笔记本电脑上运行 GPT-2，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 134.74,
    "end": 136.47
  },
  {
    "input": " and having it repeatedly predict and sample the next chunk of text ",
    "translatedText": "并让它不断地预测与取样下一个文字块，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 136.47,
    "end": 139.96
  },
  {
    "input": " to generate a story based on the seed text.",
    "translatedText": "尝试基于一段起始文本生成一个故事。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 139.96,
    "end": 142.79
  },
  {
    "input": " And the story just doesn't actually really make that much sense.",
    "translatedText": "结果呢，这个故事基本上没什么逻辑可言。  ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 142.79,
    "end": 146.71
  },
  {
    "input": " But if I swap it out for API calls to GPT-3 instead, ",
    "translatedText": "但是，如果我换成 GPT-3 的 API 调用，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 146.71,
    "end": 150.09
  },
  {
    "input": " which is the same basic model, just much bigger, suddenly, almost magically, ",
    "translatedText": "这是同样的基本模型，只是规模更大，突然间就像变魔法一样，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 150.09,
    "end": 154.93
  },
  {
    "input": " we do get a sensible story, ",
    "translatedText": "我们不仅得到了一个合乎逻辑的故事，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 154.93,
    "end": 156.49
  },
  {
    "input": " one that even seems to infer that a pi creature ",
    "translatedText": "这个故事甚至能暗示出一个 π 生物",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 156.49,
    "end": 158.81
  },
  {
    "input": " would live in a land of math and computation.",
    "translatedText": "可能居住在一个充满数学和计算的世界里。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 158.81,
    "end": 161.55
  },
  {
    "input": " This process here of repeated prediction and sampling is essentially what's happening ",
    "translatedText": "这个过程，就是通过重复的预测和选取来生成文本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 161.55,
    "end": 165.39
  },
  {
    "input": " when you interact with ChatGPT or any of these other large language models, and you see them producing one word at a time.",
    "translatedText": "正是你在使用ChatGPT或其他大型语言模型时所经历的，模型会逐字地生成文本。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 165.39,
    "end": 171.89
  },
  {
    "input": " In fact, ",
    "translatedText": "其实，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 171.89,
    "end": 172.49
  },
  {
    "input": " one feature that I would very much enjoy is ",
    "translatedText": "我特别希望能有一种功能，即",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 172.49,
    "end": 174.7
  },
  {
    "input": " the ability to see the underlying distribution for each new word that it chooses.",
    "translatedText": "能看到它在选择每个新词时的底层概率分布。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 174.7,
    "end": 179.18
  },
  {
    "input": " Let's kick things off with a very high level preview ",
    "translatedText": "我们先从宏观角度看看",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 183.921484375,
    "end": 186.32
  },
  {
    "input": " of how data flows through a transformer.",
    "translatedText": "数据是如何在 Transformer 模型中流转的。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 186.32,
    "end": 188.48
  },
  {
    "input": " We will spend much more time motivating and interpreting and expanding on the details of each step.",
    "translatedText": "接下来，我们会详细探讨、解释每一个步骤，并对其进行扩展。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 188.48,
    "end": 193.48
  },
  {
    "input": " But in broad strokes, when one of these chatbots generates a given word, ",
    "translatedText": "但是大体来说，当聊天机器人生成某个特定词汇时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 193.48,
    "end": 196.73
  },
  {
    "input": " here's what's going on under the hood.",
    "translatedText": "下面就是它底层的运行机制。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 196.73,
    "end": 198.56
  },
  {
    "input": " First, the input is broken up into a bunch of little pieces.",
    "translatedText": "首先，输入内容会被拆分成许多小片段。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 198.56,
    "end": 202.52
  },
  {
    "input": " These pieces are called tokens.",
    "translatedText": "这些小片段被称为词元 (Tokens)。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 202.52,
    "end": 203.96
  },
  {
    "input": " And in the case of text, these tend to be words or little pieces of words ",
    "translatedText": "对于文本来说，这些 Token 通常是单词、单词的一小部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 203.96,
    "end": 207.6
  },
  {
    "input": " or other common character combinations.",
    "translatedText": "或者其他常见的字符组合。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 207.6,
    "end": 209.84
  },
  {
    "input": " If images or sound are involved, ",
    "translatedText": "如果是图像或声音，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 209.84,
    "end": 212.04
  },
  {
    "input": " then tokens could be little patches of that image ",
    "translatedText": "Token 则可能代表图像的一小块区域",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 212.04,
    "end": 215.55
  },
  {
    "input": " or little chunks of that sound.",
    "translatedText": "或声音的一段小片段。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 215.55,
    "end": 217.45
  },
  {
    "input": " Each one of these tokens is then associated with a vector, ",
    "translatedText": "然后，每个 Token 会对应到一个向量上，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 217.45,
    "end": 220.49
  },
  {
    "input": " meaning some list of numbers, ",
    "translatedText": "也就是一串数字，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 220.49,
    "end": 222.31
  },
  {
    "input": " which is meant to somehow encode the meaning of that piece.",
    "translatedText": "这串数字的目的是以某种方式来表达该片段的含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 222.31,
    "end": 225.6
  },
  {
    "input": " If you think of these vectors ",
    "translatedText": "如果你把这些向量看作是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 225.6,
    "end": 226.9
  },
  {
    "input": " as giving coordinates in some very high dimensional space, ",
    "translatedText": "在一个高维空间中的坐标，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 226.9,
    "end": 230.1
  },
  {
    "input": " words with similar meanings tend ",
    "translatedText": "那么含义相似的词汇倾向于",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 230.1,
    "end": 231.73
  },
  {
    "input": " to land on vectors that are close to each other in that space.",
    "translatedText": "彼此接近的向量上。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 231.73,
    "end": 234.56
  },
  {
    "input": " This sequence of vectors then passes ",
    "translatedText": "这些向量序列接下来",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 234.56,
    "end": 236.95
  },
  {
    "input": " through an operation that's known as an attention block, ",
    "translatedText": "会经过一个称为“注意力块”的处理过程，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 236.95,
    "end": 239.48
  },
  {
    "input": " and this allows the vectors to talk to each other ",
    "translatedText": "使得向量能够相互“交流” ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 239.48,
    "end": 241.84
  },
  {
    "input": " and pass information back and forth to update their values.",
    "translatedText": "并根据彼此信息更新自身的值。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 241.84,
    "end": 244.88
  },
  {
    "input": " For example, ",
    "translatedText": "例如，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 244.88,
    "end": 245.36
  },
  {
    "input": " the meaning of the word \"model\" ",
    "translatedText": "“model”这个单词",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 245.36,
    "end": 246.95
  },
  {
    "input": " in the phrase \"a machine learning model\" ",
    "translatedText": "在“机器学习模型（model）”中的意思",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 246.95,
    "end": 249.27
  },
  {
    "input": " is different from its meaning in the phrase \"a fashion model\".",
    "translatedText": "和在“时尚模特（model）”中的意思是不同的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 249.27,
    "end": 252.04
  },
  {
    "input": " The attention block is what's responsible ",
    "translatedText": "注意力模块的作用",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 252.04,
    "end": 253.84
  },
  {
    "input": " for figuring out which words in the context are relevant to updating the meanings of which other words, ",
    "translatedText": "就是要确定上下文中哪些词对更新其他词的意义有关，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 253.84,
    "end": 259.2
  },
  {
    "input": " and how exactly those meanings should be updated.",
    "translatedText": "以及应该如何准确地更新这些含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 259.2,
    "end": 262.39
  },
  {
    "input": " And again, whenever I use the word \"meaning\", ",
    "translatedText": "每当我说到“含义”这个词时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 262.39,
    "end": 264.56
  },
  {
    "input": " this is somehow entirely encoded in the entries of those vectors.",
    "translatedText": "完全通过向量中的数字来表达。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 264.56,
    "end": 268
  },
  {
    "input": " After that, these vectors pass through a different kind of operation, ",
    "translatedText": "之后，这些向量会经过另一种处理，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 268,
    "end": 272.02
  },
  {
    "input": " and depending on the source that you're reading, ",
    "translatedText": "这个过程根据资料的不同，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 272.02,
    "end": 273.92
  },
  {
    "input": " this will be referred to as a multi-layer perceptron, ",
    "translatedText": "可能被称作多层感知机",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 273.92,
    "end": 276.68
  },
  {
    "input": " or maybe a feed-forward layer.",
    "translatedText": "或者前馈层。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 276.68,
    "end": 278.4
  },
  {
    "input": " And here, the vectors don't talk to each other, ",
    "translatedText": "这个阶段，向量不再互相“交流”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 278.4,
    "end": 280.27
  },
  {
    "input": " they all go through the same operation in parallel.",
    "translatedText": "而是并行地经历同一处理。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 280.27,
    "end": 282.96
  },
  {
    "input": " And while this block is a little bit harder to interpret, ",
    "translatedText": "虽然这个步骤比较难以理解，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 282.96,
    "end": 285.68
  },
  {
    "input": " later on we'll talk about ",
    "translatedText": "但我们会在后面讨论，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 285.68,
    "end": 286.56
  },
  {
    "input": " how this step is a little bit like ",
    "translatedText": "这个步骤有点像",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 286.56,
    "end": 288.19
  },
  {
    "input": " asking a long list of questions about each vector, ",
    "translatedText": "对每个向量提出一系列的问题，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 288.19,
    "end": 291.29
  },
  {
    "input": " and then updating them based on the answers to those questions.",
    "translatedText": "然后根据这些问题的答案来更新向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 291.29,
    "end": 294.93
  },
  {
    "input": " All of the operations in both ",
    "translatedText": "这两个处理阶段的操作",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 294.93,
    "end": 296.5
  },
  {
    "input": " of these blocks look like a giant pile of matrix multiplications, ",
    "translatedText": "本质上都是大量的矩阵乘法，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 296.5,
    "end": 300.75
  },
  {
    "input": " and our primary job is going to be to understand ",
    "translatedText": "我们要学习的主要是 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 300.75,
    "end": 303.28
  },
  {
    "input": " how to read the underlying matrices.",
    "translatedText": "如何解读这些背后的矩阵。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 303.47,
    "end": 305.44
  },
  {
    "input": " I'm glossing over some details about some normalization steps that happen in between, ",
    "translatedText": "在讲解中，我省略了一些中间步骤的归一化细节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 305.44,
    "end": 311.09
  },
  {
    "input": " but this is after all a high-level preview.",
    "translatedText": "毕竟这只是宏观概览。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 311.09,
    "end": 313.56
  },
  {
    "input": " After that, the process essentially repeats.",
    "translatedText": "接下来，过程基本上是重复的。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 313.56,
    "end": 315.69
  },
  {
    "input": " You go back and forth between attention blocks and multi-layer perceptron blocks, ",
    "translatedText": "你需要在注意力模块和多层感知机（MLP）模块之间不断切换，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 315.69,
    "end": 319.44
  },
  {
    "input": " until at the very end, ",
    "translatedText": "直到最后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 319.44,
    "end": 320.94
  },
  {
    "input": " the hope is that all ",
    "translatedText": "我们期望通过某种方式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 320.94,
    "end": 322.22
  },
  {
    "input": " of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
    "translatedText": "文章的核心意义已经被完全融入到序列的最后一个向量中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 322.22,
    "end": 328.84
  },
  {
    "input": " We then perform a certain operation on that last vector ",
    "translatedText": "然后对这最后一个向量进行特定操作，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 328.84,
    "end": 331.8
  },
  {
    "input": " that produces a probability distribution over all possible tokens, ",
    "translatedText": "产生一个覆盖所有可能 Token 的概率分布，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 331.8,
    "end": 335.44
  },
  {
    "input": " all possible little chunks of text, that might come next.",
    "translatedText": "这些 Token 代表的是可能接下来出现的任何小段文本。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 335.44,
    "end": 338.99
  },
  {
    "input": " And like I said, ",
    "translatedText": "就像我说的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 338.99,
    "end": 339.82
  },
  {
    "input": " once you have a tool that predicts what comes next given a snippet of text, ",
    "translatedText": "一旦拥有了这样一个工具，它可以根据一小段文本预测下一步，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 339.82,
    "end": 343.62
  },
  {
    "input": " you can feed it a little bit ",
    "translatedText": "你就可以给它输入一些初始文本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 343.62,
    "end": 344.7
  },
  {
    "input": " of seed text and have it repeatedly play this game of predicting what comes next, ",
    "translatedText": "让它不断地进行预测下一步，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 344.7,
    "end": 349.22
  },
  {
    "input": " sampling from the distribution, appending it, ",
    "translatedText": "从概率分布中抽样，添加到现有文本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 349.22,
    "end": 351.44
  },
  {
    "input": " and then repeating over and over.",
    "translatedText": "然后不断重复这个过程。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 351.44,
    "end": 353.59
  },
  {
    "input": " Some of you in the know may remember how long before ",
    "translatedText": "了解这一点的人可能还记得， 早在",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 353.59,
    "end": 356.07
  },
  {
    "input": " ChatGPT came into the scene, ",
    "translatedText": "ChatGPT 出现之前，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 356.07,
    "end": 358.14
  },
  {
    "input": " this is what early demos of GPT-3 looked like.",
    "translatedText": "GPT-3 的早期演示就是这样的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 358.14,
    "end": 360.88
  },
  {
    "input": " You would have it autocomplete stories and essays based on an initial snippet.",
    "translatedText": "根据一段起始文本自动补全故事和文章。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 360.88,
    "end": 364.56
  },
  {
    "input": " To make a tool like this into a chatbot, ",
    "translatedText": "将这样的工具转化为聊天机器人的一个简单方法是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 364.56,
    "end": 368.17
  },
  {
    "input": " the easiest starting point is to have a little bit of text ",
    "translatedText": "就是准备一段文本，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 368.17,
    "end": 370.6
  },
  {
    "input": " that establishes the setting of a user interacting with a helpful AI assistant, ",
    "translatedText": "设定出用户与一个有帮助的 AI 助手交互的场景，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 370.6,
    "end": 375.04
  },
  {
    "input": " what you would call the system prompt, ",
    "translatedText": "这就是所谓的系统提示。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 375.04,
    "end": 376.96
  },
  {
    "input": " and then you would use the user's initial question ",
    "translatedText": "然后，你可以利用用户的初始问题",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 376.96,
    "end": 379.55
  },
  {
    "input": " or prompt as the first bit of dialogue, ",
    "translatedText": "或提示词作为对话的开头，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 379.55,
    "end": 381.64
  },
  {
    "input": " and then you have it start predicting ",
    "translatedText": "接着让 AI 开始预测",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 381.64,
    "end": 383.58
  },
  {
    "input": " what such a helpful AI assistant would say in response.",
    "translatedText": "这个有用的 AI 助手会如何进行回应。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 383.58,
    "end": 386.73
  },
  {
    "input": " There is more to say about an added step of training that's required to make this work well, ",
    "translatedText": "为了使这个过程运行得更好，还需要额外的训练步骤，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 386.73,
    "end": 391.81
  },
  {
    "input": " but at a high level this is the general idea.",
    "translatedText": "不过总的来说，这就是基本的思路。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 391.81,
    "end": 393.99
  },
  {
    "input": " In this chapter you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, ",
    "translatedText": "在这一章节中，我们将深入讨论网络开始和结束时发生的事情，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 393.99,
    "end": 402.67
  },
  {
    "input": " and I also want to spend a lot of time reviewing ",
    "translatedText": "同时，我也会花大量的时间",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 402.67,
    "end": 404.61
  },
  {
    "input": " some important bits of background knowledge, ",
    "translatedText": "回顾一些重要的背景知识，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 404.61,
    "end": 406.9
  },
  {
    "input": " things that would have been second nature to any machine learning engineer ",
    "translatedText": "这些知识对于熟悉 Transformer 的机器学习工程师来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 406.9,
    "end": 410.8
  },
  {
    "input": " by the time transformers came around.",
    "translatedText": "都是基础常识。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 410.8,
    "end": 412.4
  },
  {
    "input": " If you're comfortable with that background knowledge and a little impatient, ",
    "translatedText": "如果你已经对背景知识比较熟悉，而且迫不及待想要了解更多，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 412.4,
    "end": 415.71
  },
  {
    "input": " you could probably feel free to skip to the next chapter, ",
    "translatedText": "那么你可以选择直接跳到下一章节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 415.71,
    "end": 417.91
  },
  {
    "input": " which is going to focus on the attention blocks, generally considered the heart of the transformer.",
    "translatedText": "这一章节将会关注 Transformer 的核心部分，即注意力模块。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 417.91,
    "end": 423.49
  },
  {
    "input": " After that I want to talk more about these multi-layer perceptron blocks, ",
    "translatedText": "在这之后，我还会详细讨论多层感知机模块，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 423.49,
    "end": 427.04
  },
  {
    "input": " how training works, ",
    "translatedText": "训练过程，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 427.04,
    "end": 427.91
  },
  {
    "input": " and a number of other details that will have been skipped up to that point.",
    "translatedText": "以及之前被省略的其他一些细节。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 427.91,
    "end": 432.05
  },
  {
    "input": " For broader context, ",
    "translatedText": "作为背景信息，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 432.05,
    "end": 433.19
  },
  {
    "input": " these videos are additions to a mini-series about deep learning, ",
    "translatedText": "这些视频是我们的深度学习系列课程的补充部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 433.19,
    "end": 436.26
  },
  {
    "input": " and it's okay if you haven't watched the previous ones, I think you can do it out of order, ",
    "translatedText": "你不一定非得按照顺序来看，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 436.26,
    "end": 440.73
  },
  {
    "input": " but before diving into transformers specifically, ",
    "translatedText": "但在深入研究 Transformer 之前，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 440.73,
    "end": 443.46
  },
  {
    "input": " I do think it's worth making sure ",
    "translatedText": "我认为有必要确保",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 443.46,
    "end": 444.83
  },
  {
    "input": " that we're on the same page about the basic premise and structure of deep learning.",
    "translatedText": "我们对深度学习的基本概念和架构有共同的理解。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 444.83,
    "end": 448.89
  },
  {
    "input": " At the risk of stating the obvious, this is one approach to machine learning, ",
    "translatedText": "这里需要明确的是，机器学习是一种方法论，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 448.89,
    "end": 453.19
  },
  {
    "input": " which describes any model where you're using data to somehow determine how a model behaves.",
    "translatedText": "它涉及到使用数据来指导模型的行为模式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 453.19,
    "end": 458.15
  },
  {
    "input": " What I mean by that is, ",
    "translatedText": "具体来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 458.15,
    "end": 459.99
  },
  {
    "input": " let's say you want a function that takes in an image ",
    "translatedText": "你可能需要一个函数，输入一张图片， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 459.99,
    "end": 462.36
  },
  {
    "input": " and it produces a label describing it, ",
    "translatedText": "输出对应的标签描述，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 462.36,
    "end": 464.47
  },
  {
    "input": " or our example of predicting the next word given a passage of text, ",
    "translatedText": "或者预测给定文本片段的下一个单词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 464.47,
    "end": 468.28
  },
  {
    "input": " or any other task that seems to require some element of intuition and pattern recognition.",
    "translatedText": "或者其他需要直觉和模式识别的任务，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 468.28,
    "end": 472.97
  },
  {
    "input": " We almost take this for granted these days, ",
    "translatedText": "虽然我们现在已经习以为常，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 472.97,
    "end": 475.04
  },
  {
    "input": " but the idea with machine learning is that rather than trying ",
    "translatedText": "机器学习的核心思想在于，我们不再尝试",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 475.04,
    "end": 477.48
  },
  {
    "input": " to explicitly define a procedure for how to do that task in code, ",
    "translatedText": "去编写固定的程序来完成这些任务，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 477.48,
    "end": 480.67
  },
  {
    "input": " which is what people would have done in the earliest days of AI, ",
    "translatedText": "这是在 AI 的最早阶段人们会做的事情。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 480.67,
    "end": 484.02
  },
  {
    "input": " instead you set up a very flexible structure with tunable parameters, ",
    "translatedText": "而是构建一个具有可调节参数的灵活结构，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 484.02,
    "end": 488.08
  },
  {
    "input": " like a bunch of knobs and dials, ",
    "translatedText": "就像一系列的旋钮和调节器，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 488.08,
    "end": 490.16
  },
  {
    "input": " and then somehow you use many examples of what the output should look like for a given input ",
    "translatedText": "然后通过大量实例输入和期望输出的学习，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 490.16,
    "end": 495.4
  },
  {
    "input": " to tweak and tune the values of those parameters ",
    "translatedText": "调整和微调参数的值，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 495.4,
    "end": 498.25
  },
  {
    "input": " to mimic this behavior.",
    "translatedText": "以此来模拟这种直觉行为。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 498.25,
    "end": 500.16
  },
  {
    "input": " For example, maybe the simplest form of machine learning is linear regression, ",
    "translatedText": "比如，可能最直观的机器学习入门模型就是线性回归了，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 500.16,
    "end": 504.9
  },
  {
    "input": " where your inputs and your outputs are each single numbers, ",
    "translatedText": "这里你把输入和输出都视为单个数字，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 504.9,
    "end": 507.57
  },
  {
    "input": " something like the square footage of a house and its price, ",
    "translatedText": "如房子的面积和价格，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 507.57,
    "end": 510.73
  },
  {
    "input": " and what you want is to find a line of best fit through this data, you know, ",
    "translatedText": "你要做的，就是找出一条最拟合这些数据的直线，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 510.73,
    "end": 515.12
  },
  {
    "input": " to predict future house prices.",
    "translatedText": "以此来预测将来的房价。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 515.12,
    "end": 517.46
  },
  {
    "input": " That line is described by two continuous parameters, ",
    "translatedText": "这条线由两个连续的参数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 517.46,
    "end": 520.4
  },
  {
    "input": " say the slope and the y-intercept, ",
    "translatedText": "即斜率和 y 轴截距。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 520.4,
    "end": 522.39
  },
  {
    "input": " and the goal of linear regression is ",
    "translatedText": "线性回归的目标就是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 522.39,
    "end": 524.76
  },
  {
    "input": " to determine those parameters to closely match the data.",
    "translatedText": "确定这些参数以尽可能匹配数据。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 524.76,
    "end": 528.91
  },
  {
    "input": " Needless to say, deep learning models get much more complicated.",
    "translatedText": "不用说，深度学习模型会更复杂。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 528.91,
    "end": 532.4
  },
  {
    "input": " GPT-3, for example, has not two, but 175 billion parameters.",
    "translatedText": "比如，GPT-3 就拥有 1750 亿个参数，而不仅仅是两个。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 532.4,
    "end": 538.12
  },
  {
    "input": " But here's the thing, ",
    "translatedText": "但值得注意的是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 538.12,
    "end": 539.2
  },
  {
    "input": " it's not a given that you can create some giant model with a huge number of parameters ",
    "translatedText": "并不是简单地构建一个参数众多的庞大模型就能有效工作，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 539.2,
    "end": 543.76
  },
  {
    "input": " without it either grossly overfitting the training data ",
    "translatedText": "这样做可能会导致模型严重过拟合训练数据，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 544.01,
    "end": 547.12
  },
  {
    "input": " or being completely intractable to train.",
    "translatedText": "或者训练起来极其困难。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 547.12,
    "end": 549.44
  },
  {
    "input": " Deep learning describes a class of models ",
    "translatedText": "深度学习涵盖了一系列",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 549.44,
    "end": 552.12
  },
  {
    "input": " that in the last couple decades have proven to scale remarkably well.",
    "translatedText": "在过去几十年里证明了具有出色扩展能力的模型类别。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 552.12,
    "end": 556.2
  },
  {
    "input": " What unifies them is that they all use the same training algorithm, ",
    "translatedText": "它们之所以能够成功，关键在于都采用了相同的训练算法：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 556.2,
    "end": 559.52
  },
  {
    "input": " it's called backpropagation, we talked about it in previous chapters, ",
    "translatedText": "即反向传播，我们在前面的章节已经介绍过这一点。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 559.52,
    "end": 563.14
  },
  {
    "input": " and the context that I want you to have as we go in is ",
    "translatedText": "你需要理解的是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 563.14,
    "end": 565.23
  },
  {
    "input": " that in order for this training algorithm to work well at scale, ",
    "translatedText": "要让这种训练算法能在大规模应用中顺利进行，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 565.23,
    "end": 568.56
  },
  {
    "input": " these models have to follow a certain specific format.",
    "translatedText": "模型必须遵循一种特定的结构。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 568.8,
    "end": 571.83
  },
  {
    "input": " And if you know this format going in, ",
    "translatedText": "如果你对这种结构有所了解，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 571.83,
    "end": 573.76
  },
  {
    "input": " it helps to explain many of the choices for how a transformer processes language, ",
    "translatedText": "就能更好地理解 Transformer 处理语言的方式及其背后的逻辑，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 573.76,
    "end": 577.76
  },
  {
    "input": " which otherwise run the risk of feeling kind of arbitrary.",
    "translatedText": "否则某些设计选择可能显得有些随意。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 577.76,
    "end": 580.56
  },
  {
    "input": " First, whatever kind of model you're making, ",
    "translatedText": "首先，不管你构建的是哪种模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 580.56,
    "end": 583.6
  },
  {
    "input": " the input has to be formatted as an array of real numbers.",
    "translatedText": "输入都必须是一个实数数组。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 583.6,
    "end": 586.69
  },
  {
    "input": " This could simply mean a list of numbers, it could be a two-dimensional array, ",
    "translatedText": "这可能只是一个数字列表，也可能是一个二维数组，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 586.69,
    "end": 591.04
  },
  {
    "input": " or very often you deal with higher dimensional arrays, ",
    "translatedText": "或者更常见的是更高维度的数组，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 591.04,
    "end": 593.76
  },
  {
    "input": " where the general term used is tensor.",
    "translatedText": "这种通用的术语我们称之为张量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 593.76,
    "end": 596.32
  },
  {
    "input": " You often think of that input data as being progressively transformed into many distinct layers, ",
    "translatedText": "这些输入数据通常会被逐步转换成多个不同的层，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 596.51,
    "end": 601.81
  },
  {
    "input": " where again, each layer is always structured as some kind of array of real numbers ",
    "translatedText": "每一层都构成了实数数组，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 601.81,
    "end": 606
  },
  {
    "input": " until you get to a final layer, ",
    "translatedText": "直到最后一层，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 606,
    "end": 607.45
  },
  {
    "input": " which you consider the output.",
    "translatedText": "你可以将其视为输出层。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 607.45,
    "end": 609.26
  },
  {
    "input": " For example, ",
    "translatedText": "例如，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 609.26,
    "end": 609.68
  },
  {
    "input": " the final layer in our text processing model is a list of numbers representing ",
    "translatedText": "我们的文本处理模型的最终输出层是一个数字列表，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 609.68,
    "end": 613.7
  },
  {
    "input": " the probability distribution for all possible next tokens.",
    "translatedText": "这些数字代表了所有可能的下一词汇的概率分布。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 613.7,
    "end": 617.1
  },
  {
    "input": " In deep learning, ",
    "translatedText": "在深度学习领域，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 617.1,
    "end": 618.17
  },
  {
    "input": " these model parameters are almost always referred to as weights.",
    "translatedText": "这些模型的参数通常被称作权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 618.17,
    "end": 621.6
  },
  {
    "input": " And this is because a key feature of these models ",
    "translatedText": "这样称呼的原因是，这些模型的一个核心特点是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 621.6,
    "end": 624.13
  },
  {
    "input": " is that the only way these parameters interact ",
    "translatedText": "这些参数与正在处理的数据之间的唯一交互方式",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 624.13,
    "end": 626.72
  },
  {
    "input": " with the data being processed is through weighted sums.",
    "translatedText": "就是通过权重和。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 626.72,
    "end": 629.96
  },
  {
    "input": " You also sprinkle some nonlinear functions throughout, ",
    "translatedText": "虽然模型中也会穿插一些非线性函数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 629.96,
    "end": 632.84
  },
  {
    "input": " but they won't depend on parameters.",
    "translatedText": "但它们并不依赖于这些参数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 632.84,
    "end": 635.14
  },
  {
    "input": " Typically, though, ",
    "translatedText": "通常来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 635.14,
    "end": 636.2
  },
  {
    "input": " instead of seeing the weighted sums all naked and written out explicitly like this, ",
    "translatedText": "我们不会直接看到这些权重和的裸露形式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 636.2,
    "end": 640.06
  },
  {
    "input": " you'll instead find them packaged together as various components in a matrix vector product.",
    "translatedText": "而是会发现它们被作为矩阵向量乘积的不同部分封装起来。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 640.06,
    "end": 645.51
  },
  {
    "input": " It amounts to saying the same thing.",
    "translatedText": "这其实是在表达同一种概念。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 645.51,
    "end": 648.18
  },
  {
    "input": " If you think back to how matrix vector multiplication works, ",
    "translatedText": "如果你回想一下矩阵向量乘法是如何运作的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 648.18,
    "end": 651.03
  },
  {
    "input": " each component in the output looks like a weighted sum.",
    "translatedText": "输出中的每一部分都像是一个权重和。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 651.03,
    "end": 654.5
  },
  {
    "input": " It's just often conceptually cleaner for you and me ",
    "translatedText": "更直观的方式是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 654.5,
    "end": 657.06
  },
  {
    "input": " to think about matrices that are filled with tunable parameters ",
    "translatedText": "将这些可调参数填充的矩阵想象成",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 657.06,
    "end": 661.34
  },
  {
    "input": " that transform vectors that are drawn from the data being processed.",
    "translatedText": "对处理中数据进行向量转换的工具。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 661.34,
    "end": 666.24
  },
  {
    "input": " For example, those 175 billion weights in GPT-3 ",
    "translatedText": "例如，GPT-3 中的那 1750 亿个权重",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 666.24,
    "end": 670.51
  },
  {
    "input": " are organized into just under 28,000 distinct matrices.",
    "translatedText": "就被组织在大约 28,000 个不同的矩阵中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 670.51,
    "end": 674.33
  },
  {
    "input": " Those matrices in turn fall into eight different categories, ",
    "translatedText": "这些矩阵又被分为八个不同的类别，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 674.33,
    "end": 677.52
  },
  {
    "input": " and what you and I are going to do ",
    "translatedText": "你和我将要做的就是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 677.52,
    "end": 678.9
  },
  {
    "input": " is step through each one of those categories ",
    "translatedText": "逐一理解这些类别，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 678.9,
    "end": 680.8
  },
  {
    "input": " to understand what that type does.",
    "translatedText": "了解每种类型的功能。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 680.8,
    "end": 682.54
  },
  {
    "input": " As we go through, I think it's kind of fun ",
    "translatedText": "接下来的过程会非常有趣，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 682.54,
    "end": 684.47
  },
  {
    "input": " to reference the specific numbers from GPT-3 ",
    "translatedText": "我们将参考 GPT-3 的具体数据",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 684.47,
    "end": 687.12
  },
  {
    "input": " to count up exactly where those 175 billion come from.",
    "translatedText": "来统计这 1750 亿是如何分配的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 687.12,
    "end": 691.68
  },
  {
    "input": " Even if nowadays there are bigger and better models, ",
    "translatedText": "即使现在有更大更好的模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 691.68,
    "end": 694.72
  },
  {
    "input": " this one has a certain charm ",
    "translatedText": "GPT-3 模型仍具有独特的魅力，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 694.72,
    "end": 695.78
  },
  {
    "input": " as the first large language model to really capture the world's attention ",
    "translatedText": "作为第一个引发全球关注的大语言模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 695.78,
    "end": 699.35
  },
  {
    "input": " outside of ML communities.",
    "translatedText": "影响力并未局限于机器学习社区。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 699.35,
    "end": 700.86
  },
  {
    "input": " Also, practically speaking, ",
    "translatedText": "实际上，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 700.86,
    "end": 702.48
  },
  {
    "input": " companies tend to keep much tighter lips around the specific numbers for more modern networks.",
    "translatedText": "对于更现代的模型，公司往往对具体的数据保持更严格的保密。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 702.48,
    "end": 707.12
  },
  {
    "input": " I just want to set the scene going in, ",
    "translatedText": "在这里，我想说明的是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 707.12,
    "end": 708.82
  },
  {
    "input": " that as you peek under the hood to see what happens inside a tool like ChatGPT, ",
    "translatedText": "当你深入探索像 ChatGPT 这样的工具的内部机制时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 708.82,
    "end": 712.96
  },
  {
    "input": " almost all of the actual computation looks like matrix-vector multiplication.",
    "translatedText": "你会发现几乎所有的计算过程都体现为矩阵和向量的乘积。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 712.96,
    "end": 717.97
  },
  {
    "input": " There's a little bit of a risk getting lost in the sea of billions of numbers, ",
    "translatedText": "在这海量的数字中，很容易迷失方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 717.97,
    "end": 721.44
  },
  {
    "input": " but you should draw a very sharp distinction in your mind between the weights of the model, ",
    "translatedText": "但你需要在心中清楚地区分两个概念：模型的权重 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 721.44,
    "end": 726.4
  },
  {
    "input": " which I'll always color in blue or red, and the data being processed, ",
    "translatedText": "（我将用蓝色或红色表示）和正在处理的数据",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 726.4,
    "end": 730.36
  },
  {
    "input": " which I'll always color in gray.",
    "translatedText": "（我将用灰色表示）。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 730.36,
    "end": 731.88
  },
  {
    "input": " The weights are the actual brains.",
    "translatedText": "权重就是模型的“大脑”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 731.88,
    "end": 733.88
  },
  {
    "input": " They are the things learned during training, and they determine how it behaves.",
    "translatedText": "这些是在训练过程中学习到的，它们决定了模型的行为模式。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 733.88,
    "end": 738.4
  },
  {
    "input": " The data being processed simply encodes ",
    "translatedText": "正在处理的数据仅仅是编码了",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 738.4,
    "end": 740.64
  },
  {
    "input": " whatever specific input is fed into the model for a given run, ",
    "translatedText": "某次操作中模型接收的具体输入， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 740.64,
    "end": 744.34
  },
  {
    "input": " like an example snippet of text.",
    "translatedText": "比如一段文本示例。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 744.34,
    "end": 746.4
  },
  {
    "input": " With all of that as foundation, ",
    "translatedText": "理解了上述基础知识后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 746.4,
    "end": 749.04
  },
  {
    "input": " let's dig into the first step of this text processing example, ",
    "translatedText": "让我们探讨文本处理示例的第一步：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 749.04,
    "end": 752.26
  },
  {
    "input": " which is to break up the input into little chunks ",
    "translatedText": "将输入分割成小片段，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 752.26,
    "end": 754.78
  },
  {
    "input": " and turn those chunks into vectors.",
    "translatedText": "并将这些片段转换成向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 754.78,
    "end": 756.62
  },
  {
    "input": " I mentioned how those chunks are called tokens, ",
    "translatedText": "我之前提到过，这些小片段被称为 Tokens，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 756.62,
    "end": 758.96
  },
  {
    "input": " which might be pieces of words or punctuation, ",
    "translatedText": "它们可能是单词的一部分或是标点符号，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 758.96,
    "end": 761.26
  },
  {
    "input": " but every now and then in this chapter and especially in the next one, ",
    "translatedText": "但在本章，特别是在下一章中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 761.26,
    "end": 764.75
  },
  {
    "input": " I'd like to just pretend that it's broken more cleanly into words.",
    "translatedText": "我倾向于简化理解，假设它们完整地对应于单词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 764.75,
    "end": 768.65
  },
  {
    "input": " Because we humans think in words, ",
    "translatedText": "因为我们人类用单词来思考，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 768.65,
    "end": 769.98
  },
  {
    "input": " this will just make it much easier to reference little examples and clarify each step.",
    "translatedText": "通过参考小例子和解释每一步可以使这个过程更加容易理解。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 769.98,
    "end": 774.18
  },
  {
    "input": " The model has a predefined vocabulary, some list of all possible words, ",
    "translatedText": "模型拥有一个预设的词汇库，包含所有可能的单词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 774.18,
    "end": 778.9
  },
  {
    "input": " say 50,000 of them, and the first matrix that we'll encounter, ",
    "translatedText": "比如说有 50,000 个。我们将首先遇到的一个矩阵",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 778.9,
    "end": 783.34
  },
  {
    "input": " known as the embedding matrix, ",
    "translatedText": "叫做嵌入矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 783.34,
    "end": 785.19
  },
  {
    "input": " has a single column for each one of these words.",
    "translatedText": "它为每个单词都分配了一个独立的列。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 785.19,
    "end": 788.45
  },
  {
    "input": " These columns are what determines what vector each word turns into in that first step.",
    "translatedText": "这些列定义了第一步中每个单词转换成的向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 788.45,
    "end": 794.39
  },
  {
    "input": " We label it we, and like all the matrices we see, ",
    "translatedText": "我们将其称为 we，就像我们看到的所有其他矩阵一样，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 794.39,
    "end": 798.65
  },
  {
    "input": " its values begin random but they're going to be learned based on data.",
    "translatedText": "它的初始值是随机的，但基于数据进行学习和调整。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 798.65,
    "end": 803.53
  },
  {
    "input": " Turning words into vectors was common practice in machine learning long before transformers, ",
    "translatedText": "在 Transformers 出现之前，机器学习中就已经普遍采用了将单词转换为向量的做法，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 803.53,
    "end": 808.53
  },
  {
    "input": " but it's a little weird if you've never seen it before, ",
    "translatedText": "虽然这对于初次接触的人来说可能有些奇怪，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 808.53,
    "end": 811.12
  },
  {
    "input": " and it sets the foundation for everything that follows, ",
    "translatedText": "但它为接下来的一切建立了基础，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 811.12,
    "end": 813.55
  },
  {
    "input": " so let's take a moment to get familiar with it.",
    "translatedText": "因此，我们需要花一些时间来熟悉它。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 813.55,
    "end": 815.77
  },
  {
    "input": " We often call this embedding a word, ",
    "translatedText": "我们通常称这种转换为词嵌入，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 815.77,
    "end": 817.56
  },
  {
    "input": " which invites you to think of these vectors very geometrically ",
    "translatedText": "这种表述让你可以从几何的角度去理解这些向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 817.56,
    "end": 820.98
  },
  {
    "input": " as points in some high dimensional space.",
    "translatedText": "把它们想象成高维空间中的点。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 820.98,
    "end": 823.35
  },
  {
    "input": " Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, ",
    "translatedText": "将三个数字看作是三维空间中的坐标点很简单，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 823.35,
    "end": 828.77
  },
  {
    "input": " but word embeddings tend to be much much higher dimensional.",
    "translatedText": "但词向量的维度远远超出这个范畴。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 828.77,
    "end": 832.4
  },
  {
    "input": " In GPT-3, they have 12,288 dimensions, and as you'll see, ",
    "translatedText": "在 GPT-3 中，它们的维度高达 12,288，如你所见，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 832.4,
    "end": 836.96
  },
  {
    "input": " it matters to work in a space that has a lot of distinct directions.",
    "translatedText": "选择一个有很多不同方向的空间进行工作是很重要的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 836.96,
    "end": 840.4
  },
  {
    "input": " In the same way that you could take a two-dimensional slice through a 3D space ",
    "translatedText": "就像你可以在三维空间中选择一个二维切片，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 840.4,
    "end": 844.81
  },
  {
    "input": " and project all the points onto that slice, ",
    "translatedText": "并将所有点投影到这个切片上一样，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 844.81,
    "end": 847.6
  },
  {
    "input": " for the sake of animating word embeddings that a simple model is giving me, ",
    "translatedText": "为了让简单模型输出的词向量能够被动态展示，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 847.76,
    "end": 851.12
  },
  {
    "input": " I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space ",
    "translatedText": "我采取了相似的方法，选择了一个高维空间中的三维“切片”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 851.12,
    "end": 856.8
  },
  {
    "input": " and projecting the word vectors down onto that and displaying the results.",
    "translatedText": "并将词向量映射到这个切片上来展示它们。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 856.8,
    "end": 860.42
  },
  {
    "input": " The big idea here is that as a model tweaks and tunes its weights ",
    "translatedText": "这里的关键思想是，模型在训练过程中调整和微调权重，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 860.42,
    "end": 864.85
  },
  {
    "input": " to determine how exactly words get embedded as vectors during training, ",
    "translatedText": " 以确定词具体如何被嵌入为向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 864.85,
    "end": 869.08
  },
  {
    "input": " it tends to settle on a set of embeddings ",
    "translatedText": "它会倾向于找到一组嵌入，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 869.08,
    "end": 871
  },
  {
    "input": " where directions in the space have a kind of semantic meaning.",
    "translatedText": "使得这个空间中的方向含有特定的语义意义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 871,
    "end": 874.61
  },
  {
    "input": " For the simple word-to-vector model I'm running here, ",
    "translatedText": "对于我目前运行的这个简单的词向量模型来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 874.61,
    "end": 877.52
  },
  {
    "input": " if I run a search for all the words whose embeddings are closest to that of \"tower\", ",
    "translatedText": "如果进行搜索，找到所有与“塔楼”最相似的词向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 877.52,
    "end": 882.08
  },
  {
    "input": " you'll notice how they all seem to give very similar tower-ish vibes.",
    "translatedText": "你会发现这些词都有着类似的“塔楼感”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 882.08,
    "end": 886.22
  },
  {
    "input": " And if you want to pull up some Python and play along at home, ",
    "translatedText": "如果你想在家里用 Python 试一试，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 886.22,
    "end": 888.58
  },
  {
    "input": " this is the specific model that I'm using to make the animations.",
    "translatedText": "这就是我用来制作动画的模型。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 888.58,
    "end": 891.57
  },
  {
    "input": " It's not a transformer, ",
    "translatedText": "虽然它不是一个 Transformer 模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 891.57,
    "end": 892.79
  },
  {
    "input": " but it's enough to illustrate the idea ",
    "translatedText": "但足以说明一个观点：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 892.79,
    "end": 894.68
  },
  {
    "input": " that directions in the space can carry semantic meaning.",
    "translatedText": "空间中的方向能够传达特定的语义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 894.68,
    "end": 898.19
  },
  {
    "input": " A very classic example of this is ",
    "translatedText": "一个经典的例子是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 898.19,
    "end": 900.06
  },
  {
    "input": " how if you take the difference between the vectors for woman and man, ",
    "translatedText": "如果你计算“女人”和“男人”向量之间的差值，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 900.06,
    "end": 903.85
  },
  {
    "input": " something you would visualize as a little vector in the space ",
    "translatedText": "你会发现这个差异可以被可视化为空间中的一个小向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 903.85,
    "end": 906.82
  },
  {
    "input": " connecting the tip of one to the tip of the other, ",
    "translatedText": "连接一个词的尖端和另一个词的尖端，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 906.82,
    "end": 909.6
  },
  {
    "input": " it's very similar to the difference between king and queen.",
    "translatedText": "这个差异与“国王”和“女王”之间的差值非常相似。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 909.6,
    "end": 913.59
  },
  {
    "input": " So let's say you didn't know the word for a female monarch, ",
    "translatedText": "所以假设你不知道表示“女性君主”的词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 913.59,
    "end": 918.02
  },
  {
    "input": " you could find it by taking king, adding this woman minus man direction, ",
    "translatedText": "你可以通过向“国王”向量添加“女人减男人”的方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 918.02,
    "end": 922.08
  },
  {
    "input": " and searching for the embeddings closest to that point.",
    "translatedText": "并搜索最接近这个点的词向量来找到它。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 922.08,
    "end": 925.42
  },
  {
    "input": " At least, kind of.",
    "translatedText": "至少在理论上是这样。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 925.42,
    "end": 926.98
  },
  {
    "input": " Despite this being a classic example for the model I'm playing with, ",
    "translatedText": "虽然这是我正在使用的模型的一个经典例子，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 926.98,
    "end": 931.2
  },
  {
    "input": " the true embedding of queen is actually a little farther off than this would suggest, ",
    "translatedText": "但实际上，真正的“女王”嵌入实际上比这种方法预想的要远一些，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 931.24,
    "end": 935.3
  },
  {
    "input": " presumably because the way queen is used in training data ",
    "translatedText": "这可能是因为在训练数据中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 935.3,
    "end": 938.39
  },
  {
    "input": " is not merely a feminine version of king.",
    "translatedText": "“女王”并不仅仅是“国王”的女性版本。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 938.39,
    "end": 940.72
  },
  {
    "input": " When I played around, ",
    "translatedText": "深入挖掘时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 940.72,
    "end": 942.58
  },
  {
    "input": " family relations seemed to illustrate the idea much better.",
    "translatedText": "我发现通过家族关系来解释这一现象似乎更为恰当。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 942.58,
    "end": 946.2
  },
  {
    "input": " The point is, it looks like during training, ",
    "translatedText": "关键在于，在训练过程中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 946.2,
    "end": 948.26
  },
  {
    "input": " the model found it advantageous to choose embeddings ",
    "translatedText": "模型发现采用这样的嵌入方式更为有利，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 948.26,
    "end": 950.77
  },
  {
    "input": " such that one direction in this space encodes gender information.",
    "translatedText": "即这个空间中的一个方向能够编码性别信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 950.77,
    "end": 955.41
  },
  {
    "input": " Another example is that if you take the embedding of Italy ",
    "translatedText": "另一个例子是，如果你从“意大利”的向量表示中",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 955.41,
    "end": 959.21
  },
  {
    "input": " and you subtract the embedding of Germany, ",
    "translatedText": "减去“德国”的向量表示，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 959.21,
    "end": 961.66
  },
  {
    "input": " and then you add that to the embedding of Hitler, ",
    "translatedText": "再加上“希特勒”的向量表示，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 961.66,
    "end": 964.8
  },
  {
    "input": " you get something very close to the embedding of Mussolini.",
    "translatedText": "结果非常接近于“墨索里尼”的向量表示。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 964.8,
    "end": 968.5
  },
  {
    "input": " It's as if the model learned to associate some directions with Italian-ness ",
    "translatedText": "这就好像模型学会了将某些方向与“意大利”特性相关联，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 968.5,
    "end": 973.1
  },
  {
    "input": " and others with WWII axis leaders.",
    "translatedText": "而将其他方向与二战轴心国的领导人相关联。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 973.1,
    "end": 975.59
  },
  {
    "input": " Maybe my favorite example in this vein is how in some models, ",
    "translatedText": "我个人最喜欢的一个例子是，在某些模型中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 975.59,
    "end": 979.41
  },
  {
    "input": " if you take the difference between Germany and Japan and add it to sushi, ",
    "translatedText": "如果你计算“德国”和“日本”的向量差值，然后加上“寿司”的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 979.41,
    "end": 983.93
  },
  {
    "input": " you end up very close to bratwurst.",
    "translatedText": "你得到的结果会非常接近“德国香肠”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 983.93,
    "end": 986.29
  },
  {
    "input": " Also in playing this game of finding nearest neighbors, ",
    "translatedText": "此外，在寻找最近邻居的过程中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 986.29,
    "end": 989.98
  },
  {
    "input": " I was very pleased to see how close cat was to both beast and monster.",
    "translatedText": "我还惊喜地发现“猫”离“野兽”和“怪物”都很近。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 989.98,
    "end": 993.84
  },
  {
    "input": " One bit of mathematical intuition that's helpful to have in mind, ",
    "translatedText": "有一个有用的数学概念，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 993.84,
    "end": 997.38
  },
  {
    "input": " especially for the next chapter, ",
    "translatedText": "尤其对于接下来的章节非常重要，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 997.38,
    "end": 998.96
  },
  {
    "input": " is how the dot product of two vectors ",
    "translatedText": "那就是两个向量的点积",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 998.96,
    "end": 1000.78
  },
  {
    "input": " can be thought of as a way to measure how well they align.",
    "translatedText": "可以被视为一种衡量它们是否对齐的方法。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1000.78,
    "end": 1003.68
  },
  {
    "input": " Computationally, ",
    "translatedText": "从计算角度看，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1003.68,
    "end": 1004.92
  },
  {
    "input": " dot products involve multiplying all the corresponding components ",
    "translatedText": "点积涉及到逐一乘以对应元素，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1004.92,
    "end": 1009.12
  },
  {
    "input": " and then adding the results, ",
    "translatedText": "然后进行求和，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1009.12,
    "end": 1010.44
  },
  {
    "input": " which is good since so much of our computation has to look like weighted sums.",
    "translatedText": "这很好，因为我们的很多计算看起来就像是权重求和。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1010.44,
    "end": 1014.24
  },
  {
    "input": " Geometrically, ",
    "translatedText": "从几何角度来看，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1014.24,
    "end": 1015.4
  },
  {
    "input": " the dot product is positive when vectors point in similar directions, ",
    "translatedText": "当两个向量指向相似方向时，点积为正；",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1015.4,
    "end": 1020.24
  },
  {
    "input": " it's zero if they're perpendicular, ",
    "translatedText": "如果它们垂直，点积为零；",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1020.24,
    "end": 1022.23
  },
  {
    "input": " and it's negative whenever they point in opposite directions.",
    "translatedText": "当它们指向相反方向时，点积为负。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1022.23,
    "end": 1025.52
  },
  {
    "input": " For example, ",
    "translatedText": "例如，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1025.52,
    "end": 1026.59
  },
  {
    "input": " let's say you were playing with this model ",
    "translatedText": "假设你在测试这个模型，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1026.59,
    "end": 1029.23
  },
  {
    "input": " and you hypothesize that the embedding of cats minus cat ",
    "translatedText": "从“cats”（复数）的向量表示中减去“cat”（单数）的向量表示",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1029.23,
    "end": 1032.97
  },
  {
    "input": " might represent a sort of plurality direction in this space.",
    "translatedText": "可能会在这个空间中找到表示复数概念的方向。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1032.97,
    "end": 1037.03
  },
  {
    "input": " To test this, ",
    "translatedText": "为了验证这个观点，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1037.03,
    "end": 1037.76
  },
  {
    "input": " I'm going to take this vector ",
    "translatedText": "我将计算某个向量",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1037.76,
    "end": 1039.21
  },
  {
    "input": " and compute its dot product against the embeddings of certain singular nouns ",
    "translatedText": "与一些特定的单数名词嵌入的点积，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1039.21,
    "end": 1043.02
  },
  {
    "input": " and compare it to the dot products with the corresponding plural nouns.",
    "translatedText": "并将其与相应的复数名词的点积进行比较。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1043.02,
    "end": 1046.56
  },
  {
    "input": " If you play around with this, ",
    "translatedText": "如果你试一试，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1046.56,
    "end": 1048.03
  },
  {
    "input": " you'll notice that the plural ones ",
    "translatedText": "你会发现复数名词的点积值",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1048.03,
    "end": 1049.65
  },
  {
    "input": " do indeed seem to consistently give higher values than the singular ones, ",
    "translatedText": "通常比单数的更高，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1049.65,
    "end": 1053.67
  },
  {
    "input": " indicating that they align more with this direction.",
    "translatedText": "这表明它们在某种方向上的对齐更为紧密。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1053.67,
    "end": 1056.37
  },
  {
    "input": " It's also fun how if you take this dot product ",
    "translatedText": "更有趣的是，如果你将这个点积操作",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1056.37,
    "end": 1058.48
  },
  {
    "input": " with the embeddings of the words one, two, three, ",
    "translatedText": "应用到“一”、“二”、“三”等词汇的嵌入上，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1058.48,
    "end": 1061.44
  },
  {
    "input": " and so on, they give increasing values, ",
    "translatedText": "会发现得到的数值是逐渐增加的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1061.44,
    "end": 1063.94
  },
  {
    "input": " so it's as if we can quantitatively measure ",
    "translatedText": "就像我们能够量化地衡量",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1063.94,
    "end": 1066.72
  },
  {
    "input": " how plural the model finds a given word.",
    "translatedText": "模型认为一个词的\"复数程度\"。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1066.72,
    "end": 1068.95
  },
  {
    "input": " Again, the specifics for how words get embedded is learned using data.",
    "translatedText": "再次说明，单词的嵌入方式是通过数据学习得到的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1068.95,
    "end": 1073.64
  },
  {
    "input": " This embedding matrix, whose columns tell us what happens to each word, ",
    "translatedText": "这种嵌入矩阵揭示了每个词汇的变化过程，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1073.64,
    "end": 1077.6
  },
  {
    "input": " is the first pile of weights in our model, and using the GPT-3 numbers, ",
    "translatedText": "它是我们模型中的第一批权重，根据 GPT-3 的数据，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1077.6,
    "end": 1081.43
  },
  {
    "input": " the vocabulary size specifically is 50,257, and again, ",
    "translatedText": "其词汇量具体为 50,257，但要注意，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1081.43,
    "end": 1085.81
  },
  {
    "input": " technically this consists not of words per se, but of tokens.",
    "translatedText": "实际上它指的不是单词本身，而是 Tokens。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1085.81,
    "end": 1089.76
  },
  {
    "input": " And the embedding dimension is 12,288.",
    "translatedText": "嵌入的维度是 12,288。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1090.4,
    "end": 1093.05
  },
  {
    "input": " Multiplying those tells us this consists of about 617 million weights.",
    "translatedText": "将这两者相乘，我们得到大约有 6.17 亿个权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1093.05,
    "end": 1097.91
  },
  {
    "input": " Let's go ahead and add this to a running tally, ",
    "translatedText": "我们将这个数字加入到我们的累计计数中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1097.91,
    "end": 1100.05
  },
  {
    "input": " remembering that by the end we should count up to 175 billion.",
    "translatedText": "最后，我们应该得到 1750 亿个权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1100.05,
    "end": 1103.79
  },
  {
    "input": " In the case of transformers, ",
    "translatedText": "在谈到 transformer 时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1103.79,
    "end": 1105.58
  },
  {
    "input": " you really want to think of the vectors in this embedding space ",
    "translatedText": "你会想到这些嵌入空间中的向量",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1105.58,
    "end": 1109.37
  },
  {
    "input": " as not merely representing individual words.",
    "translatedText": "不仅仅代表着单个词汇。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1109.37,
    "end": 1111.96
  },
  {
    "input": " For one thing, they also encode information about the position of that word, ",
    "translatedText": "它们还携带了词汇位置的信息，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1111.96,
    "end": 1116.17
  },
  {
    "input": " which we'll talk about later.",
    "translatedText": "这一点我们稍后会详细说明。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1116.17,
    "end": 1117.97
  },
  {
    "input": " But more importantly, ",
    "translatedText": "但更关键的是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1117.97,
    "end": 1119.23
  },
  {
    "input": " you should think of them as having the capacity to soak in context.",
    "translatedText": "这些向量能够吸纳并反映语境。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1119.23,
    "end": 1123.34
  },
  {
    "input": " A vector that started its life as the embedding of the word \"king\", for example, ",
    "translatedText": "举个例子，一个最初代表“国王”的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1123.34,
    "end": 1127.24
  },
  {
    "input": " might progressively get tugged and pulled by various blocks in this network ",
    "translatedText": "在网络中的各个环节的作用下，可能会逐渐变化，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1127.24,
    "end": 1131.53
  },
  {
    "input": " so that by the end it points in a much more specific and nuanced direction ",
    "translatedText": "因此最后，它指向的方向会更具体、更微妙，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1131.53,
    "end": 1135.48
  },
  {
    "input": " that somehow encodes that it was a king who lived in Scotland, ",
    "translatedText": "以某种方式编码了一位生活在苏格兰的国王，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1135.48,
    "end": 1139.2
  },
  {
    "input": " and who had achieved his post after murdering the previous king, ",
    "translatedText": "在杀死前任国王后取得其职位的国王，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1139.2,
    "end": 1142.54
  },
  {
    "input": " and who's being described in Shakespearean language.",
    "translatedText": "此人的描绘方式充满了莎士比亚式的语言。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1142.54,
    "end": 1145.36
  },
  {
    "input": " Think about your own understanding of a given word.",
    "translatedText": "想想你对某个词汇的理解通常是怎样形成的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1145.36,
    "end": 1147.91
  },
  {
    "input": " The meaning of that word is clearly informed by the surroundings, ",
    "translatedText": "那个词的意义很大程度上是由其所处的环境决定的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1147.91,
    "end": 1151.76
  },
  {
    "input": " and sometimes this includes context from a long distance away.",
    "translatedText": "有时这甚至包括来自很远的上下文。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1151.76,
    "end": 1155.11
  },
  {
    "input": " So when putting together a model that has the ability to predict what word comes next, ",
    "translatedText": "因此，在构建一个能预测下一个词汇的模型时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1155.11,
    "end": 1159.6
  },
  {
    "input": " the goal is to somehow empower it to incorporate context efficiently.",
    "translatedText": "关键目标就是让它能够高效地融合上下文信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1159.6,
    "end": 1163.97
  },
  {
    "input": " To be clear, in that very first step, ",
    "translatedText": "明确一点，在第一步，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1163.97,
    "end": 1165.71
  },
  {
    "input": " when you create the array of vectors based on the input text, ",
    "translatedText": "当我们根据输入文本创建向量数组时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1165.71,
    "end": 1168.48
  },
  {
    "input": " each one of those is simply plucked out of the embedding matrix, ",
    "translatedText": "每个向量都是直接从嵌入矩阵中选取的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1168.48,
    "end": 1171.81
  },
  {
    "input": " so initially each one can only encode the meaning of a single word ",
    "translatedText": "这意味着，起初，每个向量仅能代表一个单词的含义，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1171.81,
    "end": 1174.87
  },
  {
    "input": " without any input from its surroundings.",
    "translatedText": "而不涉及其周边环境的信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1174.87,
    "end": 1176.87
  },
  {
    "input": " But you should think of the primary goal of this network that it flows through ",
    "translatedText": "但我们的主要目标是让这些向量通过网络传递，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1176.87,
    "end": 1181.58
  },
  {
    "input": " as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
    "translatedText": "使每一个向量都能获得比单个词更丰富、更具体的含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1181.58,
    "end": 1189.36
  },
  {
    "input": " The network can only process a fixed number of vectors at a time, ",
    "translatedText": "这个网络每次只能处理一定数量的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1189.36,
    "end": 1192.58
  },
  {
    "input": " known as its context size.",
    "translatedText": "这就是所谓的上下文大小。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1192.58,
    "end": 1194.23
  },
  {
    "input": " For GPT-3, it was trained with a context size of 2048, ",
    "translatedText": "对于 GPT-3，它的训练上下文大小为 2048，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1194.23,
    "end": 1197.95
  },
  {
    "input": " so the data flowing through the network ",
    "translatedText": "意味着数据在网络中流动时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1197.95,
    "end": 1200.24
  },
  {
    "input": " always looks like this array of 2048 columns, ",
    "translatedText": "总是看起来像一串 2048 列的数组， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1200.35,
    "end": 1202.87
  },
  {
    "input": " each of which has 12,000 dimensions.",
    "translatedText": "每一列都有 12000 个维度。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1202.87,
    "end": 1205.63
  },
  {
    "input": " This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
    "translatedText": "这个上下文大小限制了 Transformer 在预测下一个词的过程中可以纳入的文本量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1205.63,
    "end": 1211.9
  },
  {
    "input": " This is why long conversations with certain chatbots, ",
    "translatedText": "这就解释了为什么如果和一些聊天机器人",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1211.9,
    "end": 1214.88
  },
  {
    "input": " like the early versions of ChatGPT, ",
    "translatedText": "比如 ChatGPT 的早期版本",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1214.88,
    "end": 1216.52
  },
  {
    "input": " often gave the feeling of the bot kind of losing the thread of conversation ",
    "translatedText": "进行长对话时，你可能会感觉到机器人似乎在对话中迷失了方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1216.52,
    "end": 1220.71
  },
  {
    "input": " as you continued too long.",
    "translatedText": "尤其是当对话持续过长时。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1220.71,
    "end": 1222.07
  },
  {
    "input": " We'll go into the details of attention in due time, but skipping ahead, ",
    "translatedText": "我们会在适当的时候详细讨论注意力机制的细节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1222.07,
    "end": 1226
  },
  {
    "input": " I want to talk for a minute about what happens at the very end.",
    "translatedText": "但先让我们简单了解一下最终阶段的处理过程。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1226,
    "end": 1229.44
  },
  {
    "input": " Remember, ",
    "translatedText": "请记住，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1229.44,
    "end": 1229.77
  },
  {
    "input": " the desired output is a probability distribution over all tokens that might come next.",
    "translatedText": "最终的目标是产生一个概率分布，预测下一个可能出现的 Token。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1229.77,
    "end": 1235.21
  },
  {
    "input": " For example, if the very last word is \"professor\", ",
    "translatedText": "举例来说，如果最后一个词是“教授”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1235.21,
    "end": 1238.32
  },
  {
    "input": " and the context includes words like \"Harry Potter\", ",
    "translatedText": "上下文中包含了“哈利·波特”等词语，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1238.32,
    "end": 1241.29
  },
  {
    "input": " and immediately preceding we see \"least favorite teacher\", ",
    "translatedText": "紧接着出现的是“最不喜欢的老师”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1241.29,
    "end": 1244.8
  },
  {
    "input": " and also if you give me some leeway ",
    "translatedText": "如果允许我稍微发挥一下，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1244.8,
    "end": 1246.05
  },
  {
    "input": " by letting me pretend that tokens simply look like full words, ",
    "translatedText": "假设 Tokens 就是完整的单词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1246.05,
    "end": 1248.97
  },
  {
    "input": " then a well-trained network that had built up knowledge of Harry Potter ",
    "translatedText": "那么，一个经过良好训练并对哈利·波特世界有所了解的网络，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1248.97,
    "end": 1252.4
  },
  {
    "input": " would presumably assign a high number to the word \"Snape\".",
    "translatedText": "会很可能给“斯内普”这个词赋予一个较高的权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1252.4,
    "end": 1255.81
  },
  {
    "input": " This involves two different steps.",
    "translatedText": "此过程涉及两个不同的步骤。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1255.81,
    "end": 1258.36
  },
  {
    "input": " The first one is to use another matrix ",
    "translatedText": "首先，使用另一个矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1258.36,
    "end": 1260.76
  },
  {
    "input": " that maps the very last vector in that context ",
    "translatedText": "将上下文中的最后一个向量映射到",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1260.76,
    "end": 1263.44
  },
  {
    "input": " to a list of 50,000 values, ",
    "translatedText": "一个包含 50,000 个值的列表，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1263.44,
    "end": 1265.51
  },
  {
    "input": " one for each token in the vocabulary.",
    "translatedText": "词汇表中的每个 token 都对应一个值。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1265.51,
    "end": 1267.57
  },
  {
    "input": " Then there's a function that normalizes this into a probability distribution, ",
    "translatedText": "接着，通过一个函数，把这些值转换成概率分布。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1267.57,
    "end": 1272.08
  },
  {
    "input": " it's called softmax and we'll talk more about it in just a second, ",
    "translatedText": "这个函数叫 softmax，我们稍后会详细讨论。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1272.08,
    "end": 1275.41
  },
  {
    "input": " but before that it might seem a little bit weird ",
    "translatedText": "但在此之前，你可能会觉得，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1275.41,
    "end": 1277.91
  },
  {
    "input": " to only use this last embedding to make a prediction, ",
    "translatedText": "仅仅基于最后一个嵌入来做出预测似乎有些奇怪，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1277.91,
    "end": 1281.07
  },
  {
    "input": " when after all in that last step there are thousands of other vectors in the layer ",
    "translatedText": "毕竟在最后一层中还有成千上万的其他向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1281.07,
    "end": 1285.34
  },
  {
    "input": " just sitting there with their own context-rich meanings.",
    "translatedText": "每个向量都蕴含着丰富的上下文意义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1285.34,
    "end": 1288.84
  },
  {
    "input": " This has to do with the fact that in the training process ",
    "translatedText": "这是因为在训练过程中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1288.84,
    "end": 1291.38
  },
  {
    "input": " it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
    "translatedText": "如果我们利用最终层的每一个向量来预测其后可能出现的内容，被证明是更高效的方法。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1291.38,
    "end": 1300.16
  },
  {
    "input": " There's a lot more to be said about training later on, ",
    "translatedText": "关于训练的更多细节我们稍后还会提到，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1300.16,
    "end": 1302.74
  },
  {
    "input": " but I just want to call that out right now.",
    "translatedText": "现在先简单指出这一点。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1302.74,
    "end": 1305.43
  },
  {
    "input": " This matrix is called the unembedding matrix and we give it the label wu.",
    "translatedText": "这个矩阵被称为 unembedding 矩阵，我们用标签 Wu 来标记它。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1305.43,
    "end": 1310.08
  },
  {
    "input": " Again, like all the weight matrices we see, its entries begin at random, ",
    "translatedText": "就像我们见过的所有权重矩阵一样，这个矩阵的初始值是随机的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1310.08,
    "end": 1313.83
  },
  {
    "input": " but they are learned during the training process.",
    "translatedText": "但在训练过程中，这些值会被更新。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1313.83,
    "end": 1316.43
  },
  {
    "input": " Keeping score on our total parameter count, ",
    "translatedText": "关于参数总数的统计，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1316.43,
    "end": 1318.15
  },
  {
    "input": " this unembedding matrix has one row for each word in the vocabulary, ",
    "translatedText": "这个 unembedding 矩阵为词汇表中的每个单词都分配了一行，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1318.15,
    "end": 1321.95
  },
  {
    "input": " and each row has the same number of elements as the embedding dimension.",
    "translatedText": "每一行包含与嵌入维度相同数量的元素。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1321.95,
    "end": 1325.94
  },
  {
    "input": " It's very similar to the embedding matrix just with the order swapped, ",
    "translatedText": "这与嵌入（embedding）矩阵非常相似，只不过是把顺序倒过来了，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1325.94,
    "end": 1329.44
  },
  {
    "input": " so it adds another 617 million parameters to the network, ",
    "translatedText": "因此它为网络增加了另外 6.17 亿个参数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1329.44,
    "end": 1333.13
  },
  {
    "input": " meaning our count so far is a little over a billion, ",
    "translatedText": "到目前为止，我们的参数总数已经超过了 10 亿，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1333.13,
    "end": 1335.92
  },
  {
    "input": " a small but not wholly insignificant fraction of the 175 billion that we'll end up with in total.",
    "translatedText": "这只是我们最终要达到的 1750 亿的一小部分。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1335.92,
    "end": 1341.97
  },
  {
    "input": " As the very last mini-lesson for this chapter, ",
    "translatedText": "在这一章的最后一个小课中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1341.97,
    "end": 1344.43
  },
  {
    "input": " I want to talk more about the softmax function, ",
    "translatedText": "我想更详细地讨论一下 softmax 函数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1344.43,
    "end": 1346.92
  },
  {
    "input": " since it makes another appearance for us once we dive into the attention blocks.",
    "translatedText": "因为它在我们探索注意力机制时会再次成为焦点。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1346.92,
    "end": 1350.56
  },
  {
    "input": " The idea is that if you want a sequence of numbers to act as a probability distribution, ",
    "translatedText": "如果你想让一串数字成为概率分布，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1350.56,
    "end": 1356.16
  },
  {
    "input": " say a distribution over all possible next words, ",
    "translatedText": "比如预测下一个可能出现的词的概率，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1356.31,
    "end": 1358.98
  },
  {
    "input": " then each value has to be between 0 and 1, ",
    "translatedText": "那么这些数字每个都得在 0 到 1 之间，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1358.98,
    "end": 1361.6
  },
  {
    "input": " and you also need all of them to add up to 1.",
    "translatedText": "并且加起来总和为 1。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1361.6,
    "end": 1364.56
  },
  {
    "input": " However, ",
    "translatedText": "但是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1364.56,
    "end": 1365.92
  },
  {
    "input": " if you're playing the deep learning game ",
    "translatedText": "如果你正在进行深度学习的实践，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1365.92,
    "end": 1367.68
  },
  {
    "input": " where everything you do looks like matrix vector multiplication, ",
    "translatedText": "你所做的每一步操作都可能看起来像是矩阵和向量的乘法，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1367.68,
    "end": 1370.64
  },
  {
    "input": " the outputs that you get by default don't abide by this at all.",
    "translatedText": "那么你得到的结果可能并不符合这个条件。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1370.64,
    "end": 1374.65
  },
  {
    "input": " The values are often negative or much bigger than 1, ",
    "translatedText": "这些值可能是负的，也可能远大于 1，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1374.65,
    "end": 1377.92
  },
  {
    "input": " and they almost certainly don't add up to 1.",
    "translatedText": "而且加起来的和几乎确定不会是 1。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1377.92,
    "end": 1380.46
  },
  {
    "input": " Softmax is the standard way ",
    "translatedText": "Softmax 就是一种标准方法，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1380.46,
    "end": 1381.69
  },
  {
    "input": " to turn an arbitrary list of numbers into a valid distribution ",
    "translatedText": "可以把任何一组数字转换成一个有效的分布，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1381.69,
    "end": 1385.33
  },
  {
    "input": " in such a way that the largest values end up closest to 1 ",
    "translatedText": "使得最大的数值非常接近 1，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1385.33,
    "end": 1389.12
  },
  {
    "input": " and the smaller values end up very close to 0.",
    "translatedText": "而较小的值将会非常接近 0。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1389.12,
    "end": 1391.24
  },
  {
    "input": " That's all you really need to know.",
    "translatedText": "了解这一点就足够了。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1391.24,
    "end": 1393.21
  },
  {
    "input": " But if you're curious, ",
    "translatedText": "但如果你感到好奇，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1393.21,
    "end": 1394.08
  },
  {
    "input": " the way that it works is to first raise e to the power of each of the numbers, ",
    "translatedText": "具体的工作原理是：首先对每个数值进行 e 的指数运算，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1394.08,
    "end": 1398.72
  },
  {
    "input": " which means you now have a list of positive values, ",
    "translatedText": "这样就得到了一组正数；",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1398.72,
    "end": 1401.28
  },
  {
    "input": " and then you can take the sum of all those positive values ",
    "translatedText": "然后取所有正数的和，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1401.28,
    "end": 1404.47
  },
  {
    "input": " and divide each term by that sum, ",
    "translatedText": "然后用每个数除以这个总和，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1404.47,
    "end": 1406.71
  },
  {
    "input": " which normalizes it into a list that adds up to 1.",
    "translatedText": "这样就把它们标准化为一个总和为 1 的列表。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1406.71,
    "end": 1409.62
  },
  {
    "input": " You'll notice that if one of the numbers in the input ",
    "translatedText": "你会注意到，如果输入中的某个数值",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1409.62,
    "end": 1412.45
  },
  {
    "input": " is meaningfully bigger than the rest, ",
    "translatedText": "明显大于其他数值，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1412.45,
    "end": 1414.49
  },
  {
    "input": " then in the output the corresponding term dominates the distribution, ",
    "translatedText": "那么在输出中，这个数值对应的项就会在分布中占主导地位，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1414.49,
    "end": 1418
  },
  {
    "input": " so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
    "translatedText": "几乎确定你在采样时会选择这个最大的输入值。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1418,
    "end": 1422.99
  },
  {
    "input": " But it's softer than just picking the max ",
    "translatedText": "但这种方法比直接挑选最大值更为细腻，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1422.99,
    "end": 1425.21
  },
  {
    "input": " in the sense that when other values are similarly large, ",
    "translatedText": "因为当其他数值也接近最大值时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1425.21,
    "end": 1428.1
  },
  {
    "input": " they also get meaningful weight in the distribution, ",
    "translatedText": "它们在整体分布中同样能获得重要的比重，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1428.1,
    "end": 1431.12
  },
  {
    "input": " and everything changes continuously ",
    "translatedText": "而且随着你不断改变输入，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1431.12,
    "end": 1432.87
  },
  {
    "input": " as you continuously vary the inputs.",
    "translatedText": "一切都会连续地变化。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1432.87,
    "end": 1434.98
  },
  {
    "input": " In some situations, ",
    "translatedText": "在一些情况下，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1434.98,
    "end": 1435.94
  },
  {
    "input": " like when ChatGPT is using this distribution to create a next word, ",
    "translatedText": "比如当 ChatGPT 利用这个分布生成下一个单词时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1435.94,
    "end": 1439.69
  },
  {
    "input": " there's room for a little bit of extra fun ",
    "translatedText": "可以为这个函数增加一些趣味性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1439.69,
    "end": 1441.98
  },
  {
    "input": " by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
    "translatedText": "可以通过在指数的分母里添加一个常量 t 来实现。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1441.98,
    "end": 1449.18
  },
  {
    "input": " We call it the temperature, ",
    "translatedText": "我们称之为“温度”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1449.18,
    "end": 1450.55
  },
  {
    "input": " since it vaguely resembles the role of temperature in certain thermodynamics equations, ",
    "translatedText": "因其在某种程度上与热力学方程中温度的作用相似。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1450.55,
    "end": 1455.3
  },
  {
    "input": " and the effect is that when t is larger, ",
    "translatedText": "它的效果是，当 t 值较大时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1455.3,
    "end": 1457.76
  },
  {
    "input": " you give more weight to the lower values, ",
    "translatedText": "会使较小的数值获得更多的权重，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1457.76,
    "end": 1459.77
  },
  {
    "input": " meaning the distribution is a little bit more uniform, and if t is smaller, ",
    "translatedText": "使得分布稍微均匀一些。而如果 t 值较小，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1459.77,
    "end": 1464.25
  },
  {
    "input": " then the bigger values will dominate more aggressively, where in the extreme, ",
    "translatedText": "则较大的数值则会更加明显地占据主导，极端情况下，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1464.25,
    "end": 1468.33
  },
  {
    "input": " setting t equal to 0 means all of the weight goes to that maximum value.",
    "translatedText": "如果把 t 设为 0，那么所有的权重都会集中在最大的值上。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1468.33,
    "end": 1472.64
  },
  {
    "input": " For example, ",
    "translatedText": "例如，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1472.64,
    "end": 1473.69
  },
  {
    "input": " I'll have GPT-3 generate a story ",
    "translatedText": "我将用 GPT-3 生成一个故事",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1473.69,
    "end": 1476.2
  },
  {
    "input": " with the seed text \"Once upon a time there was A\", ",
    "translatedText": "种子文本是\"从前，有一个 A\"",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1476.2,
    "end": 1479.6
  },
  {
    "input": " but I'm going to use different temperatures in each case.",
    "translatedText": "但我会在每次测试中使用不同的温度。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1479.6,
    "end": 1482.75
  },
  {
    "input": " Temperature 0 means that it always goes with the most predictable word, ",
    "translatedText": "温度为 0 意味着它总是选择最可预测的词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1482.75,
    "end": 1488.4
  },
  {
    "input": " and what you get ends up being a trite derivative of Goldilocks.",
    "translatedText": "而你所得到的结果就变成了一个陈词滥调的金发姑娘故事。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1488.4,
    "end": 1492.32
  },
  {
    "input": " A higher temperature gives it a chance to choose less likely words, ",
    "translatedText": "较高的温度给它提供了选择不太可能出现的词的机会，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1492.94,
    "end": 1496.64
  },
  {
    "input": " but it comes with a risk.",
    "translatedText": "但这也伴随着风险。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1496.64,
    "end": 1497.92
  },
  {
    "input": " In this case, the story starts out a bit more originally, ",
    "translatedText": "在这个例子中，故事的开始部分较为原创，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1497.92,
    "end": 1500.8
  },
  {
    "input": " about a young web artist from South Korea, ",
    "translatedText": "讲述的是韩国的一位年轻网页艺术家，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1500.8,
    "end": 1503.53
  },
  {
    "input": " but it quickly degenerates into nonsense.",
    "translatedText": "但很快就变得毫无意义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1503.53,
    "end": 1506
  },
  {
    "input": " Technically speaking, ",
    "translatedText": "严格地说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1506,
    "end": 1507.31
  },
  {
    "input": " the API doesn't actually let you pick a temperature bigger than 2.",
    "translatedText": "API 实际上并不允许你选择大于 2 的温度。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1507.31,
    "end": 1511.04
  },
  {
    "input": " There is no mathematical reason for this, ",
    "translatedText": "这个限制并没有数学上的根据，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1511.04,
    "end": 1512.95
  },
  {
    "input": " it's just an arbitrary constraint imposed, I suppose, ",
    "translatedText": "只是一个人为的限制，我猜，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1512.95,
    "end": 1515.71
  },
  {
    "input": " to keep their tool from being seen generating things that are too nonsensical, ",
    "translatedText": "目的是为了防止他们的工具产生太过荒诞的结果。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1515.71,
    "end": 1519.68
  },
  {
    "input": " so if you're curious, ",
    "translatedText": "所以，如果你感到好奇，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1519.78,
    "end": 1520.77
  },
  {
    "input": " the way this animation is actually working is ",
    "translatedText": "这个动画的工作原理是这样的：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1520.77,
    "end": 1522.8
  },
  {
    "input": " I'm taking the 20 most probable next tokens that GPT-3 generates, ",
    "translatedText": "我选择了 GPT-3 生成的可能性最高的前 20 个 Token，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1522.8,
    "end": 1526.51
  },
  {
    "input": " which seems to be the maximum they'll give me, ",
    "translatedText": "这看起来是他们能给我的最多的数量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1526.51,
    "end": 1529.36
  },
  {
    "input": " and then I tweak the probabilities based on an exponent of 1/5.",
    "translatedText": "然后，我会根据 1/5 的指数来调整这些概率。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1529.36,
    "end": 1532.48
  },
  {
    "input": " As another bit of jargon, ",
    "translatedText": "再给你介绍一个专业术语，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1532.48,
    "end": 1533.85
  },
  {
    "input": " in the same way that you might call the components of the output of this function probabilities, ",
    "translatedText": "在这个上下文中，我们通常把这个函数的输出成分称作概率，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1533.85,
    "end": 1539.18
  },
  {
    "input": " people often refer to the inputs as logits, or some people say logits, ",
    "translatedText": "人们通常将输入称为 logits，有的人说 logits，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1539.18,
    "end": 1543.63
  },
  {
    "input": " some people say logits, I'm going to say logits.",
    "translatedText": "有的人说 logits，我选择说 logits。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1543.63,
    "end": 1546.32
  },
  {
    "input": " So for instance, when you feed in some text, ",
    "translatedText": "举个例子，当你输入一段文本时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1546.41,
    "end": 1548.03
  },
  {
    "input": " you have all these word embeddings flow through the network, ",
    "translatedText": "所有这些词向量就会通过网络流动，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1548.03,
    "end": 1550.42
  },
  {
    "input": " and you do this final multiplication with the unembedding matrix, ",
    "translatedText": "并与 unembedding 矩阵进行最终的乘法运算。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1550.42,
    "end": 1553.99
  },
  {
    "input": " machine learning people would refer to the components in that raw, ",
    "translatedText": "机器学习专家会把这种原始、",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1553.99,
    "end": 1557.52
  },
  {
    "input": " unnormalized output as the logits for the next word prediction.",
    "translatedText": "未标准化的输出成分称为下一个词预测的 logits。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1557.52,
    "end": 1561.36
  },
  {
    "input": " A lot of the goal with this chapter was ",
    "translatedText": "这一章的主要目标是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1561.36,
    "end": 1564.88
  },
  {
    "input": " to lay the foundations for understanding the attention mechanism, ",
    "translatedText": "为理解注意力机制打下基础，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1564.88,
    "end": 1568.48
  },
  {
    "input": " Karate Kid wax-on-wax-off style.",
    "translatedText": "就像电影《龙威小子》里的基本功训练“上蜡刮蜡”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1568.48,
    "end": 1570.85
  },
  {
    "input": " You see, if you have a strong intuition for word embeddings, for softmax, ",
    "translatedText": "你看，如果你对词嵌入、softmax 有深入的理解，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1570.85,
    "end": 1574.89
  },
  {
    "input": " for how dot products measure similarity, ",
    "translatedText": "点积如何衡量相似度，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1574.89,
    "end": 1577.53
  },
  {
    "input": " and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, ",
    "translatedText": "以及大部分计算看起来都像是填满可调参数的矩阵乘法有着深刻的理解，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1577.53,
    "end": 1585.14
  },
  {
    "input": " then understanding the attention mechanism, ",
    "translatedText": "那么掌握注意力机制——",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1585.14,
    "end": 1587.69
  },
  {
    "input": " this cornerstone piece in the whole modern boom in AI, ",
    "translatedText": "现代 AI 浪潮中的关键技术，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1587.69,
    "end": 1590.73
  },
  {
    "input": " should be relatively smooth.",
    "translatedText": "对你来说应该会比较容易。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1590.73,
    "end": 1592.64
  },
  {
    "input": " For that, come join me in the next chapter.",
    "translatedText": "为此，欢迎在下一章中加入我。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1592.64,
    "end": 1594.4
  },
  {
    "input": " As I'm publishing this, ",
    "translatedText": "发布这篇文章时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1594.4,
    "end": 1596.57
  },
  {
    "input": " a draft of that next chapter is available for review by Patreon supporters.",
    "translatedText": "下一章的草稿已经可以供赞助者审阅。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1596.57,
    "end": 1601.54
  },
  {
    "input": " A final version should be up in public in a week or two.",
    "translatedText": "最终版应该在一两周内公开。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1601.54,
    "end": 1604.24
  },
  {
    "input": " It usually depends on how much I end up changing based on that review.",
    "translatedText": "这通常取决于我根据审阅结果做出的修改有多大。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1604.24,
    "end": 1607.61
  },
  {
    "input": " In the meantime, if you want to dive into attention, ",
    "translatedText": "与此同时，如果你想深入研究注意力机制，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1607.61,
    "end": 1610.08
  },
  {
    "input": " and if you want to help the channel out a little bit, it's there waiting.",
    "translatedText": "如果你想帮助这个频道，那么它就在那里等你。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1610.08,
    "end": 1612.72
  }
]
