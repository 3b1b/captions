1
00:00:00,000 --> 00:00:04,560
A GPT kezdőbetűk a Generative Pretrained Transformer rövidítése.

2
00:00:05,220 --> 00:00:09,020
Az első szó tehát elég egyszerű, ezek a botok új szöveget generálnak.

3
00:00:09,800 --> 00:00:13,056
Az előképzett arra utal, hogy a modell egy hatalmas adatmennyiségből 

4
00:00:13,056 --> 00:00:16,264
történő tanulási folyamaton ment keresztül, és az előtag arra utal, 

5
00:00:16,264 --> 00:00:20,040
hogy további képzéssel még van lehetőség a finomhangolásra bizonyos feladatokra.

6
00:00:20,720 --> 00:00:22,900
De az utolsó szó, ez az igazi kulcsdarab.

7
00:00:23,380 --> 00:00:26,572
A transzformátor egy speciális neurális hálózat, egy gépi tanulási modell, 

8
00:00:26,572 --> 00:00:30,361
és ez az alapvető találmány, amely a mesterséges intelligencia jelenlegi fellendülésének 

9
00:00:30,361 --> 00:00:31,000
alapját képezi.

10
00:00:31,740 --> 00:00:34,971
Ezzel a videóval és a következő fejezetekkel azt szeretném elérni, 

11
00:00:34,971 --> 00:00:39,120
hogy vizuálisan elmagyarázzuk, mi is történik valójában egy transzformátor belsejében.

12
00:00:39,700 --> 00:00:42,820
Követni fogjuk a rajta keresztül áramló adatokat, és lépésről lépésre haladunk.

13
00:00:43,440 --> 00:00:47,380
Sokféle modellt lehet építeni transzformátorok segítségével.

14
00:00:47,800 --> 00:00:50,800
Egyes modellek hangot vesznek fel és átiratot készítenek.

15
00:00:51,340 --> 00:00:53,826
Ez a mondat egy fordított irányú modellből származik, 

16
00:00:53,826 --> 00:00:56,220
amely szintetikus beszédet állít elő csak szövegből.

17
00:00:56,660 --> 00:00:59,481
Mindazok az eszközök, amelyek 2022-ben meghódították a világot, 

18
00:00:59,481 --> 00:01:03,007
mint például a Dolly és a Midjourney, amelyek egy szöveges leírást vesznek fel, 

19
00:01:03,007 --> 00:01:05,519
és egy képet állítanak elő, transzformátorokon alapulnak.

20
00:01:06,000 --> 00:01:08,280
Még ha nem is tudom teljesen rávenni, hogy megértse, 

21
00:01:08,280 --> 00:01:11,206
hogy mi a pite lénynek kellene lennie, akkor is le vagyok nyűgözve, 

22
00:01:11,206 --> 00:01:13,100
hogy ez a fajta dolog egyáltalán lehetséges.

23
00:01:13,900 --> 00:01:17,905
A Google által 2017-ben bevezetett eredeti transzformátort pedig kifejezetten arra a 

24
00:01:17,905 --> 00:01:22,100
felhasználási esetre találták ki, amikor az egyik nyelvről a másikra fordítanak szöveget.

25
00:01:22,660 --> 00:01:25,179
De az a változat, amelyre Ön és én összpontosítunk, 

26
00:01:25,179 --> 00:01:29,151
és amely az olyan eszközök alapját képezi, mint a ChatGPT, egy olyan modell lesz, 

27
00:01:29,151 --> 00:01:32,204
amely arra van kiképezve, hogy befogadjon egy szövegrészletet, 

28
00:01:32,204 --> 00:01:34,626
esetleg még néhány környező képet vagy hangot is, 

29
00:01:34,626 --> 00:01:38,260
és előrejelzést készítsen arra vonatkozóan, hogy mi következik a szövegben.

30
00:01:38,600 --> 00:01:41,076
Ez a jóslat egy valószínűségi eloszlás formájában 

31
00:01:41,076 --> 00:01:43,800
jelenik meg a következő különböző szövegdarabok között.

32
00:01:45,040 --> 00:01:47,267
Első pillantásra azt gondolhatod, hogy a következő szó 

33
00:01:47,267 --> 00:01:49,940
megjóslása egészen más célnak tűnik, mint az új szöveg generálása.

34
00:01:50,180 --> 00:01:53,777
De ha már van egy ilyen előrejelző modell, akkor egy hosszabb szövegdarabot 

35
00:01:53,777 --> 00:01:57,232
egyszerűen úgy generálhatsz, hogy adsz neki egy kezdeti szövegrészletet, 

36
00:01:57,232 --> 00:02:01,303
amivel dolgozhatsz, majd vesz egy véletlenszerű mintát az imént generált eloszlásból, 

37
00:02:01,303 --> 00:02:04,995
hozzáadod ezt a mintát a szöveghez, majd az egész folyamatot újra lefuttatod, 

38
00:02:04,995 --> 00:02:08,356
hogy új előrejelzést készítsen az új szöveg alapján, beleértve azt is, 

39
00:02:08,356 --> 00:02:09,539
amit az imént hozzáadott.

40
00:02:10,100 --> 00:02:11,889
Nem tudom, ti hogy vagytok vele, de nem igazán érzem úgy, 

41
00:02:11,889 --> 00:02:13,000
hogy ennek tényleg működnie kellene.

42
00:02:13,420 --> 00:02:16,356
Ebben az animációban például a GPT-2-t futtatom a laptopomon, 

43
00:02:16,356 --> 00:02:19,625
és többször megjósolja és mintavételezi a következő szövegrészletet, 

44
00:02:19,625 --> 00:02:22,420
hogy egy történetet generáljon a kiindulási szöveg alapján.

45
00:02:22,420 --> 00:02:26,120
A történetnek egyszerűen nincs sok értelme.

46
00:02:26,500 --> 00:02:30,457
De ha ezt kicserélem a GPT-3 API-hívásokra, ami ugyanaz az alapmodell, 

47
00:02:30,457 --> 00:02:35,417
csak sokkal nagyobb, akkor hirtelen, szinte varázsütésre egy értelmes történetet kapunk, 

48
00:02:35,417 --> 00:02:40,155
ami még arra is következtetni látszik, hogy egy pi-lény a matematika és a számítások 

49
00:02:40,155 --> 00:02:40,880
földjén élne.

50
00:02:41,580 --> 00:02:44,842
Ez az ismételt előrejelzés és mintavételezés folyamata lényegében az, 

51
00:02:44,842 --> 00:02:48,104
ami akkor történik, amikor a ChatGPT-vel vagy bármely más nagy nyelvi 

52
00:02:48,104 --> 00:02:51,880
modellel interakcióba lépünk, és látjuk, hogy egyszerre egy-egy szót produkálnak.

53
00:02:52,480 --> 00:02:55,458
Valójában az egyik funkció, amit nagyon élveznék, az az, 

54
00:02:55,458 --> 00:02:59,220
hogy minden egyes új szóhoz, amit kiválaszt, láthatnám az alapeloszlást.

55
00:03:03,820 --> 00:03:06,072
Kezdjük a dolgokat egy nagyon magas szintű előnézettel arról, 

56
00:03:06,072 --> 00:03:08,180
hogyan áramlanak az adatok egy transzformátoron keresztül.

57
00:03:08,640 --> 00:03:11,622
Sokkal több időt fogunk tölteni az egyes lépések motiválásával, 

58
00:03:11,622 --> 00:03:14,185
értelmezésével és részletezésével, de nagy vonalakban, 

59
00:03:14,185 --> 00:03:16,702
amikor az egyik ilyen chatbot generál egy adott szót, 

60
00:03:16,702 --> 00:03:18,660
a következő történik a motorháztető alatt.

61
00:03:19,080 --> 00:03:22,040
Először is, a bemenetet egy csomó kis darabra bontjuk.

62
00:03:22,620 --> 00:03:26,909
Ezeket a darabokat tokeneknek nevezzük, és a szöveg esetében ezek általában szavak, 

63
00:03:26,909 --> 00:03:29,820
szavak kis darabjai vagy más gyakori karakterkombinációk.

64
00:03:30,740 --> 00:03:33,687
Ha képek vagy hangok szerepelnek a játékban, akkor a 

65
00:03:33,687 --> 00:03:37,080
tokenek lehetnek a kép kis darabkái vagy a hang kis darabkái.

66
00:03:37,580 --> 00:03:41,718
Ezután minden egyes ilyen tokenhez egy vektor, azaz számok listája társul, 

67
00:03:41,718 --> 00:03:45,360
amelynek célja, hogy valahogyan kódolja az adott darab jelentését.

68
00:03:45,880 --> 00:03:48,707
Ha úgy gondolunk ezekre a vektorokra, mint amelyek koordinátákat adnak 

69
00:03:48,707 --> 00:03:51,773
egy nagyon nagy dimenziós térben, akkor a hasonló jelentésű szavak általában 

70
00:03:51,773 --> 00:03:54,680
olyan vektorokra kerülnek, amelyek közel vannak egymáshoz ebben a térben.

71
00:03:55,280 --> 00:03:59,037
Ez a vektorok sorozata ezután egy figyelemblokknak nevezett műveleten megy keresztül, 

72
00:03:59,037 --> 00:04:01,703
és ez lehetővé teszi, hogy a vektorok beszéljenek egymással, 

73
00:04:01,703 --> 00:04:04,500
és információt adjanak át egymásnak, hogy frissítsék értékeiket.

74
00:04:04,880 --> 00:04:09,670
Például a modell szó jelentése a gépi tanulási modell kifejezésben más, 

75
00:04:09,670 --> 00:04:11,800
mint a divatmodell kifejezésben.

76
00:04:12,260 --> 00:04:14,542
A figyelem blokk felelős azért, hogy kitalálja, 

77
00:04:14,542 --> 00:04:17,775
hogy a kontextusban mely szavak relevánsak a többi szó jelentésének 

78
00:04:17,775 --> 00:04:21,959
frissítése szempontjából, és hogy pontosan hogyan kell frissíteni ezeket a jelentéseket.

79
00:04:22,500 --> 00:04:24,784
És ismétlem, amikor a jelentés szót használom, 

80
00:04:24,784 --> 00:04:28,040
ez valahogy teljes mértékben kódolva van e vektorok bejegyzéseiben.

81
00:04:29,180 --> 00:04:32,300
Ezután ezek a vektorok egy másfajta műveleten mennek keresztül, 

82
00:04:32,300 --> 00:04:34,592
és attól függően, hogy milyen forrást olvasol, 

83
00:04:34,592 --> 00:04:38,200
ezt többrétegű perceptronként vagy esetleg feed-forward rétegként említik.

84
00:04:38,580 --> 00:04:42,660
És itt a vektorok nem beszélnek egymással, hanem párhuzamosan végzik ugyanazt a műveletet.

85
00:04:43,060 --> 00:04:46,827
És bár ezt a blokkot egy kicsit nehezebb értelmezni, később beszélni fogunk arról, 

86
00:04:46,827 --> 00:04:50,459
hogy ez a lépés egy kicsit olyan, mintha egy hosszú kérdéslistát tennénk fel az 

87
00:04:50,459 --> 00:04:54,000
egyes vektorokról, majd a kérdésekre adott válaszok alapján frissítenénk őket.

88
00:04:54,900 --> 00:04:59,807
Mindkét blokk összes művelete úgy néz ki, mint egy hatalmas halom mátrixszorzás, 

89
00:04:59,807 --> 00:05:03,017
és az elsődleges feladatunk az lesz, hogy megértsük, 

90
00:05:03,017 --> 00:05:05,320
hogyan olvassuk a mögöttes mátrixokat.

91
00:05:06,980 --> 00:05:10,680
Elmulasztok néhány részletet a közbeeső normalizálási lépésekről, 

92
00:05:10,680 --> 00:05:12,980
de ez végül is egy magas szintű előnézet.

93
00:05:13,680 --> 00:05:18,456
Ezután a folyamat lényegében megismétlődik, oda-vissza váltogatjuk a figyelmi 

94
00:05:18,456 --> 00:05:23,110
blokkokat és a többrétegű perceptron blokkokat, míg a legvégén a remény az, 

95
00:05:23,110 --> 00:05:28,500
hogy a szöveg minden lényeges jelentése valahogy belesült a szekvencia utolsó vektorába.

96
00:05:28,920 --> 00:05:32,023
Ezután elvégezünk egy bizonyos műveletet ezen az utolsó vektoron, 

97
00:05:32,023 --> 00:05:35,363
amely valószínűségi eloszlást eredményez az összes lehetséges tokenre, 

98
00:05:35,363 --> 00:05:38,420
az összes lehetséges kis szövegdarabra, amely ezután következhet.

99
00:05:38,980 --> 00:05:41,858
És ahogy mondtam, ha már van egy eszköz, amely megjósolja, 

100
00:05:41,858 --> 00:05:46,005
hogy mi következik egy szövegrészlet alapján, akkor betáplálhat egy kis magszöveget, 

101
00:05:46,005 --> 00:05:49,615
és ismételten eljátszhatja ezt a játékot, hogy megjósolja, mi következik, 

102
00:05:49,615 --> 00:05:53,080
mintavételezzen az eloszlásból, csatolja, majd újra és újra megismétli.

103
00:05:53,640 --> 00:05:57,325
Néhányan talán emlékeznek arra, hogy jóval a ChatGPT megjelenése 

104
00:05:57,325 --> 00:06:00,897
előtt a GPT-3 korai demói így néztek ki: a GPT-3 automatikusan 

105
00:06:00,897 --> 00:06:04,640
kitöltött történeteket és esszéket egy kezdeti szippantás alapján.

106
00:06:05,580 --> 00:06:08,098
Ahhoz, hogy egy ilyen eszközből chatbotot készítsünk, 

107
00:06:08,098 --> 00:06:11,736
a legegyszerűbb kiindulópont egy kis szöveg, amely meghatározza a felhasználó 

108
00:06:11,736 --> 00:06:15,327
és egy segítőkész mesterséges intelligencia asszisztens közötti interakciót, 

109
00:06:15,327 --> 00:06:18,964
amit úgy hívnánk, hogy a rendszer kérése, majd a felhasználó kezdeti kérdését 

110
00:06:18,964 --> 00:06:22,975
vagy kérését használnánk a párbeszéd első részeként, és aztán elkezdenénk megjósolni, 

111
00:06:22,975 --> 00:06:26,940
hogy mit mondana válaszul egy ilyen segítőkész mesterséges intelligencia asszisztens.

112
00:06:27,720 --> 00:06:31,197
Többet is lehetne mondani a képzés lépcsőfokáról, ami ahhoz szükséges, 

113
00:06:31,197 --> 00:06:33,940
hogy ez jól működjön, de magas szinten ez az elképzelés.

114
00:06:35,720 --> 00:06:40,039
Ebben a fejezetben mi ketten bővebben kifejtjük, hogy mi történik a hálózat legelején, 

115
00:06:40,039 --> 00:06:43,018
a hálózat legvégén, és sok időt szeretnék azzal is tölteni, 

116
00:06:43,018 --> 00:06:46,195
hogy áttekintünk néhány fontos háttérismeretet, olyan dolgokat, 

117
00:06:46,195 --> 00:06:50,316
amelyek a transzformátorok megjelenésével minden gépi tanulással foglalkozó mérnök 

118
00:06:50,316 --> 00:06:52,600
számára már második természetűek lettek volna.

119
00:06:53,060 --> 00:06:56,598
Ha ezekkel a háttérismeretekkel megbarátkozott, és egy kicsit türelmetlen, 

120
00:06:56,598 --> 00:06:59,005
akkor nyugodtan átugorhatja a következő fejezetet, 

121
00:06:59,005 --> 00:07:02,780
amely a transzformátor szívének tekintett figyelemblokkokra fog összpontosítani.

122
00:07:03,360 --> 00:07:07,447
Ezután szeretnék többet beszélni ezekről a többrétegű perceptron blokkokról, arról, 

123
00:07:07,447 --> 00:07:11,680
hogyan működik a képzés, és számos más részletről, amelyeket eddig a pontig kihagytunk.

124
00:07:12,180 --> 00:07:15,760
A tágabb kontextus érdekében ezek a videók a mélytanulásról szóló minisorozat 

125
00:07:15,760 --> 00:07:18,468
kiegészítései, és nem baj, ha nem nézted meg az előzőeket, 

126
00:07:18,468 --> 00:07:22,553
szerintem sorrendben is megteheted, de mielőtt konkrétan a transzformátorokba merülnénk, 

127
00:07:22,553 --> 00:07:26,362
úgy gondolom, érdemes megbizonyosodni arról, hogy egy oldalon állunk a mélytanulás 

128
00:07:26,362 --> 00:07:28,520
alapfeltevésével és felépítésével kapcsolatban.

129
00:07:29,020 --> 00:07:32,830
Megkockáztatva, hogy kimondom a nyilvánvalót, ez a gépi tanulás egyik megközelítése, 

130
00:07:32,830 --> 00:07:35,699
amely minden olyan modellt leír, ahol adatokat használunk arra, 

131
00:07:35,699 --> 00:07:38,300
hogy valahogyan meghatározzuk, hogyan viselkedik a modell.

132
00:07:39,140 --> 00:07:42,246
Ezalatt azt értem, hogy mondjuk egy olyan függvényt szeretnénk, 

133
00:07:42,246 --> 00:07:45,256
amely egy képet vesz fel, és előállítja a képet leíró címkét, 

134
00:07:45,256 --> 00:07:48,217
vagy a következő szó megjósolását egy szövegrészlet alapján, 

135
00:07:48,217 --> 00:07:51,760
vagy bármilyen más olyan feladatot, amelyhez szükség van némi intuícióra 

136
00:07:51,760 --> 00:07:52,780
és mintafelismerésre.

137
00:07:53,200 --> 00:07:57,420
Manapság ezt szinte természetesnek vesszük, de a gépi tanulás lényege, hogy ahelyett, 

138
00:07:57,420 --> 00:08:00,904
hogy megpróbálnánk kódban explicit módon definiálni egy eljárást arra, 

139
00:08:00,904 --> 00:08:05,321
hogyan kell elvégezni ezt a feladatot, amit az emberek az AI legkorábbi napjaiban tettek, 

140
00:08:05,321 --> 00:08:09,247
ehelyett egy nagyon rugalmas struktúrát állítunk fel hangolható paraméterekkel, 

141
00:08:09,247 --> 00:08:13,075
mint egy csomó gombot és tárcsát, majd valahogyan sok példát használunk arra, 

142
00:08:13,075 --> 00:08:15,970
hogyan kellene kinéznie a kimenetnek egy adott bemenethez, 

143
00:08:15,970 --> 00:08:19,700
hogy a paraméterek értékeit úgy hangoljuk, hogy utánozzuk ezt a viselkedést.

144
00:08:19,700 --> 00:08:23,563
Például a gépi tanulás legegyszerűbb formája a lineáris regresszió, 

145
00:08:23,563 --> 00:08:26,858
ahol a bemenetek és a kimenetek mindegyike egyetlen szám, 

146
00:08:26,858 --> 00:08:30,266
például egy ház négyzetmétere és az ára, és azt szeretnénk, 

147
00:08:30,266 --> 00:08:34,641
ha megtalálnánk a legjobb illeszkedési vonalat ezeken az adatokon keresztül, 

148
00:08:34,641 --> 00:08:36,799
hogy megjósoljuk a jövőbeli házárakat.

149
00:08:37,440 --> 00:08:41,877
Ezt az egyenest két folytonos paraméter írja le, mondjuk a meredekség és az y-intercept, 

150
00:08:41,877 --> 00:08:45,866
és a lineáris regresszió célja, hogy ezeket a paramétereket úgy határozzuk meg, 

151
00:08:45,866 --> 00:08:48,160
hogy azok pontosan megfeleljenek az adatoknak.

152
00:08:48,880 --> 00:08:52,100
Mondanom sem kell, hogy a mély tanulási modellek sokkal bonyolultabbak lesznek.

153
00:08:52,620 --> 00:08:57,660
A GPT-3 például nem két, hanem 175 milliárd paraméterrel rendelkezik.

154
00:08:58,120 --> 00:09:01,896
De itt van a dolog, ez nem egy adott, hogy létrehozhat néhány óriási 

155
00:09:01,896 --> 00:09:04,578
modellt egy hatalmas számú paraméterrel anélkül, 

156
00:09:04,578 --> 00:09:07,644
hogy ez vagy durván túlilleszkedik a képzési adatokhoz, 

157
00:09:07,644 --> 00:09:09,560
vagy teljesen nehézkes a képzéshez.

158
00:09:10,260 --> 00:09:12,576
A mélytanulás a modellek egy olyan osztályát írja le, 

159
00:09:12,576 --> 00:09:16,180
amely az elmúlt évtizedekben bebizonyította, hogy figyelemre méltóan jól skálázható.

160
00:09:16,480 --> 00:09:21,633
Ami ezeket egyesíti, az ugyanaz a képzési algoritmus, az úgynevezett backpropagation, 

161
00:09:21,633 --> 00:09:25,048
és a kontextus, amit szeretném, ha tudnának, hogy ahhoz, 

162
00:09:25,048 --> 00:09:29,722
hogy ez a képzési algoritmus jól működjön, ezeknek a modelleknek egy bizonyos 

163
00:09:29,722 --> 00:09:31,280
formátumot kell követniük.

164
00:09:31,800 --> 00:09:34,830
Ha ismeri ezt a formátumot, akkor ez segít megmagyarázni a transzformátor 

165
00:09:34,830 --> 00:09:37,574
nyelvfeldolgozási módjára vonatkozó számos választási lehetőséget, 

166
00:09:37,574 --> 00:09:40,400
amelyek egyébként azzal a veszéllyel járnak, hogy önkényesnek tűnnek.

167
00:09:41,440 --> 00:09:44,033
Először is, bármilyen modellt is készítesz, a 

168
00:09:44,033 --> 00:09:46,740
bemenetet valós számok tömbjeként kell formázni.

169
00:09:46,740 --> 00:09:50,047
Ez jelenthet számok listáját, lehet kétdimenziós tömb, 

170
00:09:50,047 --> 00:09:53,775
vagy nagyon gyakran van dolgunk magasabb dimenziós tömbökkel, 

171
00:09:53,775 --> 00:09:56,000
ahol az általános kifejezés a tenzor.

172
00:09:56,560 --> 00:10:00,727
Gyakran úgy gondolunk arra, hogy a bemeneti adatokat fokozatosan sok különböző réteggé 

173
00:10:00,727 --> 00:10:04,751
alakítjuk át, ahol minden egyes réteg mindig valós számok valamilyen tömbjeként van 

174
00:10:04,751 --> 00:10:08,680
strukturálva, amíg el nem jutunk a végső réteghez, amelyet a kimenetnek tekintünk.

175
00:10:09,280 --> 00:10:13,097
A szövegfeldolgozási modellünk utolsó rétege például egy számokból álló lista, 

176
00:10:13,097 --> 00:10:17,060
amely az összes lehetséges következő token valószínűségi eloszlását reprezentálja.

177
00:10:17,820 --> 00:10:21,380
A mélytanulásban ezeket a modellparamétereket szinte mindig súlyoknak nevezik, 

178
00:10:21,380 --> 00:10:25,077
és ez azért van így, mert ezeknek a modelleknek az egyik legfontosabb jellemzője, 

179
00:10:25,077 --> 00:10:28,863
hogy ezek a paraméterek csak súlyozott összegeken keresztül lépnek kölcsönhatásba a 

180
00:10:28,863 --> 00:10:29,900
feldolgozott adatokkal.

181
00:10:30,340 --> 00:10:34,360
Néhány nemlineáris függvényt is megszórsz, de ezek nem függnek a paraméterektől.

182
00:10:35,200 --> 00:10:40,120
Jellemzően azonban ahelyett, hogy a súlyozott összegeket csupaszon és explicit módon 

183
00:10:40,120 --> 00:10:45,330
írnánk ki, inkább egy mátrix vektorproduktum különböző összetevőiként csomagolva találjuk 

184
00:10:45,330 --> 00:10:45,620
őket.

185
00:10:46,740 --> 00:10:50,601
Ha visszagondolunk arra, hogy a mátrixvektor-szorzás hogyan működik, 

186
00:10:50,601 --> 00:10:54,240
a kimenet minden egyes összetevője egy súlyozott összegnek tűnik.

187
00:10:54,780 --> 00:10:58,756
Csak gyakran koncepcionálisan tisztább, ha olyan mátrixokra gondolunk, 

188
00:10:58,756 --> 00:11:01,668
amelyek hangolható paraméterekkel vannak feltöltve, 

189
00:11:01,668 --> 00:11:05,420
amelyek a feldolgozott adatokból származó vektorokat alakítanak át.

190
00:11:06,340 --> 00:11:14,160
Például a GPT-3-ban szereplő 175 milliárd súlyt alig 28 000 különböző mátrixba szervezik.

191
00:11:14,660 --> 00:11:18,023
Ezek a mátrixok viszont nyolc különböző kategóriába sorolhatók, 

192
00:11:18,023 --> 00:11:22,700
és mi most végigmegyünk ezeken a kategóriákon, hogy megértsük, mit csinál az adott típus.

193
00:11:23,160 --> 00:11:27,938
Ahogy végigmegyünk rajta, azt hiszem, jó móka lesz a GPT-3 konkrét számaira hivatkozni, 

194
00:11:27,938 --> 00:11:31,360
hogy pontosan megszámoljuk, honnan származik ez a 175 milliárd.

195
00:11:31,880 --> 00:11:34,317
Még ha manapság vannak is nagyobb és jobb modellek, 

196
00:11:34,317 --> 00:11:37,552
ennek a modellnek megvan az a varázsa, mint a nagy nyelvi modellnek, 

197
00:11:37,552 --> 00:11:40,740
amely valóban megragadta a világ figyelmét az ML közösségeken kívül.

198
00:11:41,440 --> 00:11:44,117
Gyakorlatilag a vállalatok sokkal szűkszavúbbak 

199
00:11:44,117 --> 00:11:46,740
a modernebb hálózatok konkrét számait illetően.

200
00:11:47,360 --> 00:11:50,655
Csak azt akarom bemutatni, hogy ha bepillantasz a motorháztető alá, 

201
00:11:50,655 --> 00:11:53,708
hogy megnézd, mi történik egy olyan eszközben, mint a ChatGPT, 

202
00:11:53,708 --> 00:11:57,440
szinte az összes tényleges számítás úgy néz ki, mint a mátrix-vektor szorzás.

203
00:11:57,900 --> 00:12:01,715
Kicsit fennáll a veszélye, hogy elveszünk a számok milliárdjainak tengerében, 

204
00:12:01,715 --> 00:12:05,432
de nagyon éles különbséget kell tennünk a fejünkben a modell súlyai között, 

205
00:12:05,432 --> 00:12:08,171
amelyeket mindig kék vagy piros színnel fogok színezni, 

206
00:12:08,171 --> 00:12:11,840
és a feldolgozott adatok között, amelyeket mindig szürkével fogok színezni.

207
00:12:12,180 --> 00:12:14,366
A súlyok a tényleges agyak, ezek azok a dolgok, 

208
00:12:14,366 --> 00:12:17,920
amelyeket a képzés során megtanult, és ezek határozzák meg, hogyan viselkedik.

209
00:12:18,280 --> 00:12:22,180
A feldolgozott adatok egyszerűen kódolják az adott futtatáshoz a 

210
00:12:22,180 --> 00:12:26,500
modellbe táplált konkrét bemeneti adatokat, például egy szöveges példát.

211
00:12:27,480 --> 00:12:32,141
Mindezzel az alapként, ássuk bele magunkat a szövegfeldolgozási példa első lépésébe, 

212
00:12:32,141 --> 00:12:36,420
ami a bemenet apró darabokra bontása és ezekből a darabokból vektorok lesznek.

213
00:12:37,020 --> 00:12:39,772
Említettem, hogy ezeket a darabokat tokeneknek hívják, 

214
00:12:39,772 --> 00:12:43,325
amelyek lehetnek szórészletek vagy írásjelek, de ebben a fejezetben és 

215
00:12:43,325 --> 00:12:46,078
különösen a következőben időnként úgy szeretnék tenni, 

216
00:12:46,078 --> 00:12:48,080
mintha tisztábban szavakra lenne bontva.

217
00:12:48,600 --> 00:12:51,274
Mivel mi emberek szavakban gondolkodunk, ez csak megkönnyíti 

218
00:12:51,274 --> 00:12:54,080
a kis példákra való hivatkozást és az egyes lépések tisztázását.

219
00:12:55,260 --> 00:12:59,910
A modellnek van egy előre meghatározott szókincse, az összes lehetséges szó egy listája, 

220
00:12:59,910 --> 00:13:03,202
mondjuk 50 000, és az első mátrix, amellyel találkozni fogunk, 

221
00:13:03,202 --> 00:13:07,800
az úgynevezett beágyazási mátrix, egyetlen oszlopot tartalmaz minden egyes ilyen szóhoz.

222
00:13:08,940 --> 00:13:11,324
Ezek az oszlopok határozzák meg, hogy az első 

223
00:13:11,324 --> 00:13:13,760
lépésben az egyes szavakból milyen vektor lesz.

224
00:13:15,100 --> 00:13:18,180
We-nek címkézzük, és mint minden mátrixot, amit látunk, 

225
00:13:18,180 --> 00:13:22,360
az értékei véletlenszerűen kezdődnek, de az adatok alapján megtanuljuk őket.

226
00:13:23,620 --> 00:13:27,697
A szavak vektorokká alakítása már jóval a transzformátorok előtt bevett gyakorlat volt 

227
00:13:27,697 --> 00:13:31,869
a gépi tanulásban, de ez egy kicsit furcsa, ha még sosem láttad, és megalapozza mindazt, 

228
00:13:31,869 --> 00:13:35,760
ami ezután következik, ezért szánjunk rá egy pillanatot, hogy megismerkedjünk vele.

229
00:13:36,040 --> 00:13:39,072
Ezt a beágyazást gyakran nevezzük szónak, ami arra hívja fel a figyelmet, 

230
00:13:39,072 --> 00:13:41,694
hogy ezeket a vektorokat nagyon geometrikusan úgy gondoljuk el, 

231
00:13:41,694 --> 00:13:43,620
mint pontokat valamilyen nagy dimenziós térben.

232
00:13:44,180 --> 00:13:47,980
Egy három számból álló listát a 3D térben lévő pontok koordinátáiként megjeleníteni 

233
00:13:47,980 --> 00:13:51,780
nem jelentene problémát, de a szóbeágyazások általában sokkal magasabb dimenziójúak.

234
00:13:52,280 --> 00:13:56,240
A GPT-3-ban 12 288 dimenzióval rendelkeznek, és mint látni fogod, 

235
00:13:56,240 --> 00:14:00,440
nem mindegy, hogy olyan térben dolgozol, ahol sok különböző irány van.

236
00:14:01,180 --> 00:14:04,639
Ugyanúgy, ahogyan egy 3D-s tér kétdimenziós szeletét vehetnénk, 

237
00:14:04,639 --> 00:14:09,397
és az összes pontot erre a szeletre vetíthetnénk, a szóbeágyazások animálása érdekében, 

238
00:14:09,397 --> 00:14:12,803
amit egy egyszerű modell ad nekem, analóg módon fogok eljárni, 

239
00:14:12,803 --> 00:14:16,965
kiválasztva egy háromdimenziós szeletet ezen a nagyon nagy dimenziójú téren, 

240
00:14:16,965 --> 00:14:20,480
és erre vetítem a szóvektorokat, és megjelenítem az eredményeket.

241
00:14:21,280 --> 00:14:24,394
A nagy ötlet itt az, hogy ahogy a modell a súlyok finomhangolásával és 

242
00:14:24,394 --> 00:14:27,509
hangolásával meghatározza, hogy pontosan hogyan ágyazódnak be a szavak 

243
00:14:27,509 --> 00:14:30,755
vektorokként a képzés során, a modell hajlamos megállapodni a beágyazások 

244
00:14:30,755 --> 00:14:34,440
olyan készletén, ahol a térben lévő irányok egyfajta szemantikai jelentéssel bírnak.

245
00:14:34,980 --> 00:14:37,555
Az itt futtatott egyszerű szó-vektor modell esetében, 

246
00:14:37,555 --> 00:14:39,939
ha lefuttatok egy keresést az összes olyan szóra, 

247
00:14:39,939 --> 00:14:43,372
amelynek beágyazása a legközelebb áll a toronyéhoz, akkor észreveheted, 

248
00:14:43,372 --> 00:14:45,900
hogy mind nagyon hasonló torony-szerű hangzást adnak.

249
00:14:46,340 --> 00:14:48,821
És ha szeretnél egy kis Pythont elővenni, és otthon is játszani, 

250
00:14:48,821 --> 00:14:51,380
ez az a konkrét modell, amit az animációk elkészítéséhez használok.

251
00:14:51,620 --> 00:14:54,951
Ez nem egy transzformátor, de elég ahhoz, hogy szemléltesse azt a gondolatot, 

252
00:14:54,951 --> 00:14:57,600
hogy a térben lévő irányok szemantikai jelentést hordozhatnak.

253
00:14:58,300 --> 00:15:03,383
Egy nagyon klasszikus példa erre, hogy ha a nő és a férfi vektorai közötti különbséget 

254
00:15:03,383 --> 00:15:06,538
vesszük, amit úgy képzelünk el, mint egy kis vektort, 

255
00:15:06,538 --> 00:15:11,447
amely az egyik és a másik csúcsát köti össze, akkor ez nagyon hasonló a király és a 

256
00:15:11,447 --> 00:15:13,200
királynő közötti különbséghez.

257
00:15:15,080 --> 00:15:19,169
Tegyük fel, hogy nem ismered a női uralkodó szavát, akkor úgy találhatod meg, 

258
00:15:19,169 --> 00:15:22,314
hogy veszed a király szót, hozzáadod ezt a nő-férfi irányt, 

259
00:15:22,314 --> 00:15:25,460
és megkeresed az ehhez a ponthoz legközelebbi beágyazásokat.

260
00:15:27,000 --> 00:15:28,200
Legalábbis valahogy így.

261
00:15:28,480 --> 00:15:31,909
Annak ellenére, hogy ez egy klasszikus példa a modellre, amellyel játszom, 

262
00:15:31,909 --> 00:15:36,024
a királynő valódi beágyazódása valójában egy kicsit távolabb van, mint ahogy ez sugallná, 

263
00:15:36,024 --> 00:15:40,139
feltehetően azért, mert a királynő a képzési adatokban használt módja nem csupán a király 

264
00:15:40,139 --> 00:15:40,780
női változata.

265
00:15:41,620 --> 00:15:45,260
Amikor játszottam, a családi kapcsolatok sokkal jobban illusztrálták a gondolatot.

266
00:15:46,340 --> 00:15:50,338
A lényeg az, hogy úgy tűnik, hogy a modell a képzés során előnyösnek találta, 

267
00:15:50,338 --> 00:15:54,900
ha a beágyazásokat úgy választja meg, hogy a tér egyik iránya a nemi információt kódolja.

268
00:15:56,800 --> 00:15:59,560
Egy másik példa: ha fogjuk Olaszország beágyazottságát, 

269
00:15:59,560 --> 00:16:01,927
és kivonjuk belőle Németország beágyazottságát, 

270
00:16:01,927 --> 00:16:05,674
majd ezt hozzáadjuk Hitler beágyazottságához, akkor valami olyasmit kapunk, 

271
00:16:05,674 --> 00:16:08,090
ami nagyon közel áll Mussolini beágyazottságához.

272
00:16:08,570 --> 00:16:12,092
Mintha a modell megtanult volna egyes irányokat az olaszsággal, 

273
00:16:12,092 --> 00:16:15,670
másokat pedig a második világháborús tengelyvezérekkel társítani.

274
00:16:16,470 --> 00:16:20,363
Talán a kedvenc példám ebben az értelemben az, hogy egyes modellekben, 

275
00:16:20,363 --> 00:16:23,872
ha a Németország és Japán közötti különbséget a sushihez adjuk, 

276
00:16:23,872 --> 00:16:26,230
akkor nagyon közel kerülünk a bratwursthoz.

277
00:16:27,350 --> 00:16:30,647
A legközelebbi szomszédok megtalálásának játékában is örömmel láttam, 

278
00:16:30,647 --> 00:16:33,850
hogy Kat milyen közel volt mind a vadállathoz, mind a szörnyeteghez.

279
00:16:34,690 --> 00:16:37,263
Egy kis matematikai intuíció, amit hasznos észben tartani, 

280
00:16:37,263 --> 00:16:40,753
különösen a következő fejezetben, hogy két vektor ponttermelése úgy tekinthető, 

281
00:16:40,753 --> 00:16:43,850
mint egy módja annak, hogy mérjük, mennyire jól illeszkednek egymáshoz.

282
00:16:44,870 --> 00:16:48,567
Számítási szempontból a ponttételek az összes megfelelő komponens szorzását, 

283
00:16:48,567 --> 00:16:50,920
majd az eredmények összeadását jelentik, ami jó, 

284
00:16:50,920 --> 00:16:54,330
mivel a számításaink nagy részének súlyozott összegeknek kell kinéznie.

285
00:16:55,190 --> 00:17:00,755
Geometriai szempontból a pontszorzat pozitív, ha a vektorok hasonló irányba mutatnak, 

286
00:17:00,755 --> 00:17:05,609
nulla, ha merőlegesek egymásra, és negatív, ha ellentétes irányba mutatnak.

287
00:17:06,550 --> 00:17:11,192
Tegyük fel például, hogy ezzel a modellel játszottál, és azt feltételezed, 

288
00:17:11,192 --> 00:17:16,452
hogy a macskák mínusz macska beágyazása egyfajta pluralitás irányt képviselhet ebben 

289
00:17:16,452 --> 00:17:17,010
a térben.

290
00:17:17,430 --> 00:17:20,316
Ennek teszteléséhez fogom ezt a vektort, és kiszámítom a 

291
00:17:20,316 --> 00:17:23,455
pontgyakoriságát bizonyos egyes számú főnevek beágyazásaival, 

292
00:17:23,455 --> 00:17:27,050
és összehasonlítom a megfelelő többes számú főnevek pontgyakoriságával.

293
00:17:27,270 --> 00:17:30,028
Ha ezzel játszadozol, észreveheted, hogy a többes számúak 

294
00:17:30,028 --> 00:17:33,453
valóban következetesen magasabb értékeket adnak, mint az egyes számúak, 

295
00:17:33,453 --> 00:17:36,070
ami azt jelzi, hogy jobban igazodnak ehhez az irányhoz.

296
00:17:37,070 --> 00:17:41,791
Az is vicces, hogy ha ezt a pontszorzatot az 1, 2, 3, stb. szavak beágyazásaival vesszük, 

297
00:17:41,791 --> 00:17:45,882
akkor növekvő értékeket kapunk, tehát mintha kvantitatív módon mérni tudnánk, 

298
00:17:45,882 --> 00:17:49,030
hogy a modell mennyire többes számúnak talál egy adott szót.

299
00:17:50,250 --> 00:17:53,570
A szavak beágyazódásának sajátosságait ismét adatok segítségével tanuljuk meg.

300
00:17:54,050 --> 00:17:56,370
Ez a beágyazási mátrix, amelynek oszlopai megmondják, 

301
00:17:56,370 --> 00:17:59,550
hogy mi történik az egyes szavakkal, a modellünkben a súlyok első halmaza.

302
00:18:00,030 --> 00:18:05,230
A GPT-3 számokat használva a szókincs mérete konkrétan 50 257, 

303
00:18:05,230 --> 00:18:09,770
és ez technikailag nem szavakból, hanem tokenekből áll.

304
00:18:10,630 --> 00:18:14,953
A beágyazási dimenzió 12 288, és ezeket megszorozva azt kapjuk, 

305
00:18:14,953 --> 00:18:17,790
hogy ez körülbelül 617 millió súlyból áll.

306
00:18:18,250 --> 00:18:20,930
Menjünk előre, és adjuk ezt össze egy futó számlához, 

307
00:18:20,930 --> 00:18:23,810
ne feledjük, hogy a végére 175 milliárdot kell számolnunk.

308
00:18:25,430 --> 00:18:29,675
A transzformátorok esetében a beágyazási térben lévő vektorokra úgy kell gondolni, 

309
00:18:29,675 --> 00:18:32,130
hogy azok nem csupán egyes szavakat képviselnek.

310
00:18:32,550 --> 00:18:35,791
Egyrészt kódolnak információt az adott szó helyzetéről is, 

311
00:18:35,791 --> 00:18:38,594
amiről később még beszélünk, de ami még fontosabb, 

312
00:18:38,594 --> 00:18:42,770
hogy úgy kell rájuk gondolni, mint amelyek képesek a kontextus felszívására.

313
00:18:43,350 --> 00:18:47,347
Egy vektor, amely például a király szó beágyazásaként kezdte életét, 

314
00:18:47,347 --> 00:18:50,882
a hálózat különböző blokkjai által fokozatosan megrántódhat, 

315
00:18:50,882 --> 00:18:54,996
hogy a végére egy sokkal specifikusabb és árnyaltabb irányba mutasson, 

316
00:18:54,996 --> 00:18:59,051
amely valahogy azt kódolja, hogy egy Skóciában élő királyról van szó, 

317
00:18:59,051 --> 00:19:02,470
aki az előző király meggyilkolása után jutott a posztjára, 

318
00:19:02,470 --> 00:19:04,730
és akit shakespeare-i nyelven írnak le.

319
00:19:05,210 --> 00:19:07,790
Gondolkodj el azon, hogy mit értesz egy adott szó alatt.

320
00:19:08,250 --> 00:19:11,297
Az adott szó jelentését egyértelműen a környezet határozza meg, 

321
00:19:11,297 --> 00:19:15,058
és néha ez a környezet nagy távolságból származó kontextust is magában foglal, 

322
00:19:15,058 --> 00:19:18,152
ezért egy olyan modell összeállításakor, amely képes megjósolni, 

323
00:19:18,152 --> 00:19:21,914
hogy melyik szó következik, a cél az, hogy valahogy képessé tegyük a modellt a 

324
00:19:21,914 --> 00:19:23,390
kontextus hatékony beépítésére.

325
00:19:24,050 --> 00:19:26,162
Hogy világos legyen, ebben a legelső lépésben, 

326
00:19:26,162 --> 00:19:29,039
amikor a bemeneti szöveg alapján létrehozza a vektorok tömbjét, 

327
00:19:29,039 --> 00:19:32,140
ezek mindegyike egyszerűen a beágyazási mátrixból kerül kiragadásra, 

328
00:19:32,140 --> 00:19:35,062
így kezdetben mindegyik csak egyetlen szó jelentését kódolhatja, 

329
00:19:35,062 --> 00:19:36,770
a környezetéből származó input nélkül.

330
00:19:37,710 --> 00:19:41,463
De úgy kell gondolni, hogy a hálózat elsődleges célja, amelyen keresztül áramlik, 

331
00:19:41,463 --> 00:19:45,216
az, hogy lehetővé tegye, hogy minden egyes vektor olyan jelentést szívjon magába, 

332
00:19:45,216 --> 00:19:48,970
amely sokkal gazdagabb és specifikusabb, mint amit a puszta szavak képviselhetnek.

333
00:19:49,510 --> 00:19:52,587
A hálózat egyszerre csak meghatározott számú vektort tud feldolgozni, 

334
00:19:52,587 --> 00:19:54,170
amit a kontextus méretének nevezünk.

335
00:19:54,510 --> 00:19:57,607
A GPT-3 esetében 2048-as kontextusmérettel lett betanítva, 

336
00:19:57,607 --> 00:20:00,442
így a hálózaton átáramló adatok mindig úgy néznek ki, 

337
00:20:00,442 --> 00:20:05,010
mint ez a 2048 oszlopból álló tömb, amelynek mindegyike 12 000 dimenzióval rendelkezik.

338
00:20:05,590 --> 00:20:09,413
Ez a kontextus mérete korlátozza, hogy a transzformátor mennyi szöveget tud beépíteni, 

339
00:20:09,413 --> 00:20:11,830
amikor a következő szóra vonatkozó előrejelzést készít.

340
00:20:12,370 --> 00:20:15,504
Ez az oka annak, hogy a hosszú beszélgetések bizonyos chatbotokkal, 

341
00:20:15,504 --> 00:20:18,915
mint például a ChatGPT korai változatai, gyakran azt az érzést keltették, 

342
00:20:18,915 --> 00:20:22,050
hogy a bot elveszíti a beszélgetés fonalát, ha túl sokáig folytatod.

343
00:20:23,030 --> 00:20:25,333
A figyelem részleteire majd a kellő időben kitérünk, 

344
00:20:25,333 --> 00:20:28,810
de előreugorva szeretnék egy percig beszélni arról, hogy mi történik a legvégén.

345
00:20:29,450 --> 00:20:32,268
Ne feledje, hogy a kívánt kimenet egy valószínűségi 

346
00:20:32,268 --> 00:20:34,870
eloszlás az összes lehetséges következő tokenre.

347
00:20:35,170 --> 00:20:39,707
Például, ha a legvégső szó a Professzor, és a kontextusban olyan szavak szerepelnek, 

348
00:20:39,707 --> 00:20:43,391
mint Harry Potter, és közvetlenül előtte a legkevésbé kedvenc tanár, 

349
00:20:43,391 --> 00:20:46,274
és ha adunk némi mozgásteret azzal, hogy úgy teszünk, 

350
00:20:46,274 --> 00:20:50,865
mintha a tokenek egyszerűen teljes szavaknak tűnnének, akkor egy jól képzett hálózat, 

351
00:20:50,865 --> 00:20:55,029
amely Harry Potterről szerzett ismereteket, feltehetően magas számot rendelne 

352
00:20:55,029 --> 00:20:55,830
a Snape szóhoz.

353
00:20:56,510 --> 00:20:57,970
Ez két különböző lépést foglal magában.

354
00:20:58,310 --> 00:21:00,749
Az első az, hogy egy másik mátrixot használunk, 

355
00:21:00,749 --> 00:21:05,069
amely az adott kontextus utolsó vektorát egy 50 000 értékből álló listára képezi le, 

356
00:21:05,069 --> 00:21:07,610
amely a szókincs minden egyes tokenjéhez tartozik.

357
00:21:08,170 --> 00:21:12,235
Ezután van egy függvény, amely ezt egy valószínűségi eloszlássá normalizálja, 

358
00:21:12,235 --> 00:21:16,145
ezt Softmaxnak hívják, és erről egy pillanat múlva többet fogunk beszélni, 

359
00:21:16,145 --> 00:21:20,314
de előtte talán egy kicsit furcsának tűnhet, hogy csak ezt az utolsó beágyazást 

360
00:21:20,314 --> 00:21:24,849
használjuk a jósláshoz, amikor az utolsó lépésben több ezer más vektor van a rétegben, 

361
00:21:24,849 --> 00:21:28,290
amelyek csak ott ülnek a saját, kontextusban gazdag jelentésükkel.

362
00:21:28,930 --> 00:21:32,962
Ez azzal függ össze, hogy a képzési folyamatban sokkal hatékonyabbnak bizonyul, 

363
00:21:32,962 --> 00:21:36,238
ha az utolsó rétegben lévő vektorok mindegyikét arra használjuk, 

364
00:21:36,238 --> 00:21:40,270
hogy egyidejűleg előrejelzést készítsünk arra, ami közvetlenül utána következik.

365
00:21:40,970 --> 00:21:45,090
Az edzésről még sok mindent el fogok mondani később, de ezt most csak szeretném kiemelni.

366
00:21:45,730 --> 00:21:49,690
Ezt a mátrixot nevezzük beágyazásmentesítő mátrixnak, és a WU jelölést adjuk neki.

367
00:21:50,210 --> 00:21:53,898
Ismét, mint az összes súlymátrix, amit látunk, a bejegyzések véletlenszerűen kezdődnek, 

368
00:21:53,898 --> 00:21:55,910
de a betanítási folyamat során megtanulják őket.

369
00:21:56,470 --> 00:21:59,625
A teljes paraméterszámot megtartva, ez a beágyazás nélküli mátrix 

370
00:21:59,625 --> 00:22:02,350
a szókincs minden egyes szavához egy sorral rendelkezik, 

371
00:22:02,350 --> 00:22:05,650
és minden sornak ugyanannyi eleme van, mint a beágyazási dimenziónak.

372
00:22:06,410 --> 00:22:10,093
Ez nagyon hasonlít a beágyazási mátrixhoz, csak a sorrend felcserélődött, 

373
00:22:10,093 --> 00:22:13,477
így további 617 millió paramétert ad a hálózathoz, ami azt jelenti, 

374
00:22:13,477 --> 00:22:16,912
hogy az eddigi számunk valamivel több mint egymilliárd, ami egy kis, 

375
00:22:16,912 --> 00:22:19,649
de nem teljesen jelentéktelen része a 175 milliárdnak, 

376
00:22:19,649 --> 00:22:21,790
amivel a végén összesen rendelkezni fogunk.

377
00:22:22,550 --> 00:22:26,696
A fejezet utolsó mini-leckéjeként szeretnék többet beszélni erről a softmax függvényről, 

378
00:22:26,696 --> 00:22:30,610
mivel ez a függvény még egyszer megjelenik, amint belemerülünk a figyelem blokkokba.

379
00:22:31,430 --> 00:22:35,441
Az ötlet az, hogy ha azt akarjuk, hogy egy számsorozat valószínűségi eloszlásként 

380
00:22:35,441 --> 00:22:39,306
működjön, mondjuk az összes lehetséges következő szóra vonatkozó eloszlásként, 

381
00:22:39,306 --> 00:22:42,926
akkor minden értéknek 0 és 1 között kell lennie, és azt is meg kell adni, 

382
00:22:42,926 --> 00:22:44,590
hogy mindegyikük összege 1 legyen.

383
00:22:45,250 --> 00:22:48,952
Ha azonban a tanulási játékot játszod, ahol minden, amit csinálsz, 

384
00:22:48,952 --> 00:22:53,760
mátrix-vektor szorzásnak tűnik, akkor az alapértelmezetten kapott kimenetek egyáltalán 

385
00:22:53,760 --> 00:22:54,810
nem tartják be ezt.

386
00:22:55,330 --> 00:22:57,666
Az értékek gyakran negatívak, vagy sokkal nagyobbak, 

387
00:22:57,666 --> 00:22:59,870
mint 1, és szinte biztosan nem adódnak össze 1-re.

388
00:23:00,510 --> 00:23:04,182
A softmax a szabványos módja annak, hogy egy tetszőleges számlistát érvényes 

389
00:23:04,182 --> 00:23:07,664
eloszlássá alakítsunk oly módon, hogy a legnagyobb értékek a legközelebb 

390
00:23:07,664 --> 00:23:11,290
kerüljenek az 1-hez, a kisebb értékek pedig nagyon közel kerüljenek a 0-hoz.

391
00:23:11,830 --> 00:23:13,070
Ennyit kell tudnod.

392
00:23:13,090 --> 00:23:15,416
De ha kíváncsiak vagytok, a módszer úgy működik, 

393
00:23:15,416 --> 00:23:18,977
hogy először minden egyes szám hatványára emeljük az e-t, ami azt jelenti, 

394
00:23:18,977 --> 00:23:21,588
hogy most már van egy pozitív értékekből álló listánk, 

395
00:23:21,588 --> 00:23:25,529
majd fogjuk az összes pozitív érték összegét, és minden egyes kifejezést elosztunk 

396
00:23:25,529 --> 00:23:29,470
ezzel az összeggel, ami normalizálja a listát egy olyan listává, amely 1-re adódik.

397
00:23:30,170 --> 00:23:33,946
Észre fogja venni, hogy ha a bemenetben az egyik szám értelmesen nagyobb, 

398
00:23:33,946 --> 00:23:37,774
mint a többi, akkor a kimeneten a megfelelő kifejezés uralja az eloszlást, 

399
00:23:37,774 --> 00:23:41,908
így ha ebből mintavételezne, akkor szinte biztosan csak a maximalizáló bemenetet 

400
00:23:41,908 --> 00:23:42,470
választaná.

401
00:23:42,990 --> 00:23:46,115
De ez puhább, mint a maximum kiválasztása abban az értelemben, 

402
00:23:46,115 --> 00:23:49,936
hogy ha más értékek is hasonlóan nagyok, akkor azok is értelmes súlyt kapnak 

403
00:23:49,936 --> 00:23:52,367
az eloszlásban, és minden folyamatosan változik, 

404
00:23:52,367 --> 00:23:54,650
ahogy folyamatosan változtatjuk a bemeneteket.

405
00:23:55,130 --> 00:23:59,920
Bizonyos helyzetekben, például amikor a ChatGPT ezt az eloszlást használja egy következő 

406
00:23:59,920 --> 00:24:02,935
szó létrehozására, van hely egy kis extra szórakozásra, 

407
00:24:02,935 --> 00:24:06,756
ha egy kis extra fűszert adunk ehhez a függvényhez, egy t konstanssal, 

408
00:24:06,756 --> 00:24:08,910
amelyet az exponensek nevezőjébe dobunk.

409
00:24:09,550 --> 00:24:13,942
Ezt nevezzük hőmérsékletnek, mivel homályosan hasonlít a hőmérséklet szerepére 

410
00:24:13,942 --> 00:24:18,056
bizonyos termodinamikai egyenletekben, és az a hatása, hogy ha t nagyobb, 

411
00:24:18,056 --> 00:24:21,003
akkor nagyobb súlyt adunk az alacsonyabb értékeknek, 

412
00:24:21,003 --> 00:24:24,839
vagyis az eloszlás egy kicsit egyenletesebb lesz, ha pedig t kisebb, 

413
00:24:24,839 --> 00:24:29,009
akkor a nagyobb értékek agresszívebben fognak dominálni, ahol a végletben, 

414
00:24:29,009 --> 00:24:32,790
ha t-t nullára állítjuk, akkor minden súly a maximális értékre megy.

415
00:24:33,470 --> 00:24:39,359
Például a GPT-3-mal generálok egy történetet a "Volt egyszer egy A" magszöveggel, 

416
00:24:39,359 --> 00:24:42,950
de minden esetben más-más hőmérsékletet használok.

417
00:24:43,630 --> 00:24:48,572
A hőmérsékleti nulla azt jelenti, hogy mindig a legkiszámíthatóbb szót választja, 

418
00:24:48,572 --> 00:24:52,370
és amit kapunk, az végül az Aranyhaj közhelyes származéka lesz.

419
00:24:53,010 --> 00:24:56,873
A magasabb hőmérséklet esélyt ad arra, hogy kevésbé valószínű szavakat válasszon, 

420
00:24:56,873 --> 00:24:57,910
de ez kockázattal jár.

421
00:24:58,230 --> 00:25:01,328
Ebben az esetben a történet eredetibben indul, 

422
00:25:01,328 --> 00:25:06,010
egy fiatal dél-koreai webes művészről szól, de hamar ostobasággá fajul.

423
00:25:06,950 --> 00:25:10,830
Technikailag az API nem engedi, hogy 2-nél nagyobb hőmérsékletet válasszon.

424
00:25:11,170 --> 00:25:14,573
Ennek nincs matematikai oka, ez csak egy önkényes korlátozás, 

425
00:25:14,573 --> 00:25:19,350
amelyet azért szabtak meg, hogy az eszközük ne generáljon túlságosan képtelen dolgokat.

426
00:25:19,870 --> 00:25:23,158
Szóval, ha kíváncsi vagy, az animáció valójában úgy működik, 

427
00:25:23,158 --> 00:25:27,794
hogy a GPT-3 által generált 20 legvalószínűbb következő tokent veszem, ami úgy tűnik, 

428
00:25:27,794 --> 00:25:31,999
hogy a maximum, amit adnak nekem, majd a valószínűségeket egy 1/5-ös exponens 

429
00:25:31,999 --> 00:25:32,970
alapján módosítom.

430
00:25:33,130 --> 00:25:37,507
Még egy kis szakzsargon: ugyanúgy, ahogyan a függvény kimenetének összetevőit 

431
00:25:37,507 --> 00:25:42,221
valószínűségeknek nevezhetjük, az emberek a bemeneteket gyakran logitoknak nevezik, 

432
00:25:42,221 --> 00:25:46,150
vagy egyesek logitoknak, mások logitoknak, én logitokat fogok mondani.

433
00:25:46,530 --> 00:25:48,816
Tehát például, amikor betáplálunk egy szöveget, 

434
00:25:48,816 --> 00:25:52,626
az összes szóbeágyazás átfolyik a hálózaton, és elvégezzük ezt a végső szorzást 

435
00:25:52,626 --> 00:25:56,531
a beágyazás nélküli mátrixszal, a gépi tanulással foglalkozó szakemberek a nyers, 

436
00:25:56,531 --> 00:25:59,103
nem normalizált kimenet összetevőire úgy hivatkoznak, 

437
00:25:59,103 --> 00:26:01,390
mint a következő szó előrejelzésének logitjaira.

438
00:26:03,330 --> 00:26:06,950
A fejezet célja nagyrészt az volt, hogy lerakjuk az alapokat a figyelem 

439
00:26:06,950 --> 00:26:10,370
mechanizmusának megértéséhez, a Karate Kid wax-on-wax-off stílusban.

440
00:26:10,850 --> 00:26:14,918
Ha van egy erős intuíciód a szóbeágyazásokhoz, a softmaxhoz, ahhoz, 

441
00:26:14,918 --> 00:26:18,388
hogy hogyan mérik a hasonlóságot a ponttételek, és ahhoz, 

442
00:26:18,388 --> 00:26:22,038
hogy a számítások többsége úgy néz ki, mint a mátrixszorzás, 

443
00:26:22,038 --> 00:26:27,303
hangolható paraméterekkel teli mátrixokkal, akkor a figyelem mechanizmusának megértése, 

444
00:26:27,303 --> 00:26:32,210
az egész modern mesterséges intelligencia boom sarokköve, viszonylag könnyen megy.

445
00:26:32,650 --> 00:26:34,510
Ehhez csatlakozzatok hozzám a következő fejezetben.

446
00:26:36,390 --> 00:26:41,210
Miközben ezt publikálom, a következő fejezet vázlata elérhető a Patreon támogatói számára.

447
00:26:41,770 --> 00:26:44,682
A végleges verziónak egy-két héten belül nyilvánosan elérhetőnek kell lennie, 

448
00:26:44,682 --> 00:26:47,370
általában attól függ, hogy mennyit változtatok a felülvizsgálat alapján.

449
00:26:47,810 --> 00:26:50,026
Addig is, ha szeretnél belevetni magad a figyelembe, 

450
00:26:50,026 --> 00:26:52,410
és ha szeretnéd egy kicsit segíteni a csatornát, ott vár.

