[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "A GPT kezdőbetűk a Generative Pretrained Transformer rövidítése.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Az első szó tehát elég egyszerű, ezek a botok új szöveget generálnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Az előképzett arra utal, hogy a modell egy hatalmas adatmennyiségből történő tanulási folyamaton ment keresztül, és az előtag arra utal, hogy további képzéssel még van lehetőség a finomhangolásra bizonyos feladatokra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "De az utolsó szó, ez az igazi kulcsdarab.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "A transzformátor egy speciális neurális hálózat, egy gépi tanulási modell, és ez az alapvető találmány, amely a mesterséges intelligencia jelenlegi fellendülésének alapját képezi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Ezzel a videóval és a következő fejezetekkel azt szeretném elérni, hogy vizuálisan elmagyarázzuk, mi is történik valójában egy transzformátor belsejében.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Követni fogjuk a rajta keresztül áramló adatokat, és lépésről lépésre haladunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Sokféle modellt lehet építeni transzformátorok segítségével.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Egyes modellek hangot vesznek fel és átiratot készítenek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Ez a mondat egy fordított irányú modellből származik, amely szintetikus beszédet állít elő csak szövegből.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Mindazok az eszközök, amelyek 2022-ben meghódították a világot, mint például a Dolly és a Midjourney, amelyek egy szöveges leírást vesznek fel, és egy képet állítanak elő, transzformátorokon alapulnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Még ha nem is tudom teljesen rávenni, hogy megértse, hogy mi a pite lénynek kellene lennie, akkor is le vagyok nyűgözve, hogy ez a fajta dolog egyáltalán lehetséges.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "A Google által 2017-ben bevezetett eredeti transzformátort pedig kifejezetten arra a felhasználási esetre találták ki, amikor az egyik nyelvről a másikra fordítanak szöveget.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "De az a változat, amelyre Ön és én összpontosítunk, és amely az olyan eszközök alapját képezi, mint a ChatGPT, egy olyan modell lesz, amely arra van kiképezve, hogy befogadjon egy szövegrészletet, esetleg még néhány környező képet vagy hangot is, és előrejelzést készítsen arra vonatkozóan, hogy mi következik a szövegben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Ez a jóslat egy valószínűségi eloszlás formájában jelenik meg a következő különböző szövegdarabok között.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "Első pillantásra azt gondolhatod, hogy a következő szó megjóslása egészen más célnak tűnik, mint az új szöveg generálása.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "De ha már van egy ilyen előrejelző modell, akkor egy hosszabb szövegdarabot egyszerűen úgy generálhatsz, hogy adsz neki egy kezdeti szövegrészletet, amivel dolgozhatsz, majd vesz egy véletlenszerű mintát az imént generált eloszlásból, hozzáadod ezt a mintát a szöveghez, majd az egész folyamatot újra lefuttatod, hogy új előrejelzést készítsen az új szöveg alapján, beleértve azt is, amit az imént hozzáadott.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Nem tudom, ti hogy vagytok vele, de nem igazán érzem úgy, hogy ennek tényleg működnie kellene.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Ebben az animációban például a GPT-2-t futtatom a laptopomon, és többször megjósolja és mintavételezi a következő szövegrészletet, hogy egy történetet generáljon a kiindulási szöveg alapján.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "A történetnek egyszerűen nincs sok értelme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "De ha ezt kicserélem a GPT-3 API-hívásokra, ami ugyanaz az alapmodell, csak sokkal nagyobb, akkor hirtelen, szinte varázsütésre egy értelmes történetet kapunk, ami még arra is következtetni látszik, hogy egy pi-lény a matematika és a számítások földjén élne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Ez az ismételt előrejelzés és mintavételezés folyamata lényegében az, ami akkor történik, amikor a ChatGPT-vel vagy bármely más nagy nyelvi modellel interakcióba lépünk, és látjuk, hogy egyszerre egy-egy szót produkálnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Valójában az egyik funkció, amit nagyon élveznék, az az, hogy minden egyes új szóhoz, amit kiválaszt, láthatnám az alapeloszlást.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Kezdjük a dolgokat egy nagyon magas szintű előnézettel arról, hogyan áramlanak az adatok egy transzformátoron keresztül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Sokkal több időt fogunk tölteni az egyes lépések motiválásával, értelmezésével és részletezésével, de nagy vonalakban, amikor az egyik ilyen chatbot generál egy adott szót, a következő történik a motorháztető alatt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Először is, a bemenetet egy csomó kis darabra bontjuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Ezeket a darabokat tokeneknek nevezzük, és a szöveg esetében ezek általában szavak, szavak kis darabjai vagy más gyakori karakterkombinációk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Ha képek vagy hangok szerepelnek a játékban, akkor a tokenek lehetnek a kép kis darabkái vagy a hang kis darabkái.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Ezután minden egyes ilyen tokenhez egy vektor, azaz számok listája társul, amelynek célja, hogy valahogyan kódolja az adott darab jelentését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Ha úgy gondolunk ezekre a vektorokra, mint amelyek koordinátákat adnak egy nagyon nagy dimenziós térben, akkor a hasonló jelentésű szavak általában olyan vektorokra kerülnek, amelyek közel vannak egymáshoz ebben a térben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Ez a vektorok sorozata ezután egy figyelemblokknak nevezett műveleten megy keresztül, és ez lehetővé teszi, hogy a vektorok beszéljenek egymással, és információt adjanak át egymásnak, hogy frissítsék értékeiket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Például a modell szó jelentése a gépi tanulási modell kifejezésben más, mint a divatmodell kifejezésben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "A figyelem blokk felelős azért, hogy kitalálja, hogy a kontextusban mely szavak relevánsak a többi szó jelentésének frissítése szempontjából, és hogy pontosan hogyan kell frissíteni ezeket a jelentéseket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "És ismétlem, amikor a jelentés szót használom, ez valahogy teljes mértékben kódolva van e vektorok bejegyzéseiben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Ezután ezek a vektorok egy másfajta műveleten mennek keresztül, és attól függően, hogy milyen forrást olvasol, ezt többrétegű perceptronként vagy esetleg feed-forward rétegként említik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "És itt a vektorok nem beszélnek egymással, hanem párhuzamosan végzik ugyanazt a műveletet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "És bár ezt a blokkot egy kicsit nehezebb értelmezni, később beszélni fogunk arról, hogy ez a lépés egy kicsit olyan, mintha egy hosszú kérdéslistát tennénk fel az egyes vektorokról, majd a kérdésekre adott válaszok alapján frissítenénk őket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Mindkét blokk összes művelete úgy néz ki, mint egy hatalmas halom mátrixszorzás, és az elsődleges feladatunk az lesz, hogy megértsük, hogyan olvassuk a mögöttes mátrixokat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Elmulasztok néhány részletet a közbeeső normalizálási lépésekről, de ez végül is egy magas szintű előnézet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Ezután a folyamat lényegében megismétlődik, oda-vissza váltogatjuk a figyelmi blokkokat és a többrétegű perceptron blokkokat, míg a legvégén a remény az, hogy a szöveg minden lényeges jelentése valahogy belesült a szekvencia utolsó vektorába.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Ezután elvégezünk egy bizonyos műveletet ezen az utolsó vektoron, amely valószínűségi eloszlást eredményez az összes lehetséges tokenre, az összes lehetséges kis szövegdarabra, amely ezután következhet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "És ahogy mondtam, ha már van egy eszköz, amely megjósolja, hogy mi következik egy szövegrészlet alapján, akkor betáplálhat egy kis magszöveget, és ismételten eljátszhatja ezt a játékot, hogy megjósolja, mi következik, mintavételezzen az eloszlásból, csatolja, majd újra és újra megismétli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Néhányan talán emlékeznek arra, hogy jóval a ChatGPT megjelenése előtt a GPT-3 korai demói így néztek ki: a GPT-3 automatikusan kitöltött történeteket és esszéket egy kezdeti szippantás alapján.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Ahhoz, hogy egy ilyen eszközből chatbotot készítsünk, a legegyszerűbb kiindulópont egy kis szöveg, amely meghatározza a felhasználó és egy segítőkész mesterséges intelligencia asszisztens közötti interakciót, amit úgy hívnánk, hogy a rendszer kérése, majd a felhasználó kezdeti kérdését vagy kérését használnánk a párbeszéd első részeként, és aztán elkezdenénk megjósolni, hogy mit mondana válaszul egy ilyen segítőkész mesterséges intelligencia asszisztens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Többet is lehetne mondani a képzés lépcsőfokáról, ami ahhoz szükséges, hogy ez jól működjön, de magas szinten ez az elképzelés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "Ebben a fejezetben mi ketten bővebben kifejtjük, hogy mi történik a hálózat legelején, a hálózat legvégén, és sok időt szeretnék azzal is tölteni, hogy áttekintünk néhány fontos háttérismeretet, olyan dolgokat, amelyek a transzformátorok megjelenésével minden gépi tanulással foglalkozó mérnök számára már második természetűek lettek volna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Ha ezekkel a háttérismeretekkel megbarátkozott, és egy kicsit türelmetlen, akkor nyugodtan átugorhatja a következő fejezetet, amely a transzformátor szívének tekintett figyelemblokkokra fog összpontosítani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Ezután szeretnék többet beszélni ezekről a többrétegű perceptron blokkokról, arról, hogyan működik a képzés, és számos más részletről, amelyeket eddig a pontig kihagytunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "A tágabb kontextus érdekében ezek a videók a mélytanulásról szóló minisorozat kiegészítései, és nem baj, ha nem nézted meg az előzőeket, szerintem sorrendben is megteheted, de mielőtt konkrétan a transzformátorokba merülnénk, úgy gondolom, érdemes megbizonyosodni arról, hogy egy oldalon állunk a mélytanulás alapfeltevésével és felépítésével kapcsolatban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Megkockáztatva, hogy kimondom a nyilvánvalót, ez a gépi tanulás egyik megközelítése, amely minden olyan modellt leír, ahol adatokat használunk arra, hogy valahogyan meghatározzuk, hogyan viselkedik a modell.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Ezalatt azt értem, hogy mondjuk egy olyan függvényt szeretnénk, amely egy képet vesz fel, és előállítja a képet leíró címkét, vagy a következő szó megjósolását egy szövegrészlet alapján, vagy bármilyen más olyan feladatot, amelyhez szükség van némi intuícióra és mintafelismerésre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Manapság ezt szinte természetesnek vesszük, de a gépi tanulás lényege, hogy ahelyett, hogy megpróbálnánk kódban explicit módon definiálni egy eljárást arra, hogyan kell elvégezni ezt a feladatot, amit az emberek az AI legkorábbi napjaiban tettek, ehelyett egy nagyon rugalmas struktúrát állítunk fel hangolható paraméterekkel, mint egy csomó gombot és tárcsát, majd valahogyan sok példát használunk arra, hogyan kellene kinéznie a kimenetnek egy adott bemenethez, hogy a paraméterek értékeit úgy hangoljuk, hogy utánozzuk ezt a viselkedést.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Például a gépi tanulás legegyszerűbb formája a lineáris regresszió, ahol a bemenetek és a kimenetek mindegyike egyetlen szám, például egy ház négyzetmétere és az ára, és azt szeretnénk, ha megtalálnánk a legjobb illeszkedési vonalat ezeken az adatokon keresztül, hogy megjósoljuk a jövőbeli házárakat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Ezt az egyenest két folytonos paraméter írja le, mondjuk a meredekség és az y-intercept, és a lineáris regresszió célja, hogy ezeket a paramétereket úgy határozzuk meg, hogy azok pontosan megfeleljenek az adatoknak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Mondanom sem kell, hogy a mély tanulási modellek sokkal bonyolultabbak lesznek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "A GPT-3 például nem két, hanem 175 milliárd paraméterrel rendelkezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "De itt van a dolog, ez nem egy adott, hogy létrehozhat néhány óriási modellt egy hatalmas számú paraméterrel anélkül, hogy ez vagy durván túlilleszkedik a képzési adatokhoz, vagy teljesen nehézkes a képzéshez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "A mélytanulás a modellek egy olyan osztályát írja le, amely az elmúlt évtizedekben bebizonyította, hogy figyelemre méltóan jól skálázható.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Ami ezeket egyesíti, az ugyanaz a képzési algoritmus, az úgynevezett backpropagation, és a kontextus, amit szeretném, ha tudnának, hogy ahhoz, hogy ez a képzési algoritmus jól működjön, ezeknek a modelleknek egy bizonyos formátumot kell követniük.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Ha ismeri ezt a formátumot, akkor ez segít megmagyarázni a transzformátor nyelvfeldolgozási módjára vonatkozó számos választási lehetőséget, amelyek egyébként azzal a veszéllyel járnak, hogy önkényesnek tűnnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Először is, bármilyen modellt is készítesz, a bemenetet valós számok tömbjeként kell formázni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Ez jelenthet számok listáját, lehet kétdimenziós tömb, vagy nagyon gyakran van dolgunk magasabb dimenziós tömbökkel, ahol az általános kifejezés a tenzor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Gyakran úgy gondolunk arra, hogy a bemeneti adatokat fokozatosan sok különböző réteggé alakítjuk át, ahol minden egyes réteg mindig valós számok valamilyen tömbjeként van strukturálva, amíg el nem jutunk a végső réteghez, amelyet a kimenetnek tekintünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "A szövegfeldolgozási modellünk utolsó rétege például egy számokból álló lista, amely az összes lehetséges következő token valószínűségi eloszlását reprezentálja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "A mélytanulásban ezeket a modellparamétereket szinte mindig súlyoknak nevezik, és ez azért van így, mert ezeknek a modelleknek az egyik legfontosabb jellemzője, hogy ezek a paraméterek csak súlyozott összegeken keresztül lépnek kölcsönhatásba a feldolgozott adatokkal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Néhány nemlineáris függvényt is megszórsz, de ezek nem függnek a paraméterektől.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Jellemzően azonban ahelyett, hogy a súlyozott összegeket csupaszon és explicit módon írnánk ki, inkább egy mátrix vektorproduktum különböző összetevőiként csomagolva találjuk őket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Ha visszagondolunk arra, hogy a mátrixvektor-szorzás hogyan működik, a kimenet minden egyes összetevője egy súlyozott összegnek tűnik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Csak gyakran koncepcionálisan tisztább, ha olyan mátrixokra gondolunk, amelyek hangolható paraméterekkel vannak feltöltve, amelyek a feldolgozott adatokból származó vektorokat alakítanak át.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Például a GPT-3-ban szereplő 175 milliárd súlyt alig 28 000 különböző mátrixba szervezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Ezek a mátrixok viszont nyolc különböző kategóriába sorolhatók, és mi most végigmegyünk ezeken a kategóriákon, hogy megértsük, mit csinál az adott típus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Ahogy végigmegyünk rajta, azt hiszem, jó móka lesz a GPT-3 konkrét számaira hivatkozni, hogy pontosan megszámoljuk, honnan származik ez a 175 milliárd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Még ha manapság vannak is nagyobb és jobb modellek, ennek a modellnek megvan az a varázsa, mint a nagy nyelvi modellnek, amely valóban megragadta a világ figyelmét az ML közösségeken kívül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Gyakorlatilag a vállalatok sokkal szűkszavúbbak a modernebb hálózatok konkrét számait illetően.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Csak azt akarom bemutatni, hogy ha bepillantasz a motorháztető alá, hogy megnézd, mi történik egy olyan eszközben, mint a ChatGPT, szinte az összes tényleges számítás úgy néz ki, mint a mátrix-vektor szorzás.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Kicsit fennáll a veszélye, hogy elveszünk a számok milliárdjainak tengerében, de nagyon éles különbséget kell tennünk a fejünkben a modell súlyai között, amelyeket mindig kék vagy piros színnel fogok színezni, és a feldolgozott adatok között, amelyeket mindig szürkével fogok színezni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "A súlyok a tényleges agyak, ezek azok a dolgok, amelyeket a képzés során megtanult, és ezek határozzák meg, hogyan viselkedik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "A feldolgozott adatok egyszerűen kódolják az adott futtatáshoz a modellbe táplált konkrét bemeneti adatokat, például egy szöveges példát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Mindezzel az alapként, ássuk bele magunkat a szövegfeldolgozási példa első lépésébe, ami a bemenet apró darabokra bontása és ezekből a darabokból vektorok lesznek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Említettem, hogy ezeket a darabokat tokeneknek hívják, amelyek lehetnek szórészletek vagy írásjelek, de ebben a fejezetben és különösen a következőben időnként úgy szeretnék tenni, mintha tisztábban szavakra lenne bontva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Mivel mi emberek szavakban gondolkodunk, ez csak megkönnyíti a kis példákra való hivatkozást és az egyes lépések tisztázását.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "A modellnek van egy előre meghatározott szókincse, az összes lehetséges szó egy listája, mondjuk 50 000, és az első mátrix, amellyel találkozni fogunk, az úgynevezett beágyazási mátrix, egyetlen oszlopot tartalmaz minden egyes ilyen szóhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Ezek az oszlopok határozzák meg, hogy az első lépésben az egyes szavakból milyen vektor lesz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "We-nek címkézzük, és mint minden mátrixot, amit látunk, az értékei véletlenszerűen kezdődnek, de az adatok alapján megtanuljuk őket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "A szavak vektorokká alakítása már jóval a transzformátorok előtt bevett gyakorlat volt a gépi tanulásban, de ez egy kicsit furcsa, ha még sosem láttad, és megalapozza mindazt, ami ezután következik, ezért szánjunk rá egy pillanatot, hogy megismerkedjünk vele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Ezt a beágyazást gyakran nevezzük szónak, ami arra hívja fel a figyelmet, hogy ezeket a vektorokat nagyon geometrikusan úgy gondoljuk el, mint pontokat valamilyen nagy dimenziós térben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Egy három számból álló listát a 3D térben lévő pontok koordinátáiként megjeleníteni nem jelentene problémát, de a szóbeágyazások általában sokkal magasabb dimenziójúak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "A GPT-3-ban 12 288 dimenzióval rendelkeznek, és mint látni fogod, nem mindegy, hogy olyan térben dolgozol, ahol sok különböző irány van.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Ugyanúgy, ahogyan egy 3D-s tér kétdimenziós szeletét vehetnénk, és az összes pontot erre a szeletre vetíthetnénk, a szóbeágyazások animálása érdekében, amit egy egyszerű modell ad nekem, analóg módon fogok eljárni, kiválasztva egy háromdimenziós szeletet ezen a nagyon nagy dimenziójú téren, és erre vetítem a szóvektorokat, és megjelenítem az eredményeket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "A nagy ötlet itt az, hogy ahogy a modell a súlyok finomhangolásával és hangolásával meghatározza, hogy pontosan hogyan ágyazódnak be a szavak vektorokként a képzés során, a modell hajlamos megállapodni a beágyazások olyan készletén, ahol a térben lévő irányok egyfajta szemantikai jelentéssel bírnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Az itt futtatott egyszerű szó-vektor modell esetében, ha lefuttatok egy keresést az összes olyan szóra, amelynek beágyazása a legközelebb áll a toronyéhoz, akkor észreveheted, hogy mind nagyon hasonló torony-szerű hangzást adnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "És ha szeretnél egy kis Pythont elővenni, és otthon is játszani, ez az a konkrét modell, amit az animációk elkészítéséhez használok.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Ez nem egy transzformátor, de elég ahhoz, hogy szemléltesse azt a gondolatot, hogy a térben lévő irányok szemantikai jelentést hordozhatnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Egy nagyon klasszikus példa erre, hogy ha a nő és a férfi vektorai közötti különbséget vesszük, amit úgy képzelünk el, mint egy kis vektort, amely az egyik és a másik csúcsát köti össze, akkor ez nagyon hasonló a király és a királynő közötti különbséghez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Tegyük fel, hogy nem ismered a női uralkodó szavát, akkor úgy találhatod meg, hogy veszed a király szót, hozzáadod ezt a nő-férfi irányt, és megkeresed az ehhez a ponthoz legközelebbi beágyazásokat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Legalábbis valahogy így.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Annak ellenére, hogy ez egy klasszikus példa a modellre, amellyel játszom, a királynő valódi beágyazódása valójában egy kicsit távolabb van, mint ahogy ez sugallná, feltehetően azért, mert a királynő a képzési adatokban használt módja nem csupán a király női változata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Amikor játszottam, a családi kapcsolatok sokkal jobban illusztrálták a gondolatot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "A lényeg az, hogy úgy tűnik, hogy a modell a képzés során előnyösnek találta, ha a beágyazásokat úgy választja meg, hogy a tér egyik iránya a nemi információt kódolja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Egy másik példa: ha fogjuk Olaszország beágyazottságát, és kivonjuk belőle Németország beágyazottságát, majd ezt hozzáadjuk Hitler beágyazottságához, akkor valami olyasmit kapunk, ami nagyon közel áll Mussolini beágyazottságához.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Mintha a modell megtanult volna egyes irányokat az olaszsággal, másokat pedig a második világháborús tengelyvezérekkel társítani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Talán a kedvenc példám ebben az értelemben az, hogy egyes modellekben, ha a Németország és Japán közötti különbséget a sushihez adjuk, akkor nagyon közel kerülünk a bratwursthoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "A legközelebbi szomszédok megtalálásának játékában is örömmel láttam, hogy Kat milyen közel volt mind a vadállathoz, mind a szörnyeteghez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Egy kis matematikai intuíció, amit hasznos észben tartani, különösen a következő fejezetben, hogy két vektor ponttermelése úgy tekinthető, mint egy módja annak, hogy mérjük, mennyire jól illeszkednek egymáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Számítási szempontból a ponttételek az összes megfelelő komponens szorzását, majd az eredmények összeadását jelentik, ami jó, mivel a számításaink nagy részének súlyozott összegeknek kell kinéznie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geometriai szempontból a pontszorzat pozitív, ha a vektorok hasonló irányba mutatnak, nulla, ha merőlegesek egymásra, és negatív, ha ellentétes irányba mutatnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Tegyük fel például, hogy ezzel a modellel játszottál, és azt feltételezed, hogy a macskák mínusz macska beágyazása egyfajta pluralitás irányt képviselhet ebben a térben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Ennek teszteléséhez fogom ezt a vektort, és kiszámítom a pontgyakoriságát bizonyos egyes számú főnevek beágyazásaival, és összehasonlítom a megfelelő többes számú főnevek pontgyakoriságával.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Ha ezzel játszadozol, észreveheted, hogy a többes számúak valóban következetesen magasabb értékeket adnak, mint az egyes számúak, ami azt jelzi, hogy jobban igazodnak ehhez az irányhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Az is vicces, hogy ha ezt a pontszorzatot az 1, 2, 3, stb. szavak beágyazásaival vesszük, akkor növekvő értékeket kapunk, tehát mintha kvantitatív módon mérni tudnánk, hogy a modell mennyire többes számúnak talál egy adott szót.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "A szavak beágyazódásának sajátosságait ismét adatok segítségével tanuljuk meg.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Ez a beágyazási mátrix, amelynek oszlopai megmondják, hogy mi történik az egyes szavakkal, a modellünkben a súlyok első halmaza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "A GPT-3 számokat használva a szókincs mérete konkrétan 50 257, és ez technikailag nem szavakból, hanem tokenekből áll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "A beágyazási dimenzió 12 288, és ezeket megszorozva azt kapjuk, hogy ez körülbelül 617 millió súlyból áll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Menjünk előre, és adjuk ezt össze egy futó számlához, ne feledjük, hogy a végére 175 milliárdot kell számolnunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "A transzformátorok esetében a beágyazási térben lévő vektorokra úgy kell gondolni, hogy azok nem csupán egyes szavakat képviselnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Egyrészt kódolnak információt az adott szó helyzetéről is, amiről később még beszélünk, de ami még fontosabb, hogy úgy kell rájuk gondolni, mint amelyek képesek a kontextus felszívására.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Egy vektor, amely például a király szó beágyazásaként kezdte életét, a hálózat különböző blokkjai által fokozatosan megrántódhat, hogy a végére egy sokkal specifikusabb és árnyaltabb irányba mutasson, amely valahogy azt kódolja, hogy egy Skóciában élő királyról van szó, aki az előző király meggyilkolása után jutott a posztjára, és akit shakespeare-i nyelven írnak le.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Gondolkodj el azon, hogy mit értesz egy adott szó alatt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Az adott szó jelentését egyértelműen a környezet határozza meg, és néha ez a környezet nagy távolságból származó kontextust is magában foglal, ezért egy olyan modell összeállításakor, amely képes megjósolni, hogy melyik szó következik, a cél az, hogy valahogy képessé tegyük a modellt a kontextus hatékony beépítésére.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Hogy világos legyen, ebben a legelső lépésben, amikor a bemeneti szöveg alapján létrehozza a vektorok tömbjét, ezek mindegyike egyszerűen a beágyazási mátrixból kerül kiragadásra, így kezdetben mindegyik csak egyetlen szó jelentését kódolhatja, a környezetéből származó input nélkül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "De úgy kell gondolni, hogy a hálózat elsődleges célja, amelyen keresztül áramlik, az, hogy lehetővé tegye, hogy minden egyes vektor olyan jelentést szívjon magába, amely sokkal gazdagabb és specifikusabb, mint amit a puszta szavak képviselhetnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "A hálózat egyszerre csak meghatározott számú vektort tud feldolgozni, amit a kontextus méretének nevezünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "A GPT-3 esetében 2048-as kontextusmérettel lett betanítva, így a hálózaton átáramló adatok mindig úgy néznek ki, mint ez a 2048 oszlopból álló tömb, amelynek mindegyike 12 000 dimenzióval rendelkezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Ez a kontextus mérete korlátozza, hogy a transzformátor mennyi szöveget tud beépíteni, amikor a következő szóra vonatkozó előrejelzést készít.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Ez az oka annak, hogy a hosszú beszélgetések bizonyos chatbotokkal, mint például a ChatGPT korai változatai, gyakran azt az érzést keltették, hogy a bot elveszíti a beszélgetés fonalát, ha túl sokáig folytatod.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "A figyelem részleteire majd a kellő időben kitérünk, de előreugorva szeretnék egy percig beszélni arról, hogy mi történik a legvégén.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Ne feledje, hogy a kívánt kimenet egy valószínűségi eloszlás az összes lehetséges következő tokenre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Például, ha a legvégső szó a Professzor, és a kontextusban olyan szavak szerepelnek, mint Harry Potter, és közvetlenül előtte a legkevésbé kedvenc tanár, és ha adunk némi mozgásteret azzal, hogy úgy teszünk, mintha a tokenek egyszerűen teljes szavaknak tűnnének, akkor egy jól képzett hálózat, amely Harry Potterről szerzett ismereteket, feltehetően magas számot rendelne a Snape szóhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Ez két különböző lépést foglal magában.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Az első az, hogy egy másik mátrixot használunk, amely az adott kontextus utolsó vektorát egy 50 000 értékből álló listára képezi le, amely a szókincs minden egyes tokenjéhez tartozik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Ezután van egy függvény, amely ezt egy valószínűségi eloszlássá normalizálja, ezt Softmaxnak hívják, és erről egy pillanat múlva többet fogunk beszélni, de előtte talán egy kicsit furcsának tűnhet, hogy csak ezt az utolsó beágyazást használjuk a jósláshoz, amikor az utolsó lépésben több ezer más vektor van a rétegben, amelyek csak ott ülnek a saját, kontextusban gazdag jelentésükkel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Ez azzal függ össze, hogy a képzési folyamatban sokkal hatékonyabbnak bizonyul, ha az utolsó rétegben lévő vektorok mindegyikét arra használjuk, hogy egyidejűleg előrejelzést készítsünk arra, ami közvetlenül utána következik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Az edzésről még sok mindent el fogok mondani később, de ezt most csak szeretném kiemelni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Ezt a mátrixot nevezzük beágyazásmentesítő mátrixnak, és a WU jelölést adjuk neki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Ismét, mint az összes súlymátrix, amit látunk, a bejegyzések véletlenszerűen kezdődnek, de a betanítási folyamat során megtanulják őket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "A teljes paraméterszámot megtartva, ez a beágyazás nélküli mátrix a szókincs minden egyes szavához egy sorral rendelkezik, és minden sornak ugyanannyi eleme van, mint a beágyazási dimenziónak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Ez nagyon hasonlít a beágyazási mátrixhoz, csak a sorrend felcserélődött, így további 617 millió paramétert ad a hálózathoz, ami azt jelenti, hogy az eddigi számunk valamivel több mint egymilliárd, ami egy kis, de nem teljesen jelentéktelen része a 175 milliárdnak, amivel a végén összesen rendelkezni fogunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "A fejezet utolsó mini-leckéjeként szeretnék többet beszélni erről a softmax függvényről, mivel ez a függvény még egyszer megjelenik, amint belemerülünk a figyelem blokkokba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Az ötlet az, hogy ha azt akarjuk, hogy egy számsorozat valószínűségi eloszlásként működjön, mondjuk az összes lehetséges következő szóra vonatkozó eloszlásként, akkor minden értéknek 0 és 1 között kell lennie, és azt is meg kell adni, hogy mindegyikük összege 1 legyen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Ha azonban a tanulási játékot játszod, ahol minden, amit csinálsz, mátrix-vektor szorzásnak tűnik, akkor az alapértelmezetten kapott kimenetek egyáltalán nem tartják be ezt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Az értékek gyakran negatívak, vagy sokkal nagyobbak, mint 1, és szinte biztosan nem adódnak össze 1-re.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "A softmax a szabványos módja annak, hogy egy tetszőleges számlistát érvényes eloszlássá alakítsunk oly módon, hogy a legnagyobb értékek a legközelebb kerüljenek az 1-hez, a kisebb értékek pedig nagyon közel kerüljenek a 0-hoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Ennyit kell tudnod.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "De ha kíváncsiak vagytok, a módszer úgy működik, hogy először minden egyes szám hatványára emeljük az e-t, ami azt jelenti, hogy most már van egy pozitív értékekből álló listánk, majd fogjuk az összes pozitív érték összegét, és minden egyes kifejezést elosztunk ezzel az összeggel, ami normalizálja a listát egy olyan listává, amely 1-re adódik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Észre fogja venni, hogy ha a bemenetben az egyik szám értelmesen nagyobb, mint a többi, akkor a kimeneten a megfelelő kifejezés uralja az eloszlást, így ha ebből mintavételezne, akkor szinte biztosan csak a maximalizáló bemenetet választaná.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "De ez puhább, mint a maximum kiválasztása abban az értelemben, hogy ha más értékek is hasonlóan nagyok, akkor azok is értelmes súlyt kapnak az eloszlásban, és minden folyamatosan változik, ahogy folyamatosan változtatjuk a bemeneteket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "Bizonyos helyzetekben, például amikor a ChatGPT ezt az eloszlást használja egy következő szó létrehozására, van hely egy kis extra szórakozásra, ha egy kis extra fűszert adunk ehhez a függvényhez, egy t konstanssal, amelyet az exponensek nevezőjébe dobunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Ezt nevezzük hőmérsékletnek, mivel homályosan hasonlít a hőmérséklet szerepére bizonyos termodinamikai egyenletekben, és az a hatása, hogy ha t nagyobb, akkor nagyobb súlyt adunk az alacsonyabb értékeknek, vagyis az eloszlás egy kicsit egyenletesebb lesz, ha pedig t kisebb, akkor a nagyobb értékek agresszívebben fognak dominálni, ahol a végletben, ha t-t nullára állítjuk, akkor minden súly a maximális értékre megy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Például a GPT-3-mal generálok egy történetet a \"Volt egyszer egy A\" magszöveggel, de minden esetben más-más hőmérsékletet használok.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "A hőmérsékleti nulla azt jelenti, hogy mindig a legkiszámíthatóbb szót választja, és amit kapunk, az végül az Aranyhaj közhelyes származéka lesz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "A magasabb hőmérséklet esélyt ad arra, hogy kevésbé valószínű szavakat válasszon, de ez kockázattal jár.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "Ebben az esetben a történet eredetibben indul, egy fiatal dél-koreai webes művészről szól, de hamar ostobasággá fajul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Technikailag az API nem engedi, hogy 2-nél nagyobb hőmérsékletet válasszon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Ennek nincs matematikai oka, ez csak egy önkényes korlátozás, amelyet azért szabtak meg, hogy az eszközük ne generáljon túlságosan képtelen dolgokat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Szóval, ha kíváncsi vagy, az animáció valójában úgy működik, hogy a GPT-3 által generált 20 legvalószínűbb következő tokent veszem, ami úgy tűnik, hogy a maximum, amit adnak nekem, majd a valószínűségeket egy 1/5-ös exponens alapján módosítom.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Még egy kis szakzsargon: ugyanúgy, ahogyan a függvény kimenetének összetevőit valószínűségeknek nevezhetjük, az emberek a bemeneteket gyakran logitoknak nevezik, vagy egyesek logitoknak, mások logitoknak, én logitokat fogok mondani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Tehát például, amikor betáplálunk egy szöveget, az összes szóbeágyazás átfolyik a hálózaton, és elvégezzük ezt a végső szorzást a beágyazás nélküli mátrixszal, a gépi tanulással foglalkozó szakemberek a nyers, nem normalizált kimenet összetevőire úgy hivatkoznak, mint a következő szó előrejelzésének logitjaira.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "A fejezet célja nagyrészt az volt, hogy lerakjuk az alapokat a figyelem mechanizmusának megértéséhez, a Karate Kid wax-on-wax-off stílusban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Ha van egy erős intuíciód a szóbeágyazásokhoz, a softmaxhoz, ahhoz, hogy hogyan mérik a hasonlóságot a ponttételek, és ahhoz, hogy a számítások többsége úgy néz ki, mint a mátrixszorzás, hangolható paraméterekkel teli mátrixokkal, akkor a figyelem mechanizmusának megértése, az egész modern mesterséges intelligencia boom sarokköve, viszonylag könnyen megy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Ehhez csatlakozzatok hozzám a következő fejezetben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Miközben ezt publikálom, a következő fejezet vázlata elérhető a Patreon támogatói számára.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "A végleges verziónak egy-két héten belül nyilvánosan elérhetőnek kell lennie, általában attól függ, hogy mennyit változtatok a felülvizsgálat alapján.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Addig is, ha szeretnél belevetni magad a figyelembe, és ha szeretnéd egy kicsit segíteni a csatornát, ott vár.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]