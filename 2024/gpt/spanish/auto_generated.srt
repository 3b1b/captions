1
00:00:00,000 --> 00:00:04,560
Las siglas GPT significan Transformador Generativo Preentrenado.

2
00:00:05,220 --> 00:00:07,243
Así que la primera palabra es bastante sencilla, 

3
00:00:07,243 --> 00:00:09,020
se trata de robots que generan texto nuevo.

4
00:00:09,800 --> 00:00:13,275
Preentrenado se refiere a que el modelo pasó por un proceso de aprendizaje 

5
00:00:13,275 --> 00:00:16,564
a partir de una cantidad masiva de datos, y el prefijo insinúa que hay 

6
00:00:16,564 --> 00:00:20,040
más margen para afinarlo en tareas específicas con entrenamiento adicional.

7
00:00:20,720 --> 00:00:22,900
Pero la última palabra, esa es la verdadera pieza clave.

8
00:00:23,380 --> 00:00:26,169
Un transformador es un tipo específico de red neuronal, 

9
00:00:26,169 --> 00:00:29,954
un modelo de aprendizaje automático, y es el invento central que subyace al 

10
00:00:29,954 --> 00:00:31,000
auge actual de la IA.

11
00:00:31,740 --> 00:00:35,238
Lo que quiero hacer con este vídeo y los capítulos siguientes es dar una 

12
00:00:35,238 --> 00:00:39,120
explicación visual de lo que ocurre realmente en el interior de un transformador.

13
00:00:39,700 --> 00:00:42,820
Vamos a seguir los datos que fluyen a través de él e iremos paso a paso.

14
00:00:43,440 --> 00:00:47,380
Hay muchos tipos diferentes de modelos que puedes construir utilizando transformadores.

15
00:00:47,800 --> 00:00:50,800
Algunos modelos captan el audio y producen una transcripción.

16
00:00:51,340 --> 00:00:53,731
Esta frase procede de un modelo que va al revés, 

17
00:00:53,731 --> 00:00:56,220
produciendo habla sintética sólo a partir de texto.

18
00:00:56,660 --> 00:01:01,190
Todas esas herramientas que tomaron el mundo por asalto en 2022, como Dolly y Midjourney, 

19
00:01:01,190 --> 00:01:05,519
que toman una descripción de texto y producen una imagen, se basan en transformadores.

20
00:01:06,000 --> 00:01:09,767
Aunque no consiga que entienda lo que se supone que es una criatura de tarta, 

21
00:01:09,767 --> 00:01:13,100
me sigue asombrando que este tipo de cosas sean remotamente posibles.

22
00:01:13,900 --> 00:01:18,029
Y el transformador original introducido en 2017 por Google se inventó 

23
00:01:18,029 --> 00:01:22,100
para el caso de uso específico de traducir texto de un idioma a otro.

24
00:01:22,660 --> 00:01:25,268
Pero la variante en la que tú y yo nos centraremos, 

25
00:01:25,268 --> 00:01:27,876
que es la que subyace en herramientas como ChatGPT, 

26
00:01:27,876 --> 00:01:30,836
será un modelo entrenado para tomar un fragmento de texto, 

27
00:01:30,836 --> 00:01:34,748
tal vez incluso con algunas imágenes o sonidos circundantes que lo acompañen, 

28
00:01:34,748 --> 00:01:38,260
y producir una predicción de lo que viene a continuación en el pasaje.

29
00:01:38,600 --> 00:01:41,365
Esa predicción adopta la forma de una distribución de probabilidad 

30
00:01:41,365 --> 00:01:43,800
sobre muchos trozos de texto diferentes que podrían seguir.

31
00:01:45,040 --> 00:01:47,331
A primera vista, podrías pensar que predecir la siguiente 

32
00:01:47,331 --> 00:01:49,940
palabra parece un objetivo muy distinto de generar un nuevo texto.

33
00:01:50,180 --> 00:01:52,972
Pero una vez que tienes un modelo de predicción como éste, 

34
00:01:52,972 --> 00:01:56,948
una forma sencilla de generar un fragmento de texto más largo es darle un fragmento 

35
00:01:56,948 --> 00:02:00,972
inicial con el que trabajar, hacer que tome una muestra aleatoria de la distribución 

36
00:02:00,972 --> 00:02:04,664
que acaba de generar, añadir esa muestra al texto y volver a ejecutar todo el 

37
00:02:04,664 --> 00:02:08,025
proceso para hacer una nueva predicción basada en todo el texto nuevo, 

38
00:02:08,025 --> 00:02:09,539
incluido lo que acaba de añadir.

39
00:02:10,100 --> 00:02:13,000
No sé a ti, pero a mí no me parece que esto deba funcionar.

40
00:02:13,420 --> 00:02:16,585
En esta animación, por ejemplo, estoy ejecutando GPT-2 en mi portátil 

41
00:02:16,585 --> 00:02:19,615
y haciendo que prediga y muestree repetidamente el siguiente trozo 

42
00:02:19,615 --> 00:02:22,420
de texto para generar una historia basada en el texto semilla.

43
00:02:22,420 --> 00:02:26,120
La historia no tiene mucho sentido.

44
00:02:26,500 --> 00:02:29,795
Pero si en lugar de eso lo cambio por llamadas a la API de GPT-3, 

45
00:02:29,795 --> 00:02:32,691
que es el mismo modelo básico, sólo que mucho más grande, 

46
00:02:32,691 --> 00:02:35,986
de repente casi por arte de magia obtenemos una historia sensata, 

47
00:02:35,986 --> 00:02:39,581
que incluso parece inferir que una criatura pi viviría en una tierra de 

48
00:02:39,581 --> 00:02:40,880
matemáticas y computación.

49
00:02:41,580 --> 00:02:45,062
Este proceso de predicción repetida y muestreo es esencialmente lo que 

50
00:02:45,062 --> 00:02:48,495
ocurre cuando interactúas con ChatGPT o con cualquiera de estos otros 

51
00:02:48,495 --> 00:02:51,880
grandes modelos lingüísticos y los ves producir una palabra cada vez.

52
00:02:52,480 --> 00:02:55,902
De hecho, una función que me gustaría mucho es la posibilidad de 

53
00:02:55,902 --> 00:02:59,220
ver la distribución subyacente de cada palabra nueva que elija.

54
00:03:03,820 --> 00:03:06,021
Empecemos con una vista previa de muy alto nivel de 

55
00:03:06,021 --> 00:03:08,180
cómo fluyen los datos a través de un transformador.

56
00:03:08,640 --> 00:03:12,954
Dedicaremos mucho más tiempo a motivar e interpretar y ampliar los detalles de cada paso, 

57
00:03:12,954 --> 00:03:16,982
pero a grandes rasgos, cuando uno de estos chatbots genera una palabra determinada, 

58
00:03:16,982 --> 00:03:18,660
esto es lo que ocurre bajo el capó.

59
00:03:19,080 --> 00:03:22,040
En primer lugar, la entrada se divide en un montón de trocitos.

60
00:03:22,620 --> 00:03:26,019
Estas piezas se denominan tokens, y en el caso del texto suelen ser 

61
00:03:26,019 --> 00:03:29,820
palabras o trocitos de palabras u otras combinaciones de caracteres comunes.

62
00:03:30,740 --> 00:03:33,678
Si se trata de imágenes o sonido, las fichas podrían ser 

63
00:03:33,678 --> 00:03:37,080
pequeños fragmentos de esa imagen o pequeños trozos de ese sonido.

64
00:03:37,580 --> 00:03:41,919
Cada una de estas fichas se asocia a un vector, es decir, a una lista de números, 

65
00:03:41,919 --> 00:03:45,360
que pretende codificar de algún modo el significado de esa pieza.

66
00:03:45,880 --> 00:03:48,895
Si piensas en estos vectores como si dieran coordenadas en algún espacio 

67
00:03:48,895 --> 00:03:51,829
de muy alta dimensión, las palabras con significados similares tienden 

68
00:03:51,829 --> 00:03:54,680
a aterrizar en vectores que están cerca unos de otros en ese espacio.

69
00:03:55,280 --> 00:03:58,311
A continuación, esta secuencia de vectores pasa por una operación que se 

70
00:03:58,311 --> 00:04:01,260
conoce como bloque de atención, y esto permite que los vectores hablen 

71
00:04:01,260 --> 00:04:04,500
entre sí y se pasen información de un lado a otro para actualizar sus valores.

72
00:04:04,880 --> 00:04:08,121
Por ejemplo, el significado de la palabra modelo en la frase un modelo de 

73
00:04:08,121 --> 00:04:11,800
aprendizaje automático es diferente de su significado en la frase un modelo de moda.

74
00:04:12,260 --> 00:04:15,367
El bloque de atención es el responsable de averiguar qué palabras 

75
00:04:15,367 --> 00:04:19,323
del contexto son relevantes para actualizar los significados de qué otras palabras, 

76
00:04:19,323 --> 00:04:21,959
y cómo deben actualizarse exactamente esos significados.

77
00:04:22,500 --> 00:04:24,798
Y de nuevo, siempre que utilizo la palabra significado, 

78
00:04:24,798 --> 00:04:28,040
éste está de algún modo totalmente codificado en las entradas de esos vectores.

79
00:04:29,180 --> 00:04:32,641
Después, estos vectores pasan por un tipo de operación diferente, 

80
00:04:32,641 --> 00:04:37,256
y dependiendo de la fuente que estés leyendo se denominará perceptrón multicapa o quizá 

81
00:04:37,256 --> 00:04:38,200
capa feed-forward.

82
00:04:38,580 --> 00:04:40,684
Y aquí los vectores no hablan entre sí, sino que 

83
00:04:40,684 --> 00:04:42,660
todos realizan la misma operación en paralelo.

84
00:04:43,060 --> 00:04:45,817
Y aunque este bloque es un poco más difícil de interpretar, 

85
00:04:45,817 --> 00:04:49,403
más adelante hablaremos de cómo el paso es un poco como hacer una larga lista 

86
00:04:49,403 --> 00:04:53,218
de preguntas sobre cada vector, y luego actualizarlos en función de las respuestas 

87
00:04:53,218 --> 00:04:54,000
a esas preguntas.

88
00:04:54,900 --> 00:04:58,335
Todas las operaciones de estos dos bloques parecen un montón 

89
00:04:58,335 --> 00:05:01,658
gigante de multiplicaciones de matrices, y nuestro trabajo 

90
00:05:01,658 --> 00:05:05,320
principal va a ser comprender cómo leer las matrices subyacentes.

91
00:05:06,980 --> 00:05:09,980
Estoy pasando por alto algunos detalles sobre algunos pasos de normalización que se 

92
00:05:09,980 --> 00:05:12,980
producen entre medias, pero al fin y al cabo esto es una vista previa de alto nivel.

93
00:05:13,680 --> 00:05:16,534
Después de eso, el proceso se repite esencialmente, 

94
00:05:16,534 --> 00:05:20,596
vas y vienes entre bloques de atención y bloques de perceptrón multicapa, 

95
00:05:20,596 --> 00:05:25,371
hasta que al final la esperanza es que todo el significado esencial del pasaje se haya 

96
00:05:25,371 --> 00:05:28,500
cocido de algún modo en el último vector de la secuencia.

97
00:05:28,920 --> 00:05:32,171
A continuación, realizamos una determinada operación sobre ese último vector 

98
00:05:32,171 --> 00:05:35,464
que produce una distribución de probabilidad sobre todos los posibles tokens, 

99
00:05:35,464 --> 00:05:38,420
todos los posibles trocitos de texto que podrían venir a continuación.

100
00:05:38,980 --> 00:05:42,325
Y como he dicho, una vez que tengas una herramienta que prediga lo que viene a 

101
00:05:42,325 --> 00:05:45,754
continuación dado un fragmento de texto, puedes alimentarla con un poco de texto 

102
00:05:45,754 --> 00:05:49,184
semilla y hacer que juegue repetidamente a este juego de predecir lo que viene a 

103
00:05:49,184 --> 00:05:51,343
continuación, tomando muestras de la distribución, 

104
00:05:51,343 --> 00:05:53,080
añadiéndolas y repitiendo una y otra vez.

105
00:05:53,640 --> 00:05:55,958
Algunos de los que sabéis quizá recordéis que, 

106
00:05:55,958 --> 00:05:59,460
mucho antes de que ChatGPT entrara en escena, así es como se veían las 

107
00:05:59,460 --> 00:06:02,962
primeras demos de GPT-3, hacías que autocompletara historias y ensayos 

108
00:06:02,962 --> 00:06:04,640
basándose en un fragmento inicial.

109
00:06:05,580 --> 00:06:08,374
Para convertir una herramienta como ésta en un chatbot, 

110
00:06:08,374 --> 00:06:12,566
el punto de partida más fácil es tener un poco de texto que establezca el escenario 

111
00:06:12,566 --> 00:06:15,411
de un usuario interactuando con un asistente de IA útil, 

112
00:06:15,411 --> 00:06:19,853
lo que llamarías la indicación del sistema, y luego utilizarías la pregunta o indicación 

113
00:06:19,853 --> 00:06:22,448
inicial del usuario como primera parte del diálogo, 

114
00:06:22,448 --> 00:06:26,940
y luego harías que empezara a predecir lo que ese asistente de IA útil diría en respuesta.

115
00:06:27,720 --> 00:06:32,206
Hay más que decir sobre un paso de entrenamiento necesario para que esto funcione bien, 

116
00:06:32,206 --> 00:06:33,940
pero a alto nivel ésta es la idea.

117
00:06:35,720 --> 00:06:39,880
En este capítulo, tú y yo vamos a ampliar los detalles de lo que ocurre al principio de 

118
00:06:39,880 --> 00:06:43,852
la red, al final de la red, y también quiero dedicar mucho tiempo a repasar algunos 

119
00:06:43,852 --> 00:06:47,918
conocimientos básicos importantes, cosas que habrían sido una segunda naturaleza para 

120
00:06:47,918 --> 00:06:51,843
cualquier ingeniero de aprendizaje automático en el momento en que aparecieron los 

121
00:06:51,843 --> 00:06:52,600
transformadores.

122
00:06:53,060 --> 00:06:56,610
Si te sientes cómodo con esos conocimientos previos y estás un poco impaciente, 

123
00:06:56,610 --> 00:07:00,338
puedes pasar al siguiente capítulo, que se va a centrar en los bloques de atención, 

124
00:07:00,338 --> 00:07:02,780
generalmente considerados el corazón del transformador.

125
00:07:03,360 --> 00:07:06,897
Después quiero hablar más sobre estos bloques de perceptrón multicapa, 

126
00:07:06,897 --> 00:07:11,082
cómo funciona el entrenamiento y otra serie de detalles que se habrán omitido hasta 

127
00:07:11,082 --> 00:07:11,680
ese momento.

128
00:07:12,180 --> 00:07:15,464
Para un contexto más amplio, estos vídeos son adiciones a una miniserie sobre 

129
00:07:15,464 --> 00:07:18,370
aprendizaje profundo, y no pasa nada si no has visto los anteriores, 

130
00:07:18,370 --> 00:07:21,444
creo que puedes hacerlo fuera de orden, pero antes de sumergirnos en los 

131
00:07:21,444 --> 00:07:24,645
transformadores específicamente, creo que merece la pena asegurarnos de que 

132
00:07:24,645 --> 00:07:27,635
estamos en la misma página sobre la premisa básica y la estructura del 

133
00:07:27,635 --> 00:07:28,520
aprendizaje profundo.

134
00:07:29,020 --> 00:07:32,741
A riesgo de decir lo obvio, éste es un enfoque del aprendizaje automático, 

135
00:07:32,741 --> 00:07:35,719
que describe cualquier modelo en el que utilizas datos para 

136
00:07:35,719 --> 00:07:38,300
determinar de algún modo cómo se comporta un modelo.

137
00:07:39,140 --> 00:07:42,605
Lo que quiero decir con esto es que, digamos que quieres una función que tome 

138
00:07:42,605 --> 00:07:44,915
una imagen y produzca una etiqueta que la describa, 

139
00:07:44,915 --> 00:07:48,292
o nuestro ejemplo de predecir la siguiente palabra dado un pasaje de texto, 

140
00:07:48,292 --> 00:07:51,580
o cualquier otra tarea que parezca requerir algún elemento de intuición y 

141
00:07:51,580 --> 00:07:52,780
reconocimiento de patrones.

142
00:07:53,200 --> 00:07:57,291
Hoy en día casi lo damos por sentado, pero la idea del aprendizaje automático es que, 

143
00:07:57,291 --> 00:08:01,050
en lugar de intentar definir explícitamente un procedimiento para realizar esa 

144
00:08:01,050 --> 00:08:04,903
tarea en código, que es lo que se habría hecho en los primeros tiempos de la IA, 

145
00:08:04,903 --> 00:08:08,138
se establece una estructura muy flexible con parámetros ajustables, 

146
00:08:08,138 --> 00:08:11,088
como un montón de mandos y diales, y luego, de alguna manera, 

147
00:08:11,088 --> 00:08:14,704
se utilizan muchos ejemplos de cómo debería ser el resultado de una entrada 

148
00:08:14,704 --> 00:08:18,463
determinada para ajustar y afinar los valores de esos parámetros con el fin de 

149
00:08:18,463 --> 00:08:19,700
imitar ese comportamiento.

150
00:08:19,700 --> 00:08:23,889
Por ejemplo, tal vez la forma más sencilla de aprendizaje automático sea la regresión 

151
00:08:23,889 --> 00:08:27,153
lineal, en la que tus entradas y salidas son números individuales, 

152
00:08:27,153 --> 00:08:30,076
algo así como los metros cuadrados de una casa y su precio, 

153
00:08:30,076 --> 00:08:34,023
y lo que quieres es encontrar una línea de mejor ajuste a través de estos datos, 

154
00:08:34,023 --> 00:08:36,799
ya sabes, para predecir los precios futuros de las casas.

155
00:08:37,440 --> 00:08:40,526
Esa recta se describe mediante dos parámetros continuos, 

156
00:08:40,526 --> 00:08:43,882
digamos la pendiente y la intersección y, y el objetivo de la 

157
00:08:43,882 --> 00:08:48,160
regresión lineal es determinar esos parámetros para que se ajusten a los datos.

158
00:08:48,880 --> 00:08:52,100
Ni que decir tiene que los modelos de aprendizaje profundo se complican mucho más.

159
00:08:52,620 --> 00:08:57,660
La GPT-3, por ejemplo, no tiene dos, sino 175.000 millones de parámetros.

160
00:08:58,120 --> 00:09:02,037
Pero la cuestión es que no es seguro que puedas crear un modelo gigantesco 

161
00:09:02,037 --> 00:09:05,746
con un gran número de parámetros sin que se ajuste excesivamente a los 

162
00:09:05,746 --> 00:09:09,560
datos de entrenamiento o sin que sea completamente imposible de entrenar.

163
00:09:10,260 --> 00:09:13,126
El aprendizaje profundo describe una clase de modelos que en 

164
00:09:13,126 --> 00:09:16,180
las dos últimas décadas han demostrado escalar notablemente bien.

165
00:09:16,480 --> 00:09:19,565
Lo que los unifica es el mismo algoritmo de entrenamiento, 

166
00:09:19,565 --> 00:09:23,278
llamado retropropagación, y el contexto que quiero que tengas a medida 

167
00:09:23,278 --> 00:09:27,828
que avanzamos es que, para que este algoritmo de entrenamiento funcione bien a escala, 

168
00:09:27,828 --> 00:09:31,280
estos modelos tienen que seguir un determinado formato específico.

169
00:09:31,800 --> 00:09:36,148
Si conoces este formato, te ayudará a explicar muchas de las opciones sobre cómo procesa 

170
00:09:36,148 --> 00:09:40,400
el lenguaje un transformador, que de otro modo corren el riesgo de parecer arbitrarias.

171
00:09:41,440 --> 00:09:43,849
En primer lugar, sea cual sea el modelo que estés haciendo, 

172
00:09:43,849 --> 00:09:46,740
la entrada tiene que estar formateada como una matriz de números reales.

173
00:09:46,740 --> 00:09:50,701
Esto puede significar una lista de números, puede ser una matriz bidimensional, 

174
00:09:50,701 --> 00:09:53,771
o muy a menudo tratas con matrices de dimensiones superiores, 

175
00:09:53,771 --> 00:09:56,000
donde el término general utilizado es tensor.

176
00:09:56,560 --> 00:10:00,508
A menudo piensas en esos datos de entrada como si se transformaran progresivamente en 

177
00:10:00,508 --> 00:10:04,640
muchas capas distintas, donde, de nuevo, cada capa siempre se estructura como una especie 

178
00:10:04,640 --> 00:10:08,680
de matriz de números reales, hasta que llegas a una capa final que consideras la salida.

179
00:10:09,280 --> 00:10:11,915
Por ejemplo, la capa final de nuestro modelo de procesamiento 

180
00:10:11,915 --> 00:10:14,679
de texto es una lista de números que representan la distribución 

181
00:10:14,679 --> 00:10:17,060
de probabilidad de todos los posibles tokens siguientes.

182
00:10:17,820 --> 00:10:21,816
En el aprendizaje profundo, estos parámetros del modelo casi siempre se denominan pesos, 

183
00:10:21,816 --> 00:10:25,768
y esto se debe a que una característica clave de estos modelos es que la única forma en 

184
00:10:25,768 --> 00:10:29,406
que estos parámetros interactúan con los datos que se procesan es mediante sumas 

185
00:10:29,406 --> 00:10:29,900
ponderadas.

186
00:10:30,340 --> 00:10:34,360
También salpicarás algunas funciones no lineales, pero no dependerán de parámetros.

187
00:10:35,200 --> 00:10:38,465
Sin embargo, normalmente, en lugar de ver las sumas ponderadas 

188
00:10:38,465 --> 00:10:41,058
desnudas y escritas explícitamente de esta forma, 

189
00:10:41,058 --> 00:10:45,620
las encontrarás empaquetadas como varios componentes en un producto vectorial matricial.

190
00:10:46,740 --> 00:10:50,388
Equivale a decir lo mismo, si recuerdas cómo funciona la multiplicación 

191
00:10:50,388 --> 00:10:54,240
vectorial matricial, cada componente de la salida parece una suma ponderada.

192
00:10:54,780 --> 00:10:59,946
Para ti y para mí suele ser conceptualmente más limpio pensar en matrices llenas de 

193
00:10:59,946 --> 00:11:05,420
parámetros sintonizables que transforman vectores extraídos de los datos que se procesan.

194
00:11:06,340 --> 00:11:10,082
Por ejemplo, esos 175.000 millones de pesos de la GPT-3 

195
00:11:10,082 --> 00:11:14,160
están organizados en algo menos de 28.000 matrices distintas.

196
00:11:14,660 --> 00:11:17,680
Esas matrices se dividen a su vez en ocho categorías diferentes, 

197
00:11:17,680 --> 00:11:21,724
y lo que vamos a hacer tú y yo es recorrer cada una de esas categorías para comprender 

198
00:11:21,724 --> 00:11:22,700
lo que hace ese tipo.

199
00:11:23,160 --> 00:11:27,006
A medida que avanzamos, creo que es divertido hacer referencia a las cifras 

200
00:11:27,006 --> 00:11:31,360
concretas de la GPT-3 para contar exactamente de dónde proceden esos 175.000 millones.

201
00:11:31,880 --> 00:11:34,446
Aunque hoy en día hay modelos más grandes y mejores, 

202
00:11:34,446 --> 00:11:37,350
éste tiene cierto encanto como modelo de gran lenguaje para 

203
00:11:37,350 --> 00:11:40,740
captar realmente la atención del mundo fuera de las comunidades de ML.

204
00:11:41,440 --> 00:11:44,108
Además, en la práctica, las empresas tienden a mantener los labios mucho 

205
00:11:44,108 --> 00:11:46,740
más apretados en torno a las cifras concretas de las redes más modernas.

206
00:11:47,360 --> 00:11:50,648
Sólo quiero explicar que, cuando miras bajo el capó para ver 

207
00:11:50,648 --> 00:11:53,558
lo que ocurre dentro de una herramienta como ChatGPT, 

208
00:11:53,558 --> 00:11:57,440
casi todo el cálculo real parece una multiplicación vectorial matricial.

209
00:11:57,900 --> 00:12:01,935
Hay un poco de riesgo de perderse en el mar de miles de millones de números, 

210
00:12:01,935 --> 00:12:06,337
pero debes trazar una distinción muy nítida en tu mente entre las ponderaciones del 

211
00:12:06,337 --> 00:12:10,267
modelo, que siempre colorearé en azul o rojo, y los datos que se procesan, 

212
00:12:10,267 --> 00:12:11,840
que siempre colorearé en gris.

213
00:12:12,180 --> 00:12:16,435
Los pesos son los cerebros reales, son las cosas aprendidas durante el entrenamiento, 

214
00:12:16,435 --> 00:12:17,920
y determinan cómo se comporta.

215
00:12:18,280 --> 00:12:22,159
Los datos que se procesan simplemente codifican cualquier entrada específica que se 

216
00:12:22,159 --> 00:12:24,745
introduzca en el modelo para una ejecución determinada, 

217
00:12:24,745 --> 00:12:26,500
como un fragmento de texto de ejemplo.

218
00:12:27,480 --> 00:12:30,490
Con todo esto como base, vamos a profundizar en el primer paso de 

219
00:12:30,490 --> 00:12:33,409
este ejemplo de procesamiento de texto, que consiste en dividir 

220
00:12:33,409 --> 00:12:36,420
la entrada en pequeños trozos y convertir esos trozos en vectores.

221
00:12:37,020 --> 00:12:39,370
Ya he mencionado que esos trozos se llaman tokens, 

222
00:12:39,370 --> 00:12:43,103
que pueden ser trozos de palabras o signos de puntuación, pero de vez en cuando, 

223
00:12:43,103 --> 00:12:45,268
en este capítulo y sobre todo en el siguiente, 

224
00:12:45,268 --> 00:12:48,080
me gustaría fingir que se divide más limpiamente en palabras.

225
00:12:48,600 --> 00:12:51,380
Como los humanos pensamos con palabras, esto sólo hará que sea mucho 

226
00:12:51,380 --> 00:12:54,080
más fácil hacer referencia a pequeños ejemplos y aclarar cada paso.

227
00:12:55,260 --> 00:12:59,888
El modelo tiene un vocabulario predefinido, una lista de todas las palabras posibles, 

228
00:12:59,888 --> 00:13:02,848
digamos 50.000, y la primera matriz que encontraremos, 

229
00:13:02,848 --> 00:13:07,046
conocida como matriz de incrustación, tiene una sola columna para cada una de 

230
00:13:07,046 --> 00:13:07,800
esas palabras.

231
00:13:08,940 --> 00:13:11,523
Estas columnas son las que determinan en qué vector 

232
00:13:11,523 --> 00:13:13,760
se convierte cada palabra en ese primer paso.

233
00:13:15,100 --> 00:13:17,923
La etiquetamos We, y como todas las matrices que vemos, 

234
00:13:17,923 --> 00:13:22,360
sus valores empiezan siendo aleatorios, pero se van aprendiendo en función de los datos.

235
00:13:23,620 --> 00:13:26,696
Convertir palabras en vectores era una práctica habitual en el aprendizaje 

236
00:13:26,696 --> 00:13:28,623
automático mucho antes de los transformadores, 

237
00:13:28,623 --> 00:13:30,797
pero es un poco extraño si nunca lo has visto antes, 

238
00:13:30,797 --> 00:13:33,094
y sienta las bases de todo lo que viene a continuación, 

239
00:13:33,094 --> 00:13:35,760
así que vamos a dedicarle un momento para familiarizarnos con él.

240
00:13:36,040 --> 00:13:38,294
A menudo llamamos palabra a esta incrustación, 

241
00:13:38,294 --> 00:13:42,084
lo que te invita a pensar en estos vectores muy geométricamente como puntos en 

242
00:13:42,084 --> 00:13:43,620
algún espacio de alta dimensión.

243
00:13:44,180 --> 00:13:46,751
Visualizar una lista de tres números como coordenadas de puntos en 

244
00:13:46,751 --> 00:13:48,901
un espacio tridimensional no supondría ningún problema, 

245
00:13:48,901 --> 00:13:51,780
pero las incrustaciones de palabras suelen tener una dimensión mucho mayor.

246
00:13:52,280 --> 00:13:55,518
En GPT-3 tienen 12.288 dimensiones, y como verás, 

247
00:13:55,518 --> 00:14:00,440
es importante trabajar en un espacio que tenga muchas direcciones distintas.

248
00:14:01,180 --> 00:14:04,657
Del mismo modo que puedes tomar un corte bidimensional en un espacio 

249
00:14:04,657 --> 00:14:07,579
tridimensional y proyectar todos los puntos en ese corte, 

250
00:14:07,579 --> 00:14:11,711
para animar las incrustaciones de palabras que me proporciona un modelo sencillo, 

251
00:14:11,711 --> 00:14:15,491
voy a hacer algo análogo eligiendo un corte tridimensional en este espacio 

252
00:14:15,491 --> 00:14:19,220
de dimensiones muy elevadas, proyectando los vectores de palabras en él y 

253
00:14:19,220 --> 00:14:20,480
mostrando los resultados.

254
00:14:21,280 --> 00:14:24,644
La gran idea aquí es que, a medida que un modelo ajusta y afina sus pesos para 

255
00:14:24,644 --> 00:14:27,881
determinar cómo se incrustan exactamente las palabras como vectores durante 

256
00:14:27,881 --> 00:14:31,118
el entrenamiento, tiende a asentarse en un conjunto de incrustaciones en el 

257
00:14:31,118 --> 00:14:34,440
que las direcciones en el espacio tienen una especie de significado semántico.

258
00:14:34,980 --> 00:14:37,816
Para el modelo simple de palabra a vector que utilizo aquí, 

259
00:14:37,816 --> 00:14:41,503
si realizo una búsqueda de todas las palabras cuyas incrustaciones se acercan 

260
00:14:41,503 --> 00:14:45,190
más a la de torre, verás que todas parecen dar unas vibraciones muy parecidas 

261
00:14:45,190 --> 00:14:45,900
a las de torre.

262
00:14:46,340 --> 00:14:48,600
Y si quieres sacar algo de Python y seguir el juego en casa, 

263
00:14:48,600 --> 00:14:51,380
éste es el modelo concreto que estoy utilizando para hacer las animaciones.

264
00:14:51,620 --> 00:14:54,519
No es un transformador, pero basta para ilustrar la idea de que 

265
00:14:54,519 --> 00:14:57,600
las direcciones en el espacio pueden tener un significado semántico.

266
00:14:58,300 --> 00:15:03,206
Un ejemplo muy clásico de esto es cómo si tomas la diferencia entre los vectores 

267
00:15:03,206 --> 00:15:08,051
de mujer y hombre, algo que visualizarías como un pequeño vector que conecta la 

268
00:15:08,051 --> 00:15:13,200
punta de uno con la punta del otro, es muy similar a la diferencia entre rey y reina.

269
00:15:15,080 --> 00:15:18,648
Así que digamos que no conoces la palabra para una monarca mujer, 

270
00:15:18,648 --> 00:15:21,783
podrías encontrarla tomando rey, añadiendo esta dirección 

271
00:15:21,783 --> 00:15:25,460
mujer-hombre y buscando las incrustaciones más cercanas a ese punto.

272
00:15:27,000 --> 00:15:28,200
Al menos, algo así.

273
00:15:28,480 --> 00:15:31,504
A pesar de ser un ejemplo clásico para el modelo con el que estoy jugando, 

274
00:15:31,504 --> 00:15:34,529
la verdadera incrustación de reina está en realidad un poco más alejada de 

275
00:15:34,529 --> 00:15:37,473
lo que esto sugeriría, presumiblemente porque la forma en que se utiliza 

276
00:15:37,473 --> 00:15:40,780
reina en los datos de entrenamiento no es simplemente una versión femenina de rey.

277
00:15:41,620 --> 00:15:45,260
Cuando jugué, las relaciones familiares parecían ilustrar mucho mejor la idea.

278
00:15:46,340 --> 00:15:49,020
La cuestión es que parece que, durante el entrenamiento, 

279
00:15:49,020 --> 00:15:51,889
el modelo encontró ventajoso elegir incrustaciones tales que 

280
00:15:51,889 --> 00:15:54,900
una dirección de este espacio codifica la información de género.

281
00:15:56,800 --> 00:16:00,042
Otro ejemplo es que si tomas la incrustación de Italia, 

282
00:16:00,042 --> 00:16:04,731
y le restas la incrustación de Alemania, y la sumas a la incrustación de Hitler, 

283
00:16:04,731 --> 00:16:08,090
obtienes algo muy parecido a la incrustación de Mussolini.

284
00:16:08,570 --> 00:16:12,120
Es como si el modelo hubiera aprendido a asociar unas direcciones con la 

285
00:16:12,120 --> 00:16:15,670
italianidad y otras con los líderes del eje de la Segunda Guerra Mundial.

286
00:16:16,470 --> 00:16:19,243
Quizá mi ejemplo favorito en este sentido sea cómo, 

287
00:16:19,243 --> 00:16:23,936
en algunos modelos, si tomas la diferencia entre Alemania y Japón y la añades al sushi, 

288
00:16:23,936 --> 00:16:26,230
acabas muy cerca de la salchicha bratwurst.

289
00:16:27,350 --> 00:16:30,447
También al jugar a este juego de encontrar a los vecinos más cercanos, 

290
00:16:30,447 --> 00:16:33,850
me complació ver lo cerca que estaba Kat tanto de la bestia como del monstruo.

291
00:16:34,690 --> 00:16:37,252
Una intuición matemática que conviene tener presente, 

292
00:16:37,252 --> 00:16:40,337
sobre todo para el próximo capítulo, es que el producto punto de 

293
00:16:40,337 --> 00:16:43,850
dos vectores puede considerarse una forma de medir lo bien que se alinean.

294
00:16:44,870 --> 00:16:47,828
Computacionalmente, los productos punto implican multiplicar todos los 

295
00:16:47,828 --> 00:16:51,037
componentes correspondientes y luego sumar los resultados, lo cual es bueno, 

296
00:16:51,037 --> 00:16:54,330
ya que gran parte de nuestros cálculos tienen que parecerse a sumas ponderadas.

297
00:16:55,190 --> 00:16:58,456
Geométricamente, el producto punto es positivo cuando los 

298
00:16:58,456 --> 00:17:01,723
vectores apuntan en direcciones similares, es cero si son 

299
00:17:01,723 --> 00:17:05,609
perpendiculares y es negativo cuando apuntan en direcciones opuestas.

300
00:17:06,550 --> 00:17:09,590
Por ejemplo, supongamos que estás jugando con este modelo, 

301
00:17:09,590 --> 00:17:13,042
y planteas la hipótesis de que la incrustación de gatos menos gato 

302
00:17:13,042 --> 00:17:17,010
podría representar una especie de dirección de la pluralidad en este espacio.

303
00:17:17,430 --> 00:17:20,651
Para comprobarlo, voy a tomar este vector y calcular su producto escalar 

304
00:17:20,651 --> 00:17:23,210
con las incrustaciones de ciertos sustantivos singulares, 

305
00:17:23,210 --> 00:17:27,050
y compararlo con los productos escalares con los sustantivos plurales correspondientes.

306
00:17:27,270 --> 00:17:29,988
Si juegas con esto, te darás cuenta de que, efectivamente, 

307
00:17:29,988 --> 00:17:33,674
los plurales parecen dar sistemáticamente valores más altos que los singulares, 

308
00:17:33,674 --> 00:17:36,070
lo que indica que se alinean más con esta dirección.

309
00:17:37,070 --> 00:17:41,008
También es divertido ver cómo si tomas este producto punto con las incrustaciones 

310
00:17:41,008 --> 00:17:43,650
de las palabras 1, 2, 3, etc., dan valores crecientes, 

311
00:17:43,650 --> 00:17:47,396
así que es como si pudiéramos medir cuantitativamente lo plural que el modelo 

312
00:17:47,396 --> 00:17:49,030
encuentra una palabra determinada.

313
00:17:50,250 --> 00:17:53,570
De nuevo, los detalles de cómo se incrustan las palabras se aprenden utilizando datos.

314
00:17:54,050 --> 00:17:57,561
Esta matriz de incrustación, cuyas columnas nos dicen qué ocurre con cada palabra, 

315
00:17:57,561 --> 00:17:59,550
es el primer montón de pesos de nuestro modelo.

316
00:18:00,030 --> 00:18:05,228
Utilizando las cifras de la GPT-3, el tamaño del vocabulario en concreto es de 50.257, 

317
00:18:05,228 --> 00:18:09,770
y de nuevo, técnicamente esto no consiste en palabras en sí, sino en tokens.

318
00:18:10,630 --> 00:18:14,210
La dimensión de incrustación es 12.288, y multiplicando 

319
00:18:14,210 --> 00:18:17,790
éstas nos dice que consta de unos 617 millones de pesos.

320
00:18:18,250 --> 00:18:20,847
Sigamos adelante y añadamos esto a una cuenta corriente, 

321
00:18:20,847 --> 00:18:23,810
recordando que al final deberíamos contar hasta 175.000 millones.

322
00:18:25,430 --> 00:18:28,707
En el caso de los transformadores, debes pensar que los vectores de 

323
00:18:28,707 --> 00:18:32,130
este espacio de incrustación no sólo representan palabras individuales.

324
00:18:32,550 --> 00:18:36,388
Por un lado, también codifican información sobre la posición de esa palabra, 

325
00:18:36,388 --> 00:18:39,629
de lo que hablaremos más adelante, pero lo más importante es que 

326
00:18:39,629 --> 00:18:42,770
debes pensar que tienen la capacidad de empaparse del contexto.

327
00:18:43,350 --> 00:18:47,398
Un vector que empezó su vida como la incrustación de la palabra rey, por ejemplo, 

328
00:18:47,398 --> 00:18:51,497
puede ser progresivamente tironeado y arrastrado por diversos bloques de esta red, 

329
00:18:51,497 --> 00:18:55,644
de modo que al final apunte en una dirección mucho más específica y matizada que de 

330
00:18:55,644 --> 00:18:59,002
algún modo codifique que se trataba de un rey que vivía en Escocia, 

331
00:18:59,002 --> 00:19:02,113
y que había alcanzado su puesto tras asesinar al rey anterior, 

332
00:19:02,113 --> 00:19:04,730
y que está siendo descrito en lenguaje shakesperiano.

333
00:19:05,210 --> 00:19:07,790
Piensa en tu propia comprensión de una palabra determinada.

334
00:19:08,250 --> 00:19:11,624
El significado de esa palabra está claramente informado por el entorno, 

335
00:19:11,624 --> 00:19:14,484
y a veces esto incluye el contexto desde una gran distancia, 

336
00:19:14,484 --> 00:19:18,233
por lo que al elaborar un modelo que tenga la capacidad de predecir qué palabra 

337
00:19:18,233 --> 00:19:22,077
viene a continuación, el objetivo es capacitarlo de algún modo para que incorpore 

338
00:19:22,077 --> 00:19:23,390
el contexto de forma eficaz.

339
00:19:24,050 --> 00:19:27,240
Para que quede claro, en ese primer paso, cuando creas la matriz de vectores 

340
00:19:27,240 --> 00:19:30,430
basándote en el texto de entrada, cada uno de ellos se extrae simplemente de 

341
00:19:30,430 --> 00:19:33,372
la matriz de incrustación, por lo que inicialmente cada uno sólo puede 

342
00:19:33,372 --> 00:19:36,770
codificar el significado de una sola palabra sin ninguna aportación de su entorno.

343
00:19:37,710 --> 00:19:41,336
Pero debes pensar que el objetivo principal de esta red por la que fluye es 

344
00:19:41,336 --> 00:19:45,105
permitir que cada uno de esos vectores se impregne de un significado mucho más 

345
00:19:45,105 --> 00:19:48,970
rico y específico que el que podrían representar las meras palabras individuales.

346
00:19:49,510 --> 00:19:52,492
La red sólo puede procesar un número fijo de vectores a la vez, 

347
00:19:52,492 --> 00:19:54,170
conocido como su tamaño de contexto.

348
00:19:54,510 --> 00:19:57,495
Para la GPT-3 se entrenó con un tamaño de contexto de 2048, 

349
00:19:57,495 --> 00:20:00,979
de modo que los datos que fluyen por la red siempre tienen el aspecto 

350
00:20:00,979 --> 00:20:05,010
de esta matriz de 2048 columnas, cada una de las cuales tiene 12.000 dimensiones.

351
00:20:05,590 --> 00:20:08,820
Este tamaño de contexto limita la cantidad de texto que el transformador 

352
00:20:08,820 --> 00:20:11,830
puede incorporar cuando hace una predicción de la siguiente palabra.

353
00:20:12,370 --> 00:20:15,009
Por eso, las conversaciones largas con ciertos chatbots, 

354
00:20:15,009 --> 00:20:18,159
como las primeras versiones de ChatGPT, a menudo daban la sensación 

355
00:20:18,159 --> 00:20:22,050
de que el bot perdía el hilo de la conversación cuando continuabas demasiado tiempo.

356
00:20:23,030 --> 00:20:25,684
Entraremos en los detalles de la atención a su debido tiempo, 

357
00:20:25,684 --> 00:20:28,810
pero saltando adelante quiero hablar un minuto de lo que ocurre al final.

358
00:20:29,450 --> 00:20:32,402
Recuerda que la salida deseada es una distribución de probabilidad 

359
00:20:32,402 --> 00:20:34,870
sobre todas las fichas que podrían venir a continuación.

360
00:20:35,170 --> 00:20:37,481
Por ejemplo, si la última palabra es Profesor, 

361
00:20:37,481 --> 00:20:39,941
y el contexto incluye palabras como Harry Potter, 

362
00:20:39,941 --> 00:20:42,597
e inmediatamente antes vemos profesor menos favorito, 

363
00:20:42,597 --> 00:20:46,631
y también si me das cierto margen de maniobra permitiéndome fingir que los tokens 

364
00:20:46,631 --> 00:20:50,763
simplemente parecen palabras completas, entonces una red bien entrenada que hubiera 

365
00:20:50,763 --> 00:20:54,895
acumulado conocimientos sobre Harry Potter asignaría presumiblemente un número alto 

366
00:20:54,895 --> 00:20:55,830
a la palabra Snape.

367
00:20:56,510 --> 00:20:57,970
Esto implica dos pasos diferentes.

368
00:20:58,310 --> 00:21:02,808
La primera consiste en utilizar otra matriz que mapea el último vector de 

369
00:21:02,808 --> 00:21:07,610
ese contexto a una lista de 50.000 valores, uno por cada token del vocabulario.

370
00:21:08,170 --> 00:21:12,225
Luego hay una función que normaliza esto en una distribución de probabilidad, 

371
00:21:12,225 --> 00:21:15,188
se llama Softmax y hablaremos más de ella en un segundo, 

372
00:21:15,188 --> 00:21:18,879
pero antes de eso puede parecer un poco raro utilizar sólo esta última 

373
00:21:18,879 --> 00:21:22,883
incrustación para hacer una predicción, cuando después de todo en ese último 

374
00:21:22,883 --> 00:21:26,678
paso hay miles de otros vectores en la capa ahí sentados con sus propios 

375
00:21:26,678 --> 00:21:28,290
significados ricos en contexto.

376
00:21:28,930 --> 00:21:32,726
Esto tiene que ver con el hecho de que en el proceso de entrenamiento resulta 

377
00:21:32,726 --> 00:21:36,473
mucho más eficaz si utilizas cada uno de esos vectores en la capa final para 

378
00:21:36,473 --> 00:21:40,270
hacer simultáneamente una predicción de lo que vendría inmediatamente después.

379
00:21:40,970 --> 00:21:43,234
Hay mucho más que decir sobre el entrenamiento más adelante, 

380
00:21:43,234 --> 00:21:45,090
pero sólo quiero llamarlo la atención ahora mismo.

381
00:21:45,730 --> 00:21:49,690
Esta matriz se denomina matriz de no incrustación y le damos la etiqueta WU.

382
00:21:50,210 --> 00:21:52,440
De nuevo, como todas las matrices de pesos que vemos, 

383
00:21:52,440 --> 00:21:55,910
sus entradas empiezan al azar, pero se aprenden durante el proceso de entrenamiento.

384
00:21:56,470 --> 00:21:58,721
Siguiendo con nuestro recuento total de parámetros, 

385
00:21:58,721 --> 00:22:02,185
esta matriz de desincrustación tiene una fila por cada palabra del vocabulario, 

386
00:22:02,185 --> 00:22:05,650
y cada fila tiene el mismo número de elementos que la dimensión de incrustación.

387
00:22:06,410 --> 00:22:10,185
Es muy parecida a la matriz de incrustación, sólo que con el orden intercambiado, 

388
00:22:10,185 --> 00:22:12,948
por lo que añade otros 617 millones de parámetros a la red, 

389
00:22:12,948 --> 00:22:16,724
lo que significa que nuestro recuento hasta ahora es de algo más de mil millones, 

390
00:22:16,724 --> 00:22:20,454
una fracción pequeña pero no del todo insignificante de los 175 mil millones que 

391
00:22:20,454 --> 00:22:21,790
acabaremos teniendo en total.

392
00:22:22,550 --> 00:22:26,510
Como última minilección de este capítulo, quiero hablar más de esta función softmax, 

393
00:22:26,510 --> 00:22:30,610
ya que vuelve a aparecer para nosotros cuando nos sumerjamos en los bloques de atención.

394
00:22:31,430 --> 00:22:35,816
La idea es que si quieres que una secuencia de números actúe como una distribución de 

395
00:22:35,816 --> 00:22:40,152
probabilidad, digamos una distribución sobre todas las posibles palabras siguientes, 

396
00:22:40,152 --> 00:22:44,590
entonces cada valor tiene que estar entre 0 y 1, y también necesitas que todos sumen 1.

397
00:22:45,250 --> 00:22:48,453
Sin embargo, si estás jugando al juego del aprendizaje en el que 

398
00:22:48,453 --> 00:22:51,360
todo lo que haces parece una multiplicación matriz-vector, 

399
00:22:51,360 --> 00:22:54,810
las salidas que obtienes por defecto no se atienen a esto en absoluto.

400
00:22:55,330 --> 00:22:59,870
Los valores suelen ser negativos, o mucho mayores que 1, y casi seguro que no suman 1.

401
00:23:00,510 --> 00:23:04,119
Softmax es la forma estándar de convertir una lista arbitraria de números 

402
00:23:04,119 --> 00:23:07,729
en una distribución válida de forma que los valores más grandes acaben lo 

403
00:23:07,729 --> 00:23:11,290
más cerca posible de 1, y los valores más pequeños acaben muy cerca de 0.

404
00:23:11,830 --> 00:23:13,070
Eso es todo lo que necesitas saber.

405
00:23:13,090 --> 00:23:17,006
Pero si tienes curiosidad, la forma en que funciona es elevar primero e a la 

406
00:23:17,006 --> 00:23:21,127
potencia de cada uno de los números, lo que significa que ahora tienes una lista 

407
00:23:21,127 --> 00:23:25,349
de valores positivos, y luego puedes tomar la suma de todos esos valores positivos 

408
00:23:25,349 --> 00:23:29,470
y dividir cada término por esa suma, lo que lo normaliza en una lista que suma 1.

409
00:23:30,170 --> 00:23:34,148
Te darás cuenta de que si uno de los números de la entrada es significativamente mayor 

410
00:23:34,148 --> 00:23:37,714
que el resto, en la salida el término correspondiente domina la distribución, 

411
00:23:37,714 --> 00:23:41,509
por lo que si tomaras muestras de ella, casi seguro que sólo estarías eligiendo la 

412
00:23:41,509 --> 00:23:42,470
entrada maximizadora.

413
00:23:42,990 --> 00:23:45,396
Pero es más suave que elegir simplemente el máximo, 

414
00:23:45,396 --> 00:23:48,449
en el sentido de que cuando otros valores son igualmente grandes, 

415
00:23:48,449 --> 00:23:51,179
también obtienen un peso significativo en la distribución, 

416
00:23:51,179 --> 00:23:54,650
y todo cambia continuamente a medida que varías continuamente las entradas.

417
00:23:55,130 --> 00:23:59,723
En algunas situaciones, como cuando ChatGPT utiliza esta distribución para crear una 

418
00:23:59,723 --> 00:24:04,262
siguiente palabra, hay lugar para un poco más de diversión añadiendo un poco más de 

419
00:24:04,262 --> 00:24:08,910
picante a esta función, con una constante t lanzada al denominador de esos exponentes.

420
00:24:09,550 --> 00:24:14,023
Lo llamamos temperatura, ya que se parece vagamente al papel de la temperatura en 

421
00:24:14,023 --> 00:24:18,114
ciertas ecuaciones de termodinámica, y el efecto es que cuando t es mayor, 

422
00:24:18,114 --> 00:24:22,642
se da más peso a los valores más bajos, lo que significa que la distribución es un 

423
00:24:22,642 --> 00:24:27,170
poco más uniforme, y si t es menor, entonces los valores más grandes dominarán más 

424
00:24:27,170 --> 00:24:31,753
agresivamente, donde en el extremo, fijar t igual a cero significa que todo el peso 

425
00:24:31,753 --> 00:24:32,790
va al valor máximo.

426
00:24:33,470 --> 00:24:39,377
Por ejemplo, haré que GPT-3 genere una historia con el texto semilla Érase una vez A, 

427
00:24:39,377 --> 00:24:42,950
pero utilizaré temperaturas diferentes en cada caso.

428
00:24:43,630 --> 00:24:48,091
Temperatura cero significa que siempre va con la palabra más predecible, 

429
00:24:48,091 --> 00:24:52,370
y lo que obtienes acaba siendo un trillado derivado de Ricitos de Oro.

430
00:24:53,010 --> 00:24:56,800
Una temperatura más alta le da la oportunidad de elegir palabras menos probables, 

431
00:24:56,800 --> 00:24:57,910
pero conlleva un riesgo.

432
00:24:58,230 --> 00:25:01,320
En este caso, la historia comienza de forma más original, 

433
00:25:01,320 --> 00:25:06,010
sobre un joven artista web de Corea del Sur, pero rápidamente degenera en un sinsentido.

434
00:25:06,950 --> 00:25:10,830
Técnicamente hablando, la API no te permite elegir una temperatura superior a 2.

435
00:25:11,170 --> 00:25:15,060
No hay ninguna razón matemática para ello, es sólo una restricción arbitraria 

436
00:25:15,060 --> 00:25:19,350
impuesta para evitar que su herramienta se vea generando cosas demasiado disparatadas.

437
00:25:19,870 --> 00:25:24,236
Así que, si tienes curiosidad, la forma en que funciona realmente esta animación es que 

438
00:25:24,236 --> 00:25:27,660
estoy tomando las 20 próximas fichas más probables que genera GPT-3, 

439
00:25:27,660 --> 00:25:32,076
que parece ser el máximo que me darán, y luego ajusto las probabilidades basándome en un 

440
00:25:32,076 --> 00:25:32,970
exponente de 1 5º.

441
00:25:33,130 --> 00:25:37,340
Como otro poco de jerga, del mismo modo que podrías llamar probabilidades a 

442
00:25:37,340 --> 00:25:41,606
los componentes de la salida de esta función, la gente suele referirse a las 

443
00:25:41,606 --> 00:25:46,150
entradas como logits, o algunos dicen logits, otros logits, yo voy a decir logits.

444
00:25:46,530 --> 00:25:50,374
Así, por ejemplo, cuando introduces un texto, haces que todas estas palabras incrustadas 

445
00:25:50,374 --> 00:25:53,830
fluyan a través de la red y realizas esta multiplicación final con la matriz no 

446
00:25:53,830 --> 00:25:57,545
incrustada, la gente del aprendizaje automático se referiría a los componentes de esa 

447
00:25:57,545 --> 00:26:01,390
salida bruta, no normalizada, como los logits para la predicción de la siguiente palabra.

448
00:26:03,330 --> 00:26:06,605
Gran parte del objetivo de este capítulo era sentar las bases para 

449
00:26:06,605 --> 00:26:10,370
comprender el mecanismo de la atención, al estilo Karate Kid cera sobre cera.

450
00:26:10,850 --> 00:26:14,402
Verás, si tienes una fuerte intuición para las incrustaciones de palabras, 

451
00:26:14,402 --> 00:26:17,764
para el softmax, para cómo los productos de puntos miden la similitud, 

452
00:26:17,764 --> 00:26:21,222
y también la premisa subyacente de que la mayoría de los cálculos tienen 

453
00:26:21,222 --> 00:26:25,011
que parecerse a la multiplicación de matrices con matrices llenas de parámetros 

454
00:26:25,011 --> 00:26:28,042
sintonizables, entonces comprender el mecanismo de la atención, 

455
00:26:28,042 --> 00:26:32,210
esta pieza angular en todo el auge moderno de la IA, debería ser relativamente sencillo.

456
00:26:32,650 --> 00:26:34,510
Para ello, acompáñame en el próximo capítulo.

457
00:26:36,390 --> 00:26:38,740
Mientras publico esto, un borrador de ese próximo capítulo 

458
00:26:38,740 --> 00:26:41,210
está disponible para que lo revisen los seguidores de Patreon.

459
00:26:41,770 --> 00:26:44,469
En una semana o dos debería aparecer una versión final en público, 

460
00:26:44,469 --> 00:26:47,370
normalmente depende de cuánto acabe cambiando basándome en esa revisión.

461
00:26:47,810 --> 00:26:50,047
Mientras tanto, si quieres sumergirte en la atención, 

462
00:26:50,047 --> 00:26:52,410
y si quieres ayudar un poco al canal, está ahí esperando.

