[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Las siglas GPT significan Transformador Generativo Preentrenado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Así que la primera palabra es bastante sencilla, se trata de robots que generan texto nuevo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Preentrenado se refiere a que el modelo pasó por un proceso de aprendizaje a partir de una cantidad masiva de datos, y el prefijo insinúa que hay más margen para afinarlo en tareas específicas con entrenamiento adicional.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Pero la última palabra, esa es la verdadera pieza clave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Un transformador es un tipo específico de red neuronal, un modelo de aprendizaje automático, y es el invento central que subyace al auge actual de la IA.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Lo que quiero hacer con este vídeo y los capítulos siguientes es dar una explicación visual de lo que ocurre realmente en el interior de un transformador.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Vamos a seguir los datos que fluyen a través de él e iremos paso a paso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Hay muchos tipos diferentes de modelos que puedes construir utilizando transformadores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Algunos modelos captan el audio y producen una transcripción.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Esta frase procede de un modelo que va al revés, produciendo habla sintética sólo a partir de texto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Todas esas herramientas que tomaron el mundo por asalto en 2022, como Dolly y Midjourney, que toman una descripción de texto y producen una imagen, se basan en transformadores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Aunque no consiga que entienda lo que se supone que es una criatura de tarta, me sigue asombrando que este tipo de cosas sean remotamente posibles.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "Y el transformador original introducido en 2017 por Google se inventó para el caso de uso específico de traducir texto de un idioma a otro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Pero la variante en la que tú y yo nos centraremos, que es la que subyace en herramientas como ChatGPT, será un modelo entrenado para tomar un fragmento de texto, tal vez incluso con algunas imágenes o sonidos circundantes que lo acompañen, y producir una predicción de lo que viene a continuación en el pasaje.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Esa predicción adopta la forma de una distribución de probabilidad sobre muchos trozos de texto diferentes que podrían seguir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "A primera vista, podrías pensar que predecir la siguiente palabra parece un objetivo muy distinto de generar un nuevo texto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Pero una vez que tienes un modelo de predicción como éste, una forma sencilla de generar un fragmento de texto más largo es darle un fragmento inicial con el que trabajar, hacer que tome una muestra aleatoria de la distribución que acaba de generar, añadir esa muestra al texto y volver a ejecutar todo el proceso para hacer una nueva predicción basada en todo el texto nuevo, incluido lo que acaba de añadir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "No sé a ti, pero a mí no me parece que esto deba funcionar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "En esta animación, por ejemplo, estoy ejecutando GPT-2 en mi portátil y haciendo que prediga y muestree repetidamente el siguiente trozo de texto para generar una historia basada en el texto semilla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "La historia no tiene mucho sentido.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Pero si en lugar de eso lo cambio por llamadas a la API de GPT-3, que es el mismo modelo básico, sólo que mucho más grande, de repente casi por arte de magia obtenemos una historia sensata, que incluso parece inferir que una criatura pi viviría en una tierra de matemáticas y computación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Este proceso de predicción repetida y muestreo es esencialmente lo que ocurre cuando interactúas con ChatGPT o con cualquiera de estos otros grandes modelos lingüísticos y los ves producir una palabra cada vez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "De hecho, una función que me gustaría mucho es la posibilidad de ver la distribución subyacente de cada palabra nueva que elija.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Empecemos con una vista previa de muy alto nivel de cómo fluyen los datos a través de un transformador.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Dedicaremos mucho más tiempo a motivar e interpretar y ampliar los detalles de cada paso, pero a grandes rasgos, cuando uno de estos chatbots genera una palabra determinada, esto es lo que ocurre bajo el capó.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "En primer lugar, la entrada se divide en un montón de trocitos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Estas piezas se denominan tokens, y en el caso del texto suelen ser palabras o trocitos de palabras u otras combinaciones de caracteres comunes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Si se trata de imágenes o sonido, las fichas podrían ser pequeños fragmentos de esa imagen o pequeños trozos de ese sonido.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Cada una de estas fichas se asocia a un vector, es decir, a una lista de números, que pretende codificar de algún modo el significado de esa pieza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Si piensas en estos vectores como si dieran coordenadas en algún espacio de muy alta dimensión, las palabras con significados similares tienden a aterrizar en vectores que están cerca unos de otros en ese espacio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "A continuación, esta secuencia de vectores pasa por una operación que se conoce como bloque de atención, y esto permite que los vectores hablen entre sí y se pasen información de un lado a otro para actualizar sus valores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Por ejemplo, el significado de la palabra modelo en la frase un modelo de aprendizaje automático es diferente de su significado en la frase un modelo de moda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "El bloque de atención es el responsable de averiguar qué palabras del contexto son relevantes para actualizar los significados de qué otras palabras, y cómo deben actualizarse exactamente esos significados.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "Y de nuevo, siempre que utilizo la palabra significado, éste está de algún modo totalmente codificado en las entradas de esos vectores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Después, estos vectores pasan por un tipo de operación diferente, y dependiendo de la fuente que estés leyendo se denominará perceptrón multicapa o quizá capa feed-forward.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "Y aquí los vectores no hablan entre sí, sino que todos realizan la misma operación en paralelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Y aunque este bloque es un poco más difícil de interpretar, más adelante hablaremos de cómo el paso es un poco como hacer una larga lista de preguntas sobre cada vector, y luego actualizarlos en función de las respuestas a esas preguntas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Todas las operaciones de estos dos bloques parecen un montón gigante de multiplicaciones de matrices, y nuestro trabajo principal va a ser comprender cómo leer las matrices subyacentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Estoy pasando por alto algunos detalles sobre algunos pasos de normalización que se producen entre medias, pero al fin y al cabo esto es una vista previa de alto nivel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Después de eso, el proceso se repite esencialmente, vas y vienes entre bloques de atención y bloques de perceptrón multicapa, hasta que al final la esperanza es que todo el significado esencial del pasaje se haya cocido de algún modo en el último vector de la secuencia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "A continuación, realizamos una determinada operación sobre ese último vector que produce una distribución de probabilidad sobre todos los posibles tokens, todos los posibles trocitos de texto que podrían venir a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "Y como he dicho, una vez que tengas una herramienta que prediga lo que viene a continuación dado un fragmento de texto, puedes alimentarla con un poco de texto semilla y hacer que juegue repetidamente a este juego de predecir lo que viene a continuación, tomando muestras de la distribución, añadiéndolas y repitiendo una y otra vez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Algunos de los que sabéis quizá recordéis que, mucho antes de que ChatGPT entrara en escena, así es como se veían las primeras demos de GPT-3, hacías que autocompletara historias y ensayos basándose en un fragmento inicial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Para convertir una herramienta como ésta en un chatbot, el punto de partida más fácil es tener un poco de texto que establezca el escenario de un usuario interactuando con un asistente de IA útil, lo que llamarías la indicación del sistema, y luego utilizarías la pregunta o indicación inicial del usuario como primera parte del diálogo, y luego harías que empezara a predecir lo que ese asistente de IA útil diría en respuesta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Hay más que decir sobre un paso de entrenamiento necesario para que esto funcione bien, pero a alto nivel ésta es la idea.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "En este capítulo, tú y yo vamos a ampliar los detalles de lo que ocurre al principio de la red, al final de la red, y también quiero dedicar mucho tiempo a repasar algunos conocimientos básicos importantes, cosas que habrían sido una segunda naturaleza para cualquier ingeniero de aprendizaje automático en el momento en que aparecieron los transformadores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Si te sientes cómodo con esos conocimientos previos y estás un poco impaciente, puedes pasar al siguiente capítulo, que se va a centrar en los bloques de atención, generalmente considerados el corazón del transformador.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Después quiero hablar más sobre estos bloques de perceptrón multicapa, cómo funciona el entrenamiento y otra serie de detalles que se habrán omitido hasta ese momento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Para un contexto más amplio, estos vídeos son adiciones a una miniserie sobre aprendizaje profundo, y no pasa nada si no has visto los anteriores, creo que puedes hacerlo fuera de orden, pero antes de sumergirnos en los transformadores específicamente, creo que merece la pena asegurarnos de que estamos en la misma página sobre la premisa básica y la estructura del aprendizaje profundo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "A riesgo de decir lo obvio, éste es un enfoque del aprendizaje automático, que describe cualquier modelo en el que utilizas datos para determinar de algún modo cómo se comporta un modelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Lo que quiero decir con esto es que, digamos que quieres una función que tome una imagen y produzca una etiqueta que la describa, o nuestro ejemplo de predecir la siguiente palabra dado un pasaje de texto, o cualquier otra tarea que parezca requerir algún elemento de intuición y reconocimiento de patrones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Hoy en día casi lo damos por sentado, pero la idea del aprendizaje automático es que, en lugar de intentar definir explícitamente un procedimiento para realizar esa tarea en código, que es lo que se habría hecho en los primeros tiempos de la IA, se establece una estructura muy flexible con parámetros ajustables, como un montón de mandos y diales, y luego, de alguna manera, se utilizan muchos ejemplos de cómo debería ser el resultado de una entrada determinada para ajustar y afinar los valores de esos parámetros con el fin de imitar ese comportamiento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Por ejemplo, tal vez la forma más sencilla de aprendizaje automático sea la regresión lineal, en la que tus entradas y salidas son números individuales, algo así como los metros cuadrados de una casa y su precio, y lo que quieres es encontrar una línea de mejor ajuste a través de estos datos, ya sabes, para predecir los precios futuros de las casas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Esa recta se describe mediante dos parámetros continuos, digamos la pendiente y la intersección y, y el objetivo de la regresión lineal es determinar esos parámetros para que se ajusten a los datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Ni que decir tiene que los modelos de aprendizaje profundo se complican mucho más.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "La GPT-3, por ejemplo, no tiene dos, sino 175.000 millones de parámetros.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Pero la cuestión es que no es seguro que puedas crear un modelo gigantesco con un gran número de parámetros sin que se ajuste excesivamente a los datos de entrenamiento o sin que sea completamente imposible de entrenar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "El aprendizaje profundo describe una clase de modelos que en las dos últimas décadas han demostrado escalar notablemente bien.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Lo que los unifica es el mismo algoritmo de entrenamiento, llamado retropropagación, y el contexto que quiero que tengas a medida que avanzamos es que, para que este algoritmo de entrenamiento funcione bien a escala, estos modelos tienen que seguir un determinado formato específico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Si conoces este formato, te ayudará a explicar muchas de las opciones sobre cómo procesa el lenguaje un transformador, que de otro modo corren el riesgo de parecer arbitrarias.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "En primer lugar, sea cual sea el modelo que estés haciendo, la entrada tiene que estar formateada como una matriz de números reales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Esto puede significar una lista de números, puede ser una matriz bidimensional, o muy a menudo tratas con matrices de dimensiones superiores, donde el término general utilizado es tensor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "A menudo piensas en esos datos de entrada como si se transformaran progresivamente en muchas capas distintas, donde, de nuevo, cada capa siempre se estructura como una especie de matriz de números reales, hasta que llegas a una capa final que consideras la salida.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Por ejemplo, la capa final de nuestro modelo de procesamiento de texto es una lista de números que representan la distribución de probabilidad de todos los posibles tokens siguientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "En el aprendizaje profundo, estos parámetros del modelo casi siempre se denominan pesos, y esto se debe a que una característica clave de estos modelos es que la única forma en que estos parámetros interactúan con los datos que se procesan es mediante sumas ponderadas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "También salpicarás algunas funciones no lineales, pero no dependerán de parámetros.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Sin embargo, normalmente, en lugar de ver las sumas ponderadas desnudas y escritas explícitamente de esta forma, las encontrarás empaquetadas como varios componentes en un producto vectorial matricial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Equivale a decir lo mismo, si recuerdas cómo funciona la multiplicación vectorial matricial, cada componente de la salida parece una suma ponderada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Para ti y para mí suele ser conceptualmente más limpio pensar en matrices llenas de parámetros sintonizables que transforman vectores extraídos de los datos que se procesan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Por ejemplo, esos 175.000 millones de pesos de la GPT-3 están organizados en algo menos de 28.000 matrices distintas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Esas matrices se dividen a su vez en ocho categorías diferentes, y lo que vamos a hacer tú y yo es recorrer cada una de esas categorías para comprender lo que hace ese tipo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "A medida que avanzamos, creo que es divertido hacer referencia a las cifras concretas de la GPT-3 para contar exactamente de dónde proceden esos 175.000 millones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Aunque hoy en día hay modelos más grandes y mejores, éste tiene cierto encanto como modelo de gran lenguaje para captar realmente la atención del mundo fuera de las comunidades de ML.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Además, en la práctica, las empresas tienden a mantener los labios mucho más apretados en torno a las cifras concretas de las redes más modernas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Sólo quiero explicar que, cuando miras bajo el capó para ver lo que ocurre dentro de una herramienta como ChatGPT, casi todo el cálculo real parece una multiplicación vectorial matricial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Hay un poco de riesgo de perderse en el mar de miles de millones de números, pero debes trazar una distinción muy nítida en tu mente entre las ponderaciones del modelo, que siempre colorearé en azul o rojo, y los datos que se procesan, que siempre colorearé en gris.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Los pesos son los cerebros reales, son las cosas aprendidas durante el entrenamiento, y determinan cómo se comporta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Los datos que se procesan simplemente codifican cualquier entrada específica que se introduzca en el modelo para una ejecución determinada, como un fragmento de texto de ejemplo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Con todo esto como base, vamos a profundizar en el primer paso de este ejemplo de procesamiento de texto, que consiste en dividir la entrada en pequeños trozos y convertir esos trozos en vectores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Ya he mencionado que esos trozos se llaman tokens, que pueden ser trozos de palabras o signos de puntuación, pero de vez en cuando, en este capítulo y sobre todo en el siguiente, me gustaría fingir que se divide más limpiamente en palabras.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Como los humanos pensamos con palabras, esto sólo hará que sea mucho más fácil hacer referencia a pequeños ejemplos y aclarar cada paso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "El modelo tiene un vocabulario predefinido, una lista de todas las palabras posibles, digamos 50.000, y la primera matriz que encontraremos, conocida como matriz de incrustación, tiene una sola columna para cada una de esas palabras.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Estas columnas son las que determinan en qué vector se convierte cada palabra en ese primer paso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "La etiquetamos We, y como todas las matrices que vemos, sus valores empiezan siendo aleatorios, pero se van aprendiendo en función de los datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Convertir palabras en vectores era una práctica habitual en el aprendizaje automático mucho antes de los transformadores, pero es un poco extraño si nunca lo has visto antes, y sienta las bases de todo lo que viene a continuación, así que vamos a dedicarle un momento para familiarizarnos con él.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "A menudo llamamos palabra a esta incrustación, lo que te invita a pensar en estos vectores muy geométricamente como puntos en algún espacio de alta dimensión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Visualizar una lista de tres números como coordenadas de puntos en un espacio tridimensional no supondría ningún problema, pero las incrustaciones de palabras suelen tener una dimensión mucho mayor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "En GPT-3 tienen 12.288 dimensiones, y como verás, es importante trabajar en un espacio que tenga muchas direcciones distintas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Del mismo modo que puedes tomar un corte bidimensional en un espacio tridimensional y proyectar todos los puntos en ese corte, para animar las incrustaciones de palabras que me proporciona un modelo sencillo, voy a hacer algo análogo eligiendo un corte tridimensional en este espacio de dimensiones muy elevadas, proyectando los vectores de palabras en él y mostrando los resultados.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "La gran idea aquí es que, a medida que un modelo ajusta y afina sus pesos para determinar cómo se incrustan exactamente las palabras como vectores durante el entrenamiento, tiende a asentarse en un conjunto de incrustaciones en el que las direcciones en el espacio tienen una especie de significado semántico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Para el modelo simple de palabra a vector que utilizo aquí, si realizo una búsqueda de todas las palabras cuyas incrustaciones se acercan más a la de torre, verás que todas parecen dar unas vibraciones muy parecidas a las de torre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Y si quieres sacar algo de Python y seguir el juego en casa, éste es el modelo concreto que estoy utilizando para hacer las animaciones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "No es un transformador, pero basta para ilustrar la idea de que las direcciones en el espacio pueden tener un significado semántico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Un ejemplo muy clásico de esto es cómo si tomas la diferencia entre los vectores de mujer y hombre, algo que visualizarías como un pequeño vector que conecta la punta de uno con la punta del otro, es muy similar a la diferencia entre rey y reina.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Así que digamos que no conoces la palabra para una monarca mujer, podrías encontrarla tomando rey, añadiendo esta dirección mujer-hombre y buscando las incrustaciones más cercanas a ese punto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Al menos, algo así.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "A pesar de ser un ejemplo clásico para el modelo con el que estoy jugando, la verdadera incrustación de reina está en realidad un poco más alejada de lo que esto sugeriría, presumiblemente porque la forma en que se utiliza reina en los datos de entrenamiento no es simplemente una versión femenina de rey.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Cuando jugué, las relaciones familiares parecían ilustrar mucho mejor la idea.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "La cuestión es que parece que, durante el entrenamiento, el modelo encontró ventajoso elegir incrustaciones tales que una dirección de este espacio codifica la información de género.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Otro ejemplo es que si tomas la incrustación de Italia, y le restas la incrustación de Alemania, y la sumas a la incrustación de Hitler, obtienes algo muy parecido a la incrustación de Mussolini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Es como si el modelo hubiera aprendido a asociar unas direcciones con la italianidad y otras con los líderes del eje de la Segunda Guerra Mundial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Quizá mi ejemplo favorito en este sentido sea cómo, en algunos modelos, si tomas la diferencia entre Alemania y Japón y la añades al sushi, acabas muy cerca de la salchicha bratwurst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "También al jugar a este juego de encontrar a los vecinos más cercanos, me complació ver lo cerca que estaba Kat tanto de la bestia como del monstruo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Una intuición matemática que conviene tener presente, sobre todo para el próximo capítulo, es que el producto punto de dos vectores puede considerarse una forma de medir lo bien que se alinean.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Computacionalmente, los productos punto implican multiplicar todos los componentes correspondientes y luego sumar los resultados, lo cual es bueno, ya que gran parte de nuestros cálculos tienen que parecerse a sumas ponderadas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geométricamente, el producto punto es positivo cuando los vectores apuntan en direcciones similares, es cero si son perpendiculares y es negativo cuando apuntan en direcciones opuestas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Por ejemplo, supongamos que estás jugando con este modelo, y planteas la hipótesis de que la incrustación de gatos menos gato podría representar una especie de dirección de la pluralidad en este espacio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Para comprobarlo, voy a tomar este vector y calcular su producto escalar con las incrustaciones de ciertos sustantivos singulares, y compararlo con los productos escalares con los sustantivos plurales correspondientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Si juegas con esto, te darás cuenta de que, efectivamente, los plurales parecen dar sistemáticamente valores más altos que los singulares, lo que indica que se alinean más con esta dirección.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "También es divertido ver cómo si tomas este producto punto con las incrustaciones de las palabras 1, 2, 3, etc., dan valores crecientes, así que es como si pudiéramos medir cuantitativamente lo plural que el modelo encuentra una palabra determinada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "De nuevo, los detalles de cómo se incrustan las palabras se aprenden utilizando datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Esta matriz de incrustación, cuyas columnas nos dicen qué ocurre con cada palabra, es el primer montón de pesos de nuestro modelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Utilizando las cifras de la GPT-3, el tamaño del vocabulario en concreto es de 50.257, y de nuevo, técnicamente esto no consiste en palabras en sí, sino en tokens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "La dimensión de incrustación es 12.288, y multiplicando éstas nos dice que consta de unos 617 millones de pesos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Sigamos adelante y añadamos esto a una cuenta corriente, recordando que al final deberíamos contar hasta 175.000 millones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "En el caso de los transformadores, debes pensar que los vectores de este espacio de incrustación no sólo representan palabras individuales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Por un lado, también codifican información sobre la posición de esa palabra, de lo que hablaremos más adelante, pero lo más importante es que debes pensar que tienen la capacidad de empaparse del contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Un vector que empezó su vida como la incrustación de la palabra rey, por ejemplo, puede ser progresivamente tironeado y arrastrado por diversos bloques de esta red, de modo que al final apunte en una dirección mucho más específica y matizada que de algún modo codifique que se trataba de un rey que vivía en Escocia, y que había alcanzado su puesto tras asesinar al rey anterior, y que está siendo descrito en lenguaje shakesperiano.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Piensa en tu propia comprensión de una palabra determinada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "El significado de esa palabra está claramente informado por el entorno, y a veces esto incluye el contexto desde una gran distancia, por lo que al elaborar un modelo que tenga la capacidad de predecir qué palabra viene a continuación, el objetivo es capacitarlo de algún modo para que incorpore el contexto de forma eficaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Para que quede claro, en ese primer paso, cuando creas la matriz de vectores basándote en el texto de entrada, cada uno de ellos se extrae simplemente de la matriz de incrustación, por lo que inicialmente cada uno sólo puede codificar el significado de una sola palabra sin ninguna aportación de su entorno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Pero debes pensar que el objetivo principal de esta red por la que fluye es permitir que cada uno de esos vectores se impregne de un significado mucho más rico y específico que el que podrían representar las meras palabras individuales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "La red sólo puede procesar un número fijo de vectores a la vez, conocido como su tamaño de contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Para la GPT-3 se entrenó con un tamaño de contexto de 2048, de modo que los datos que fluyen por la red siempre tienen el aspecto de esta matriz de 2048 columnas, cada una de las cuales tiene 12.000 dimensiones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Este tamaño de contexto limita la cantidad de texto que el transformador puede incorporar cuando hace una predicción de la siguiente palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Por eso, las conversaciones largas con ciertos chatbots, como las primeras versiones de ChatGPT, a menudo daban la sensación de que el bot perdía el hilo de la conversación cuando continuabas demasiado tiempo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Entraremos en los detalles de la atención a su debido tiempo, pero saltando adelante quiero hablar un minuto de lo que ocurre al final.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Recuerda que la salida deseada es una distribución de probabilidad sobre todas las fichas que podrían venir a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Por ejemplo, si la última palabra es Profesor, y el contexto incluye palabras como Harry Potter, e inmediatamente antes vemos profesor menos favorito, y también si me das cierto margen de maniobra permitiéndome fingir que los tokens simplemente parecen palabras completas, entonces una red bien entrenada que hubiera acumulado conocimientos sobre Harry Potter asignaría presumiblemente un número alto a la palabra Snape.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Esto implica dos pasos diferentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "La primera consiste en utilizar otra matriz que mapea el último vector de ese contexto a una lista de 50.000 valores, uno por cada token del vocabulario.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Luego hay una función que normaliza esto en una distribución de probabilidad, se llama Softmax y hablaremos más de ella en un segundo, pero antes de eso puede parecer un poco raro utilizar sólo esta última incrustación para hacer una predicción, cuando después de todo en ese último paso hay miles de otros vectores en la capa ahí sentados con sus propios significados ricos en contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Esto tiene que ver con el hecho de que en el proceso de entrenamiento resulta mucho más eficaz si utilizas cada uno de esos vectores en la capa final para hacer simultáneamente una predicción de lo que vendría inmediatamente después.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Hay mucho más que decir sobre el entrenamiento más adelante, pero sólo quiero llamarlo la atención ahora mismo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Esta matriz se denomina matriz de no incrustación y le damos la etiqueta WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "De nuevo, como todas las matrices de pesos que vemos, sus entradas empiezan al azar, pero se aprenden durante el proceso de entrenamiento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Siguiendo con nuestro recuento total de parámetros, esta matriz de desincrustación tiene una fila por cada palabra del vocabulario, y cada fila tiene el mismo número de elementos que la dimensión de incrustación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Es muy parecida a la matriz de incrustación, sólo que con el orden intercambiado, por lo que añade otros 617 millones de parámetros a la red, lo que significa que nuestro recuento hasta ahora es de algo más de mil millones, una fracción pequeña pero no del todo insignificante de los 175 mil millones que acabaremos teniendo en total.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "Como última minilección de este capítulo, quiero hablar más de esta función softmax, ya que vuelve a aparecer para nosotros cuando nos sumerjamos en los bloques de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "La idea es que si quieres que una secuencia de números actúe como una distribución de probabilidad, digamos una distribución sobre todas las posibles palabras siguientes, entonces cada valor tiene que estar entre 0 y 1, y también necesitas que todos sumen 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Sin embargo, si estás jugando al juego del aprendizaje en el que todo lo que haces parece una multiplicación matriz-vector, las salidas que obtienes por defecto no se atienen a esto en absoluto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Los valores suelen ser negativos, o mucho mayores que 1, y casi seguro que no suman 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax es la forma estándar de convertir una lista arbitraria de números en una distribución válida de forma que los valores más grandes acaben lo más cerca posible de 1, y los valores más pequeños acaben muy cerca de 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Eso es todo lo que necesitas saber.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Pero si tienes curiosidad, la forma en que funciona es elevar primero e a la potencia de cada uno de los números, lo que significa que ahora tienes una lista de valores positivos, y luego puedes tomar la suma de todos esos valores positivos y dividir cada término por esa suma, lo que lo normaliza en una lista que suma 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Te darás cuenta de que si uno de los números de la entrada es significativamente mayor que el resto, en la salida el término correspondiente domina la distribución, por lo que si tomaras muestras de ella, casi seguro que sólo estarías eligiendo la entrada maximizadora.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Pero es más suave que elegir simplemente el máximo, en el sentido de que cuando otros valores son igualmente grandes, también obtienen un peso significativo en la distribución, y todo cambia continuamente a medida que varías continuamente las entradas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "En algunas situaciones, como cuando ChatGPT utiliza esta distribución para crear una siguiente palabra, hay lugar para un poco más de diversión añadiendo un poco más de picante a esta función, con una constante t lanzada al denominador de esos exponentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Lo llamamos temperatura, ya que se parece vagamente al papel de la temperatura en ciertas ecuaciones de termodinámica, y el efecto es que cuando t es mayor, se da más peso a los valores más bajos, lo que significa que la distribución es un poco más uniforme, y si t es menor, entonces los valores más grandes dominarán más agresivamente, donde en el extremo, fijar t igual a cero significa que todo el peso va al valor máximo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Por ejemplo, haré que GPT-3 genere una historia con el texto semilla Érase una vez A, pero utilizaré temperaturas diferentes en cada caso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Temperatura cero significa que siempre va con la palabra más predecible, y lo que obtienes acaba siendo un trillado derivado de Ricitos de Oro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Una temperatura más alta le da la oportunidad de elegir palabras menos probables, pero conlleva un riesgo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "En este caso, la historia comienza de forma más original, sobre un joven artista web de Corea del Sur, pero rápidamente degenera en un sinsentido.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Técnicamente hablando, la API no te permite elegir una temperatura superior a 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "No hay ninguna razón matemática para ello, es sólo una restricción arbitraria impuesta para evitar que su herramienta se vea generando cosas demasiado disparatadas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Así que, si tienes curiosidad, la forma en que funciona realmente esta animación es que estoy tomando las 20 próximas fichas más probables que genera GPT-3, que parece ser el máximo que me darán, y luego ajusto las probabilidades basándome en un exponente de 1 5º.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Como otro poco de jerga, del mismo modo que podrías llamar probabilidades a los componentes de la salida de esta función, la gente suele referirse a las entradas como logits, o algunos dicen logits, otros logits, yo voy a decir logits.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Así, por ejemplo, cuando introduces un texto, haces que todas estas palabras incrustadas fluyan a través de la red y realizas esta multiplicación final con la matriz no incrustada, la gente del aprendizaje automático se referiría a los componentes de esa salida bruta, no normalizada, como los logits para la predicción de la siguiente palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Gran parte del objetivo de este capítulo era sentar las bases para comprender el mecanismo de la atención, al estilo Karate Kid cera sobre cera.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Verás, si tienes una fuerte intuición para las incrustaciones de palabras, para el softmax, para cómo los productos de puntos miden la similitud, y también la premisa subyacente de que la mayoría de los cálculos tienen que parecerse a la multiplicación de matrices con matrices llenas de parámetros sintonizables, entonces comprender el mecanismo de la atención, esta pieza angular en todo el auge moderno de la IA, debería ser relativamente sencillo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Para ello, acompáñame en el próximo capítulo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Mientras publico esto, un borrador de ese próximo capítulo está disponible para que lo revisen los seguidores de Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "En una semana o dos debería aparecer una versión final en público, normalmente depende de cuánto acabe cambiando basándome en esa revisión.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Mientras tanto, si quieres sumergirte en la atención, y si quieres ayudar un poco al canal, está ahí esperando.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]