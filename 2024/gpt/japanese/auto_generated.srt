1
00:00:00,000 --> 00:00:01,843
頭文字のGPTはGenerative 

2
00:00:01,843 --> 00:00:04,560
Pretrained Transformerの略である。

3
00:00:05,220 --> 00:00:07,119
つまり、最初の言葉は簡単で、これは新

4
00:00:07,119 --> 00:00:09,020
しいテキストを生成するボットなのだ。

5
00:00:09,800 --> 00:00:13,213
Pretrainedとは、モデルが大量のデータから学習するプ

6
00:00:13,213 --> 00:00:16,626
ロセスを経たことを意味し、Prefixは、追加トレーニングに

7
00:00:16,626 --> 00:00:20,040
よって特定のタスクを微調整する余地があることを示唆している。

8
00:00:20,720 --> 00:00:22,900
しかし、最後の言葉、それこそが本当に重要なピースなのだ。

9
00:00:23,380 --> 00:00:25,878
トランスフォーマーは特定の種類のニューラ

10
00:00:25,878 --> 00:00:28,376
ルネットワーク、機械学習モデルであり、現

11
00:00:28,376 --> 00:00:31,000
在のAIブームの根底にある中核的な発明だ。

12
00:00:31,740 --> 00:00:35,362
このビデオと次の章で私がしたいことは、トランスの内部で

13
00:00:35,362 --> 00:00:39,120
実際に何が起こっているのかを視覚的に説明することである。

14
00:00:39,700 --> 00:00:42,820
そこに流れるデータを追いながら、一歩一歩進んでいく。

15
00:00:43,440 --> 00:00:47,380
変圧器を使って作ることができる模型にはいろいろな種類がある。

16
00:00:47,800 --> 00:00:50,800
音声を取り込み、トランスクリプトを作成するモデルもある。

17
00:00:51,340 --> 00:00:53,780
この文章は、逆にテキストだけで合

18
00:00:53,780 --> 00:00:56,220
成音声を生成するモデルのものだ。

19
00:00:56,660 --> 00:00:59,613
2022年に一世を風靡したドリーやミッドジャーニー

20
00:00:59,613 --> 00:01:02,566
のような、テキスト描写を取り込んで画像を生成するツ

21
00:01:02,566 --> 00:01:05,519
ールはすべてトランスフォーマーをベースにしている。

22
00:01:06,000 --> 00:01:09,550
パイという生き物が何であるかを理解させることはで

23
00:01:09,550 --> 00:01:13,100
きなくても、こんなことが可能なのかと驚かされる。

24
00:01:13,900 --> 00:01:16,633
そして、グーグルが2017年に発表したオリジナルのト

25
00:01:16,633 --> 00:01:19,366
ランスフォーマーは、ある言語から別の言語へテキストを

26
00:01:19,366 --> 00:01:22,100
翻訳するという特定のユースケースのために発明された。

27
00:01:22,660 --> 00:01:26,522
しかし、あなたや私が注目するのは、ChatGPTのよ

28
00:01:26,522 --> 00:01:30,385
うなツールの根底にあるタイプで、テキストの一部を取り

29
00:01:30,385 --> 00:01:34,248
込み、それに付随する画像や音声を取り込み、その文章の

30
00:01:34,248 --> 00:01:38,260
次に何が来るかを予測するように訓練されたモデルである。

31
00:01:38,600 --> 00:01:41,139
その予測は、その後に続く可能性のあるさまざ

32
00:01:41,139 --> 00:01:43,800
まなテキストの塊に対する確率分布の形をとる。

33
00:01:45,040 --> 00:01:47,490
一見すると、次の単語を予測することは、新しいテキストを生成す

34
00:01:47,490 --> 00:01:49,940
ることとはまったく異なる目標のように感じられるかもしれない。

35
00:01:50,180 --> 00:01:56,560
しかし、このような予測モデルができたら、長いテキストを生成

36
00:01:56,560 --> 00:02:02,940
する簡単な方法は、最初のスニペットを与え、生成した分布から

37
00:02:02,940 --> 00:02:09,539
ランダムなサンプルを取り、そのサンプルをテキストに追加する。

38
00:02:10,100 --> 00:02:11,506
あなたのことは知らないが、これが

39
00:02:11,506 --> 00:02:13,000
実際に機能するとはとても思えない。

40
00:02:13,420 --> 00:02:16,420
例えばこのアニメーションでは、私のノートパソコンでGPT-

41
00:02:16,420 --> 00:02:19,420
2を動かし、次のテキストの塊を繰り返し予測してサンプリング

42
00:02:19,420 --> 00:02:22,420
させ、種となるテキストに基づいてストーリーを生成している。

43
00:02:22,420 --> 00:02:26,120
ストーリーはあまり意味がない。

44
00:02:26,500 --> 00:02:30,095
しかし、その代わりにGPT-3のAPI呼び出し

45
00:02:30,095 --> 00:02:33,690
に置き換えてみると、同じ基本モデルでありながら

46
00:02:33,690 --> 00:02:37,285
、はるかに大きくなり、突然、ほとんど魔法のよう

47
00:02:37,285 --> 00:02:40,880
に、理にかなったストーリーを得ることができる。

48
00:02:41,580 --> 00:02:44,930
予測とサンプリングを繰り返すこのプロセスは、ChatG

49
00:02:44,930 --> 00:02:48,281
PTや他の大規模な言語モデルと対話し、彼らが一度に1つ

50
00:02:48,281 --> 00:02:51,880
の単語を生成するのを見るとき、本質的に起こっていることだ。

51
00:02:52,480 --> 00:02:55,788
実際、私がとても楽しみたい機能のひとつは、新しい単語を

52
00:02:55,788 --> 00:02:59,220
選ぶたびに、その基礎となる分布を見ることができることだ。

53
00:03:03,820 --> 00:03:06,000
まずは、データがトランスフォーマーをどのように通過

54
00:03:06,000 --> 00:03:08,180
するか、非常にハイレベルなプレビューから始めよう。

55
00:03:08,640 --> 00:03:11,099
各ステップの詳細については、動機づけや解釈、展開にもっ

56
00:03:11,099 --> 00:03:13,558
と多くの時間を費やすことになるだろうが、大まかに言えば

57
00:03:13,558 --> 00:03:16,018
、これらのチャットボットの1つが与えられた単語を生成す

58
00:03:16,018 --> 00:03:18,660
るとき、ボンネットの下で何が起こっているかは以下の通りだ。

59
00:03:19,080 --> 00:03:22,040
まず、入力は小さな断片に分割される。

60
00:03:22,620 --> 00:03:26,220
これらの断片はトークンと呼ばれ、テキストの場合、単語や単語

61
00:03:26,220 --> 00:03:29,820
の小片、その他の一般的な文字の組み合わせになる傾向がある。

62
00:03:30,740 --> 00:03:33,910
イメージやサウンドが関係している場合、トークンはイメ

63
00:03:33,910 --> 00:03:37,080
ージの小さなパッチやサウンドの小さなチャンクになる。

64
00:03:37,580 --> 00:03:40,173
これらのトークンはそれぞれ、ベクター、つま

65
00:03:40,173 --> 00:03:42,766
り数字のリストと関連付けられ、何らかの形で

66
00:03:42,766 --> 00:03:45,360
その部分の意味を暗号化することを意味する。

67
00:03:45,880 --> 00:03:48,813
これらのベクトルを、ある非常に高次元の空間における座標

68
00:03:48,813 --> 00:03:51,746
を与えるものと考えれば、似たような意味を持つ単語は、そ

69
00:03:51,746 --> 00:03:54,680
の空間において互いに近いベクトルに着地する傾向がある。

70
00:03:55,280 --> 00:03:58,262
この一連のベクトルは、次にアテンション・ブロ

71
00:03:58,262 --> 00:04:01,245
ックと呼ばれる処理を通過し、これによりベクト

72
00:04:01,245 --> 00:04:04,500
ル同士が会話し、情報をやり取りして値を更新する。

73
00:04:04,880 --> 00:04:07,186
例えば、機械学習モデルというフレーズにおけ

74
00:04:07,186 --> 00:04:09,493
るモデルという単語の意味は、ファッションモ

75
00:04:09,493 --> 00:04:11,800
デルというフレーズにおける意味とは異なる。

76
00:04:12,260 --> 00:04:15,456
アテンション・ブロックは、文脈の中でどの単語が他のどの単語

77
00:04:15,456 --> 00:04:18,653
の意味を更新するのに関連するのか、そしてそれらの意味を具体

78
00:04:18,653 --> 00:04:21,959
的にどのように更新すべきなのかを見つけ出す役割を担っている。

79
00:04:22,500 --> 00:04:24,287
そしてまた、私が意味という言葉を使うとき

80
00:04:24,287 --> 00:04:26,074
はいつでも、この意味はこれらのベクトルの

81
00:04:26,074 --> 00:04:28,040
エントリーに完全に符号化されているのである。

82
00:04:29,180 --> 00:04:32,186
その後、これらのベクトルは別の種類の演算を経て、

83
00:04:32,186 --> 00:04:35,193
あなたが読んでいる情報源によっては、多層パーセプ

84
00:04:35,193 --> 00:04:38,200
トロン、あるいはフィードフォワード層と呼ばれる。

85
00:04:38,580 --> 00:04:40,567
そしてここでは、ベクターは互いに会話す

86
00:04:40,567 --> 00:04:42,660
ることなく、すべて並列に同じ操作を行う。

87
00:04:43,060 --> 00:04:46,706
このブロックの解釈は少し難しいが、後ほど、このステップが、

88
00:04:46,706 --> 00:04:50,353
各ベクトルについて長い質問リストを問いかけ、その答えに基づ

89
00:04:50,353 --> 00:04:54,000
いてベクトルを更新していくようなものであることを説明する。

90
00:04:54,900 --> 00:05:00,110
これらのブロックの演算はすべて、

91
00:05:00,110 --> 00:05:05,320
行列の掛け算の山のように見える。

92
00:05:06,980 --> 00:05:09,925
その間に行われるいくつかの正規化ステップの詳細は割愛す

93
00:05:09,925 --> 00:05:12,980
るが、結局のところ、これはハイレベルなプレビューなのだ。

94
00:05:13,680 --> 00:05:17,353
その後、アテンション・ブロックと多層パーセプトロン・ブロッ

95
00:05:17,353 --> 00:05:21,026
クを行ったり来たりしながら、基本的にこのプロセスを繰り返す

96
00:05:21,026 --> 00:05:24,700
のだが、最後の最後には、その文章の本質的な意味のすべてが、

97
00:05:24,700 --> 00:05:28,500
どうにかして最後のベクトルに焼き付いていることが期待される。

98
00:05:28,920 --> 00:05:32,046
次に、この最後のベクトルに対してある演算を行い、次に

99
00:05:32,046 --> 00:05:35,173
来る可能性のあるすべてのトークン、つまり可能性のある

100
00:05:35,173 --> 00:05:38,420
すべての小さなテキストの塊に対する確率分布を生成する。

101
00:05:38,980 --> 00:05:41,777
そして、私が言ったように、テキストの断片から次に何

102
00:05:41,777 --> 00:05:44,575
が来るかを予測するツールを手に入れたら、そのツール

103
00:05:44,575 --> 00:05:47,372
に少量のシードテキストを与え、次に何が来るかを予測

104
00:05:47,372 --> 00:05:50,170
し、分布からサンプリングし、それを追加し、何度も何

105
00:05:50,170 --> 00:05:53,080
度も繰り返すというゲームを繰り返させることができる。

106
00:05:53,640 --> 00:05:59,140
ChatGPTが登場するずっと前、GPT-3の初期の

107
00:05:59,140 --> 00:06:04,640
デモがこんな感じだったのを覚えている人もいるだろう。

108
00:06:05,580 --> 00:06:09,057
このようなツールをチャットボットにするには、最も簡単な出

109
00:06:09,057 --> 00:06:12,534
発点は、ユーザーが親切なAIアシスタントと対話するという

110
00:06:12,534 --> 00:06:16,011
設定を確立するちょっとしたテキスト、システム・プロンプト

111
00:06:16,011 --> 00:06:17,998
と呼ぶべきものを用意することだ。

112
00:06:17,998 --> 00:06:21,475
そして、ユーザーの最初の質問やプロンプトを対話の最初のビ

113
00:06:21,475 --> 00:06:24,953
ットとして使い、そのような親切なAIアシスタントが返答と

114
00:06:24,953 --> 00:06:26,940
して何を言うかを予測させるのだ。

115
00:06:27,720 --> 00:06:29,760
これをうまく機能させるために必要なトレーニ

116
00:06:29,760 --> 00:06:31,801
ングのステップについては、もっと言うべきこ

117
00:06:31,801 --> 00:06:33,940
とがあるが、高いレベルではこれがアイデアだ。

118
00:06:35,720 --> 00:06:39,817
この章では、ネットワークの一番最初と一番最後に何が

119
00:06:39,817 --> 00:06:43,914
起こるかについて詳しく説明し、トランスフォーマーが

120
00:06:43,914 --> 00:06:48,011
登場する頃には機械学習エンジニアなら誰でも知ってい

121
00:06:48,011 --> 00:06:52,600
たような、重要な背景知識の復習にも多くの時間を割きたい。

122
00:06:53,060 --> 00:06:57,811
次の章では、一般的にトランスの心臓部と考えら

123
00:06:57,811 --> 00:07:02,780
れているアテンション・ブロックに焦点を当てる。

124
00:07:03,360 --> 00:07:06,061
この後、この多層パーセプトロン・ブロックについて、

125
00:07:06,061 --> 00:07:08,762
トレーニングがどのように機能するのか、そしてそれま

126
00:07:08,762 --> 00:07:11,680
で飛ばされてきた他の多くの詳細についてもっと話したい。

127
00:07:12,180 --> 00:07:20,211
しかし、トランスフォーマーに特化する前に、ディープラーニン

128
00:07:20,211 --> 00:07:28,520
グの基本的な前提や構造について確認しておく価値があると思う。

129
00:07:29,020 --> 00:07:32,022
明らかなことを言うかもしれないが、これは機械

130
00:07:32,022 --> 00:07:35,024
学習の1つのアプローチであり、モデルの振る舞

131
00:07:35,024 --> 00:07:38,300
いを決定するためにデータを使用するモデルを指す。

132
00:07:39,140 --> 00:07:43,631
つまり、画像を取り込んで、それを説明するラベルを生成す

133
00:07:43,631 --> 00:07:48,122
る関数や、文章から次の単語を予測する例など、直感やパタ

134
00:07:48,122 --> 00:07:52,780
ーン認識の要素を必要とするようなタスクが必要だとしよう。

135
00:07:53,200 --> 00:07:59,825
しかし、機械学習の考え方は、そのタスクをどのように行

136
00:07:59,825 --> 00:08:06,450
うかの手順をコードで明示的に定義しようとするのではな

137
00:08:06,450 --> 00:08:13,075
く、AIの初期に人々が行っていたような、調整可能なパ

138
00:08:13,075 --> 00:08:19,700
ラメータを持つ非常に柔軟な構造を設定することである。

139
00:08:19,700 --> 00:08:28,250
例えば、機械学習の最も単純な形は線形回帰で、入力と出力

140
00:08:28,250 --> 00:08:36,799
はそれぞれ単一の数値、例えば家の面積とその価格である。

141
00:08:37,440 --> 00:08:40,968
その直線は2つの連続的なパラメータ、例えば傾きとy切

142
00:08:40,968 --> 00:08:44,496
片によって記述され、線形回帰の目標は、データに密接に

143
00:08:44,496 --> 00:08:48,160
一致するようにそれらのパラメータを決定することである。

144
00:08:48,880 --> 00:08:50,438
言うまでもなく、ディープラーニ

145
00:08:50,438 --> 00:08:52,100
ング・モデルはもっと複雑になる。

146
00:08:52,620 --> 00:08:55,140
例えばGPT-3には2つどころか、

147
00:08:55,140 --> 00:08:57,660
1750億ものパラメーターがある。

148
00:08:58,120 --> 00:09:00,885
しかし、ここで重要なのは、膨大な数のパラメー

149
00:09:00,885 --> 00:09:03,651
ターを持つ巨大なモデルを作成しても、それが訓

150
00:09:03,651 --> 00:09:06,417
練データに著しくオーバーフィットしたり、訓練

151
00:09:06,417 --> 00:09:09,560
に全く手こずったりしないとは限らないということだ。

152
00:09:10,260 --> 00:09:13,220
ディープラーニングは、ここ数十年で驚くほどうまくスケール

153
00:09:13,220 --> 00:09:16,180
することが証明されたモデルのクラスについて説明している。

154
00:09:16,480 --> 00:09:21,357
そして、このトレーニング・アルゴリズムがスケールアップして

155
00:09:21,357 --> 00:09:26,234
うまく機能するためには、これらのモデルはある特定のフォーマ

156
00:09:26,234 --> 00:09:31,280
ットに従わなければならない、ということをご理解いただきたい。

157
00:09:31,800 --> 00:09:36,018
この形式を知っていれば、トランスフォーマーがどのよう

158
00:09:36,018 --> 00:09:40,400
に言語を処理するかの選択肢の多くを説明するのに役立つ。

159
00:09:41,440 --> 00:09:44,090
まず、どんなモデルを作るにせよ、入力は実数の

160
00:09:44,090 --> 00:09:46,740
配列としてフォーマットされなければならない。

161
00:09:46,740 --> 00:09:49,784
これは数値のリストを意味することもあれば、2次元

162
00:09:49,784 --> 00:09:52,828
配列であることもある。また、一般的にテンソルとい

163
00:09:52,828 --> 00:09:56,000
う用語が使われる高次元配列を扱うことも非常に多い。

164
00:09:56,560 --> 00:10:02,619
その入力データは、多くの異なるレイヤーに徐

165
00:10:02,619 --> 00:10:08,680
々に変換されていくと考えるのが普通である。

166
00:10:09,280 --> 00:10:13,170
例えば、我々のテキスト処理モデルの最終レイヤーは、次に起こ

167
00:10:13,170 --> 00:10:17,060
りうるすべてのトークンの確率分布を表す数値のリストである。

168
00:10:17,820 --> 00:10:20,812
ディープラーニングでは、これらのモデルパラメータはほと

169
00:10:20,812 --> 00:10:23,804
んどの場合、重みと呼ばれる。これは、これらのモデルの重

170
00:10:23,804 --> 00:10:26,796
要な特徴が、これらのパラメータが処理されるデータと相互

171
00:10:26,796 --> 00:10:29,900
作用する唯一の方法が、重み付き和であることにあるためだ。

172
00:10:30,340 --> 00:10:32,295
非線形関数もいくつか散りばめられてい

173
00:10:32,295 --> 00:10:34,360
るが、それらはパラメータに依存しない。

174
00:10:35,200 --> 00:10:38,623
しかし一般的には、加重和をすべて裸にしてこのよ

175
00:10:38,623 --> 00:10:42,047
うに明示的に書き出すのではなく、行列のベクトル

176
00:10:42,047 --> 00:10:45,620
積のさまざまな成分としてまとめて見ることになる。

177
00:10:46,740 --> 00:10:50,406
行列のベクトル乗算がどのように機能するかを思

178
00:10:50,406 --> 00:10:54,240
い返せば、出力の各成分は加重和のように見える。

179
00:10:54,780 --> 00:10:58,326
処理されるデータから引き出されるベクトルを変換する

180
00:10:58,326 --> 00:11:01,873
、調整可能なパラメーターで満たされた行列について考

181
00:11:01,873 --> 00:11:05,420
える方が、概念的にすっきりしていることが多いのだ。

182
00:11:06,340 --> 00:11:10,159
例えば、GPT-3の1,750億の重みは、

183
00:11:10,159 --> 00:11:14,160
28,000弱の異なる行列に整理されている。

184
00:11:14,660 --> 00:11:17,303
これらのマトリックスは8つの異なるカテゴリーに分

185
00:11:17,303 --> 00:11:19,946
類され、私たちはそれぞれのカテゴリーを順に見てい

186
00:11:19,946 --> 00:11:22,700
き、そのタイプが何をするのかを理解することになる。

187
00:11:23,160 --> 00:11:27,188
GPT-3の具体的な数字を参照しながら、その1750億が

188
00:11:27,188 --> 00:11:31,360
どこから来たのかをカウントアップしていくのも楽しいと思う。

189
00:11:31,880 --> 00:11:34,833
今でこそ、より大規模で優れたモデルがあるにせよ、こ

190
00:11:34,833 --> 00:11:37,786
のモデルはMLコミュニティ以外の世界の注目を集める

191
00:11:37,786 --> 00:11:40,740
大規模言語モデルとして、ある種の魅力を持っている。

192
00:11:41,440 --> 00:11:44,035
また、現実的に言えば、最新のネットワークでは、各

193
00:11:44,035 --> 00:11:46,740
社とも具体的な数字については口をつぐむ傾向にある。

194
00:11:47,360 --> 00:11:50,644
ChatGPTのようなツールの内部で何が起こっているのか、

195
00:11:50,644 --> 00:11:53,928
フードの下を覗いてみると、実際の計算のほとんどすべてが行列

196
00:11:53,928 --> 00:11:57,213
のベクトル乗算のように見えるということを、これから説明した

197
00:11:57,213 --> 00:11:57,440
い。

198
00:11:57,900 --> 00:12:02,493
何十億という数字の海に迷い込むリスクは少しあるが、モデルの

199
00:12:02,493 --> 00:12:07,087
重み（私は常に青か赤で色付けする）と、処理されるデータ（私

200
00:12:07,087 --> 00:12:11,840
は常にグレーで色付けする）を頭の中で鋭く区別する必要がある。

201
00:12:12,180 --> 00:12:14,993
ウェイトは実際の頭脳であり、トレーニング中に学習し

202
00:12:14,993 --> 00:12:17,920
たものであり、それがどのように振る舞うかを決定する。

203
00:12:18,280 --> 00:12:20,939
処理されるデータは、テキストのスニペットの例

204
00:12:20,939 --> 00:12:23,598
のように、与えられた実行のためにモデルに供給

205
00:12:23,598 --> 00:12:26,500
されるどんな特定の入力でも、単にエンコードする。

206
00:12:27,480 --> 00:12:31,814
入力を小さなチャンクに分割し、そ

207
00:12:31,814 --> 00:12:36,420
のチャンクをベクトルに変えるのだ。

208
00:12:37,020 --> 00:12:39,755
これらのチャンクがトークンと呼ばれ、単語の断片

209
00:12:39,755 --> 00:12:42,490
であったり、句読点であったりすることは前述した

210
00:12:42,490 --> 00:12:45,225
が、この章、そして特に次の章では、時々、もっと

211
00:12:45,225 --> 00:12:48,080
きれいに単語に分割されているように見せかけたい。

212
00:12:48,600 --> 00:12:51,290
私たち人間は言葉で考えるから、ちょっとした例を参照した

213
00:12:51,290 --> 00:12:54,080
り、各ステップを明確にしたりすることが、より簡単になる。

214
00:12:55,260 --> 00:13:01,402
このモデルには、あらかじめ定義された語彙があり、

215
00:13:01,402 --> 00:13:07,800
たとえば50,000個の単語からなるリストがある。

216
00:13:08,940 --> 00:13:11,350
これらの列が、最初のステップで各単語がど

217
00:13:11,350 --> 00:13:13,760
のようなベクトルに変化するかを決定する。

218
00:13:15,100 --> 00:13:18,658
私たちが目にするすべての行列と同じように、その値も

219
00:13:18,658 --> 00:13:22,360
最初はランダムだが、データに基づいて学習されていく。

220
00:13:23,620 --> 00:13:27,575
単語をベクトルに変換することは、トランスフォーマーよりもず

221
00:13:27,575 --> 00:13:31,531
っと以前から機械学習では一般的に行われていたことだが、初め

222
00:13:31,531 --> 00:13:35,487
て目にすると少し奇妙で、この後に続くすべての基礎となるもの

223
00:13:35,487 --> 00:13:35,760
だ。

224
00:13:36,040 --> 00:13:38,525
私たちはよくこの埋め込みを単語と呼ぶが、

225
00:13:38,525 --> 00:13:41,010
これはこれらのベクトルをある高次元空間の

226
00:13:41,010 --> 00:13:43,620
点として非常に幾何学的に考えることを誘う。

227
00:13:44,180 --> 00:13:46,671
3つの数字のリストを3次元空間の点の座標

228
00:13:46,671 --> 00:13:49,163
として視覚化することは問題ないが、単語の

229
00:13:49,163 --> 00:13:51,780
埋め込みははるかに高次元になる傾向がある。

230
00:13:52,280 --> 00:13:56,288
GPT-3では12,288の次元があり、おわかりのように

231
00:13:56,288 --> 00:14:00,440
、多くの明確な方向性を持つ空間で作業することが重要なのだ。

232
00:14:01,180 --> 00:14:05,040
3次元空間を2次元的にスライスして、そのスライスに

233
00:14:05,040 --> 00:14:08,900
すべての点を投影するのと同じように、単純なモデルが

234
00:14:08,900 --> 00:14:12,760
与えてくれる単語の埋め込みをアニメーション化するた

235
00:14:12,760 --> 00:14:16,620
めに、この非常に高次元の空間を3次元的にスライスし

236
00:14:16,620 --> 00:14:20,480
て、そこに単語のベクトルを投影し、結果を表示する。

237
00:14:21,280 --> 00:14:24,514
ここでの大きなアイデアは、学習中に単語がどのようにベクトル

238
00:14:24,514 --> 00:14:27,748
として埋め込まれるかを決定するために、モデルが重みを微調整

239
00:14:27,748 --> 00:14:30,982
し、チューニングするにつれ、空間内の方向が一種の意味的な意

240
00:14:30,982 --> 00:14:34,216
味を持つような埋め込みセットに落ち着く傾向があるということ

241
00:14:34,216 --> 00:14:34,440
だ。

242
00:14:34,980 --> 00:14:37,682
私がここで実行している単純な単語対ベクトルモデルの

243
00:14:37,682 --> 00:14:40,385
場合、埋め込みがtowerのそれに最も近い単語をす

244
00:14:40,385 --> 00:14:43,088
べて検索してみると、どの単語も非常によく似たtow

245
00:14:43,088 --> 00:14:45,900
erっぽい雰囲気を醸し出していることに気づくだろう。

246
00:14:46,340 --> 00:14:48,814
Pythonを立ち上げて、自宅で一緒に遊びたいなら、こ

247
00:14:48,814 --> 00:14:51,380
れがアニメーションを作るのに使っている具体的なモデルだ。

248
00:14:51,620 --> 00:14:54,543
これは変圧器ではないが、空間の方向が意味的な

249
00:14:54,543 --> 00:14:57,600
意味を持ちうるという考えを説明するには十分だ。

250
00:14:58,300 --> 00:15:03,201
その典型的な例が、女性と男性のベクトルの違いを、片

251
00:15:03,201 --> 00:15:08,102
方の先端ともう片方の先端を結ぶ小さなベクトルとして

252
00:15:08,102 --> 00:15:13,200
視覚化した場合、それは王と女王の違いによく似ている。

253
00:15:15,080 --> 00:15:18,540
つまり、女性君主を意味する単語を知らなかったとすると、k

254
00:15:18,540 --> 00:15:22,000
ingを取り、このwoman-manの方向を加え、その点

255
00:15:22,000 --> 00:15:25,460
に最も近い埋め込みを検索することで見つけることができる。

256
00:15:27,000 --> 00:15:28,200
少なくとも、そんな感じだ。

257
00:15:28,480 --> 00:15:31,474
これは私が遊んでいるモデルの典型的な例であるにもかかわら

258
00:15:31,474 --> 00:15:34,469
ず、クイーンの真の埋め込みは、これが示唆するよりも実際に

259
00:15:34,469 --> 00:15:37,464
は少しずれている。おそらく、トレーニングデータでクイーン

260
00:15:37,464 --> 00:15:40,459
が単にキングの女性版として使われているわけではないからだ

261
00:15:40,459 --> 00:15:40,780
ろう。

262
00:15:41,620 --> 00:15:43,390
いろいろやってみると、家族関係の方が

263
00:15:43,390 --> 00:15:45,260
この考えをよく表しているように思えた。

264
00:15:46,340 --> 00:15:49,193
要するに、モデルはトレーニング中に、この空間

265
00:15:49,193 --> 00:15:52,046
のある方向が性別情報をエンコードするような埋

266
00:15:52,046 --> 00:15:54,900
め込みを選択するのが有利だとわかったようだ。

267
00:15:56,800 --> 00:15:59,622
もう一つの例は、イタリアのエンベッディングからド

268
00:15:59,622 --> 00:16:02,444
イツのエンベッディングを引いて、それをヒトラーの

269
00:16:02,444 --> 00:16:05,267
エンベッディングに足すと、ムッソリーニのエンベッ

270
00:16:05,267 --> 00:16:08,090
ディングに非常に近いものが得られるということだ。

271
00:16:08,570 --> 00:16:12,059
このモデルは、ある方向はイタリアらしさを、他の方向は第二次

272
00:16:12,059 --> 00:16:15,670
世界大戦の枢軸国の指導者を連想させることを学んだかのようだ。

273
00:16:16,470 --> 00:16:21,264
この系統で私が一番好きな例は、ドイツと日本の違いを寿司に

274
00:16:21,264 --> 00:16:26,230
加えると、ブラートヴルストに非常に近くなるというモデルだ。

275
00:16:27,350 --> 00:16:30,530
また、この最も近い隣人を探すゲームでは、キャッ

276
00:16:30,530 --> 00:16:33,850
トが野獣と怪物の両方に近かったことが嬉しかった。

277
00:16:34,690 --> 00:16:37,703
特に次の章で役に立つ数学的直観のひとつは、2つのベ

278
00:16:37,703 --> 00:16:40,716
クトルの内積が、それらのベクトルがどれだけうまく整

279
00:16:40,716 --> 00:16:43,850
列しているかを測る方法として考えられるということだ。

280
00:16:44,870 --> 00:16:49,478
計算上、ドット積は対応するすべての成分

281
00:16:49,478 --> 00:16:54,330
を掛け合わせ、その結果を足すことになる。

282
00:16:55,190 --> 00:16:58,663
幾何学的には、ベクトルが同じような方向を向い

283
00:16:58,663 --> 00:17:02,136
ている場合、内積は正になり、垂直な場合はゼロ

284
00:17:02,136 --> 00:17:05,609
になり、反対方向を向いている場合は負になる。

285
00:17:06,550 --> 00:17:10,036
例えば、あなたがこのモデルで遊んでいて、猫から猫を引

286
00:17:10,036 --> 00:17:13,523
いた埋め込みが、この空間におけるある種の複数性の方向

287
00:17:13,523 --> 00:17:17,010
を表しているのではないかという仮説を立てたとしよう。

288
00:17:17,430 --> 00:17:20,636
これをテストするために、このベクトルを取り出し、

289
00:17:20,636 --> 00:17:23,843
ある単数名詞の埋め込みに対してそのドット積を計算

290
00:17:23,843 --> 00:17:27,050
し、対応する複数名詞とのドット積と比較してみる。

291
00:17:27,270 --> 00:17:31,670
これを弄ってみると、確かに複数形の方が単数形より

292
00:17:31,670 --> 00:17:36,070
も一貫して高い値を示していることに気づくだろう。

293
00:17:37,070 --> 00:17:40,060
また、1、2、3......と単語の埋め込みと

294
00:17:40,060 --> 00:17:43,830
この点積を取ると、どんどん値が大きくなっていくのも面白い。

295
00:17:43,830 --> 00:17:46,820
つまり、モデルがある単語をどれだけ複数個見つけ

296
00:17:46,820 --> 00:17:49,030
たかを定量的に測れるようなものだ。

297
00:17:50,250 --> 00:17:51,862
ここでも、単語がどのように埋め込ま

298
00:17:51,862 --> 00:17:53,570
れるかは、データを使って学習される。

299
00:17:54,050 --> 00:17:56,800
この埋め込み行列は、その列が各単語に何が起こるかを示

300
00:17:56,800 --> 00:17:59,550
しており、我々のモデルにおける重みの最初の山となる。

301
00:18:00,030 --> 00:18:04,814
GPT-3の数字を使うと、具体的な語彙数は50,257と

302
00:18:04,814 --> 00:18:09,770
なる。これも厳密には単語ではなくトークンで構成されている。

303
00:18:10,630 --> 00:18:14,144
埋め込み次元は12,288であり、これを掛け合わせると

304
00:18:14,144 --> 00:18:17,790
、約6億1700万個の重みで構成されていることがわかる。

305
00:18:18,250 --> 00:18:20,970
これを集計に加え、最終的には1,750億ドル（

306
00:18:20,970 --> 00:18:23,810
約17兆7,500億円）になることを思い出そう。

307
00:18:25,430 --> 00:18:28,780
トランスフォーマーの場合、この埋め込み空間のベクト

308
00:18:28,780 --> 00:18:32,130
ルは、単に個々の単語を表すものではないと考えたい。

309
00:18:32,550 --> 00:18:35,915
ひとつには、その単語の位置に関する情報も符号化されるか

310
00:18:35,915 --> 00:18:39,280
らで、これについては後述するが、それよりも重要なのは、

311
00:18:39,280 --> 00:18:42,770
文脈を染み込ませる能力があると考えるべきだということだ。

312
00:18:43,350 --> 00:18:46,830
例えば、kingという単語を埋め込むことから始まったベク

313
00:18:46,830 --> 00:18:50,310
トルは、次第にこのネットワーク内の様々なブロックに引っ張

314
00:18:50,310 --> 00:18:53,791
られ、引っ張られ、最後にはより具体的でニュアンスのある方

315
00:18:53,791 --> 00:18:57,271
向を指し示すようになり、スコットランドに住んでいた王で、

316
00:18:57,271 --> 00:19:00,752
前の王を殺害した後にその地位を獲得し、シェイクスピア語で

317
00:19:00,752 --> 00:19:04,232
表現されている王であることをコード化するようになるかもし

318
00:19:04,232 --> 00:19:04,730
れない。

319
00:19:05,210 --> 00:19:07,790
ある単語に対する自分の理解について考えてみよう。

320
00:19:08,250 --> 00:19:15,682
次に来る単語を予測する能力を持つモデルを構築する場合、

321
00:19:15,682 --> 00:19:23,390
目標は文脈を効率的に取り入れる力をどうにか与えることだ。

322
00:19:24,050 --> 00:19:26,534
はっきりさせておきたいのは、その最初のステップで、

323
00:19:26,534 --> 00:19:29,018
入力テキストに基づいてベクトルの配列を作成するとき

324
00:19:29,018 --> 00:19:31,503
、それらのひとつひとつは単に埋め込み行列から抜き出

325
00:19:31,503 --> 00:19:33,987
されるだけなので、最初はそれぞれが周囲からの入力な

326
00:19:33,987 --> 00:19:36,770
しにひとつの単語の意味だけをエンコードすることができる。

327
00:19:37,710 --> 00:19:41,463
しかし、このネットワークは、それぞれのベクトルが、単なる

328
00:19:41,463 --> 00:19:45,216
個々の言葉が表すよりもはるかに豊かで具体的な意味を吸収で

329
00:19:45,216 --> 00:19:48,970
きるようにすることを第一の目的としていると考えるべきだ。

330
00:19:49,510 --> 00:19:51,790
ネットワークが一度に処理できるベクトル数は決ま

331
00:19:51,790 --> 00:19:54,170
っており、コンテキストサイズとして知られている。

332
00:19:54,510 --> 00:19:57,965
GPT-3は2048のコンテキスト・サイズで学習され

333
00:19:57,965 --> 00:20:01,421
たので、ネットワークを流れるデータは常にこの2048

334
00:20:01,421 --> 00:20:05,010
列の配列のようになり、それぞれが12000次元を持つ。

335
00:20:05,590 --> 00:20:08,648
このコンテキストの大きさによって、変換器が次の単語

336
00:20:08,648 --> 00:20:11,830
の予測をする際に取り込めるテキストの量が制限される。

337
00:20:12,370 --> 00:20:15,596
そのため、ChatGPTの初期バージョンのように、ある種

338
00:20:15,596 --> 00:20:18,823
のチャットボットとの長時間の会話は、長く続けるうちにボッ

339
00:20:18,823 --> 00:20:22,050
トが会話の糸口を見失うような感覚を与えることが多かった。

340
00:20:23,030 --> 00:20:25,920
注目の詳細についてはいずれ触れるとして、最後の

341
00:20:25,920 --> 00:20:28,810
最後に何が起こるかについて少し触れておきたい。

342
00:20:29,450 --> 00:20:32,106
覚えておいてほしいのは、望ましい出力とは、次に来る

343
00:20:32,106 --> 00:20:34,870
可能性のあるすべてのトークンに対する確率分布である。

344
00:20:35,170 --> 00:20:38,105
例えば、一番最後の単語がProfessorで、文脈にH

345
00:20:38,105 --> 00:20:41,041
arry Potterのような単語が含まれ、その直前に

346
00:20:41,041 --> 00:20:43,977
were favorite teacherがあり、さら

347
00:20:43,977 --> 00:20:46,913
にトークンが単純に完全な単語に見えるように見せかけて多

348
00:20:46,913 --> 00:20:49,197
少の余裕を持たせてくれるなら、Harry 

349
00:20:49,197 --> 00:20:52,132
Potterに関する知識を蓄積したよく訓練されたネット

350
00:20:52,132 --> 00:20:55,068
ワークは、おそらくSnapeという単語に高い数字を割り

351
00:20:55,068 --> 00:20:55,830
当てるだろう。

352
00:20:56,510 --> 00:20:57,970
これには2つの異なるステップがある。

353
00:20:58,310 --> 00:21:01,409
最初のものは、そのコンテキストの一番最後のベクトルを

354
00:21:01,409 --> 00:21:04,510
、語彙の各トークンに対して1つずつ、50,000個の

355
00:21:04,510 --> 00:21:07,610
値のリストにマッピングする別の行列を使うことである。

356
00:21:08,170 --> 00:21:14,876
ソフトマックスと呼ばれるもので、これについてはもう少し後

357
00:21:14,876 --> 00:21:21,583
で詳しく説明する。しかしその前に、予測を行うためにこの最

358
00:21:21,583 --> 00:21:28,290
後の埋め込みだけを使うのは少し奇妙に思えるかもしれない。

359
00:21:28,930 --> 00:21:32,603
これは、学習プロセスにおいて、最終層の各ベクト

360
00:21:32,603 --> 00:21:36,277
ルを使って、その直後の予測も同時に行う方がはる

361
00:21:36,277 --> 00:21:40,270
かに効率的であることが判明したことと関係している。

362
00:21:40,970 --> 00:21:42,979
トレーニングについては後述することがたく

363
00:21:42,979 --> 00:21:45,090
さんあるが、今はただ、そのことを訴えたい。

364
00:21:45,730 --> 00:21:47,653
この行列をUnembedding行

365
00:21:47,653 --> 00:21:49,690
列と呼び、WUというラベルを付ける。

366
00:21:50,210 --> 00:21:52,048
繰り返すが、我々が目にするすべてのウェイ

367
00:21:52,048 --> 00:21:53,887
ト行列のように、そのエントリーはランダム

368
00:21:53,887 --> 00:21:55,910
に始まるが、トレーニングの過程で学習される。

369
00:21:56,470 --> 00:21:59,482
パラメーターの総数から計算すると、この埋め

370
00:21:59,482 --> 00:22:02,494
込み解除行列は、語彙の各単語に対して1行を

371
00:22:02,494 --> 00:22:05,650
持ち、各行は埋め込み次元と同じ要素数を持つ。

372
00:22:06,410 --> 00:22:09,486
埋め込み行列とよく似ているが、順番が入れ替わっているだ

373
00:22:09,486 --> 00:22:12,562
けなので、ネットワークにさらに6億1700万個のパラメ

374
00:22:12,562 --> 00:22:15,638
ータが追加されることになる。つまり、ここまでのカウント

375
00:22:15,638 --> 00:22:18,714
は10億個強で、最終的に合計1750億個になるのだから

376
00:22:18,714 --> 00:22:21,790
、わずかではあるが、まったく取るに足らない数ではない。

377
00:22:22,550 --> 00:22:26,481
この章の最後のミニレッスンとして、このソ

378
00:22:26,481 --> 00:22:30,610
フトマックス関数についてもう少し話したい。

379
00:22:31,430 --> 00:22:34,657
この考え方は、ある数列を確率分布として機能させたい場

380
00:22:34,657 --> 00:22:37,885
合、例えば、次に起こりうるすべての単語に対する分布の

381
00:22:37,885 --> 00:22:41,113
場合、それぞれの値は0から1の間でなければならず、ま

382
00:22:41,113 --> 00:22:44,590
た、すべての値を足すと1になる必要がある、というものだ。

383
00:22:45,250 --> 00:22:48,436
しかし、やることなすことすべてが行列とベクトルの

384
00:22:48,436 --> 00:22:51,623
掛け算に見えるような学習ゲームをしている場合、デ

385
00:22:51,623 --> 00:22:54,810
フォルトで得られる出力はまったくこれに従わない。

386
00:22:55,330 --> 00:22:57,549
その値は多くの場合マイナスか、1よりはるかに

387
00:22:57,549 --> 00:22:59,870
大きく、ほぼ間違いなく足し算で1にはならない。

388
00:23:00,510 --> 00:23:04,049
ソフトマックスは、任意の数値のリストを有効な

389
00:23:04,049 --> 00:23:07,589
分布に変える標準的な方法で、最も大きな値は1

390
00:23:07,589 --> 00:23:11,290
に最も近くなり、小さな値は0に非常に近くなる。

391
00:23:11,830 --> 00:23:13,070
それだけが本当に必要なことだ。

392
00:23:13,090 --> 00:23:17,065
しかし、気になるのであれば、その方法は、まずeを各

393
00:23:17,065 --> 00:23:21,041
数値のべき乗にすることである。つまり、正の値のリス

394
00:23:21,041 --> 00:23:25,017
トができ、次にそれらの正の値の合計をとり、各項をそ

395
00:23:25,017 --> 00:23:29,470
の合計で割ることで、足すと1になるリストに正規化される。

396
00:23:30,170 --> 00:23:33,158
入力の数値の1つが残りの数値より有意に大きい場合、出

397
00:23:33,158 --> 00:23:36,147
力では対応する項が分布を支配することに気づくだろう。

398
00:23:36,147 --> 00:23:39,136
したがって、もしあなたがこの分布からサンプリングする

399
00:23:39,136 --> 00:23:42,470
のであれば、ほぼ間違いなく最大化する入力を選ぶだけだろう。

400
00:23:42,990 --> 00:23:46,786
しかし、他の値が同様に大きい場合、分布の中で意味のある重

401
00:23:46,786 --> 00:23:50,582
みを得るという意味では、ただ最大値を選ぶよりもソフトであ

402
00:23:50,582 --> 00:23:54,650
り、入力を連続的に変化させることですべてが連続的に変化する。

403
00:23:55,130 --> 00:23:59,723
ChatGPTがこの分布を使って次の単語を作るような状況で

404
00:23:59,723 --> 00:24:04,316
は、この関数にちょっとしたスパイスを加えて、指数の分母に定

405
00:24:04,316 --> 00:24:08,910
数tを入れることで、ちょっとした楽しみを増やす余地がある。

406
00:24:09,550 --> 00:24:15,183
tが大きいと、低い値に重みを与えることになり、分

407
00:24:15,183 --> 00:24:20,817
布が少し均一になる。tが小さいと、大きな値がより

408
00:24:20,817 --> 00:24:26,451
積極的に支配することになり、極端に言えば、tをゼ

409
00:24:26,451 --> 00:24:32,790
ロに設定すると、すべての重みが最大値に行くことになる。

410
00:24:33,470 --> 00:24:36,537
例えば、GPT-3に『昔々あるところにAがい

411
00:24:36,537 --> 00:24:39,604
た』という種明かしのテキストでストーリーを生

412
00:24:39,604 --> 00:24:42,950
成させるが、それぞれのケースで異なる温度を使う。

413
00:24:43,630 --> 00:24:46,495
温度ゼロということは、常に最も予測しやす

414
00:24:46,495 --> 00:24:49,361
い言葉を使うということであり、結局はゴル

415
00:24:49,361 --> 00:24:52,370
ディロックスの陳腐な派生語になってしまう。

416
00:24:53,010 --> 00:24:55,405
温度が高ければ高いほど、可能性の低い言葉を選

417
00:24:55,405 --> 00:24:57,910
ぶチャンスが生まれるが、それにはリスクが伴う。

418
00:24:58,230 --> 00:25:02,049
この作品の場合、最初は韓国の若いウェブ・アーティストの

419
00:25:02,049 --> 00:25:06,010
話から始まるのだが、すぐにナンセンスな話になってしまう。

420
00:25:06,950 --> 00:25:08,834
技術的に言えば、APIは実際には2

421
00:25:08,834 --> 00:25:10,830
より大きな温度を選ぶことはできない。

422
00:25:11,170 --> 00:25:13,896
これには数学的な理由はなく、彼らのツールがあまりに

423
00:25:13,896 --> 00:25:16,623
ナンセンスなものを生成していると見られないようにす

424
00:25:16,623 --> 00:25:19,350
るために、恣意的な制約が課せられているだけなのだ。

425
00:25:19,870 --> 00:25:26,420
このアニメーションは、GPT-3が生成する次のトークンのう

426
00:25:26,420 --> 00:25:32,970
ち、最も確率が高いと思われる20個のトークンを使っている。

427
00:25:33,130 --> 00:25:39,529
もうひとつ専門用語を使うと、この関数の出力の構成要素を確率

428
00:25:39,529 --> 00:25:46,150
と呼ぶのと同じように、人々は入力をロジットと呼ぶことが多い。

429
00:25:46,530 --> 00:25:53,829
例えば、テキストを入力し、すべての単語の埋め込みをネット

430
00:25:53,829 --> 00:26:01,390
ワークに流し、最後に埋め込みを解除した行列と掛け算をする。

431
00:26:03,330 --> 00:26:05,641
この章のゴールの多くは、空手キッドのワックス

432
00:26:05,641 --> 00:26:07,953
・オン・ワックス・オフ・スタイルで、注目のメ

433
00:26:07,953 --> 00:26:10,370
カニズムを理解するための基礎を築くことだった。

434
00:26:10,850 --> 00:26:13,884
もしあなたが、単語の埋め込み、ソフトマックス、ドット

435
00:26:13,884 --> 00:26:16,919
積がどのように類似性を測定するかについての強い直感を

436
00:26:16,919 --> 00:26:19,954
持っていて、さらに、ほとんどの計算が、調整可能なパラ

437
00:26:19,954 --> 00:26:22,989
メーターでいっぱいの行列を使った行列の掛け算のように

438
00:26:22,989 --> 00:26:26,023
見えなければならないという根本的な前提を持っているな

439
00:26:26,023 --> 00:26:29,058
らば、現代のAIブーム全体の基礎となる部分である注意

440
00:26:29,058 --> 00:26:32,210
のメカニズムを理解することは、比較的スムーズなはずだ。

441
00:26:32,650 --> 00:26:34,510
それについては、次の章でご一緒しよう。

442
00:26:36,390 --> 00:26:38,800
この原稿を書いている今、その次の章の草稿がパト

443
00:26:38,800 --> 00:26:41,210
ロン・サポーターのレビュー用に公開されている。

444
00:26:41,770 --> 00:26:44,570
最終的なバージョンは1、2週間以内に公開されるはずで、

445
00:26:44,570 --> 00:26:47,370
通常はそのレビューに基づいてどれだけ変更するかによる。

446
00:26:47,810 --> 00:26:50,110
その間、もしあなたがアテンションに飛び込みたいなら、そして

447
00:26:50,110 --> 00:26:52,410
チャンネルを少しでも助けたいなら、それはそこで待っている。

