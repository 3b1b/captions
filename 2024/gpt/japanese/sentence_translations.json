[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "頭文字のGPTはGenerative Pretrained Transformerの略である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "つまり、最初の言葉は簡単で、これは新しいテキストを生成するボットなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Pretrainedとは、モデルが大量のデータから学習するプロセスを経たことを意味し、Prefixは、追加トレーニングによって特定のタスクを微調整する余地があることを示唆している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "しかし、最後の言葉、それこそが本当に重要なピースなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "トランスフォーマーは特定の種類のニューラルネットワーク、機械学習モデルであり、現在のAIブームの根底にある中核的な発明だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "このビデオと次の章で私がしたいことは、トランスの内部で実際に何が起こっているのかを視覚的に説明することである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "そこに流れるデータを追いながら、一歩一歩進んでいく。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "変圧器を使って作ることができる模型にはいろいろな種類がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "音声を取り込み、トランスクリプトを作成するモデルもある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "この文章は、逆にテキストだけで合成音声を生成するモデルのものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "2022年に一世を風靡したドリーやミッドジャーニーのような、テキスト描写を取り込んで画像を生成するツールはすべてトランスフォーマーをベースにしている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "パイという生き物が何であるかを理解させることはできなくても、こんなことが可能なのかと驚かされる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "そして、グーグルが2017年に発表したオリジナルのトランスフォーマーは、ある言語から別の言語へテキストを翻訳するという特定のユースケースのために発明された。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "しかし、あなたや私が注目するのは、ChatGPTのようなツールの根底にあるタイプで、テキストの一部を取り込み、それに付随する画像や音声を取り込み、その文章の次に何が来るかを予測するように訓練されたモデルである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "その予測は、その後に続く可能性のあるさまざまなテキストの塊に対する確率分布の形をとる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "一見すると、次の単語を予測することは、新しいテキストを生成することとはまったく異なる目標のように感じられるかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "しかし、このような予測モデルができたら、長いテキストを生成する簡単な方法は、最初のスニペットを与え、生成した分布からランダムなサンプルを取り、そのサンプルをテキストに追加する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "あなたのことは知らないが、これが実際に機能するとはとても思えない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "例えばこのアニメーションでは、私のノートパソコンでGPT-2を動かし、次のテキストの塊を繰り返し予測してサンプリングさせ、種となるテキストに基づいてストーリーを生成している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "ストーリーはあまり意味がない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "しかし、その代わりにGPT-3のAPI呼び出しに置き換えてみると、同じ基本モデルでありながら、はるかに大きくなり、突然、ほとんど魔法のように、理にかなったストーリーを得ることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "予測とサンプリングを繰り返すこのプロセスは、ChatGPTや他の大規模な言語モデルと対話し、彼らが一度に1つの単語を生成するのを見るとき、本質的に起こっていることだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "実際、私がとても楽しみたい機能のひとつは、新しい単語を選ぶたびに、その基礎となる分布を見ることができることだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "まずは、データがトランスフォーマーをどのように通過するか、非常にハイレベルなプレビューから始めよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "各ステップの詳細については、動機づけや解釈、展開にもっと多くの時間を費やすことになるだろうが、大まかに言えば、これらのチャットボットの1つが与えられた単語を生成するとき、ボンネットの下で何が起こっているかは以下の通りだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "まず、入力は小さな断片に分割される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "これらの断片はトークンと呼ばれ、テキストの場合、単語や単語の小片、その他の一般的な文字の組み合わせになる傾向がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "イメージやサウンドが関係している場合、トークンはイメージの小さなパッチやサウンドの小さなチャンクになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "これらのトークンはそれぞれ、ベクター、つまり数字のリストと関連付けられ、何らかの形でその部分の意味を暗号化することを意味する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "これらのベクトルを、ある非常に高次元の空間における座標を与えるものと考えれば、似たような意味を持つ単語は、その空間において互いに近いベクトルに着地する傾向がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "この一連のベクトルは、次にアテンション・ブロックと呼ばれる処理を通過し、これによりベクトル同士が会話し、情報をやり取りして値を更新する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "例えば、機械学習モデルというフレーズにおけるモデルという単語の意味は、ファッションモデルというフレーズにおける意味とは異なる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "アテンション・ブロックは、文脈の中でどの単語が他のどの単語の意味を更新するのに関連するのか、そしてそれらの意味を具体的にどのように更新すべきなのかを見つけ出す役割を担っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "そしてまた、私が意味という言葉を使うときはいつでも、この意味はこれらのベクトルのエントリーに完全に符号化されているのである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "その後、これらのベクトルは別の種類の演算を経て、あなたが読んでいる情報源によっては、多層パーセプトロン、あるいはフィードフォワード層と呼ばれる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "そしてここでは、ベクターは互いに会話することなく、すべて並列に同じ操作を行う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "このブロックの解釈は少し難しいが、後ほど、このステップが、各ベクトルについて長い質問リストを問いかけ、その答えに基づいてベクトルを更新していくようなものであることを説明する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "これらのブロックの演算はすべて、行列の掛け算の山のように見える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "その間に行われるいくつかの正規化ステップの詳細は割愛するが、結局のところ、これはハイレベルなプレビューなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "その後、アテンション・ブロックと多層パーセプトロン・ブロックを行ったり来たりしながら、基本的にこのプロセスを繰り返すのだが、最後の最後には、その文章の本質的な意味のすべてが、どうにかして最後のベクトルに焼き付いていることが期待される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "次に、この最後のベクトルに対してある演算を行い、次に来る可能性のあるすべてのトークン、つまり可能性のあるすべての小さなテキストの塊に対する確率分布を生成する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "そして、私が言ったように、テキストの断片から次に何が来るかを予測するツールを手に入れたら、そのツールに少量のシードテキストを与え、次に何が来るかを予測し、分布からサンプリングし、それを追加し、何度も何度も繰り返すというゲームを繰り返させることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "ChatGPTが登場するずっと前、GPT-3の初期のデモがこんな感じだったのを覚えている人もいるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "このようなツールをチャットボットにするには、最も簡単な出発点は、ユーザーが親切なAIアシスタントと対話するという設定を確立するちょっとしたテキスト、システム・プロンプトと呼ぶべきものを用意することだ。そして、ユーザーの最初の質問やプロンプトを対話の最初のビットとして使い、そのような親切なAIアシスタントが返答として何を言うかを予測させるのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "これをうまく機能させるために必要なトレーニングのステップについては、もっと言うべきことがあるが、高いレベルではこれがアイデアだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "この章では、ネットワークの一番最初と一番最後に何が起こるかについて詳しく説明し、トランスフォーマーが登場する頃には機械学習エンジニアなら誰でも知っていたような、重要な背景知識の復習にも多くの時間を割きたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "次の章では、一般的にトランスの心臓部と考えられているアテンション・ブロックに焦点を当てる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "この後、この多層パーセプトロン・ブロックについて、トレーニングがどのように機能するのか、そしてそれまで飛ばされてきた他の多くの詳細についてもっと話したい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "しかし、トランスフォーマーに特化する前に、ディープラーニングの基本的な前提や構造について確認しておく価値があると思う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "明らかなことを言うかもしれないが、これは機械学習の1つのアプローチであり、モデルの振る舞いを決定するためにデータを使用するモデルを指す。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "つまり、画像を取り込んで、それを説明するラベルを生成する関数や、文章から次の単語を予測する例など、直感やパターン認識の要素を必要とするようなタスクが必要だとしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "しかし、機械学習の考え方は、そのタスクをどのように行うかの手順をコードで明示的に定義しようとするのではなく、AIの初期に人々が行っていたような、調整可能なパラメータを持つ非常に柔軟な構造を設定することである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "例えば、機械学習の最も単純な形は線形回帰で、入力と出力はそれぞれ単一の数値、例えば家の面積とその価格である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "その直線は2つの連続的なパラメータ、例えば傾きとy切片によって記述され、線形回帰の目標は、データに密接に一致するようにそれらのパラメータを決定することである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "言うまでもなく、ディープラーニング・モデルはもっと複雑になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "例えばGPT-3には2つどころか、1750億ものパラメーターがある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "しかし、ここで重要なのは、膨大な数のパラメーターを持つ巨大なモデルを作成しても、それが訓練データに著しくオーバーフィットしたり、訓練に全く手こずったりしないとは限らないということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "ディープラーニングは、ここ数十年で驚くほどうまくスケールすることが証明されたモデルのクラスについて説明している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "そして、このトレーニング・アルゴリズムがスケールアップしてうまく機能するためには、これらのモデルはある特定のフォーマットに従わなければならない、ということをご理解いただきたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "この形式を知っていれば、トランスフォーマーがどのように言語を処理するかの選択肢の多くを説明するのに役立つ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "まず、どんなモデルを作るにせよ、入力は実数の配列としてフォーマットされなければならない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "これは数値のリストを意味することもあれば、2次元配列であることもある。また、一般的にテンソルという用語が使われる高次元配列を扱うことも非常に多い。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "その入力データは、多くの異なるレイヤーに徐々に変換されていくと考えるのが普通である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "例えば、我々のテキスト処理モデルの最終レイヤーは、次に起こりうるすべてのトークンの確率分布を表す数値のリストである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "ディープラーニングでは、これらのモデルパラメータはほとんどの場合、重みと呼ばれる。これは、これらのモデルの重要な特徴が、これらのパラメータが処理されるデータと相互作用する唯一の方法が、重み付き和であることにあるためだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "非線形関数もいくつか散りばめられているが、それらはパラメータに依存しない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "しかし一般的には、加重和をすべて裸にしてこのように明示的に書き出すのではなく、行列のベクトル積のさまざまな成分としてまとめて見ることになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "行列のベクトル乗算がどのように機能するかを思い返せば、出力の各成分は加重和のように見える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "処理されるデータから引き出されるベクトルを変換する、調整可能なパラメーターで満たされた行列について考える方が、概念的にすっきりしていることが多いのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "例えば、GPT-3の1,750億の重みは、28,000弱の異なる行列に整理されている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "これらのマトリックスは8つの異なるカテゴリーに分類され、私たちはそれぞれのカテゴリーを順に見ていき、そのタイプが何をするのかを理解することになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "GPT-3の具体的な数字を参照しながら、その1750億がどこから来たのかをカウントアップしていくのも楽しいと思う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "今でこそ、より大規模で優れたモデルがあるにせよ、このモデルはMLコミュニティ以外の世界の注目を集める大規模言語モデルとして、ある種の魅力を持っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "また、現実的に言えば、最新のネットワークでは、各社とも具体的な数字については口をつぐむ傾向にある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "ChatGPTのようなツールの内部で何が起こっているのか、フードの下を覗いてみると、実際の計算のほとんどすべてが行列のベクトル乗算のように見えるということを、これから説明したい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "何十億という数字の海に迷い込むリスクは少しあるが、モデルの重み（私は常に青か赤で色付けする）と、処理されるデータ（私は常にグレーで色付けする）を頭の中で鋭く区別する必要がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "ウェイトは実際の頭脳であり、トレーニング中に学習したものであり、それがどのように振る舞うかを決定する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "処理されるデータは、テキストのスニペットの例のように、与えられた実行のためにモデルに供給されるどんな特定の入力でも、単にエンコードする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "入力を小さなチャンクに分割し、そのチャンクをベクトルに変えるのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "これらのチャンクがトークンと呼ばれ、単語の断片であったり、句読点であったりすることは前述したが、この章、そして特に次の章では、時々、もっときれいに単語に分割されているように見せかけたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "私たち人間は言葉で考えるから、ちょっとした例を参照したり、各ステップを明確にしたりすることが、より簡単になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "このモデルには、あらかじめ定義された語彙があり、たとえば50,000個の単語からなるリストがある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "これらの列が、最初のステップで各単語がどのようなベクトルに変化するかを決定する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "私たちが目にするすべての行列と同じように、その値も最初はランダムだが、データに基づいて学習されていく。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "単語をベクトルに変換することは、トランスフォーマーよりもずっと以前から機械学習では一般的に行われていたことだが、初めて目にすると少し奇妙で、この後に続くすべての基礎となるものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "私たちはよくこの埋め込みを単語と呼ぶが、これはこれらのベクトルをある高次元空間の点として非常に幾何学的に考えることを誘う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "3つの数字のリストを3次元空間の点の座標として視覚化することは問題ないが、単語の埋め込みははるかに高次元になる傾向がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "GPT-3では12,288の次元があり、おわかりのように、多くの明確な方向性を持つ空間で作業することが重要なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "3次元空間を2次元的にスライスして、そのスライスにすべての点を投影するのと同じように、単純なモデルが与えてくれる単語の埋め込みをアニメーション化するために、この非常に高次元の空間を3次元的にスライスして、そこに単語のベクトルを投影し、結果を表示する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "ここでの大きなアイデアは、学習中に単語がどのようにベクトルとして埋め込まれるかを決定するために、モデルが重みを微調整し、チューニングするにつれ、空間内の方向が一種の意味的な意味を持つような埋め込みセットに落ち着く傾向があるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "私がここで実行している単純な単語対ベクトルモデルの場合、埋め込みがtowerのそれに最も近い単語をすべて検索してみると、どの単語も非常によく似たtowerっぽい雰囲気を醸し出していることに気づくだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Pythonを立ち上げて、自宅で一緒に遊びたいなら、これがアニメーションを作るのに使っている具体的なモデルだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "これは変圧器ではないが、空間の方向が意味的な意味を持ちうるという考えを説明するには十分だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "その典型的な例が、女性と男性のベクトルの違いを、片方の先端ともう片方の先端を結ぶ小さなベクトルとして視覚化した場合、それは王と女王の違いによく似ている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "つまり、女性君主を意味する単語を知らなかったとすると、kingを取り、このwoman-manの方向を加え、その点に最も近い埋め込みを検索することで見つけることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "少なくとも、そんな感じだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "これは私が遊んでいるモデルの典型的な例であるにもかかわらず、クイーンの真の埋め込みは、これが示唆するよりも実際には少しずれている。おそらく、トレーニングデータでクイーンが単にキングの女性版として使われているわけではないからだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "いろいろやってみると、家族関係の方がこの考えをよく表しているように思えた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "要するに、モデルはトレーニング中に、この空間のある方向が性別情報をエンコードするような埋め込みを選択するのが有利だとわかったようだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "もう一つの例は、イタリアのエンベッディングからドイツのエンベッディングを引いて、それをヒトラーのエンベッディングに足すと、ムッソリーニのエンベッディングに非常に近いものが得られるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "このモデルは、ある方向はイタリアらしさを、他の方向は第二次世界大戦の枢軸国の指導者を連想させることを学んだかのようだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "この系統で私が一番好きな例は、ドイツと日本の違いを寿司に加えると、ブラートヴルストに非常に近くなるというモデルだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "また、この最も近い隣人を探すゲームでは、キャットが野獣と怪物の両方に近かったことが嬉しかった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "特に次の章で役に立つ数学的直観のひとつは、2つのベクトルの内積が、それらのベクトルがどれだけうまく整列しているかを測る方法として考えられるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "計算上、ドット積は対応するすべての成分を掛け合わせ、その結果を足すことになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "幾何学的には、ベクトルが同じような方向を向いている場合、内積は正になり、垂直な場合はゼロになり、反対方向を向いている場合は負になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "例えば、あなたがこのモデルで遊んでいて、猫から猫を引いた埋め込みが、この空間におけるある種の複数性の方向を表しているのではないかという仮説を立てたとしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "これをテストするために、このベクトルを取り出し、ある単数名詞の埋め込みに対してそのドット積を計算し、対応する複数名詞とのドット積と比較してみる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "これを弄ってみると、確かに複数形の方が単数形よりも一貫して高い値を示していることに気づくだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "また、1、2、3......と単語の埋め込みとこの点積を取ると、どんどん値が大きくなっていくのも面白い。つまり、モデルがある単語をどれだけ複数個見つけたかを定量的に測れるようなものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "ここでも、単語がどのように埋め込まれるかは、データを使って学習される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "この埋め込み行列は、その列が各単語に何が起こるかを示しており、我々のモデルにおける重みの最初の山となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "GPT-3の数字を使うと、具体的な語彙数は50,257となる。これも厳密には単語ではなくトークンで構成されている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "埋め込み次元は12,288であり、これを掛け合わせると、約6億1700万個の重みで構成されていることがわかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "これを集計に加え、最終的には1,750億ドル（約17兆7,500億円）になることを思い出そう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "トランスフォーマーの場合、この埋め込み空間のベクトルは、単に個々の単語を表すものではないと考えたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "ひとつには、その単語の位置に関する情報も符号化されるからで、これについては後述するが、それよりも重要なのは、文脈を染み込ませる能力があると考えるべきだということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "例えば、kingという単語を埋め込むことから始まったベクトルは、次第にこのネットワーク内の様々なブロックに引っ張られ、引っ張られ、最後にはより具体的でニュアンスのある方向を指し示すようになり、スコットランドに住んでいた王で、前の王を殺害した後にその地位を獲得し、シェイクスピア語で表現されている王であることをコード化するようになるかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "ある単語に対する自分の理解について考えてみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "次に来る単語を予測する能力を持つモデルを構築する場合、目標は文脈を効率的に取り入れる力をどうにか与えることだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "はっきりさせておきたいのは、その最初のステップで、入力テキストに基づいてベクトルの配列を作成するとき、それらのひとつひとつは単に埋め込み行列から抜き出されるだけなので、最初はそれぞれが周囲からの入力なしにひとつの単語の意味だけをエンコードすることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "しかし、このネットワークは、それぞれのベクトルが、単なる個々の言葉が表すよりもはるかに豊かで具体的な意味を吸収できるようにすることを第一の目的としていると考えるべきだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "ネットワークが一度に処理できるベクトル数は決まっており、コンテキストサイズとして知られている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "GPT-3は2048のコンテキスト・サイズで学習されたので、ネットワークを流れるデータは常にこの2048列の配列のようになり、それぞれが12000次元を持つ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "このコンテキストの大きさによって、変換器が次の単語の予測をする際に取り込めるテキストの量が制限される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "そのため、ChatGPTの初期バージョンのように、ある種のチャットボットとの長時間の会話は、長く続けるうちにボットが会話の糸口を見失うような感覚を与えることが多かった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "注目の詳細についてはいずれ触れるとして、最後の最後に何が起こるかについて少し触れておきたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "覚えておいてほしいのは、望ましい出力とは、次に来る可能性のあるすべてのトークンに対する確率分布である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "例えば、一番最後の単語がProfessorで、文脈にHarry Potterのような単語が含まれ、その直前にwere favorite teacherがあり、さらにトークンが単純に完全な単語に見えるように見せかけて多少の余裕を持たせてくれるなら、Harry Potterに関する知識を蓄積したよく訓練されたネットワークは、おそらくSnapeという単語に高い数字を割り当てるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "これには2つの異なるステップがある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "最初のものは、そのコンテキストの一番最後のベクトルを、語彙の各トークンに対して1つずつ、50,000個の値のリストにマッピングする別の行列を使うことである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "ソフトマックスと呼ばれるもので、これについてはもう少し後で詳しく説明する。しかしその前に、予測を行うためにこの最後の埋め込みだけを使うのは少し奇妙に思えるかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "これは、学習プロセスにおいて、最終層の各ベクトルを使って、その直後の予測も同時に行う方がはるかに効率的であることが判明したことと関係している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "トレーニングについては後述することがたくさんあるが、今はただ、そのことを訴えたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "この行列をUnembedding行列と呼び、WUというラベルを付ける。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "繰り返すが、我々が目にするすべてのウェイト行列のように、そのエントリーはランダムに始まるが、トレーニングの過程で学習される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "パラメーターの総数から計算すると、この埋め込み解除行列は、語彙の各単語に対して1行を持ち、各行は埋め込み次元と同じ要素数を持つ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "埋め込み行列とよく似ているが、順番が入れ替わっているだけなので、ネットワークにさらに6億1700万個のパラメータが追加されることになる。つまり、ここまでのカウントは10億個強で、最終的に合計1750億個になるのだから、わずかではあるが、まったく取るに足らない数ではない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "この章の最後のミニレッスンとして、このソフトマックス関数についてもう少し話したい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "この考え方は、ある数列を確率分布として機能させたい場合、例えば、次に起こりうるすべての単語に対する分布の場合、それぞれの値は0から1の間でなければならず、また、すべての値を足すと1になる必要がある、というものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "しかし、やることなすことすべてが行列とベクトルの掛け算に見えるような学習ゲームをしている場合、デフォルトで得られる出力はまったくこれに従わない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "その値は多くの場合マイナスか、1よりはるかに大きく、ほぼ間違いなく足し算で1にはならない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "ソフトマックスは、任意の数値のリストを有効な分布に変える標準的な方法で、最も大きな値は1に最も近くなり、小さな値は0に非常に近くなる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "それだけが本当に必要なことだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "しかし、気になるのであれば、その方法は、まずeを各数値のべき乗にすることである。つまり、正の値のリストができ、次にそれらの正の値の合計をとり、各項をその合計で割ることで、足すと1になるリストに正規化される。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "入力の数値の1つが残りの数値より有意に大きい場合、出力では対応する項が分布を支配することに気づくだろう。したがって、もしあなたがこの分布からサンプリングするのであれば、ほぼ間違いなく最大化する入力を選ぶだけだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "しかし、他の値が同様に大きい場合、分布の中で意味のある重みを得るという意味では、ただ最大値を選ぶよりもソフトであり、入力を連続的に変化させることですべてが連続的に変化する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "ChatGPTがこの分布を使って次の単語を作るような状況では、この関数にちょっとしたスパイスを加えて、指数の分母に定数tを入れることで、ちょっとした楽しみを増やす余地がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "tが大きいと、低い値に重みを与えることになり、分布が少し均一になる。tが小さいと、大きな値がより積極的に支配することになり、極端に言えば、tをゼロに設定すると、すべての重みが最大値に行くことになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "例えば、GPT-3に『昔々あるところにAがいた』という種明かしのテキストでストーリーを生成させるが、それぞれのケースで異なる温度を使う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "温度ゼロということは、常に最も予測しやすい言葉を使うということであり、結局はゴルディロックスの陳腐な派生語になってしまう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "温度が高ければ高いほど、可能性の低い言葉を選ぶチャンスが生まれるが、それにはリスクが伴う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "この作品の場合、最初は韓国の若いウェブ・アーティストの話から始まるのだが、すぐにナンセンスな話になってしまう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "技術的に言えば、APIは実際には2より大きな温度を選ぶことはできない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "これには数学的な理由はなく、彼らのツールがあまりにナンセンスなものを生成していると見られないようにするために、恣意的な制約が課せられているだけなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "このアニメーションは、GPT-3が生成する次のトークンのうち、最も確率が高いと思われる20個のトークンを使っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "もうひとつ専門用語を使うと、この関数の出力の構成要素を確率と呼ぶのと同じように、人々は入力をロジットと呼ぶことが多い。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "例えば、テキストを入力し、すべての単語の埋め込みをネットワークに流し、最後に埋め込みを解除した行列と掛け算をする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "この章のゴールの多くは、空手キッドのワックス・オン・ワックス・オフ・スタイルで、注目のメカニズムを理解するための基礎を築くことだった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "もしあなたが、単語の埋め込み、ソフトマックス、ドット積がどのように類似性を測定するかについての強い直感を持っていて、さらに、ほとんどの計算が、調整可能なパラメーターでいっぱいの行列を使った行列の掛け算のように見えなければならないという根本的な前提を持っているならば、現代のAIブーム全体の基礎となる部分である注意のメカニズムを理解することは、比較的スムーズなはずだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "それについては、次の章でご一緒しよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "この原稿を書いている今、その次の章の草稿がパトロン・サポーターのレビュー用に公開されている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "最終的なバージョンは1、2週間以内に公開されるはずで、通常はそのレビューに基づいてどれだけ変更するかによる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "その間、もしあなたがアテンションに飛び込みたいなら、そしてチャンネルを少しでも助けたいなら、それはそこで待っている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]