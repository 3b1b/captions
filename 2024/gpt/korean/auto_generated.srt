1
00:00:00,000 --> 00:00:04,560
GPT란 생성형 사전 훈련 트랜스포머의 약자입니다.

2
00:00:05,220 --> 00:00:06,965
첫 단어는 간단히, 이것이 새 

3
00:00:06,965 --> 00:00:09,020
텍스트를 생성하는 봇이라는 뜻입니다.

4
00:00:09,800 --> 00:00:12,285
사전 학습은 모델이 방대한 양의 데이터로부터 

5
00:00:12,285 --> 00:00:14,273
학습하는 과정을 거쳤음을 의미하며, 

6
00:00:14,273 --> 00:00:16,659
이 접두사는 추가 학습을 통해 특정 작업에 

7
00:00:16,659 --> 00:00:19,145
대해 미세 조정할 수 있는 여지가 더 많다는 

8
00:00:19,145 --> 00:00:20,040
것을 암시합니다.

9
00:00:20,720 --> 00:00:22,900
하지만 마지막 단어가 진짜 핵심입니다.

10
00:00:23,380 --> 00:00:27,261
트랜스포머는 머신러닝 모델인 신경망의 일종으로, 

11
00:00:27,261 --> 00:00:31,000
현재 AI 붐의 근간이 되는 핵심 발명품입니다.

12
00:00:31,740 --> 00:00:34,270
이 동영상과 다음 장에서 제가 하고자 하는 

13
00:00:34,270 --> 00:00:36,800
것은 트랜스포머 내부에서 실제로 어떤 일이 

14
00:00:36,800 --> 00:00:39,120
일어나는지 시각적으로 설명하는 것입니다.

15
00:00:39,700 --> 00:00:41,307
트랜스포머에 흘러가는 데이터를 

16
00:00:41,307 --> 00:00:42,820
따라 단계별로 진행하겠습니다.

17
00:00:43,440 --> 00:00:45,250
트랜스포머를 사용하여 만들 수 

18
00:00:45,250 --> 00:00:47,380
있는 모델의 종류는 매우 다양합니다.

19
00:00:47,800 --> 00:00:50,800
일부 모델은 오디오를 받아 녹취록을 생성합니다.

20
00:00:51,340 --> 00:00:54,098
이 문장은 텍스트만으로 합성 음성을 생성하는, 

21
00:00:54,098 --> 00:00:56,220
반대 방향의 모델에서 나온 것입니다.

22
00:00:56,660 --> 00:00:59,736
텍스트 설명을 받아 이미지를 생성하는 돌리와 

23
00:00:59,736 --> 00:01:02,689
미드저니처럼 2022년에 전 세계를 강타한 

24
00:01:02,689 --> 00:01:05,519
도구는 모두 트랜스포머를 기반으로 합니다.

25
00:01:06,000 --> 00:01:08,592
파이 생물이 무엇인지 이해하지 못한다고는 

26
00:01:08,592 --> 00:01:10,958
해도 이런 일이 원격으로도 가능하다는 

27
00:01:10,958 --> 00:01:13,100
사실에 놀라움을 금할 수 없습니다.

28
00:01:13,900 --> 00:01:16,702
그리고 2017년에 Google이 도입한 최초의 

29
00:01:16,702 --> 00:01:19,401
트랜스포머는 한 언어에서 다른 언어로 텍스트를 

30
00:01:19,401 --> 00:01:22,100
번역하는 특정 사용 사례를 위해 개발되었습니다.

31
00:01:22,660 --> 00:01:25,700
하지만 여러분과 제가 집중할 변형 모델은 

32
00:01:25,700 --> 00:01:29,534
ChatGPT와 같은 도구의 기반이 되는 유형으로, 

33
00:01:29,534 --> 00:01:33,368
텍스트 조각을 (어쩌면 주변 이미지나 소리도 함께) 

34
00:01:33,368 --> 00:01:36,937
받아 그 구절에서 다음에 나올 내용을 예측하도록 

35
00:01:36,937 --> 00:01:38,260
학습된 모델입니다.

36
00:01:38,600 --> 00:01:41,100
이러한 예측은 다음에 이어질 수 있는 다양한 

37
00:01:41,100 --> 00:01:43,800
텍스트 청크에 대한 확률 분포의 형태를 취합니다.

38
00:01:45,040 --> 00:01:46,541
언뜻 보기에 다음 단어를 예측하는 

39
00:01:46,541 --> 00:01:48,201
것은 새로운 텍스트를 생성하는 것과는 

40
00:01:48,201 --> 00:01:49,940
매우 다른 목표처럼 느껴질 수 있습니다.

41
00:01:50,180 --> 00:01:53,186
하지만 이와 같은 예측 모델이 있으면 더 긴 

42
00:01:53,186 --> 00:01:56,673
텍스트를 생성할 때 작업할 초기 스니펫을 제공하고, 

43
00:01:56,673 --> 00:01:59,559
방금 생성한 분포에서 무작위 샘플을 가져와 

44
00:01:59,559 --> 00:02:02,926
텍스트에 해당 샘플을 추가한 다음 전체 프로세스를 

45
00:02:02,926 --> 00:02:06,052
다시 실행하여 방금 추가된 내용을 포함한 모든 

46
00:02:06,052 --> 00:02:09,539
새 텍스트를 기반으로 새로운 예측을 수행하면 됩니다.

47
00:02:10,100 --> 00:02:11,519
여러분은 어떤지 모르겠지만, 이런 방식이 

48
00:02:11,519 --> 00:02:13,000
실제로 작동할 것 같이 느껴지지는 않습니다.

49
00:02:13,420 --> 00:02:15,579
예를 들어, 이 애니메이션에서는 노트북에서 

50
00:02:15,579 --> 00:02:17,829
GPT-2를 실행하고 시드 텍스트를 기반으로 

51
00:02:17,829 --> 00:02:20,079
스토리를 생성하기 위해 다음 텍스트 덩어리를 

52
00:02:20,079 --> 00:02:22,420
반복적으로 예측하고 샘플링하도록 하고 있습니다.

53
00:02:22,420 --> 00:02:26,120
이 이야기는 그다지 말이 되지 않습니다.

54
00:02:26,500 --> 00:02:30,190
하지만 기본 모델은 같지만 훨씬 더 큰 GPT-3에 

55
00:02:30,190 --> 00:02:33,626
대한 API 호출로 바꾸면 갑자기 거의 마술처럼 

56
00:02:33,626 --> 00:02:36,935
파이라는 생물이 수학과 계산의 땅에 살고 있을 

57
00:02:36,935 --> 00:02:39,989
것이라는 추론까지 가능한 합리적인 이야기가 

58
00:02:39,989 --> 00:02:40,880
만들어집니다.

59
00:02:41,580 --> 00:02:44,003
여기서 반복되는 예측 및 샘플링 프로세스는 

60
00:02:44,003 --> 00:02:46,629
여러분이 ChatGPT 또는 다른 대규모 언어 

61
00:02:46,629 --> 00:02:49,052
모델과 상호 작용할 때 한 번에 한 단어씩 

62
00:02:49,052 --> 00:02:51,880
생성되는 것을 볼 때 본질적으로 일어나는 일입니다.

63
00:02:52,480 --> 00:02:54,654
실제로 제가 매우 좋아하는 기능 중 

64
00:02:54,654 --> 00:02:56,828
하나는 새로운 단어를 선택할 때마다 

65
00:02:56,828 --> 00:02:59,220
그 기본 분포를 볼 수 있는 기능입니다.

66
00:03:03,820 --> 00:03:06,044
트랜스포머를 통해 데이터가 어떻게 흘러가는지 

67
00:03:06,044 --> 00:03:08,180
아주 높은 수준의 미리보기로 시작하겠습니다.

68
00:03:08,640 --> 00:03:11,231
각 단계의 세부 사항에 대한 동기를 부여하고 해석하고 

69
00:03:11,231 --> 00:03:13,563
확장하는 데 훨씬 더 많은 시간을 할애하겠지만, 

70
00:03:13,563 --> 00:03:16,155
크게 보면 이러한 챗봇 중 하나가 특정 단어를 생성할 

71
00:03:16,155 --> 00:03:18,660
때 내부에서 어떤 일이 일어나는지는 다음과 같습니다.

72
00:03:19,080 --> 00:03:22,040
먼저, 입력이 여러 개의 작은 조각으로 나뉩니다.

73
00:03:22,620 --> 00:03:24,710
이러한 조각을 토큰이라고 하며, 

74
00:03:24,710 --> 00:03:27,149
텍스트의 경우 단어 또는 단어의 작은 

75
00:03:27,149 --> 00:03:29,820
조각 또는 기타 일반적인 문자 조합입니다.

76
00:03:30,740 --> 00:03:33,910
이미지나 소리의 경우 토큰은 해당 이미지의 작은 

77
00:03:33,910 --> 00:03:37,080
조각이나 사운드의 작은 덩어리가 될 수 있습니다.

78
00:03:37,580 --> 00:03:41,399
이러한 각 토큰은 벡터, 즉 해당 토큰의 의미를 

79
00:03:41,399 --> 00:03:45,360
어떻게든 인코딩하기 위한 숫자 리스트와 연관됩니다.

80
00:03:45,880 --> 00:03:48,747
이러한 벡터를 어떠한 매우 고차원적인 공간에 좌표를 

81
00:03:48,747 --> 00:03:51,713
제공한다고 생각하면, 비슷한 의미를 가진 단어는 해당 

82
00:03:51,713 --> 00:03:54,680
공간에서 서로 가까운 벡터에 위치하는 경향이 있습니다.

83
00:03:55,280 --> 00:03:58,034
이 일련의 벡터는 어텐션 블록이라고 하는 

84
00:03:58,034 --> 00:04:00,788
연산을 통과하며, 이를 통해 벡터가 서로 

85
00:04:00,788 --> 00:04:03,661
대화하고 정보를 주고받으며 값을 업데이트할 

86
00:04:03,661 --> 00:04:04,500
수 있습니다.

87
00:04:04,880 --> 00:04:07,153
예를 들어, 머신러닝 모델이라는 문구에서 

88
00:04:07,153 --> 00:04:09,427
모델이라는 단어의 의미는 패션 모델이라는 

89
00:04:09,427 --> 00:04:11,800
문구에서 모델이라는 단어의 의미와 다릅니다.

90
00:04:12,260 --> 00:04:15,604
어텐션 블록은 문맥에서 어떤 단어가 다른 단어의 의미 

91
00:04:15,604 --> 00:04:18,838
업데이트와 관련이 있는지, 그리고 그 의미를 정확히 

92
00:04:18,838 --> 00:04:21,959
어떻게 업데이트해야 하는지 파악하는 역할을 합니다.

93
00:04:22,500 --> 00:04:24,317
그리고 다시 말하지만, 제가 의미라는 

94
00:04:24,317 --> 00:04:26,135
단어를 사용할 때마다 이것은 어떻게든 

95
00:04:26,135 --> 00:04:28,040
해당 벡터의 항목에 완전히 인코딩됩니다.

96
00:04:29,180 --> 00:04:32,262
그 후 이러한 벡터는 다른 종류의 연산을 거치게 

97
00:04:32,262 --> 00:04:35,117
되며, 읽는 소스에 따라 이를 다층 퍼셉트론 

98
00:04:35,117 --> 00:04:38,200
또는 피드 포워드 레이어라고 부를 수도 있습니다.

99
00:04:38,580 --> 00:04:40,518
여기서 벡터는 서로 대화하지 않고 

100
00:04:40,518 --> 00:04:42,660
모두 동일한 연산을 병렬로 진행합니다.

101
00:04:43,060 --> 00:04:45,762
이 블록은 해석하기가 조금 어렵지만, 

102
00:04:45,762 --> 00:04:49,237
나중에 각 벡터에 대해 많은 질문들을 한 뒤 그 

103
00:04:49,237 --> 00:04:52,970
질문에 대한 답을 바탕으로 업데이트하는 단계에 대해 

104
00:04:52,970 --> 00:04:54,000
설명하겠습니다.

105
00:04:54,900 --> 00:04:58,128
이 두 블록의 모든 연산은 행렬 곱셈의 

106
00:04:58,128 --> 00:05:01,210
거대한 더미처럼 보이며, 우리의 주요 

107
00:05:01,210 --> 00:05:05,320
임무는 기본 행렬을 읽는 방법을 이해하는 것입니다.

108
00:05:06,980 --> 00:05:09,773
그 사이에 일어나는 몇 가지 정규화 단계에 대해 

109
00:05:09,773 --> 00:05:12,152
자세히 설명했지만, 이것은 결국 개략적인 

110
00:05:12,152 --> 00:05:12,980
미리보기입니다.

111
00:05:13,680 --> 00:05:16,989
그 후에는 기본적으로 어텐션 블록과 다층 

112
00:05:16,989 --> 00:05:20,730
퍼셉트론 블록 사이를 오가는 과정이 반복되며, 

113
00:05:20,730 --> 00:05:24,615
마지막에는 구절의 모든 본질적인 의미가 시퀀스의 

114
00:05:24,615 --> 00:05:28,500
맨 마지막 벡터에 어떻게든 구워지기를 희망합니다.

115
00:05:28,920 --> 00:05:32,199
그런 다음 마지막 벡터에 대해 특정 연산을 수행하여 

116
00:05:32,199 --> 00:05:35,253
가능한 모든 토큰, 즉 다음에 올 수 있는 모든 

117
00:05:35,253 --> 00:05:38,420
작은 텍스트 덩어리에 대한 확률 분포를 생성합니다.

118
00:05:38,980 --> 00:05:41,553
앞서 말했듯이 텍스트 조각이 주어졌을 때 

119
00:05:41,553 --> 00:05:44,351
다음에 나올 내용을 예측하는 도구가 있다면, 

120
00:05:44,351 --> 00:05:47,149
여기에 약간의 시드 텍스트를 입력하고 다음에 

121
00:05:47,149 --> 00:05:49,722
나올 내용을 예측하고 분포에서 샘플링하고 

122
00:05:49,722 --> 00:05:53,080
추가한 다음 반복해서 반복하는 게임을 할 수 있습니다.

123
00:05:53,640 --> 00:05:56,031
ChatGPT가 등장하기 오래 전, 

124
00:05:56,031 --> 00:05:58,661
GPT-3의 초기 데모는 초기 스니펫을 

125
00:05:58,661 --> 00:06:01,411
기반으로 이야기와 에세이를 자동 완성하는 

126
00:06:01,411 --> 00:06:04,640
기능이었던 것을 기억하시는 분들도 계실 것입니다.

127
00:06:05,580 --> 00:06:08,534
이와 같은 도구를 챗봇으로 만들려면 가장 쉬운 

128
00:06:08,534 --> 00:06:11,601
시작점은 사용자가 유용한 AI 어시스턴트와 상호 

129
00:06:11,601 --> 00:06:14,214
작용하는 설정을 설정하는 약간의 텍스트, 

130
00:06:14,214 --> 00:06:17,055
즉 시스템 프롬프트라고 할 수 있는 텍스트를 

131
00:06:17,055 --> 00:06:20,009
작성한 다음 사용자의 초기 질문이나 프롬프트를 

132
00:06:20,009 --> 00:06:23,077
첫 번째 대화의 일부로 사용한 다음 유용한 AI 

133
00:06:23,077 --> 00:06:26,031
어시스턴트가 응답으로 무엇을 말할지 예측하도록 

134
00:06:26,031 --> 00:06:26,940
하는 것입니다.

135
00:06:27,720 --> 00:06:29,586
이를 잘 작동시키기 위해 필요한 교육 

136
00:06:29,586 --> 00:06:31,896
단계에 대해서는 더 많은 이야기가 필요하지만, 

137
00:06:31,896 --> 00:06:33,940
개략적인 수준에서는 이것이 아이디어입니다.

138
00:06:35,720 --> 00:06:38,168
이 장에서는 네트워크의 맨 처음, 

139
00:06:38,168 --> 00:06:41,647
네트워크의 맨 끝에서 일어나는 일에 대해 자세히 

140
00:06:41,647 --> 00:06:44,868
살펴보고, 트랜스포머가 등장할 무렵에는 모든 

141
00:06:44,868 --> 00:06:48,090
머신 러닝 엔지니어에게 당연시되었을 몇 가지 

142
00:06:48,090 --> 00:06:51,311
중요한 배경 지식을 검토하는 데 많은 시간을 

143
00:06:51,311 --> 00:06:52,600
할애하고자 합니다.

144
00:06:53,060 --> 00:06:56,188
이러한 배경 지식에 익숙하고 조금 조바심이 난다면 

145
00:06:56,188 --> 00:06:59,539
일반적으로 트랜스포머의 핵심이라고 할 수 있는 어텐션 

146
00:06:59,539 --> 00:07:02,780
블록에 초점을 맞춘 다음 장으로 건너뛰셔도 좋습니다.

147
00:07:03,360 --> 00:07:05,782
그 다음에는 이러한 다층 퍼셉트론 블록, 

148
00:07:05,782 --> 00:07:08,520
훈련의 작동 방식 및 지금까지 건너뛰었던 기타 

149
00:07:08,520 --> 00:07:11,680
여러 세부 사항에 대해 더 자세히 이야기하고 싶습니다.

150
00:07:12,180 --> 00:07:15,401
이 동영상은 딥러닝에 대한 미니 시리즈에 추가되는 

151
00:07:15,401 --> 00:07:18,623
것으로, 이전 동영상을 보지 않으셨더라도 괜찮지만 

152
00:07:18,623 --> 00:07:21,500
트랜스포머에 대해 구체적으로 알아보기 전에, 

153
00:07:21,500 --> 00:07:24,492
딥러닝의 기본 전제와 구조에 대해 같은 맥락을 

154
00:07:24,492 --> 00:07:27,829
가지기 위해 이전 영상들을 확인할만한 가치가 있다고 

155
00:07:27,829 --> 00:07:28,520
생각합니다.

156
00:07:29,020 --> 00:07:31,082
너무 뻔한 이야기일 수 있지만, 

157
00:07:31,082 --> 00:07:33,946
이것은 데이터를 사용하여 모델의 작동 방식을 

158
00:07:33,946 --> 00:07:36,925
결정하는 모든 모델을 설명하는 머신 러닝의 한 

159
00:07:36,925 --> 00:07:38,300
가지 접근 방식입니다.

160
00:07:39,140 --> 00:07:42,643
즉, 이미지를 입력받아 이미지를 설명하는 레이블을 

161
00:07:42,643 --> 00:07:45,772
생성하는 함수나 텍스트 구절이 주어지면 다음 

162
00:07:45,772 --> 00:07:49,025
단어를 예측하는 예제 또는 직관 및 패턴 인식 

163
00:07:49,025 --> 00:07:52,780
요소가 필요한 기타 작업을 원한다고 가정해 보겠습니다.

164
00:07:53,200 --> 00:07:55,418
요즘은 거의 당연하게 여기지만, 

165
00:07:55,418 --> 00:07:58,500
머신 러닝의 개념은 초창기 AI에서 사람들이 

166
00:07:58,500 --> 00:08:01,458
했던 것처럼 코드에서 해당 작업을 수행하는 

167
00:08:01,458 --> 00:08:03,799
방법을 명시적으로 정의하는 대신, 

168
00:08:03,799 --> 00:08:07,004
여러 개의 노브와 다이얼처럼 조정 가능한 매개 

169
00:08:07,004 --> 00:08:10,455
변수를 사용하여 매우 유연한 구조를 설정한 다음, 

170
00:08:10,455 --> 00:08:13,660
주어진 입력에 대한 출력이 어떤 모습일지 많은 

171
00:08:13,660 --> 00:08:16,988
예를 사용하여 이러한 동작을 모방하기 위해 매개 

172
00:08:16,988 --> 00:08:19,700
변수 값을 조정하고 조정한다는 점입니다.

173
00:08:19,700 --> 00:08:23,204
예를 들어, 가장 간단한 형태의 머신 러닝은 

174
00:08:23,204 --> 00:08:26,427
선형 회귀로, 입력과 출력은 각각 주택의 

175
00:08:26,427 --> 00:08:29,231
면적과 가격과 같은 단일 숫자이며, 

176
00:08:29,231 --> 00:08:32,595
원하는 것은 이 데이터를 통해 미래의 주택 

177
00:08:32,595 --> 00:08:36,799
가격을 예측하기 위해 가장 적합한 선을 찾는 것입니다.

178
00:08:37,440 --> 00:08:40,719
이 선은 기울기와 y-절편이라는 두 개의 연속 

179
00:08:40,719 --> 00:08:44,250
매개변수로 설명되며, 선형 회귀의 목표는 데이터와 

180
00:08:44,250 --> 00:08:47,529
밀접하게 일치하도록 이러한 매개변수를 결정하는 

181
00:08:47,529 --> 00:08:48,160
것입니다.

182
00:08:48,880 --> 00:08:52,100
말할 필요도 없이 딥러닝 모델은 훨씬 더 복잡해집니다.

183
00:08:52,620 --> 00:08:54,846
예를 들어 GPT-3에는 두 개가 

184
00:08:54,846 --> 00:08:57,660
아니라 1750억 개의 매개변수가 있습니다.

185
00:08:58,120 --> 00:09:01,806
하지만 중요한 사실은, 엄청난 수의 파라미터를 가진 

186
00:09:01,806 --> 00:09:05,365
거대한 모델을 만들면, 훈련 데이터에 과적합되거나 

187
00:09:05,365 --> 00:09:08,924
훈련이 완전히 불가능해지지 않는다는 보장이 없다는 

188
00:09:08,924 --> 00:09:09,560
것입니다.

189
00:09:10,260 --> 00:09:12,983
딥러닝은 지난 수십 년 동안 놀랍도록 잘 

190
00:09:12,983 --> 00:09:16,180
확장되는 것으로 입증된 모델 종류들을 설명합니다.

191
00:09:16,480 --> 00:09:20,217
이들을 통합하는 것은 역전파라고 하는 동일한 

192
00:09:20,217 --> 00:09:23,506
학습 알고리즘이며, 이 학습 알고리즘이 

193
00:09:23,506 --> 00:09:27,094
대규모로 잘 작동하려면 이러한 모델이 특정 

194
00:09:27,094 --> 00:09:31,280
형식을 따라야 한다는 점을 이해해 주시기 바랍니다.

195
00:09:31,800 --> 00:09:34,437
이 형식을 알고 있으면 자의적으로 느껴질 

196
00:09:34,437 --> 00:09:37,189
위험이 있는 트랜스포머의 언어 처리 방식에 

197
00:09:37,189 --> 00:09:40,400
대한 많은 선택 사항을 설명하는 데 도움이 됩니다.

198
00:09:41,440 --> 00:09:43,950
먼저, 어떤 모델을 만들든 입력 

199
00:09:43,950 --> 00:09:46,740
형식은 실수 배열로 지정해야 합니다.

200
00:09:46,740 --> 00:09:49,478
이는 숫자 목록을 의미할 수도 있고, 

201
00:09:49,478 --> 00:09:52,739
2차원 배열일 수도 있으며, 보통은 텐서라고 

202
00:09:52,739 --> 00:09:56,000
부르는 고차원 배열을 다루는 경우가 많습니다.

203
00:09:56,560 --> 00:10:00,289
입력 데이터가 점진적으로 여러 계층으로 변환되고, 

204
00:10:00,289 --> 00:10:03,352
각 계층은 출력으로 간주되는 최종 계층에 

205
00:10:03,352 --> 00:10:06,282
도달할 때까지 항상 일종의 실수 배열로 

206
00:10:06,282 --> 00:10:08,680
구조화된다고 생각할 수 있습니다.

207
00:10:09,280 --> 00:10:11,913
예를 들어, 텍스트 처리 모델의 마지막 

208
00:10:11,913 --> 00:10:14,546
레이어는 가능한 모든 다음 토큰에 대한 

209
00:10:14,546 --> 00:10:17,060
확률 분포를 나타내는 숫자 목록입니다.

210
00:10:17,820 --> 00:10:20,696
딥러닝에서 이러한 모델 매개변수는 거의 항상 

211
00:10:20,696 --> 00:10:23,572
가중치라고 불리며, 이는 이러한 모델이 처리 

212
00:10:23,572 --> 00:10:26,563
중인 데이터와 상호작용하는 유일한 방식이 가중 

213
00:10:26,563 --> 00:10:29,900
합계를 통해서만 이루어지는 것이 핵심이기 때문입니다.

214
00:10:30,340 --> 00:10:32,450
비선형 함수를 곳곳에 뿌리기도 하지만 

215
00:10:32,450 --> 00:10:34,360
이는 매개변수에 의존하지 않습니다.

216
00:10:35,200 --> 00:10:38,575
일반적으로 가중 합계를 이렇게 명시적으로 

217
00:10:38,575 --> 00:10:41,657
적나라하게 표시하는 대신 다양한 구성 

218
00:10:41,657 --> 00:10:45,620
요소를 행렬 벡터 곱셈으로 함께 묶어 표시합니다.

219
00:10:46,740 --> 00:10:50,640
행렬 벡터 곱셈의 작동 방식을 떠올리면 출력의 

220
00:10:50,640 --> 00:10:54,240
각 구성 요소가 가중 합으로 보일 것입니다.

221
00:10:54,780 --> 00:10:58,326
처리 중인 데이터에서 도출된 벡터를 변환하는, 

222
00:10:58,326 --> 00:11:01,600
조정 가능한 매개변수로 채워진 행렬에 대해 

223
00:11:01,600 --> 00:11:05,420
생각하는 것이 개념적으로 더 깔끔할 때가 많습니다.

224
00:11:06,340 --> 00:11:09,032
예를 들어, GPT-3의 1,750억 

225
00:11:09,032 --> 00:11:11,467
개의 가중치는 28,000개가 안 

226
00:11:11,467 --> 00:11:14,160
되는 개별 행렬들로 구성되어 있습니다.

227
00:11:14,660 --> 00:11:17,204
이러한 행렬은 차례로 8가지 범주로 나뉘며, 

228
00:11:17,204 --> 00:11:19,646
여러분과 제가 할 일은 각 범주를 단계별로 

229
00:11:19,646 --> 00:11:22,700
살펴보고 해당 유형이 무엇을 하는지 이해하는 것입니다.

230
00:11:23,160 --> 00:11:25,893
진행하면서 GPT-3의 구체적인 수치를 

231
00:11:25,893 --> 00:11:28,378
참고하여 1,750억 달러의 출처를 

232
00:11:28,378 --> 00:11:31,360
정확히 세어보는 것도 재미있을 것 같습니다.

233
00:11:31,880 --> 00:11:34,571
요즘에는 더 크고 더 좋은 모델이 있지만, 

234
00:11:34,571 --> 00:11:37,375
이 모델은 ML 커뮤니티 외부에서 전 세계의 

235
00:11:37,375 --> 00:11:40,740
관심을 끈 대형 언어 모델로서 확실한 매력이 있습니다.

236
00:11:41,440 --> 00:11:43,290
또한, 실제로 기업들은 최신 네트워크의 

237
00:11:43,290 --> 00:11:45,225
경우 특정 숫자에 대해 훨씬 더 엄격하게 

238
00:11:45,225 --> 00:11:46,740
입 다물고 있는 경향이 있습니다.

239
00:11:47,360 --> 00:11:50,306
ChatGPT와 같은 도구 내부를 

240
00:11:50,306 --> 00:11:53,718
들여다보면 거의 모든 실제 계산이 행렬 

241
00:11:53,718 --> 00:11:57,440
벡터 곱셈처럼 보인다고 설정하고 싶었습니다.

242
00:11:57,900 --> 00:12:01,319
수십억 개의 숫자의 바다에서 길을 잃을 위험이 

243
00:12:01,319 --> 00:12:04,343
약간 있지만, 머릿속에서 항상 파란색이나 

244
00:12:04,343 --> 00:12:07,105
빨간색으로 표시하는 모델의 가중치와, 

245
00:12:07,105 --> 00:12:10,393
항상 회색으로 표시하는 처리 중인 데이터를  

246
00:12:10,393 --> 00:12:11,840
잘 구분해야 합니다.

247
00:12:12,180 --> 00:12:14,246
가중치는 실제 두뇌에 해당하며, 

248
00:12:14,246 --> 00:12:17,231
훈련 중에 학습한 내용을 바탕으로 작동 방식을 

249
00:12:17,231 --> 00:12:17,920
결정합니다.

250
00:12:18,280 --> 00:12:20,840
처리 중인 데이터는 텍스트 스니펫 

251
00:12:20,840 --> 00:12:23,535
예시처럼 간단히 실행을 위해 모델에 

252
00:12:23,535 --> 00:12:26,500
입력되는 특정 입력을 인코드한 것입니다.

253
00:12:27,480 --> 00:12:30,172
이 모든 것을 기초로 하고, 이 텍스트 처리 

254
00:12:30,172 --> 00:12:32,973
예제의 첫 번째 단계인, 입력을 작은 덩어리로 

255
00:12:32,973 --> 00:12:35,558
나누고 그 덩어리를 벡터로 변환하는 과정을 

256
00:12:35,558 --> 00:12:36,420
살펴보겠습니다.

257
00:12:37,020 --> 00:12:39,562
이러한 덩어리를 토큰이라고 하는데, 

258
00:12:39,562 --> 00:12:42,232
단어 조각이나 구두점일 수도 있지만, 

259
00:12:42,232 --> 00:12:45,918
이 장과 특히 다음 장에서는 좀 더 깔끔하게 단어로 

260
00:12:45,918 --> 00:12:48,080
나누었다고 생각하고 싶었습니다.

261
00:12:48,600 --> 00:12:51,495
인간은 말로 생각하기 때문에 작은 예시를 참조하고 

262
00:12:51,495 --> 00:12:54,080
각 단계를 명확히 하는 것이 훨씬 쉬워집니다.

263
00:12:55,260 --> 00:12:57,587
이 모델에는 미리 정의된 어휘, 

264
00:12:57,587 --> 00:13:00,948
가능한 모든 단어 목록(예: 50,000개)이 

265
00:13:00,948 --> 00:13:03,921
있으며, 임베딩 행렬이라고 하는 첫 번째 

266
00:13:03,921 --> 00:13:07,800
행렬에는 이러한 단어 각각에 대한 단일 열이 있습니다.

267
00:13:08,940 --> 00:13:11,189
각 열은 첫 번째 단계에서 각 단어가 

268
00:13:11,189 --> 00:13:13,760
어떤 벡터로 변하는지를 결정하는 요소입니다.

269
00:13:15,100 --> 00:13:17,555
이 행렬에 WE라는 레이블을 붙이고 다른 

270
00:13:17,555 --> 00:13:19,797
모든 행렬과 마찬가지로 값은 무작위로 

271
00:13:19,797 --> 00:13:22,360
시작하여 데이터를 기반으로 학습할 것입니다.

272
00:13:23,620 --> 00:13:26,578
단어를 벡터로 바꾸는 것은 트랜스포머 이전부터 머신 

273
00:13:26,578 --> 00:13:28,618
러닝에서 흔히 사용되던 방식이지만, 

274
00:13:28,618 --> 00:13:31,475
처음 접하는 분들에게는 다소 생소할 수 있고 이후 

275
00:13:31,475 --> 00:13:34,433
모든 작업의 기초가 되는 것이므로 잠시 시간을 내어 

276
00:13:34,433 --> 00:13:35,760
익숙해지도록 하겠습니다.

277
00:13:36,040 --> 00:13:39,006
우리는 흔히 이를 임베딩이라는 단어로 부르는데, 

278
00:13:39,006 --> 00:13:41,422
이는 벡터를 매우 기하학적으로 고차원인 

279
00:13:41,422 --> 00:13:43,620
공간의 점으로 생각하도록 유도합니다.

280
00:13:44,180 --> 00:13:46,480
세 개의 숫자 목록을 3D 공간에서 점의 

281
00:13:46,480 --> 00:13:48,980
좌표로 시각화하는 것은 문제가 되지 않지만, 

282
00:13:48,980 --> 00:13:51,780
단어 임베딩은 훨씬 더 고차원적인 경향이 있습니다.

283
00:13:52,280 --> 00:13:55,777
GPT-3에서는 12,288개의 차원이 있으며, 

284
00:13:55,777 --> 00:13:58,626
보시다시피 여러 방향이 뚜렷한 공간에서 

285
00:13:58,626 --> 00:14:00,440
작업하는 것이 중요합니다.

286
00:14:01,180 --> 00:14:04,313
3D 공간에서 2차원 슬라이스를 가져와 모든 

287
00:14:04,313 --> 00:14:07,571
점을 그 슬라이스에 투영하는 것과 마찬가지로, 

288
00:14:07,571 --> 00:14:11,080
간단한 모델에서 제공하는 단어 임베딩 애니메이션을 

289
00:14:11,080 --> 00:14:14,339
위해 이 고차원 공간을 통해 3차원 슬라이스를 

290
00:14:14,339 --> 00:14:17,597
선택하고 그 위에 단어 벡터를 투영하여 결과를 

291
00:14:17,597 --> 00:14:20,480
표시하는 유사한 작업을 수행하려고 합니다.

292
00:14:21,280 --> 00:14:23,868
여기서 중요한 아이디어는 모델이 학습 중에 

293
00:14:23,868 --> 00:14:26,241
단어가 벡터로 정확히 임베드되는 방식을 

294
00:14:26,241 --> 00:14:29,154
결정하기 위해 가중치를 조정하고 조정함에 따라, 

295
00:14:29,154 --> 00:14:31,635
공간의 방향이 일종의 의미적 의미를 갖는 

296
00:14:31,635 --> 00:14:34,440
임베딩 세트에 정착하는 경향이 있다는 것입니다.

297
00:14:34,980 --> 00:14:38,303
여기서 실행 중인 간단한 단어-벡터 모델의 경우, 

298
00:14:38,303 --> 00:14:41,152
임베딩이 '탑'에 가장 가까운 모든 단어를 

299
00:14:41,152 --> 00:14:43,644
검색해 보면 모두 매우 '탑' 스러운 

300
00:14:43,644 --> 00:14:45,900
느낌을 주는 것을 알 수 있습니다.

301
00:14:46,340 --> 00:14:48,722
집에서 파이썬을 불러와서 따라 해보고 싶다면, 

302
00:14:48,722 --> 00:14:51,380
여기 애니메이션을 만드는 데 사용한 모델이 있습니다.

303
00:14:51,620 --> 00:14:54,506
트랜스포머는 아니지만 공간의 방향이 의미적 의미를 

304
00:14:54,506 --> 00:14:57,600
전달할 수 있다는 아이디어를 설명하는 데는 충분합니다.

305
00:14:58,300 --> 00:15:03,146
아주 고전적인 예로 여성과 남성의 벡터의 차이를 

306
00:15:03,146 --> 00:15:07,814
한쪽 끝과 다른 쪽 끝을 연결하는 작은 벡터로 

307
00:15:07,814 --> 00:15:13,200
시각화하면, 이는 왕과 여왕의 차이와 매우 유사합니다.

308
00:15:15,080 --> 00:15:17,870
예를 들어 여성 군주에 대한 단어를 모른다고 

309
00:15:17,870 --> 00:15:20,102
가정하면, 왕(king)을 가져와서 

310
00:15:20,102 --> 00:15:22,781
여성-남성 방향을 추가하고 그 지점에 가장 

311
00:15:22,781 --> 00:15:25,460
가까운 임베딩을 검색하면 찾을 수 있습니다.

312
00:15:27,000 --> 00:15:28,200
적어도 그런 식이죠.

313
00:15:28,480 --> 00:15:31,213
이것이 제가 사용하는 모델의 전형적인 예시이긴 

314
00:15:31,213 --> 00:15:34,262
하지만, 실제 여왕의 임베딩은 이보다 조금 더 멀리 

315
00:15:34,262 --> 00:15:36,995
떨어져 있는데, 아마도 훈련 데이터에서 여왕이 

316
00:15:36,995 --> 00:15:39,833
사용되는 방식이 단순히 왕의 여성 버전이 아니기 

317
00:15:39,833 --> 00:15:40,780
때문일 것입니다.

318
00:15:41,620 --> 00:15:43,390
가족 관계는 이 아이디어를 훨씬 

319
00:15:43,390 --> 00:15:45,260
더 잘 설명해 주는 것 같았습니다.

320
00:15:46,340 --> 00:15:49,295
요점은, 모델이 학습 과정에서 이 공간의 한 방향이 

321
00:15:49,295 --> 00:15:52,046
성별 정보를 인코딩하도록 임베딩을 선택하는 것이 

322
00:15:52,046 --> 00:15:54,900
유리하다는 것을 알게 된 것으로 보인다는 것입니다.

323
00:15:56,800 --> 00:16:00,273
또 다른 예로 이탈리아의 임베딩에서 독일의 

324
00:16:00,273 --> 00:16:04,326
임베딩을 빼고 히틀러의 임베딩을 더하면 무솔리니의 

325
00:16:04,326 --> 00:16:08,090
임베딩에 매우 근접한 결과를 얻을 수 있습니다.

326
00:16:08,570 --> 00:16:10,904
마치 모델이 어떤 방향은 이탈리아스러움과, 

327
00:16:10,904 --> 00:16:13,141
어떤 방향은 제2차 세계대전의 축을 이끈 

328
00:16:13,141 --> 00:16:15,670
지도자와 연관 짓는 법을 배운 것처럼 말입니다.

329
00:16:16,470 --> 00:16:19,723
이런 맥락에서 제가 가장 좋아하는 예는 

330
00:16:19,723 --> 00:16:22,828
독일과 일본의 차이를 초밥에 적용하면 

331
00:16:22,828 --> 00:16:26,230
브랫부르스트에 매우 가까워진다는 점입니다.

332
00:16:27,350 --> 00:16:29,551
또한 가장 가까운 이웃을 찾는 놀이를 

333
00:16:29,551 --> 00:16:31,648
하면서 고양이가 짐승과 괴물 모두에 

334
00:16:31,648 --> 00:16:33,850
얼마나 가까운지 알게 되어 기뻤습니다.

335
00:16:34,690 --> 00:16:37,079
특히 다음 장에서 염두에 두면 도움이 되는 

336
00:16:37,079 --> 00:16:39,469
수학적 직관 하나는, 두 벡터의 도트 곱은 

337
00:16:39,469 --> 00:16:41,858
그 벡터들이 얼마나 잘 정렬되는지 측정하는 

338
00:16:41,858 --> 00:16:43,850
방법으로 생각할 수 있다는 것입니다.

339
00:16:44,870 --> 00:16:46,983
계산적으로 도트 곱은 모든 해당 구성 

340
00:16:46,983 --> 00:16:49,298
요소를 곱한 다음 결과를 더하는 방식으로 

341
00:16:49,298 --> 00:16:51,713
이루어지는데, 이는 우리의 대부분의 계산이 

342
00:16:51,713 --> 00:16:54,330
가중 합처럼 보여야 하기 때문에 좋은 일입니다.

343
00:16:55,190 --> 00:16:58,719
기하학적으로 도트 곱은 벡터가 비슷한 

344
00:16:58,719 --> 00:17:02,248
방향을 가리키면 양수, 수직이면 0, 

345
00:17:02,248 --> 00:17:05,609
반대 방향을 가리키면 음수가 됩니다.

346
00:17:06,550 --> 00:17:10,522
예를 들어, 이 모델을 가지고 놀면서 '고양이들'에서 

347
00:17:10,522 --> 00:17:13,699
'고양이'를 뺀 결과가 이 공간에서 일종의 

348
00:17:13,699 --> 00:17:17,010
복수형의 방향을 나타낸다고 가정해 보겠습니다.

349
00:17:17,430 --> 00:17:20,751
이를 테스트하기 위해 이 벡터를 가지고 특정 단수형 

350
00:17:20,751 --> 00:17:23,728
명사의 임베딩에 대한 도트 곱을 계산한 다음, 

351
00:17:23,728 --> 00:17:27,050
해당 명사의 복수형과의 도트 곱과 비교해 보겠습니다.

352
00:17:27,270 --> 00:17:30,272
이 기능을 사용해 보면 복수형이 단수형보다 일관되게 

353
00:17:30,272 --> 00:17:32,860
더 높은 값을 제공하는 것을 알 수 있으며, 

354
00:17:32,860 --> 00:17:35,448
이는 복수형이 이 방향에 더 부합한다는 것을 

355
00:17:35,448 --> 00:17:36,070
나타냅니다.

356
00:17:37,070 --> 00:17:40,832
1, 2, 3 등의 단어와 도트 곱을 취하면 점차 

357
00:17:40,832 --> 00:17:44,595
값이 증가하기 때문에 모델이 주어진 단어를 얼마나 

358
00:17:44,595 --> 00:17:48,089
복수로 찾는지 정량적으로 측정할 수 있는 것도 

359
00:17:48,089 --> 00:17:49,030
재미있습니다.

360
00:17:50,250 --> 00:17:51,982
다시 말하지만, 단어가 어떻게 임베딩되는지 

361
00:17:51,982 --> 00:17:53,570
세부사항은 데이터를 사용하여 학습합니다.

362
00:17:54,050 --> 00:17:56,846
각 단어에 어떤 일이 일어나는지 알려주는 열로 구성된 

363
00:17:56,846 --> 00:17:59,550
이 임베딩 행렬은 모델의 첫 번째 가중치 더미입니다.

364
00:18:00,030 --> 00:18:03,146
GPT-3의 숫자를 가져오면 어휘의 크기는 

365
00:18:03,146 --> 00:18:05,354
구체적으로 50,257개이며, 

366
00:18:05,354 --> 00:18:08,471
엄밀히 말하면 단어 자체가 아니라 토큰으로 

367
00:18:08,471 --> 00:18:09,770
구성되어 있습니다.

368
00:18:10,630 --> 00:18:12,789
임베딩 차원은 12,288개이며, 

369
00:18:12,789 --> 00:18:15,176
이를 곱하면 약 6억 1700만 개의 

370
00:18:15,176 --> 00:18:17,790
가중치로 구성되어 있음을 알 수 있습니다.

371
00:18:18,250 --> 00:18:20,138
이제 이것을 집계에 추가하면서, 

372
00:18:20,138 --> 00:18:22,865
마지막에는 1,750억 개로 계산되어야 한다는 

373
00:18:22,865 --> 00:18:23,810
점을 기억합시다.

374
00:18:25,430 --> 00:18:27,736
트랜스포머의 경우, 이 임베딩 공간에 

375
00:18:27,736 --> 00:18:29,713
있는 벡터가 단순히 개별 단어를 

376
00:18:29,713 --> 00:18:32,130
나타내는 것이 아니라고 생각하면 됩니다.

377
00:18:32,550 --> 00:18:36,092
나중에 설명하겠지만 단어의 위치에 대한 정보도 

378
00:18:36,092 --> 00:18:39,227
인코딩하며, 더 중요한 것은 이 벡터들에 

379
00:18:39,227 --> 00:18:42,770
문맥을 흡수하는 능력이 있다고 생각해야 합니다.

380
00:18:43,350 --> 00:18:46,953
예를 들어, 왕이라는 단어를 임베딩하는 것으로 시작한 

381
00:18:46,953 --> 00:18:50,436
벡터는 이 네트워크의 다양한 블록에 의해 점차적으로 

382
00:18:50,436 --> 00:18:53,919
밀고 당겨져서 마지막에는 훨씬 더 구체적이고 미묘한 

383
00:18:53,919 --> 00:18:57,162
방향을 가리키면서 스코틀랜드에 살았고 전임 왕을 

384
00:18:57,162 --> 00:19:00,526
살해한 후 그 자리에 오른 왕이었으며 셰익스피어의 

385
00:19:00,526 --> 00:19:03,889
언어로 묘사되고 있다는 것을 어떻게든 인코딩할 수 

386
00:19:03,889 --> 00:19:04,730
있게 됩니다.

387
00:19:05,210 --> 00:19:07,790
여러분이 주어진 단어들을 이해하는 법을 생각해 보세요.

388
00:19:08,250 --> 00:19:11,835
그 단어의 의미는 주변에 의해 명확하게 알려지며 

389
00:19:11,835 --> 00:19:14,624
때로는 먼 거리의 문맥도 포함되므로, 

390
00:19:14,624 --> 00:19:18,343
다음에 나올 단어를 예측할 수 있는 모델을 구성할 

391
00:19:18,343 --> 00:19:21,929
때 목표는 어떻게든 문맥을 효율적으로 통합할 수 

392
00:19:21,929 --> 00:19:23,390
있게 하는 것입니다.

393
00:19:24,050 --> 00:19:27,397
명확하게 말하면, 첫 단계에서 입력 텍스트를 기반으로 

394
00:19:27,397 --> 00:19:30,521
벡터 배열을 만들 때는 각 벡터를 임베딩 행렬에서 

395
00:19:30,521 --> 00:19:33,534
단순히 뽑아내기 때문에 처음에는 각 벡터가 주변 

396
00:19:33,534 --> 00:19:36,770
입력 없이 단일 단어의 의미만 인코딩할 수 있습니다.

397
00:19:37,710 --> 00:19:40,525
하지만 이 네트워크의 주요 목표는 각각의 

398
00:19:40,525 --> 00:19:43,462
벡터가 단순한 개별 단어가 나타내는 것보다 

399
00:19:43,462 --> 00:19:46,155
훨씬 더 풍부하고 구체적인 의미를 담을 

400
00:19:46,155 --> 00:19:48,970
수 있도록 하는 것이라고 생각해야 합니다.

401
00:19:49,510 --> 00:19:51,840
네트워크는 컨텍스트 크기로 알려진 고정된 

402
00:19:51,840 --> 00:19:54,170
수의 벡터만 한 번에 처리할 수 있습니다.

403
00:19:54,510 --> 00:19:57,273
GPT-3의 경우 컨텍스트 크기가 2048로 

404
00:19:57,273 --> 00:19:59,594
학습되었으므로 네트워크를 통해 흐르는 

405
00:19:59,594 --> 00:20:02,467
데이터는 항상 2048 열의 배열처럼 보이며, 

406
00:20:02,467 --> 00:20:05,010
각 열은 12,000개의 차원을 가집니다.

407
00:20:05,590 --> 00:20:08,478
이 컨텍스트 크기는 트랜스포머가 다음 단어를 

408
00:20:08,478 --> 00:20:11,830
예측할 때 포함할 수 있는 텍스트의 양을 제한합니다.

409
00:20:12,370 --> 00:20:14,738
이 때문에 ChatGPT의 초기 버전처럼 

410
00:20:14,738 --> 00:20:16,901
특정 챗봇과 긴 대화를 이어가다 보면 

411
00:20:16,901 --> 00:20:19,269
대화가 너무 길어지면서 봇이 대화의 끈을 

412
00:20:19,269 --> 00:20:22,050
놓아버린다는 느낌을 주는 경우가 종종 있었습니다.

413
00:20:23,030 --> 00:20:24,755
어텐션에 대한 자세한 내용은 추후에 

414
00:20:24,755 --> 00:20:26,567
설명할 예정이지만, 우선 마지막 단계 

415
00:20:26,567 --> 00:20:28,810
어떤 일이 일어나는지 잠시 이야기하고 싶습니다.

416
00:20:29,450 --> 00:20:32,102
원하는 출력은 다음에 나올 수 있는 모든 

417
00:20:32,102 --> 00:20:34,870
토큰에 대한 확률 분포라는 점을 기억하세요.

418
00:20:35,170 --> 00:20:38,425
예를 들어, 맨 마지막 단어가 교수이고 문맥에 

419
00:20:38,425 --> 00:20:41,806
해리포터와 같은 단어가 포함되어 있고 바로 앞에 

420
00:20:41,806 --> 00:20:45,061
가장 싫어하는 선생님이 표시되며 토큰이 단순히 

421
00:20:45,061 --> 00:20:48,442
완전한 단어처럼 보이도록 약간의 여유를 준다면, 

422
00:20:48,442 --> 00:20:51,948
해리포터에 대한 지식을 쌓은 잘 훈련된 네트워크는 

423
00:20:51,948 --> 00:20:55,203
아마도 스네이프라는 단어에 높은 숫자를 할당할 

424
00:20:55,203 --> 00:20:55,830
것입니다.

425
00:20:56,510 --> 00:20:57,970
여기에는 두 가지 단계가 포함됩니다.

426
00:20:58,310 --> 00:21:01,180
첫 번째는 해당 컨텍스트의 맨 마지막 벡터를 

427
00:21:01,180 --> 00:21:04,280
어휘의 각 토큰에 대해 하나씩 매핑하는 또 다른 

428
00:21:04,280 --> 00:21:07,610
50,000개의 값을 가진 행렬을 사용하는 것입니다.

429
00:21:08,170 --> 00:21:11,176
그런 다음 이를 확률 분포로 정규화하는 함수가 

430
00:21:11,176 --> 00:21:14,298
있는데, 이를 Softmax라고 하며 잠시 후에 

431
00:21:14,298 --> 00:21:17,304
자세히 설명하겠습니다. 하지만 그 전에 마지막 

432
00:21:17,304 --> 00:21:20,542
임베딩만 사용하여 예측을 하는 것이 조금 이상하게 

433
00:21:20,542 --> 00:21:23,780
보일 수 있는데, 마지막 단계에서 레이어에는 수천 

434
00:21:23,780 --> 00:21:27,249
개의 다른 벡터가 각자의 맥락이 풍부한 의미를 가지고 

435
00:21:27,249 --> 00:21:28,290
있기 때문입니다.

436
00:21:28,930 --> 00:21:32,757
이는 학습 과정에서 최종 레이어 있는 각 벡터를 

437
00:21:32,757 --> 00:21:36,301
사용하여 그 직후에 올 것을 동시에 예측하는 

438
00:21:36,301 --> 00:21:40,270
것이 훨씬 더 효율적이라는 사실과 관련이 있습니다.

439
00:21:40,970 --> 00:21:43,030
나중에 학습에 대해 더 자세히 설명할 것이 

440
00:21:43,030 --> 00:21:45,090
많지만 지금은 이 부분만 언급하고 싶습니다.

441
00:21:45,730 --> 00:21:47,763
이 행렬을 임베딩 해제 행렬이라고 

442
00:21:47,763 --> 00:21:49,690
하며 WU라는 레이블을 붙입니다.

443
00:21:50,210 --> 00:21:52,196
다시 말하지만, 우리가 보는 모든 가중치 

444
00:21:52,196 --> 00:21:54,182
행렬과 마찬가지로, 그 항목들은 무작위로 

445
00:21:54,182 --> 00:21:55,910
시작하지만 학습 과정에서 학습됩니다.

446
00:21:56,470 --> 00:21:58,460
총 매개변수 집계에 추가해보면, 

447
00:21:58,460 --> 00:22:01,557
이 임베딩 해제 행렬은 어휘의 각 단어마다 하나의 

448
00:22:01,557 --> 00:22:04,654
행을 가지며, 각 행은 임베딩 차원과 동일한 수의 

449
00:22:04,654 --> 00:22:05,650
요소를 가집니다.

450
00:22:06,410 --> 00:22:09,116
순서가 바뀌었을 뿐 임베딩 행렬과 매우 

451
00:22:09,116 --> 00:22:12,192
유사하므로 네트워크에 6억 1,700만 개의 

452
00:22:12,192 --> 00:22:15,391
매개변수가 추가되며, 즉 지금까지 집계된 수는 

453
00:22:15,391 --> 00:22:18,837
10억 개가 조금 넘습니다. 이는 총 1,750억 

454
00:22:18,837 --> 00:22:21,790
개에 비하면 작지만 미미한 일부는 아닙니다.

455
00:22:22,550 --> 00:22:25,140
이 장의 마지막 미니 레슨으로 소프트맥스 기능에 

456
00:22:25,140 --> 00:22:27,155
대해 더 자세히 이야기하려고 하는데, 

457
00:22:27,155 --> 00:22:30,034
어텐션 블록에 들어가면 이 기능이 또 한 번 등장하기 

458
00:22:30,034 --> 00:22:30,610
때문입니다.

459
00:22:31,430 --> 00:22:35,514
일련의 숫자가 확률 분포, 즉 가능한 모든 다음 

460
00:22:35,514 --> 00:22:39,749
단어에 대한 분포로 작동하려면 각 값이 0에서 1 

461
00:22:39,749 --> 00:22:43,984
사이여야 하며, 모든 값을 더했을 때 1이 되어야 

462
00:22:43,984 --> 00:22:44,590
합니다.

463
00:22:45,250 --> 00:22:50,113
그러나 행렬-벡터 곱셈같은 학습 놀이를 하다 보면, 

464
00:22:50,113 --> 00:22:54,810
기본적으로 얻는 출력은 이를 전혀 따르지 않습니다.

465
00:22:55,330 --> 00:22:57,883
값은 음수이거나 1보다 훨씬 큰 경우가 많으며, 

466
00:22:57,883 --> 00:22:59,870
거의 확실하게 1로 합산되지 않습니다.

467
00:23:00,510 --> 00:23:03,928
소프트맥스는 임의의 숫자 목록을 가장 큰 값은 

468
00:23:03,928 --> 00:23:07,346
1에 가장 가깝고 작은 값은 0에 매우 가깝게 

469
00:23:07,346 --> 00:23:11,290
되는 방식으로 유효한 분포로 변환하는 표준 방법입니다.

470
00:23:11,830 --> 00:23:13,070
이 정도만 알아두면 됩니다.

471
00:23:13,090 --> 00:23:16,715
궁금하신 분들을 위해, 작동 방법을 알려드리자면 

472
00:23:16,715 --> 00:23:19,803
먼저 각 숫자를 e의 거듭제곱으로 올리면 

473
00:23:19,803 --> 00:23:22,891
양수 값의 목록이 생기고, 그런 다음 그 

474
00:23:22,891 --> 00:23:25,844
모든 양수 값의 합을 구해 각 항을 그 

475
00:23:25,844 --> 00:23:29,470
합으로 나누면 총합이 1인 목록으로 정규화됩니다.

476
00:23:30,170 --> 00:23:33,996
입력의 숫자 중 하나가 나머지 숫자보다 의미 있게 

477
00:23:33,996 --> 00:23:37,960
크면 출력에서 해당 항이 분포를 지배하므로 샘플링을 

478
00:23:37,960 --> 00:23:41,786
한다면 거의 확실하게 최대값의 입력을 선택하게 될 

479
00:23:41,786 --> 00:23:42,470
것입니다.

480
00:23:42,990 --> 00:23:45,991
하지만 다른 값도 비슷하게 크면 분포에서 의미 

481
00:23:45,991 --> 00:23:48,646
있는 가중치를 가지며, 입력을 연속적으로 

482
00:23:48,646 --> 00:23:51,302
변경하면 모든 것이 연속적으로 변화한다는 

483
00:23:51,302 --> 00:23:54,650
점에서 단순히 최대값을 선택하는 것보다 부드럽습니다.

484
00:23:55,130 --> 00:23:58,707
ChatGPT가 이 분포를 사용하여 다음 단어를 

485
00:23:58,707 --> 00:24:02,285
생성하는 경우와 같이 일부 상황에서는 이 함수의 

486
00:24:02,285 --> 00:24:05,862
지수부 분모에 상수 t를 넣는다는 약간의 양념을 

487
00:24:05,862 --> 00:24:08,910
추가하여 약간의 재미를 더할 수 있습니다.

488
00:24:09,550 --> 00:24:13,401
이것을 온도라고 부릅니다. 특정한 열역학 방정식에서 

489
00:24:13,401 --> 00:24:16,853
온도의 역할과 대략적으로 유사하기 때문입니다. 

490
00:24:16,853 --> 00:24:20,572
그 효과는 t가 크면 낮은 값에 더 많은 가중치를 

491
00:24:20,572 --> 00:24:23,361
부여하여 분포가 조금 더 균일해지고, 

492
00:24:23,361 --> 00:24:27,212
t가 작으면 큰 값이 더 공격적으로 지배하게 되며, 

493
00:24:27,212 --> 00:24:30,665
극단적으로 t를 0으로 설정하면 모든 가중치가 

494
00:24:30,665 --> 00:24:32,790
최대값으로 이동하는 것입니다.

495
00:24:33,470 --> 00:24:36,588
예를 들어, GPT-3가 '옛날 옛적에 A가 

496
00:24:36,588 --> 00:24:39,831
있었다'라는 시드 텍스트로 스토리를 생성하도록 

497
00:24:39,831 --> 00:24:42,950
하되 각 사례마다 다른 온도를 사용하겠습니다.

498
00:24:43,630 --> 00:24:48,156
온도 0은 항상 가장 예상 가능한 단어만을 사용하기 

499
00:24:48,156 --> 00:24:52,370
때문에 결국 골디락스의 진부한 파생으로 끝납니다.

500
00:24:53,010 --> 00:24:55,510
온도가 높을수록 가능성이 낮은 단어를 선택할 

501
00:24:55,510 --> 00:24:57,910
확률이 높아지지만, 그만큼 위험이 따릅니다.

502
00:24:58,230 --> 00:25:02,258
이 경우, 한국의 젊은 웹 아티스트에 대한 이야기로 

503
00:25:02,258 --> 00:25:06,010
시작하지만 곧 말도 안 되는 이야기로 변질됩니다.

504
00:25:06,950 --> 00:25:08,978
기술적으로 말하자면, API에서는 실제로 

505
00:25:08,978 --> 00:25:10,830
2보다 큰 온도를 선택할 수 없습니다.

506
00:25:11,170 --> 00:25:13,896
여기에는 수학적 이유가 있는 것이 아니라 

507
00:25:13,896 --> 00:25:16,623
도구가 너무 무의미한 것을 생성하는 것을 

508
00:25:16,623 --> 00:25:19,350
막기 위해 임의로 설정한 제약 조건입니다.

509
00:25:19,870 --> 00:25:23,317
이 애니메이션이 실제로 작동하는 방식이 궁금하시다면, 

510
00:25:23,317 --> 00:25:26,420
GPT-3가 생성하는 다음 토큰 중 가장 확률이 

511
00:25:26,420 --> 00:25:29,407
높은 20개(가져올 수 있는 최대치)의 토큰을 

512
00:25:29,407 --> 00:25:32,165
가져온 다음 지수 1/5를 기준으로 확률을 

513
00:25:32,165 --> 00:25:32,970
조정했습니다.

514
00:25:33,130 --> 00:25:36,230
또 다른 전문 용어로, 이 함수의 출력 구성 

515
00:25:36,230 --> 00:25:39,206
요소를 확률이라고 부르는 것과 마찬가지로, 

516
00:25:39,206 --> 00:25:42,554
입력은 보통 로짓이라고 부릅니다. 누구는 로짓, 

517
00:25:42,554 --> 00:25:46,150
누구는 로깃이라고 하는데, 저는 로짓이라고 부릅니다.

518
00:25:46,530 --> 00:25:49,620
예를 들어, 어떤 텍스트를 입력하면 네트워크에 

519
00:25:49,620 --> 00:25:51,879
모든 단어 임베딩이 흐르게 되고, 

520
00:25:51,879 --> 00:25:54,732
임베딩 해제 행렬로 최종 곱셈을 수행하면, 

521
00:25:54,732 --> 00:25:57,704
머신러닝을 하는 사람은 정규화되지 않은 원시 

522
00:25:57,704 --> 00:26:00,914
출력을 다음 단어 예측을 위한 로짓으로 참조하게 

523
00:26:00,914 --> 00:26:01,390
됩니다.

524
00:26:03,330 --> 00:26:05,750
이 장의 많은 목표는 어텐션 메커니즘을 

525
00:26:05,750 --> 00:26:08,500
이해하기 위한 기초를 마련하는 것이었습니다. 

526
00:26:08,500 --> 00:26:10,370
자세한 설명 없이 기본적으로요.

527
00:26:10,850 --> 00:26:14,863
단어 임베딩, 소프트맥스, 도트 곱셈으로 유사성을 

528
00:26:14,863 --> 00:26:19,021
측정하는 방법, 그리고 대부분의 계산이 조정 가능한 

529
00:26:19,021 --> 00:26:23,035
매개변수로 가득 찬 행렬의 곱셈처럼 보여야 한다는 

530
00:26:23,035 --> 00:26:25,758
기본 전제에 대한 직관이 있다면, 

531
00:26:25,758 --> 00:26:29,772
최근 AI 붐의 초석인 어텐션 메커니즘을 이해하는 

532
00:26:29,772 --> 00:26:32,210
것은 비교적 순조로울 것입니다.

533
00:26:32,650 --> 00:26:34,510
이를 위해 다음 장에서 저와 함께하세요.

534
00:26:36,390 --> 00:26:38,390
이 글을 게시하면서 다음 챕터의 초안을 

535
00:26:38,390 --> 00:26:40,664
Patreon 후원자분들이 검토할 수 있도록 

536
00:26:40,664 --> 00:26:41,210
공개합니다.

537
00:26:41,770 --> 00:26:44,028
최종 버전은 1~2주 후에 공개될 예정이며, 

538
00:26:44,028 --> 00:26:45,924
보통 그 검토를 바탕으로 얼마나 많이 

539
00:26:45,924 --> 00:26:47,370
수정하느냐에 따라 달라집니다.

540
00:26:47,810 --> 00:26:49,197
그동안 관심을 갖고 싶거나 채널에 

541
00:26:49,197 --> 00:26:50,657
조금이나마 도움을 주고 싶으시다면, 

542
00:26:50,657 --> 00:26:52,410
바로 이 채널이 여러분을 기다리고 있습니다.

