[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "GPT란 생성형 사전 훈련 트랜스포머의 약자입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "첫 단어는 간단히, 이것이 새 텍스트를 생성하는 봇이라는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "사전 학습은 모델이 방대한 양의 데이터로부터 학습하는 과정을 거쳤음을 의미하며, 이 접두사는 추가 학습을 통해 특정 작업에 대해 미세 조정할 수 있는 여지가 더 많다는 것을 암시합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "하지만 마지막 단어가 진짜 핵심입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "트랜스포머는 머신러닝 모델인 신경망의 일종으로, 현재 AI 붐의 근간이 되는 핵심 발명품입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "이 동영상과 다음 장에서 제가 하고자 하는 것은 트랜스포머 내부에서 실제로 어떤 일이 일어나는지 시각적으로 설명하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "트랜스포머에 흘러가는 데이터를 따라 단계별로 진행하겠습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "트랜스포머를 사용하여 만들 수 있는 모델의 종류는 매우 다양합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "일부 모델은 오디오를 받아 녹취록을 생성합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "이 문장은 텍스트만으로 합성 음성을 생성하는, 반대 방향의 모델에서 나온 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "텍스트 설명을 받아 이미지를 생성하는 돌리와 미드저니처럼 2022년에 전 세계를 강타한 도구는 모두 트랜스포머를 기반으로 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "파이 생물이 무엇인지 이해하지 못한다고는 해도 이런 일이 원격으로도 가능하다는 사실에 놀라움을 금할 수 없습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "그리고 2017년에 Google이 도입한 최초의 트랜스포머는 한 언어에서 다른 언어로 텍스트를 번역하는 특정 사용 사례를 위해 개발되었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "하지만 여러분과 제가 집중할 변형 모델은 ChatGPT와 같은 도구의 기반이 되는 유형으로, 텍스트 조각을 (어쩌면 주변 이미지나 소리도 함께) 받아 그 구절에서 다음에 나올 내용을 예측하도록 학습된 모델입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "이러한 예측은 다음에 이어질 수 있는 다양한 텍스트 청크에 대한 확률 분포의 형태를 취합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "언뜻 보기에 다음 단어를 예측하는 것은 새로운 텍스트를 생성하는 것과는 매우 다른 목표처럼 느껴질 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "하지만 이와 같은 예측 모델이 있으면 더 긴 텍스트를 생성할 때 작업할 초기 스니펫을 제공하고, 방금 생성한 분포에서 무작위 샘플을 가져와 텍스트에 해당 샘플을 추가한 다음 전체 프로세스를 다시 실행하여 방금 추가된 내용을 포함한 모든 새 텍스트를 기반으로 새로운 예측을 수행하면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "여러분은 어떤지 모르겠지만, 이런 방식이 실제로 작동할 것 같이 느껴지지는 않습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "예를 들어, 이 애니메이션에서는 노트북에서 GPT-2를 실행하고 시드 텍스트를 기반으로 스토리를 생성하기 위해 다음 텍스트 덩어리를 반복적으로 예측하고 샘플링하도록 하고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "이 이야기는 그다지 말이 되지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "하지만 기본 모델은 같지만 훨씬 더 큰 GPT-3에 대한 API 호출로 바꾸면 갑자기 거의 마술처럼 파이라는 생물이 수학과 계산의 땅에 살고 있을 것이라는 추론까지 가능한 합리적인 이야기가 만들어집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "여기서 반복되는 예측 및 샘플링 프로세스는 여러분이 ChatGPT 또는 다른 대규모 언어 모델과 상호 작용할 때 한 번에 한 단어씩 생성되는 것을 볼 때 본질적으로 일어나는 일입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "실제로 제가 매우 좋아하는 기능 중 하나는 새로운 단어를 선택할 때마다 그 기본 분포를 볼 수 있는 기능입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "트랜스포머를 통해 데이터가 어떻게 흘러가는지 아주 높은 수준의 미리보기로 시작하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "각 단계의 세부 사항에 대한 동기를 부여하고 해석하고 확장하는 데 훨씬 더 많은 시간을 할애하겠지만, 크게 보면 이러한 챗봇 중 하나가 특정 단어를 생성할 때 내부에서 어떤 일이 일어나는지는 다음과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "먼저, 입력이 여러 개의 작은 조각으로 나뉩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "이러한 조각을 토큰이라고 하며, 텍스트의 경우 단어 또는 단어의 작은 조각 또는 기타 일반적인 문자 조합입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "이미지나 소리의 경우 토큰은 해당 이미지의 작은 조각이나 사운드의 작은 덩어리가 될 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "이러한 각 토큰은 벡터, 즉 해당 토큰의 의미를 어떻게든 인코딩하기 위한 숫자 리스트와 연관됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "이러한 벡터를 어떠한 매우 고차원적인 공간에 좌표를 제공한다고 생각하면, 비슷한 의미를 가진 단어는 해당 공간에서 서로 가까운 벡터에 위치하는 경향이 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "이 일련의 벡터는 어텐션 블록이라고 하는 연산을 통과하며, 이를 통해 벡터가 서로 대화하고 정보를 주고받으며 값을 업데이트할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "예를 들어, 머신러닝 모델이라는 문구에서 모델이라는 단어의 의미는 패션 모델이라는 문구에서 모델이라는 단어의 의미와 다릅니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "어텐션 블록은 문맥에서 어떤 단어가 다른 단어의 의미 업데이트와 관련이 있는지, 그리고 그 의미를 정확히 어떻게 업데이트해야 하는지 파악하는 역할을 합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "그리고 다시 말하지만, 제가 의미라는 단어를 사용할 때마다 이것은 어떻게든 해당 벡터의 항목에 완전히 인코딩됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "그 후 이러한 벡터는 다른 종류의 연산을 거치게 되며, 읽는 소스에 따라 이를 다층 퍼셉트론 또는 피드 포워드 레이어라고 부를 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "여기서 벡터는 서로 대화하지 않고 모두 동일한 연산을 병렬로 진행합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "이 블록은 해석하기가 조금 어렵지만, 나중에 각 벡터에 대해 많은 질문들을 한 뒤 그 질문에 대한 답을 바탕으로 업데이트하는 단계에 대해 설명하겠습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "이 두 블록의 모든 연산은 행렬 곱셈의 거대한 더미처럼 보이며, 우리의 주요 임무는 기본 행렬을 읽는 방법을 이해하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "그 사이에 일어나는 몇 가지 정규화 단계에 대해 자세히 설명했지만, 이것은 결국 개략적인 미리보기입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "그 후에는 기본적으로 어텐션 블록과 다층 퍼셉트론 블록 사이를 오가는 과정이 반복되며, 마지막에는 구절의 모든 본질적인 의미가 시퀀스의 맨 마지막 벡터에 어떻게든 구워지기를 희망합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "그런 다음 마지막 벡터에 대해 특정 연산을 수행하여 가능한 모든 토큰, 즉 다음에 올 수 있는 모든 작은 텍스트 덩어리에 대한 확률 분포를 생성합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "앞서 말했듯이 텍스트 조각이 주어졌을 때 다음에 나올 내용을 예측하는 도구가 있다면, 여기에 약간의 시드 텍스트를 입력하고 다음에 나올 내용을 예측하고 분포에서 샘플링하고 추가한 다음 반복해서 반복하는 게임을 할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "ChatGPT가 등장하기 오래 전, GPT-3의 초기 데모는 초기 스니펫을 기반으로 이야기와 에세이를 자동 완성하는 기능이었던 것을 기억하시는 분들도 계실 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "이와 같은 도구를 챗봇으로 만들려면 가장 쉬운 시작점은 사용자가 유용한 AI 어시스턴트와 상호 작용하는 설정을 설정하는 약간의 텍스트, 즉 시스템 프롬프트라고 할 수 있는 텍스트를 작성한 다음 사용자의 초기 질문이나 프롬프트를 첫 번째 대화의 일부로 사용한 다음 유용한 AI 어시스턴트가 응답으로 무엇을 말할지 예측하도록 하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "이를 잘 작동시키기 위해 필요한 교육 단계에 대해서는 더 많은 이야기가 필요하지만, 개략적인 수준에서는 이것이 아이디어입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "이 장에서는 네트워크의 맨 처음, 네트워크의 맨 끝에서 일어나는 일에 대해 자세히 살펴보고, 트랜스포머가 등장할 무렵에는 모든 머신 러닝 엔지니어에게 당연시되었을 몇 가지 중요한 배경 지식을 검토하는 데 많은 시간을 할애하고자 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "이러한 배경 지식에 익숙하고 조금 조바심이 난다면 일반적으로 트랜스포머의 핵심이라고 할 수 있는 어텐션 블록에 초점을 맞춘 다음 장으로 건너뛰셔도 좋습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "그 다음에는 이러한 다층 퍼셉트론 블록, 훈련의 작동 방식 및 지금까지 건너뛰었던 기타 여러 세부 사항에 대해 더 자세히 이야기하고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "이 동영상은 딥러닝에 대한 미니 시리즈에 추가되는 것으로, 이전 동영상을 보지 않으셨더라도 괜찮지만 트랜스포머에 대해 구체적으로 알아보기 전에, 딥러닝의 기본 전제와 구조에 대해 같은 맥락을 가지기 위해 이전 영상들을 확인할만한 가치가 있다고 생각합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "너무 뻔한 이야기일 수 있지만, 이것은 데이터를 사용하여 모델의 작동 방식을 결정하는 모든 모델을 설명하는 머신 러닝의 한 가지 접근 방식입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "즉, 이미지를 입력받아 이미지를 설명하는 레이블을 생성하는 함수나 텍스트 구절이 주어지면 다음 단어를 예측하는 예제 또는 직관 및 패턴 인식 요소가 필요한 기타 작업을 원한다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "요즘은 거의 당연하게 여기지만, 머신 러닝의 개념은 초창기 AI에서 사람들이 했던 것처럼 코드에서 해당 작업을 수행하는 방법을 명시적으로 정의하는 대신, 여러 개의 노브와 다이얼처럼 조정 가능한 매개 변수를 사용하여 매우 유연한 구조를 설정한 다음, 주어진 입력에 대한 출력이 어떤 모습일지 많은 예를 사용하여 이러한 동작을 모방하기 위해 매개 변수 값을 조정하고 조정한다는 점입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "예를 들어, 가장 간단한 형태의 머신 러닝은 선형 회귀로, 입력과 출력은 각각 주택의 면적과 가격과 같은 단일 숫자이며, 원하는 것은 이 데이터를 통해 미래의 주택 가격을 예측하기 위해 가장 적합한 선을 찾는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "이 선은 기울기와 y-절편이라는 두 개의 연속 매개변수로 설명되며, 선형 회귀의 목표는 데이터와 밀접하게 일치하도록 이러한 매개변수를 결정하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "말할 필요도 없이 딥러닝 모델은 훨씬 더 복잡해집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "예를 들어 GPT-3에는 두 개가 아니라 1750억 개의 매개변수가 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "하지만 중요한 사실은, 엄청난 수의 파라미터를 가진 거대한 모델을 만들면, 훈련 데이터에 과적합되거나 훈련이 완전히 불가능해지지 않는다는 보장이 없다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "딥러닝은 지난 수십 년 동안 놀랍도록 잘 확장되는 것으로 입증된 모델 종류들을 설명합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "이들을 통합하는 것은 역전파라고 하는 동일한 학습 알고리즘이며, 이 학습 알고리즘이 대규모로 잘 작동하려면 이러한 모델이 특정 형식을 따라야 한다는 점을 이해해 주시기 바랍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "이 형식을 알고 있으면 자의적으로 느껴질 위험이 있는 트랜스포머의 언어 처리 방식에 대한 많은 선택 사항을 설명하는 데 도움이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "먼저, 어떤 모델을 만들든 입력 형식은 실수 배열로 지정해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "이는 숫자 목록을 의미할 수도 있고, 2차원 배열일 수도 있으며, 보통은 텐서라고 부르는 고차원 배열을 다루는 경우가 많습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "입력 데이터가 점진적으로 여러 계층으로 변환되고, 각 계층은 출력으로 간주되는 최종 계층에 도달할 때까지 항상 일종의 실수 배열로 구조화된다고 생각할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "예를 들어, 텍스트 처리 모델의 마지막 레이어는 가능한 모든 다음 토큰에 대한 확률 분포를 나타내는 숫자 목록입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "딥러닝에서 이러한 모델 매개변수는 거의 항상 가중치라고 불리며, 이는 이러한 모델이 처리 중인 데이터와 상호작용하는 유일한 방식이 가중 합계를 통해서만 이루어지는 것이 핵심이기 때문입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "비선형 함수를 곳곳에 뿌리기도 하지만 이는 매개변수에 의존하지 않습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "일반적으로 가중 합계를 이렇게 명시적으로 적나라하게 표시하는 대신 다양한 구성 요소를 행렬 벡터 곱셈으로 함께 묶어 표시합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "행렬 벡터 곱셈의 작동 방식을 떠올리면 출력의 각 구성 요소가 가중 합으로 보일 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "처리 중인 데이터에서 도출된 벡터를 변환하는, 조정 가능한 매개변수로 채워진 행렬에 대해 생각하는 것이 개념적으로 더 깔끔할 때가 많습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "예를 들어, GPT-3의 1,750억 개의 가중치는 28,000개가 안 되는 개별 행렬들로 구성되어 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "이러한 행렬은 차례로 8가지 범주로 나뉘며, 여러분과 제가 할 일은 각 범주를 단계별로 살펴보고 해당 유형이 무엇을 하는지 이해하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "진행하면서 GPT-3의 구체적인 수치를 참고하여 1,750억 달러의 출처를 정확히 세어보는 것도 재미있을 것 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "요즘에는 더 크고 더 좋은 모델이 있지만, 이 모델은 ML 커뮤니티 외부에서 전 세계의 관심을 끈 대형 언어 모델로서 확실한 매력이 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "또한, 실제로 기업들은 최신 네트워크의 경우 특정 숫자에 대해 훨씬 더 엄격하게 입 다물고 있는 경향이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "ChatGPT와 같은 도구 내부를 들여다보면 거의 모든 실제 계산이 행렬 벡터 곱셈처럼 보인다고 설정하고 싶었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "수십억 개의 숫자의 바다에서 길을 잃을 위험이 약간 있지만, 머릿속에서 항상 파란색이나 빨간색으로 표시하는 모델의 가중치와, 항상 회색으로 표시하는 처리 중인 데이터를  잘 구분해야 합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "가중치는 실제 두뇌에 해당하며, 훈련 중에 학습한 내용을 바탕으로 작동 방식을 결정합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "처리 중인 데이터는 텍스트 스니펫 예시처럼 간단히 실행을 위해 모델에 입력되는 특정 입력을 인코드한 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "이 모든 것을 기초로 하고, 이 텍스트 처리 예제의 첫 번째 단계인, 입력을 작은 덩어리로 나누고 그 덩어리를 벡터로 변환하는 과정을 살펴보겠습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "이러한 덩어리를 토큰이라고 하는데, 단어 조각이나 구두점일 수도 있지만, 이 장과 특히 다음 장에서는 좀 더 깔끔하게 단어로 나누었다고 생각하고 싶었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "인간은 말로 생각하기 때문에 작은 예시를 참조하고 각 단계를 명확히 하는 것이 훨씬 쉬워집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "이 모델에는 미리 정의된 어휘, 가능한 모든 단어 목록(예: 50,000개)이 있으며, 임베딩 행렬이라고 하는 첫 번째 행렬에는 이러한 단어 각각에 대한 단일 열이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "각 열은 첫 번째 단계에서 각 단어가 어떤 벡터로 변하는지를 결정하는 요소입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "이 행렬에 WE라는 레이블을 붙이고 다른 모든 행렬과 마찬가지로 값은 무작위로 시작하여 데이터를 기반으로 학습할 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "단어를 벡터로 바꾸는 것은 트랜스포머 이전부터 머신 러닝에서 흔히 사용되던 방식이지만, 처음 접하는 분들에게는 다소 생소할 수 있고 이후 모든 작업의 기초가 되는 것이므로 잠시 시간을 내어 익숙해지도록 하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "우리는 흔히 이를 임베딩이라는 단어로 부르는데, 이는 벡터를 매우 기하학적으로 고차원인 공간의 점으로 생각하도록 유도합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "세 개의 숫자 목록을 3D 공간에서 점의 좌표로 시각화하는 것은 문제가 되지 않지만, 단어 임베딩은 훨씬 더 고차원적인 경향이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "GPT-3에서는 12,288개의 차원이 있으며, 보시다시피 여러 방향이 뚜렷한 공간에서 작업하는 것이 중요합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "3D 공간에서 2차원 슬라이스를 가져와 모든 점을 그 슬라이스에 투영하는 것과 마찬가지로, 간단한 모델에서 제공하는 단어 임베딩 애니메이션을 위해 이 고차원 공간을 통해 3차원 슬라이스를 선택하고 그 위에 단어 벡터를 투영하여 결과를 표시하는 유사한 작업을 수행하려고 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "여기서 중요한 아이디어는 모델이 학습 중에 단어가 벡터로 정확히 임베드되는 방식을 결정하기 위해 가중치를 조정하고 조정함에 따라, 공간의 방향이 일종의 의미적 의미를 갖는 임베딩 세트에 정착하는 경향이 있다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "여기서 실행 중인 간단한 단어-벡터 모델의 경우, 임베딩이 '탑'에 가장 가까운 모든 단어를 검색해 보면 모두 매우 '탑' 스러운 느낌을 주는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "집에서 파이썬을 불러와서 따라 해보고 싶다면, 여기 애니메이션을 만드는 데 사용한 모델이 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "트랜스포머는 아니지만 공간의 방향이 의미적 의미를 전달할 수 있다는 아이디어를 설명하는 데는 충분합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "아주 고전적인 예로 여성과 남성의 벡터의 차이를 한쪽 끝과 다른 쪽 끝을 연결하는 작은 벡터로 시각화하면, 이는 왕과 여왕의 차이와 매우 유사합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "예를 들어 여성 군주에 대한 단어를 모른다고 가정하면, 왕(king)을 가져와서 여성-남성 방향을 추가하고 그 지점에 가장 가까운 임베딩을 검색하면 찾을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "적어도 그런 식이죠.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "이것이 제가 사용하는 모델의 전형적인 예시이긴 하지만, 실제 여왕의 임베딩은 이보다 조금 더 멀리 떨어져 있는데, 아마도 훈련 데이터에서 여왕이 사용되는 방식이 단순히 왕의 여성 버전이 아니기 때문일 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "가족 관계는 이 아이디어를 훨씬 더 잘 설명해 주는 것 같았습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "요점은, 모델이 학습 과정에서 이 공간의 한 방향이 성별 정보를 인코딩하도록 임베딩을 선택하는 것이 유리하다는 것을 알게 된 것으로 보인다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "또 다른 예로 이탈리아의 임베딩에서 독일의 임베딩을 빼고 히틀러의 임베딩을 더하면 무솔리니의 임베딩에 매우 근접한 결과를 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "마치 모델이 어떤 방향은 이탈리아스러움과, 어떤 방향은 제2차 세계대전의 축을 이끈 지도자와 연관 짓는 법을 배운 것처럼 말입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "이런 맥락에서 제가 가장 좋아하는 예는 독일과 일본의 차이를 초밥에 적용하면 브랫부르스트에 매우 가까워진다는 점입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "또한 가장 가까운 이웃을 찾는 놀이를 하면서 고양이가 짐승과 괴물 모두에 얼마나 가까운지 알게 되어 기뻤습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "특히 다음 장에서 염두에 두면 도움이 되는 수학적 직관 하나는, 두 벡터의 도트 곱은 그 벡터들이 얼마나 잘 정렬되는지 측정하는 방법으로 생각할 수 있다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "계산적으로 도트 곱은 모든 해당 구성 요소를 곱한 다음 결과를 더하는 방식으로 이루어지는데, 이는 우리의 대부분의 계산이 가중 합처럼 보여야 하기 때문에 좋은 일입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "기하학적으로 도트 곱은 벡터가 비슷한 방향을 가리키면 양수, 수직이면 0, 반대 방향을 가리키면 음수가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "예를 들어, 이 모델을 가지고 놀면서 '고양이들'에서 '고양이'를 뺀 결과가 이 공간에서 일종의 복수형의 방향을 나타낸다고 가정해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "이를 테스트하기 위해 이 벡터를 가지고 특정 단수형 명사의 임베딩에 대한 도트 곱을 계산한 다음, 해당 명사의 복수형과의 도트 곱과 비교해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "이 기능을 사용해 보면 복수형이 단수형보다 일관되게 더 높은 값을 제공하는 것을 알 수 있으며, 이는 복수형이 이 방향에 더 부합한다는 것을 나타냅니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "1, 2, 3 등의 단어와 도트 곱을 취하면 점차 값이 증가하기 때문에 모델이 주어진 단어를 얼마나 복수로 찾는지 정량적으로 측정할 수 있는 것도 재미있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "다시 말하지만, 단어가 어떻게 임베딩되는지 세부사항은 데이터를 사용하여 학습합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "각 단어에 어떤 일이 일어나는지 알려주는 열로 구성된 이 임베딩 행렬은 모델의 첫 번째 가중치 더미입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "GPT-3의 숫자를 가져오면 어휘의 크기는 구체적으로 50,257개이며, 엄밀히 말하면 단어 자체가 아니라 토큰으로 구성되어 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "임베딩 차원은 12,288개이며, 이를 곱하면 약 6억 1700만 개의 가중치로 구성되어 있음을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "이제 이것을 집계에 추가하면서, 마지막에는 1,750억 개로 계산되어야 한다는 점을 기억합시다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "트랜스포머의 경우, 이 임베딩 공간에 있는 벡터가 단순히 개별 단어를 나타내는 것이 아니라고 생각하면 됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "나중에 설명하겠지만 단어의 위치에 대한 정보도 인코딩하며, 더 중요한 것은 이 벡터들에 문맥을 흡수하는 능력이 있다고 생각해야 합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "예를 들어, 왕이라는 단어를 임베딩하는 것으로 시작한 벡터는 이 네트워크의 다양한 블록에 의해 점차적으로 밀고 당겨져서 마지막에는 훨씬 더 구체적이고 미묘한 방향을 가리키면서 스코틀랜드에 살았고 전임 왕을 살해한 후 그 자리에 오른 왕이었으며 셰익스피어의 언어로 묘사되고 있다는 것을 어떻게든 인코딩할 수 있게 됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "여러분이 주어진 단어들을 이해하는 법을 생각해 보세요.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "그 단어의 의미는 주변에 의해 명확하게 알려지며 때로는 먼 거리의 문맥도 포함되므로, 다음에 나올 단어를 예측할 수 있는 모델을 구성할 때 목표는 어떻게든 문맥을 효율적으로 통합할 수 있게 하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "명확하게 말하면, 첫 단계에서 입력 텍스트를 기반으로 벡터 배열을 만들 때는 각 벡터를 임베딩 행렬에서 단순히 뽑아내기 때문에 처음에는 각 벡터가 주변 입력 없이 단일 단어의 의미만 인코딩할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "하지만 이 네트워크의 주요 목표는 각각의 벡터가 단순한 개별 단어가 나타내는 것보다 훨씬 더 풍부하고 구체적인 의미를 담을 수 있도록 하는 것이라고 생각해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "네트워크는 컨텍스트 크기로 알려진 고정된 수의 벡터만 한 번에 처리할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "GPT-3의 경우 컨텍스트 크기가 2048로 학습되었으므로 네트워크를 통해 흐르는 데이터는 항상 2048 열의 배열처럼 보이며, 각 열은 12,000개의 차원을 가집니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "이 컨텍스트 크기는 트랜스포머가 다음 단어를 예측할 때 포함할 수 있는 텍스트의 양을 제한합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "이 때문에 ChatGPT의 초기 버전처럼 특정 챗봇과 긴 대화를 이어가다 보면 대화가 너무 길어지면서 봇이 대화의 끈을 놓아버린다는 느낌을 주는 경우가 종종 있었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "어텐션에 대한 자세한 내용은 추후에 설명할 예정이지만, 우선 마지막 단계 어떤 일이 일어나는지 잠시 이야기하고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "원하는 출력은 다음에 나올 수 있는 모든 토큰에 대한 확률 분포라는 점을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "예를 들어, 맨 마지막 단어가 교수이고 문맥에 해리포터와 같은 단어가 포함되어 있고 바로 앞에 가장 싫어하는 선생님이 표시되며 토큰이 단순히 완전한 단어처럼 보이도록 약간의 여유를 준다면, 해리포터에 대한 지식을 쌓은 잘 훈련된 네트워크는 아마도 스네이프라는 단어에 높은 숫자를 할당할 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "여기에는 두 가지 단계가 포함됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "첫 번째는 해당 컨텍스트의 맨 마지막 벡터를 어휘의 각 토큰에 대해 하나씩 매핑하는 또 다른 50,000개의 값을 가진 행렬을 사용하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "그런 다음 이를 확률 분포로 정규화하는 함수가 있는데, 이를 Softmax라고 하며 잠시 후에 자세히 설명하겠습니다. 하지만 그 전에 마지막 임베딩만 사용하여 예측을 하는 것이 조금 이상하게 보일 수 있는데, 마지막 단계에서 레이어에는 수천 개의 다른 벡터가 각자의 맥락이 풍부한 의미를 가지고 있기 때문입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "이는 학습 과정에서 최종 레이어 있는 각 벡터를 사용하여 그 직후에 올 것을 동시에 예측하는 것이 훨씬 더 효율적이라는 사실과 관련이 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "나중에 학습에 대해 더 자세히 설명할 것이 많지만 지금은 이 부분만 언급하고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "이 행렬을 임베딩 해제 행렬이라고 하며 WU라는 레이블을 붙입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "다시 말하지만, 우리가 보는 모든 가중치 행렬과 마찬가지로, 그 항목들은 무작위로 시작하지만 학습 과정에서 학습됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "총 매개변수 집계에 추가해보면, 이 임베딩 해제 행렬은 어휘의 각 단어마다 하나의 행을 가지며, 각 행은 임베딩 차원과 동일한 수의 요소를 가집니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "순서가 바뀌었을 뿐 임베딩 행렬과 매우 유사하므로 네트워크에 6억 1,700만 개의 매개변수가 추가되며, 즉 지금까지 집계된 수는 10억 개가 조금 넘습니다. 이는 총 1,750억 개에 비하면 작지만 미미한 일부는 아닙니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "이 장의 마지막 미니 레슨으로 소프트맥스 기능에 대해 더 자세히 이야기하려고 하는데, 어텐션 블록에 들어가면 이 기능이 또 한 번 등장하기 때문입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "일련의 숫자가 확률 분포, 즉 가능한 모든 다음 단어에 대한 분포로 작동하려면 각 값이 0에서 1 사이여야 하며, 모든 값을 더했을 때 1이 되어야 합니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "그러나 행렬-벡터 곱셈같은 학습 놀이를 하다 보면, 기본적으로 얻는 출력은 이를 전혀 따르지 않습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "값은 음수이거나 1보다 훨씬 큰 경우가 많으며, 거의 확실하게 1로 합산되지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "소프트맥스는 임의의 숫자 목록을 가장 큰 값은 1에 가장 가깝고 작은 값은 0에 매우 가깝게 되는 방식으로 유효한 분포로 변환하는 표준 방법입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "이 정도만 알아두면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "궁금하신 분들을 위해, 작동 방법을 알려드리자면 먼저 각 숫자를 e의 거듭제곱으로 올리면 양수 값의 목록이 생기고, 그런 다음 그 모든 양수 값의 합을 구해 각 항을 그 합으로 나누면 총합이 1인 목록으로 정규화됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "입력의 숫자 중 하나가 나머지 숫자보다 의미 있게 크면 출력에서 해당 항이 분포를 지배하므로 샘플링을 한다면 거의 확실하게 최대값의 입력을 선택하게 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "하지만 다른 값도 비슷하게 크면 분포에서 의미 있는 가중치를 가지며, 입력을 연속적으로 변경하면 모든 것이 연속적으로 변화한다는 점에서 단순히 최대값을 선택하는 것보다 부드럽습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "ChatGPT가 이 분포를 사용하여 다음 단어를 생성하는 경우와 같이 일부 상황에서는 이 함수의 지수부 분모에 상수 t를 넣는다는 약간의 양념을 추가하여 약간의 재미를 더할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "이것을 온도라고 부릅니다. 특정한 열역학 방정식에서 온도의 역할과 대략적으로 유사하기 때문입니다. 그 효과는 t가 크면 낮은 값에 더 많은 가중치를 부여하여 분포가 조금 더 균일해지고, t가 작으면 큰 값이 더 공격적으로 지배하게 되며, 극단적으로 t를 0으로 설정하면 모든 가중치가 최대값으로 이동하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "예를 들어, GPT-3가 '옛날 옛적에 A가 있었다'라는 시드 텍스트로 스토리를 생성하도록 하되 각 사례마다 다른 온도를 사용하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "온도 0은 항상 가장 예상 가능한 단어만을 사용하기 때문에 결국 골디락스의 진부한 파생으로 끝납니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "온도가 높을수록 가능성이 낮은 단어를 선택할 확률이 높아지지만, 그만큼 위험이 따릅니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "이 경우, 한국의 젊은 웹 아티스트에 대한 이야기로 시작하지만 곧 말도 안 되는 이야기로 변질됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "기술적으로 말하자면, API에서는 실제로 2보다 큰 온도를 선택할 수 없습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "여기에는 수학적 이유가 있는 것이 아니라 도구가 너무 무의미한 것을 생성하는 것을 막기 위해 임의로 설정한 제약 조건입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "이 애니메이션이 실제로 작동하는 방식이 궁금하시다면, GPT-3가 생성하는 다음 토큰 중 가장 확률이 높은 20개(가져올 수 있는 최대치)의 토큰을 가져온 다음 지수 1/5를 기준으로 확률을 조정했습니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "또 다른 전문 용어로, 이 함수의 출력 구성 요소를 확률이라고 부르는 것과 마찬가지로, 입력은 보통 로짓이라고 부릅니다. 누구는 로짓, 누구는 로깃이라고 하는데, 저는 로짓이라고 부릅니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "예를 들어, 어떤 텍스트를 입력하면 네트워크에 모든 단어 임베딩이 흐르게 되고, 임베딩 해제 행렬로 최종 곱셈을 수행하면, 머신러닝을 하는 사람은 정규화되지 않은 원시 출력을 다음 단어 예측을 위한 로짓으로 참조하게 됩니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "이 장의 많은 목표는 어텐션 메커니즘을 이해하기 위한 기초를 마련하는 것이었습니다. 자세한 설명 없이 기본적으로요.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "단어 임베딩, 소프트맥스, 도트 곱셈으로 유사성을 측정하는 방법, 그리고 대부분의 계산이 조정 가능한 매개변수로 가득 찬 행렬의 곱셈처럼 보여야 한다는 기본 전제에 대한 직관이 있다면, 최근 AI 붐의 초석인 어텐션 메커니즘을 이해하는 것은 비교적 순조로울 것입니다.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "이를 위해 다음 장에서 저와 함께하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "이 글을 게시하면서 다음 챕터의 초안을 Patreon 후원자분들이 검토할 수 있도록 공개합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "최종 버전은 1~2주 후에 공개될 예정이며, 보통 그 검토를 바탕으로 얼마나 많이 수정하느냐에 따라 달라집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "그동안 관심을 갖고 싶거나 채널에 조금이나마 도움을 주고 싶으시다면, 바로 이 채널이 여러분을 기다리고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
