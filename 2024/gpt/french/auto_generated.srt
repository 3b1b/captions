1
00:00:00,000 --> 00:00:02,349
Les initiales GPT signifient Generative Pretrained 

2
00:00:02,349 --> 00:00:04,560
Transformer (transformateur génératif préformé).

3
00:00:05,220 --> 00:00:09,020
Ce premier mot est donc assez simple, ce sont des bots qui génèrent de nouveaux textes.

4
00:00:09,800 --> 00:00:13,101
Le préfixe indique que le modèle a été soumis à un processus d'apprentissage à 

5
00:00:13,101 --> 00:00:16,529
partir d'une quantité massive de données, et ce préfixe insinue qu'il y a plus de 

6
00:00:16,529 --> 00:00:20,040
place pour l'affiner sur des tâches spécifiques avec un entraînement supplémentaire.

7
00:00:20,720 --> 00:00:22,900
Mais le dernier mot, c'est la vraie pièce maîtresse.

8
00:00:23,380 --> 00:00:26,163
Un transformateur est un type spécifique de réseau neuronal, 

9
00:00:26,163 --> 00:00:29,950
un modèle d'apprentissage automatique, et c'est l'invention centrale qui sous-tend 

10
00:00:29,950 --> 00:00:31,000
l'essor actuel de l'IA.

11
00:00:31,740 --> 00:00:34,834
Ce que je veux faire avec cette vidéo et les chapitres suivants, 

12
00:00:34,834 --> 00:00:39,120
c'est expliquer visuellement ce qui se passe réellement à l'intérieur d'un transformateur.

13
00:00:39,700 --> 00:00:42,820
Nous allons suivre les données qui y circulent et procéder étape par étape.

14
00:00:43,440 --> 00:00:45,388
Il existe de nombreuses sortes de modèles que 

15
00:00:45,388 --> 00:00:47,380
tu peux construire à l'aide de transformateurs.

16
00:00:47,800 --> 00:00:50,800
Certains modèles prennent des données audio et produisent une transcription.

17
00:00:51,340 --> 00:00:53,609
Cette phrase provient d'un modèle qui va dans l'autre sens, 

18
00:00:53,609 --> 00:00:56,220
en produisant un discours synthétique uniquement à partir d'un texte.

19
00:00:56,660 --> 00:00:59,519
Tous ces outils qui ont pris le monde d'assaut en 2022 comme 

20
00:00:59,519 --> 00:01:02,285
Dolly et Midjourney qui prennent en compte une description 

21
00:01:02,285 --> 00:01:05,519
textuelle et produisent une image sont basés sur des transformateurs.

22
00:01:06,000 --> 00:01:09,819
Même si je n'arrive pas à lui faire comprendre ce qu'est censée être la créature Pi, 

23
00:01:09,819 --> 00:01:13,100
je suis toujours époustouflée de voir que ce genre de chose est possible.

24
00:01:13,900 --> 00:01:18,025
Et le transformateur original introduit en 2017 par Google a été inventé pour le 

25
00:01:18,025 --> 00:01:22,100
cas d'utilisation spécifique de la traduction de texte d'une langue à une autre.

26
00:01:22,660 --> 00:01:25,742
Mais la variante sur laquelle toi et moi allons nous concentrer, 

27
00:01:25,742 --> 00:01:28,918
et qui est le type d'outil qui sous-tend des outils comme ChatGPT, 

28
00:01:28,918 --> 00:01:31,906
sera un modèle qui est entraîné à prendre un morceau de texte, 

29
00:01:31,906 --> 00:01:34,893
peut-être même avec des images ou des sons qui l'accompagnent, 

30
00:01:34,893 --> 00:01:38,260
et à produire une prédiction pour ce qui vient ensuite dans le passage.

31
00:01:38,600 --> 00:01:41,199
Cette prédiction prend la forme d'une distribution de probabilités 

32
00:01:41,199 --> 00:01:43,800
sur de nombreux morceaux de texte différents qui pourraient suivre.

33
00:01:45,040 --> 00:01:47,490
À première vue, tu pourrais penser que prédire le mot suivant semble 

34
00:01:47,490 --> 00:01:49,940
être un objectif très différent de celui de générer un nouveau texte.

35
00:01:50,180 --> 00:01:52,989
Mais une fois que tu as un modèle de prédiction comme celui-ci, 

36
00:01:52,989 --> 00:01:56,896
une façon simple de générer un texte plus long est de lui donner un extrait initial avec 

37
00:01:56,896 --> 00:02:00,847
lequel travailler, de lui demander de prendre un échantillon aléatoire de la distribution 

38
00:02:00,847 --> 00:02:03,481
qu'il vient de générer, d'ajouter cet échantillon au texte, 

39
00:02:03,481 --> 00:02:07,344
puis de relancer tout le processus pour faire une nouvelle prédiction basée sur tout le 

40
00:02:07,344 --> 00:02:09,539
nouveau texte, y compris ce qu'il vient d'ajouter.

41
00:02:10,100 --> 00:02:11,536
Je ne sais pas ce qu'il en est pour toi, mais je n'ai 

42
00:02:11,536 --> 00:02:13,000
vraiment pas l'impression que cela devrait fonctionner.

43
00:02:13,420 --> 00:02:16,524
Dans cette animation, par exemple, j'exécute GPT-2 sur mon ordinateur portable 

44
00:02:16,524 --> 00:02:19,590
et je lui demande de prédire et d'échantillonner de façon répétée le prochain 

45
00:02:19,590 --> 00:02:22,420
morceau de texte pour générer une histoire basée sur le texte de départ.

46
00:02:22,420 --> 00:02:26,120
L'histoire n'a pas vraiment de sens.

47
00:02:26,500 --> 00:02:31,165
Mais si je le remplace par des appels d'API à GPT-3, qui est le même modèle de base, 

48
00:02:31,165 --> 00:02:36,104
juste beaucoup plus grand, soudain, presque par magie, nous obtenons une histoire sensée, 

49
00:02:36,104 --> 00:02:40,880
qui semble même déduire qu'une créature pi vivrait dans un pays de maths et de calculs.

50
00:02:41,580 --> 00:02:45,325
Ce processus de prédiction et d'échantillonnage répétés est essentiellement 

51
00:02:45,325 --> 00:02:48,775
ce qui se passe lorsque tu interagis avec ChatGPT ou tout autre grand 

52
00:02:48,775 --> 00:02:51,880
modèle de langage et que tu les vois produire un mot à la fois.

53
00:02:52,480 --> 00:02:55,850
En fait, une fonction que j'apprécierais beaucoup est la possibilité de 

54
00:02:55,850 --> 00:02:59,220
voir la distribution sous-jacente pour chaque nouveau mot qu'il choisit.

55
00:03:03,820 --> 00:03:05,898
Commençons par un aperçu de très haut niveau de la 

56
00:03:05,898 --> 00:03:08,180
façon dont les données circulent dans un transformateur.

57
00:03:08,640 --> 00:03:10,902
Nous passerons beaucoup plus de temps à motiver, 

58
00:03:10,902 --> 00:03:14,827
interpréter et développer les détails de chaque étape, mais dans les grandes lignes, 

59
00:03:14,827 --> 00:03:18,660
lorsqu'un de ces chatbots génère un mot donné, voici ce qui se passe sous le capot.

60
00:03:19,080 --> 00:03:22,040
Tout d'abord, l'entrée est décomposée en un tas de petits morceaux.

61
00:03:22,620 --> 00:03:25,188
Ces morceaux sont appelés jetons, et dans le cas d'un texte, 

62
00:03:25,188 --> 00:03:28,809
il s'agit généralement de mots ou de petits morceaux de mots ou d'autres combinaisons 

63
00:03:28,809 --> 00:03:29,820
de caractères courantes.

64
00:03:30,740 --> 00:03:33,832
S'il s'agit d'images ou de sons, les jetons peuvent être de 

65
00:03:33,832 --> 00:03:37,080
petites parties de cette image ou de petits morceaux de ce son.

66
00:03:37,580 --> 00:03:40,053
Chacun de ces jetons est ensuite associé à un vecteur, 

67
00:03:40,053 --> 00:03:43,920
c'est-à-dire à une liste de nombres, qui est censé coder d'une manière ou d'une autre 

68
00:03:43,920 --> 00:03:45,360
la signification de cette pièce.

69
00:03:45,880 --> 00:03:48,875
Si tu considères que ces vecteurs donnent des coordonnées dans un espace à très 

70
00:03:48,875 --> 00:03:51,759
haute dimension, les mots ayant des significations similaires ont tendance à 

71
00:03:51,759 --> 00:03:54,680
atterrir sur des vecteurs qui sont proches les uns des autres dans cet espace.

72
00:03:55,280 --> 00:03:58,397
Cette séquence de vecteurs passe ensuite par une opération connue sous 

73
00:03:58,397 --> 00:04:01,514
le nom de bloc d'attention, ce qui permet aux vecteurs de se parler et 

74
00:04:01,514 --> 00:04:04,500
de se transmettre des informations pour mettre à jour leurs valeurs.

75
00:04:04,880 --> 00:04:08,453
Par exemple, le sens du mot modèle dans l'expression un modèle d'apprentissage 

76
00:04:08,453 --> 00:04:11,800
automatique est différent de son sens dans l'expression un modèle de mode.

77
00:04:12,260 --> 00:04:15,415
Le bloc d'attention est chargé de déterminer quels mots du contexte 

78
00:04:15,415 --> 00:04:18,850
sont pertinents pour mettre à jour la signification de quels autres mots, 

79
00:04:18,850 --> 00:04:21,959
et comment exactement ces significations doivent être mises à jour.

80
00:04:22,500 --> 00:04:24,851
Et encore une fois, chaque fois que j'utilise le mot sens, 

81
00:04:24,851 --> 00:04:28,040
celui-ci est en quelque sorte entièrement codé dans les entrées de ces vecteurs.

82
00:04:29,180 --> 00:04:32,378
Ensuite, ces vecteurs passent par un autre type d'opération, 

83
00:04:32,378 --> 00:04:36,731
et selon la source que tu lis, on parlera d'un perceptron multicouche ou peut-être 

84
00:04:36,731 --> 00:04:38,200
d'une couche d'anticipation.

85
00:04:38,580 --> 00:04:40,659
Et ici, les vecteurs ne communiquent pas entre eux, 

86
00:04:40,659 --> 00:04:42,660
ils subissent tous la même opération en parallèle.

87
00:04:43,060 --> 00:04:45,886
Et bien que ce bloc soit un peu plus difficile à interpréter, 

88
00:04:45,886 --> 00:04:49,487
nous verrons plus loin que cette étape revient un peu à poser une longue liste 

89
00:04:49,487 --> 00:04:53,270
de questions sur chaque vecteur, puis à les mettre à jour en fonction des réponses 

90
00:04:53,270 --> 00:04:54,000
à ces questions.

91
00:04:54,900 --> 00:04:58,338
Toutes les opérations dans ces deux blocs ressemblent à une pile 

92
00:04:58,338 --> 00:05:01,829
géante de multiplications de matrices, et notre travail principal 

93
00:05:01,829 --> 00:05:05,320
va consister à comprendre comment lire les matrices sous-jacentes.

94
00:05:06,980 --> 00:05:09,820
J'omets certains détails concernant les étapes de normalisation qui se 

95
00:05:09,820 --> 00:05:12,980
déroulent entre les deux, mais il s'agit après tout d'un aperçu de haut niveau.

96
00:05:13,680 --> 00:05:16,190
Après cela, le processus se répète essentiellement, 

97
00:05:16,190 --> 00:05:20,245
tu vas et viens entre les blocs d'attention et les blocs de perceptron multicouche, 

98
00:05:20,245 --> 00:05:23,624
jusqu'à ce qu'à la toute fin, l'espoir est que toute la signification 

99
00:05:23,624 --> 00:05:27,389
essentielle du passage a été en quelque sorte incorporée dans le tout dernier 

100
00:05:27,389 --> 00:05:28,500
vecteur de la séquence.

101
00:05:28,920 --> 00:05:31,970
Nous effectuons ensuite une certaine opération sur ce dernier vecteur 

102
00:05:31,970 --> 00:05:35,238
qui produit une distribution de probabilité sur tous les tokens possibles, 

103
00:05:35,238 --> 00:05:38,420
tous les petits morceaux de texte possibles qui pourraient venir ensuite.

104
00:05:38,980 --> 00:05:42,595
Et comme je l'ai dit, une fois que tu as un outil qui prédit ce qui vient après 

105
00:05:42,595 --> 00:05:46,120
un extrait de texte, tu peux lui donner un peu de texte de départ et le faire 

106
00:05:46,120 --> 00:05:49,419
jouer à plusieurs reprises à ce jeu de prédiction de ce qui vient après, 

107
00:05:49,419 --> 00:05:53,080
d'échantillonnage de la distribution, d'ajout, et de répétition encore et encore.

108
00:05:53,640 --> 00:05:57,208
Certains d'entre vous se souviennent peut-être que bien avant que ChatGPT n'entre en 

109
00:05:57,208 --> 00:06:00,273
scène, voici à quoi ressemblaient les premières démonstrations de GPT-3, 

110
00:06:00,273 --> 00:06:03,968
qui permettaient de compléter automatiquement des histoires et des essais à partir d'un 

111
00:06:03,968 --> 00:06:04,640
extrait initial.

112
00:06:05,580 --> 00:06:08,073
Pour transformer un outil comme celui-ci en chatbot, 

113
00:06:08,073 --> 00:06:11,508
le point de départ le plus simple est d'avoir un petit bout de texte qui 

114
00:06:11,508 --> 00:06:15,366
établit le cadre d'une interaction entre un utilisateur et un assistant IA utile, 

115
00:06:15,366 --> 00:06:18,894
ce que tu appellerais l'invite du système, puis tu utiliserais la question 

116
00:06:18,894 --> 00:06:22,188
ou l'invite initiale de l'utilisateur comme premier bout de dialogue, 

117
00:06:22,188 --> 00:06:25,810
puis tu ferais en sorte qu'il commence à prédire ce qu'un assistant IA aussi 

118
00:06:25,810 --> 00:06:26,940
utile dirait en réponse.

119
00:06:27,720 --> 00:06:32,068
Il y a plus à dire sur l'étape de formation nécessaire pour que cela fonctionne bien, 

120
00:06:32,068 --> 00:06:33,940
mais à un niveau élevé, c'est l'idée.

121
00:06:35,720 --> 00:06:39,070
Dans ce chapitre, toi et moi allons nous étendre sur les détails de ce qui se 

122
00:06:39,070 --> 00:06:41,518
passe au tout début du réseau, à la toute fin du réseau, 

123
00:06:41,518 --> 00:06:44,954
et je veux aussi passer beaucoup de temps à revoir certains éléments importants 

124
00:06:44,954 --> 00:06:48,304
de connaissances de base, des choses qui auraient été une seconde nature pour 

125
00:06:48,304 --> 00:06:51,354
n'importe quel ingénieur en apprentissage automatique au moment où les 

126
00:06:51,354 --> 00:06:52,600
transformateurs sont apparus.

127
00:06:53,060 --> 00:06:56,575
Si tu te sens à l'aise avec ces connaissances de base et que tu es un peu impatient, 

128
00:06:56,575 --> 00:06:59,677
sens-toi libre de passer au chapitre suivant, qui va se concentrer sur les 

129
00:06:59,677 --> 00:07:02,780
blocs d'attention, généralement considérés comme le cœur du transformateur.

130
00:07:03,360 --> 00:07:06,696
Après cela, je veux parler davantage de ces blocs de perceptron multicouche, 

131
00:07:06,696 --> 00:07:09,643
du fonctionnement de l'entraînement et d'un certain nombre d'autres 

132
00:07:09,643 --> 00:07:11,680
détails qui auront été sautés jusqu'à ce point.

133
00:07:12,180 --> 00:07:15,020
Pour un contexte plus large, ces vidéos sont des ajouts à une mini-série sur 

134
00:07:15,020 --> 00:07:18,229
l'apprentissage profond, et ce n'est pas grave si tu n'as pas regardé les précédentes, 

135
00:07:18,229 --> 00:07:19,999
je pense que tu peux le faire dans le désordre, 

136
00:07:19,999 --> 00:07:22,323
mais avant de plonger dans les transformateurs en particulier, 

137
00:07:22,323 --> 00:07:25,606
je pense qu'il vaut la peine de s'assurer que nous sommes sur la même longueur d'onde en 

138
00:07:25,606 --> 00:07:28,520
ce qui concerne le principe de base et la structure de l'apprentissage profond.

139
00:07:29,020 --> 00:07:32,757
Au risque d'énoncer l'évidence, il s'agit d'une approche de l'apprentissage automatique, 

140
00:07:32,757 --> 00:07:35,906
qui décrit tout modèle dans lequel tu utilises des données pour déterminer 

141
00:07:35,906 --> 00:07:38,300
d'une manière ou d'une autre le comportement d'un modèle.

142
00:07:39,140 --> 00:07:42,459
Ce que je veux dire par là, c'est que tu veux une fonction qui prend une 

143
00:07:42,459 --> 00:07:44,686
image et qui produit une étiquette la décrivant, 

144
00:07:44,686 --> 00:07:47,960
ou notre exemple de prédiction du mot suivant dans un passage de texte, 

145
00:07:47,960 --> 00:07:51,325
ou toute autre tâche qui semble nécessiter un certain élément d'intuition 

146
00:07:51,325 --> 00:07:52,780
et de reconnaissance des formes.

147
00:07:53,200 --> 00:07:55,688
Nous considérons presque cela comme acquis de nos jours, 

148
00:07:55,688 --> 00:07:59,137
mais l'idée de l'apprentissage automatique est qu'au lieu d'essayer de définir 

149
00:07:59,137 --> 00:08:02,149
explicitement une procédure pour effectuer cette tâche dans le code, 

150
00:08:02,149 --> 00:08:04,900
ce que les gens auraient fait dans les premiers jours de l'IA, 

151
00:08:04,900 --> 00:08:08,218
tu mets en place une structure très flexible avec des paramètres réglables, 

152
00:08:08,218 --> 00:08:11,754
comme un tas de boutons et de cadrans, et ensuite, d'une manière ou d'une autre, 

153
00:08:11,754 --> 00:08:15,421
tu utilises de nombreux exemples de ce à quoi la sortie devrait ressembler pour une 

154
00:08:15,421 --> 00:08:19,132
entrée donnée, pour ajuster et régler les valeurs de ces paramètres afin d'imiter ce 

155
00:08:19,132 --> 00:08:19,700
comportement.

156
00:08:19,700 --> 00:08:22,923
Par exemple, la forme la plus simple d'apprentissage automatique est 

157
00:08:22,923 --> 00:08:26,147
peut-être la régression linéaire, où tes entrées et tes sorties sont 

158
00:08:26,147 --> 00:08:30,305
chacune des nombres uniques, quelque chose comme la superficie d'une maison et son prix, 

159
00:08:30,305 --> 00:08:34,463
et ce que tu veux, c'est trouver une ligne de meilleur ajustement à travers ces données, 

160
00:08:34,463 --> 00:08:36,799
tu sais, pour prédire les prix futurs des maisons.

161
00:08:37,440 --> 00:08:40,047
Cette ligne est décrite par deux paramètres continus, 

162
00:08:40,047 --> 00:08:43,717
à savoir la pente et l'ordonnée à l'origine, et l'objectif de la régression 

163
00:08:43,717 --> 00:08:47,001
linéaire est de déterminer ces paramètres pour qu'ils correspondent 

164
00:08:47,001 --> 00:08:48,160
étroitement aux données.

165
00:08:48,880 --> 00:08:52,100
Inutile de dire que les modèles d'apprentissage profond se complexifient énormément.

166
00:08:52,620 --> 00:08:57,660
Le GPT-3, par exemple, n'a pas deux, mais 175 milliards de paramètres.

167
00:08:58,120 --> 00:09:02,002
Mais voilà, il n'est pas évident que tu puisses créer un modèle géant avec 

168
00:09:02,002 --> 00:09:05,729
un grand nombre de paramètres sans qu'il ne surajoute grossièrement les 

169
00:09:05,729 --> 00:09:09,560
données d'apprentissage ou qu'il ne soit complètement impossible à former.

170
00:09:10,260 --> 00:09:12,730
L'apprentissage profond décrit une classe de modèles qui, 

171
00:09:12,730 --> 00:09:16,180
au cours des deux dernières décennies, se sont révélés remarquablement évolutifs.

172
00:09:16,480 --> 00:09:20,680
Ce qui les unit, c'est le même algorithme d'apprentissage, appelé rétropropagation, 

173
00:09:20,680 --> 00:09:24,530
et le contexte que je veux que tu aies au fur et à mesure que nous avançons, 

174
00:09:24,530 --> 00:09:28,480
c'est que pour que cet algorithme d'apprentissage fonctionne bien à l'échelle, 

175
00:09:28,480 --> 00:09:31,280
ces modèles doivent suivre un certain format spécifique.

176
00:09:31,800 --> 00:09:36,075
Si tu connais ce format au départ, cela permet d'expliquer un grand nombre des choix de 

177
00:09:36,075 --> 00:09:40,400
traitement de la langue par un transformateur, qui risquent sinon de sembler arbitraires.

178
00:09:41,440 --> 00:09:43,832
Tout d'abord, quel que soit le modèle que tu fais, 

179
00:09:43,832 --> 00:09:46,740
l'entrée doit être formatée comme un tableau de nombres réels.

180
00:09:46,740 --> 00:09:51,396
Il peut s'agir d'une liste de nombres, d'un tableau à deux dimensions ou, très souvent, 

181
00:09:51,396 --> 00:09:56,000
de tableaux à plus haute dimension, pour lesquels le terme général utilisé est tenseur.

182
00:09:56,560 --> 00:09:59,503
Tu considères souvent que ces données d'entrée sont progressivement 

183
00:09:59,503 --> 00:10:02,403
transformées en plusieurs couches distinctes, où chaque couche est 

184
00:10:02,403 --> 00:10:05,217
toujours structurée comme une sorte de tableau de nombres réels, 

185
00:10:05,217 --> 00:10:08,680
jusqu'à ce que tu arrives à une couche finale que tu considères comme la sortie.

186
00:10:09,280 --> 00:10:11,944
Par exemple, la dernière couche de notre modèle de traitement 

187
00:10:11,944 --> 00:10:14,652
de texte est une liste de nombres représentant la distribution 

188
00:10:14,652 --> 00:10:17,060
de probabilité pour tous les prochains tokens possibles.

189
00:10:17,820 --> 00:10:21,680
Dans l'apprentissage profond, ces paramètres de modèle sont presque toujours appelés 

190
00:10:21,680 --> 00:10:25,721
poids, et ce parce qu'une caractéristique clé de ces modèles est que la seule façon dont 

191
00:10:25,721 --> 00:10:29,445
ces paramètres interagissent avec les données traitées est par le biais de sommes 

192
00:10:29,445 --> 00:10:29,900
pondérées.

193
00:10:30,340 --> 00:10:32,671
Tu saupoudres également quelques fonctions non linéaires, 

194
00:10:32,671 --> 00:10:34,360
mais elles ne dépendent pas de paramètres.

195
00:10:35,200 --> 00:10:38,587
Généralement, au lieu de voir les sommes pondérées toutes nues et 

196
00:10:38,587 --> 00:10:42,129
écrites explicitement comme ceci, tu les trouveras plutôt regroupées 

197
00:10:42,129 --> 00:10:45,620
sous forme de divers composants dans un produit vectoriel matriciel.

198
00:10:46,740 --> 00:10:49,267
Cela revient à dire la même chose, si tu repenses à la façon 

199
00:10:49,267 --> 00:10:51,712
dont fonctionne la multiplication vectorielle matricielle, 

200
00:10:51,712 --> 00:10:54,240
chaque composant de la sortie ressemble à une somme pondérée.

201
00:10:54,780 --> 00:10:58,691
Pour toi et moi, il est souvent plus simple, d'un point de vue conceptuel, 

202
00:10:58,691 --> 00:11:01,925
de penser à des matrices remplies de paramètres réglables qui 

203
00:11:01,925 --> 00:11:05,420
transforment les vecteurs tirés des données en cours de traitement.

204
00:11:06,340 --> 00:11:10,178
Par exemple, les 175 milliards de poids du GPT-3 sont 

205
00:11:10,178 --> 00:11:14,160
organisés en un peu moins de 28 000 matrices distinctes.

206
00:11:14,660 --> 00:11:17,468
Ces matrices se répartissent à leur tour en huit catégories différentes, 

207
00:11:17,468 --> 00:11:20,199
et ce que nous allons faire, toi et moi, c'est passer en revue chacune 

208
00:11:20,199 --> 00:11:22,700
de ces catégories pour comprendre ce que fait ce type de matrice.

209
00:11:23,160 --> 00:11:27,187
Au fur et à mesure que nous avançons, je pense qu'il est amusant de se référer aux 

210
00:11:27,187 --> 00:11:31,360
chiffres spécifiques de GPT-3 pour compter exactement d'où viennent ces 175 milliards.

211
00:11:31,880 --> 00:11:34,804
Même s'il existe aujourd'hui des modèles plus grands et meilleurs, 

212
00:11:34,804 --> 00:11:37,641
celui-ci a un certain charme en tant que modèle de grande langue 

213
00:11:37,641 --> 00:11:40,740
pour vraiment capter l'attention du monde en dehors des communautés ML.

214
00:11:41,440 --> 00:11:44,008
De plus, d'un point de vue pratique, les entreprises ont tendance à garder les 

215
00:11:44,008 --> 00:11:46,740
lèvres beaucoup plus serrées sur les chiffres spécifiques des réseaux plus modernes.

216
00:11:47,360 --> 00:11:50,782
Je veux juste te montrer que lorsque tu regardes sous le capot pour voir 

217
00:11:50,782 --> 00:11:53,408
ce qui se passe à l'intérieur d'un outil comme ChatGPT, 

218
00:11:53,408 --> 00:11:57,440
presque tous les calculs ressemblent à des multiplications de matrices et de vecteurs.

219
00:11:57,900 --> 00:12:01,734
Il y a un petit risque de se perdre dans la mer de milliards de chiffres, 

220
00:12:01,734 --> 00:12:06,346
mais tu dois faire une distinction très nette dans ton esprit entre les poids du modèle, 

221
00:12:06,346 --> 00:12:10,078
que je colorerai toujours en bleu ou en rouge, et les données traitées, 

222
00:12:10,078 --> 00:12:11,840
que je colorerai toujours en gris.

223
00:12:12,180 --> 00:12:16,240
Les poids sont les cerveaux réels, ce sont les choses apprises pendant l'entraînement, 

224
00:12:16,240 --> 00:12:17,920
et ils déterminent son comportement.

225
00:12:18,280 --> 00:12:22,502
Les données traitées codent simplement l'entrée spécifique introduite dans 

226
00:12:22,502 --> 00:12:26,500
le modèle pour une exécution donnée, comme un exemple de bout de texte.

227
00:12:27,480 --> 00:12:30,350
Avec tout cela comme base, entrons dans la première étape de 

228
00:12:30,350 --> 00:12:33,126
cet exemple de traitement de texte, qui consiste à diviser 

229
00:12:33,126 --> 00:12:36,420
l'entrée en petits morceaux et à transformer ces morceaux en vecteurs.

230
00:12:37,020 --> 00:12:39,531
J'ai mentionné que ces morceaux sont appelés des jetons, 

231
00:12:39,531 --> 00:12:43,100
qui peuvent être des morceaux de mots ou de ponctuation, mais de temps en temps, 

232
00:12:43,100 --> 00:12:46,714
dans ce chapitre et surtout dans le prochain, j'aimerais faire semblant que c'est 

233
00:12:46,714 --> 00:12:48,080
divisé plus proprement en mots.

234
00:12:48,600 --> 00:12:50,414
Comme nous, les humains, pensons avec des mots, 

235
00:12:50,414 --> 00:12:52,946
cela facilitera grandement la référence à de petits exemples et la 

236
00:12:52,946 --> 00:12:54,080
clarification de chaque étape.

237
00:12:55,260 --> 00:12:59,580
Le modèle possède un vocabulaire prédéfini, une liste de tous les mots possibles, 

238
00:12:59,580 --> 00:13:03,479
disons 50 000 d'entre eux, et la première matrice que nous rencontrerons, 

239
00:13:03,479 --> 00:13:07,800
appelée matrice d'intégration, comporte une seule colonne pour chacun de ces mots.

240
00:13:08,940 --> 00:13:11,195
Ce sont ces colonnes qui déterminent le vecteur en 

241
00:13:11,195 --> 00:13:13,760
lequel chaque mot se transforme dans cette première étape.

242
00:13:15,100 --> 00:13:18,211
Nous l'appelons We, et comme toutes les matrices que nous voyons, 

243
00:13:18,211 --> 00:13:22,360
ses valeurs commencent au hasard, mais elles vont être apprises en fonction des données.

244
00:13:23,620 --> 00:13:26,394
Transformer des mots en vecteurs était une pratique courante en 

245
00:13:26,394 --> 00:13:28,909
apprentissage automatique bien avant les transformateurs, 

246
00:13:28,909 --> 00:13:31,597
mais c'est un peu bizarre si tu ne l'as jamais vu auparavant, 

247
00:13:31,597 --> 00:13:34,545
et cela pose les bases de tout ce qui suit, alors prenons un moment 

248
00:13:34,545 --> 00:13:35,760
pour nous familiariser avec.

249
00:13:36,040 --> 00:13:38,199
Nous appelons souvent cet encastrement un mot, 

250
00:13:38,199 --> 00:13:42,104
ce qui t'invite à considérer ces vecteurs de façon très géométrique comme des points 

251
00:13:42,104 --> 00:13:43,620
dans un espace à haute dimension.

252
00:13:44,180 --> 00:13:46,648
Visualiser une liste de trois nombres comme des coordonnées de 

253
00:13:46,648 --> 00:13:48,685
points dans l'espace 3D ne poserait aucun problème, 

254
00:13:48,685 --> 00:13:51,780
mais les enchâssements de mots ont tendance à être beaucoup plus dimensionnels.

255
00:13:52,280 --> 00:13:55,666
Dans GPT-3, il y a 12 288 dimensions, et comme tu le verras, 

256
00:13:55,666 --> 00:14:00,440
il est important de travailler dans un espace qui a beaucoup de directions distinctes.

257
00:14:01,180 --> 00:14:04,956
De la même façon que tu peux prendre une tranche bidimensionnelle dans un espace 

258
00:14:04,956 --> 00:14:07,287
3D et projeter tous les points sur cette tranche, 

259
00:14:07,287 --> 00:14:10,457
pour animer les enchâssements de mots qu'un modèle simple me donne, 

260
00:14:10,457 --> 00:14:14,326
je vais faire une chose analogue en choisissant une tranche tridimensionnelle dans 

261
00:14:14,326 --> 00:14:18,055
cet espace à très haute dimension, et en projetant les vecteurs de mots vers le 

262
00:14:18,055 --> 00:14:20,480
bas sur cette tranche et en affichant les résultats.

263
00:14:21,280 --> 00:14:24,580
L'idée principale ici est qu'au fur et à mesure qu'un modèle ajuste et règle 

264
00:14:24,580 --> 00:14:28,010
ses poids pour déterminer comment les mots sont intégrés sous forme de vecteurs 

265
00:14:28,010 --> 00:14:31,267
au cours de la formation, il tend à se fixer sur un ensemble d'intégrations 

266
00:14:31,267 --> 00:14:34,440
où les directions dans l'espace ont une sorte de signification sémantique.

267
00:14:34,980 --> 00:14:37,442
Pour le modèle simple mot-vecteur que j'utilise ici, 

268
00:14:37,442 --> 00:14:40,974
si je lance une recherche pour tous les mots dont l'intégration est la plus 

269
00:14:40,974 --> 00:14:44,831
proche de celle de tour, tu remarqueras qu'ils semblent tous donner une impression 

270
00:14:44,831 --> 00:14:45,900
de tour très similaire.

271
00:14:46,340 --> 00:14:48,612
Et si tu veux utiliser Python et t'amuser à la maison, 

272
00:14:48,612 --> 00:14:51,380
voici le modèle spécifique que j'utilise pour faire les animations.

273
00:14:51,620 --> 00:14:54,610
Ce n'est pas un transformateur, mais c'est suffisant pour illustrer l'idée 

274
00:14:54,610 --> 00:14:57,600
que les directions dans l'espace peuvent être porteuses de sens sémantique.

275
00:14:58,300 --> 00:15:03,248
Un exemple très classique de ceci est la différence entre les vecteurs de la femme et de 

276
00:15:03,248 --> 00:15:08,140
l'homme, quelque chose que tu visualiserais comme un petit vecteur reliant la pointe de 

277
00:15:08,140 --> 00:15:12,866
l'un à la pointe de l'autre, c'est très similaire à la différence entre le roi et la 

278
00:15:12,866 --> 00:15:13,200
reine.

279
00:15:15,080 --> 00:15:18,696
Supposons donc que tu ne connaisses pas le mot désignant une femme monarque, 

280
00:15:18,696 --> 00:15:21,843
tu pourrais le trouver en prenant roi, en ajoutant cette direction 

281
00:15:21,843 --> 00:15:25,460
femme-homme et en recherchant les encastrements les plus proches de ce point.

282
00:15:27,000 --> 00:15:28,200
Du moins, en quelque sorte.

283
00:15:28,480 --> 00:15:31,645
Bien qu'il s'agisse d'un exemple classique pour le modèle avec lequel je joue, 

284
00:15:31,645 --> 00:15:35,251
l'intégration réelle de la reine est en fait un peu plus éloignée que ce qui est suggéré, 

285
00:15:35,251 --> 00:15:38,295
probablement parce que la façon dont la reine est utilisée dans les données 

286
00:15:38,295 --> 00:15:40,780
de formation n'est pas simplement une version féminine du roi.

287
00:15:41,620 --> 00:15:43,594
Lorsque je me suis amusé, les relations familiales 

288
00:15:43,594 --> 00:15:45,260
semblaient illustrer beaucoup mieux l'idée.

289
00:15:46,340 --> 00:15:48,712
Le fait est qu'il semble que pendant la formation, 

290
00:15:48,712 --> 00:15:51,457
le modèle a trouvé avantageux de choisir des encastrements 

291
00:15:51,457 --> 00:15:54,900
tels qu'une direction dans cet espace encode des informations sur le sexe.

292
00:15:56,800 --> 00:15:59,970
Un autre exemple est que si tu prends l'encastrement de l'Italie, 

293
00:15:59,970 --> 00:16:02,276
que tu soustrais l'encastrement de l'Allemagne, 

294
00:16:02,276 --> 00:16:04,678
et que tu ajoutes cela à l'encastrement d'Hitler, 

295
00:16:04,678 --> 00:16:08,090
tu obtiens quelque chose de très proche de l'encastrement de Mussolini.

296
00:16:08,570 --> 00:16:12,587
C'est comme si le modèle avait appris à associer certaines directions à l'italianité, 

297
00:16:12,587 --> 00:16:15,670
et d'autres aux dirigeants de l'axe de la Seconde Guerre mondiale.

298
00:16:16,470 --> 00:16:19,246
Mon exemple préféré dans cette veine est peut-être la façon dont, 

299
00:16:19,246 --> 00:16:22,654
dans certains modèles, si tu prends la différence entre l'Allemagne et le Japon, 

300
00:16:22,654 --> 00:16:26,230
et que tu l'ajoutes aux sushis, tu te retrouves très proche de la saucisse bratwurst.

301
00:16:27,350 --> 00:16:29,822
En jouant à ce jeu de recherche des voisins les plus proches, 

302
00:16:29,822 --> 00:16:33,092
j'ai également été heureuse de voir à quel point Kat était proche à la fois de la 

303
00:16:33,092 --> 00:16:33,850
bête et du monstre.

304
00:16:34,690 --> 00:16:37,313
Une intuition mathématique qu'il est utile d'avoir à l'esprit, 

305
00:16:37,313 --> 00:16:40,352
en particulier pour le chapitre suivant, est que le produit en points de 

306
00:16:40,352 --> 00:16:43,850
deux vecteurs peut être considéré comme un moyen de mesurer leur degré d'alignement.

307
00:16:44,870 --> 00:16:47,986
D'un point de vue informatique, les produits points impliquent la multiplication de 

308
00:16:47,986 --> 00:16:50,360
tous les composants correspondants et l'addition des résultats, 

309
00:16:50,360 --> 00:16:53,476
ce qui est une bonne chose, puisqu'une grande partie de nos calculs doit ressembler 

310
00:16:53,476 --> 00:16:54,330
à des sommes pondérées.

311
00:16:55,190 --> 00:16:58,551
Géométriquement, le produit du point est positif lorsque les vecteurs 

312
00:16:58,551 --> 00:17:02,392
pointent dans des directions similaires, il est nul s'ils sont perpendiculaires 

313
00:17:02,392 --> 00:17:05,609
et il est négatif lorsqu'ils pointent dans des directions opposées.

314
00:17:06,550 --> 00:17:09,233
Par exemple, disons que tu joues avec ce modèle, 

315
00:17:09,233 --> 00:17:12,574
et que tu émets l'hypothèse que l'intégration de chats moins 

316
00:17:12,574 --> 00:17:17,010
chat pourrait représenter une sorte de direction de la pluralité dans cet espace.

317
00:17:17,430 --> 00:17:20,683
Pour tester cela, je vais prendre ce vecteur et calculer son produit 

318
00:17:20,683 --> 00:17:23,607
point par rapport aux embeddings de certains noms singuliers, 

319
00:17:23,607 --> 00:17:27,050
et le comparer aux produits points avec les noms pluriels correspondants.

320
00:17:27,270 --> 00:17:30,174
Si tu t'amuses avec ça, tu remarqueras que les pluriels semblent en 

321
00:17:30,174 --> 00:17:33,378
effet donner systématiquement des valeurs plus élevées que les singuliers, 

322
00:17:33,378 --> 00:17:36,070
ce qui indique qu'ils s'alignent davantage sur cette direction.

323
00:17:37,070 --> 00:17:41,101
C'est aussi amusant de voir que si tu fais le produit de ce point avec l'intégration des 

324
00:17:41,101 --> 00:17:44,273
mots 1, 2, 3, et ainsi de suite, ils donnent des valeurs croissantes, 

325
00:17:44,273 --> 00:17:48,214
c'est comme si nous pouvions mesurer quantitativement à quel point le modèle trouve un 

326
00:17:48,214 --> 00:17:49,030
mot donné pluriel.

327
00:17:50,250 --> 00:17:51,940
Encore une fois, les spécificités de la façon dont les 

328
00:17:51,940 --> 00:17:53,570
mots sont intégrés sont apprises à l'aide de données.

329
00:17:54,050 --> 00:17:56,724
Cette matrice d'intégration, dont les colonnes nous indiquent ce qu'il 

330
00:17:56,724 --> 00:17:59,550
advient de chaque mot, constitue la première pile de poids de notre modèle.

331
00:18:00,030 --> 00:18:03,259
En utilisant les chiffres du GPT-3, la taille du vocabulaire 

332
00:18:03,259 --> 00:18:06,699
est spécifiquement de 50 257, et encore une fois, techniquement, 

333
00:18:06,699 --> 00:18:09,770
il ne s'agit pas de mots en tant que tels, mais de jetons.

334
00:18:10,630 --> 00:18:14,235
La dimension d'intégration est de 12 288, et la multiplication de ces 

335
00:18:14,235 --> 00:18:17,790
dimensions nous indique qu'il s'agit d'environ 617 millions de poids.

336
00:18:18,250 --> 00:18:20,474
Allons-y et ajoutons ceci à un décompte en cours, 

337
00:18:20,474 --> 00:18:23,810
en nous rappelant qu'à la fin, nous devrions compter jusqu'à 175 milliards.

338
00:18:25,430 --> 00:18:28,780
Dans le cas des transformateurs, tu dois vraiment penser que les vecteurs de 

339
00:18:28,780 --> 00:18:32,130
cet espace d'intégration ne représentent pas simplement des mots individuels.

340
00:18:32,550 --> 00:18:36,669
D'une part, ils encodent également des informations sur la position de ce mot, 

341
00:18:36,669 --> 00:18:39,172
ce dont nous parlerons plus tard, mais surtout, 

342
00:18:39,172 --> 00:18:42,770
tu dois considérer qu'ils ont la capacité de s'imprégner du contexte.

343
00:18:43,350 --> 00:18:47,320
Un vecteur qui a commencé sa vie comme l'intégration du mot roi, par exemple, 

344
00:18:47,320 --> 00:18:51,800
pourrait progressivement être tiré par divers blocs de ce réseau, de sorte qu'à la fin, 

345
00:18:51,800 --> 00:18:56,127
il pointe dans une direction beaucoup plus spécifique et nuancée qui code en quelque 

346
00:18:56,127 --> 00:18:58,875
sorte qu'il s'agissait d'un roi qui vivait en Écosse, 

347
00:18:58,875 --> 00:19:02,286
qui avait obtenu son poste après avoir assassiné le roi précédent, 

348
00:19:02,286 --> 00:19:04,730
et qui est décrit dans la langue de Shakespeare.

349
00:19:05,210 --> 00:19:07,790
Réfléchis à ta propre compréhension d'un mot donné.

350
00:19:08,250 --> 00:19:12,194
La signification de ce mot est clairement influencée par l'environnement, 

351
00:19:12,194 --> 00:19:14,807
et parfois cela inclut un contexte très éloigné, 

352
00:19:14,807 --> 00:19:19,071
donc en mettant en place un modèle qui a la capacité de prédire le mot suivant, 

353
00:19:19,071 --> 00:19:23,390
l'objectif est de le rendre capable d'incorporer le contexte de manière efficace.

354
00:19:24,050 --> 00:19:25,989
Pour être clair, dans cette toute première étape, 

355
00:19:25,989 --> 00:19:28,626
lorsque tu crées le tableau de vecteurs basé sur le texte d'entrée, 

356
00:19:28,626 --> 00:19:31,379
chacun d'entre eux est simplement extrait de la matrice d'intégration, 

357
00:19:31,379 --> 00:19:34,637
de sorte qu'au départ, chacun d'entre eux ne peut encoder que la signification d'un 

358
00:19:34,637 --> 00:19:36,770
seul mot sans aucune contribution de son environnement.

359
00:19:37,710 --> 00:19:41,416
Mais tu dois considérer que l'objectif premier de ce réseau par lequel il passe 

360
00:19:41,416 --> 00:19:45,170
est de permettre à chacun de ces vecteurs de s'imprégner d'un sens beaucoup plus 

361
00:19:45,170 --> 00:19:48,970
riche et spécifique que ce que de simples mots individuels pourraient représenter.

362
00:19:49,510 --> 00:19:52,867
Le réseau ne peut traiter qu'un nombre fixe de vecteurs à la fois, 

363
00:19:52,867 --> 00:19:54,170
appelé taille du contexte.

364
00:19:54,510 --> 00:19:57,621
Pour GPT-3, il a été formé avec une taille de contexte de 2048, 

365
00:19:57,621 --> 00:20:01,266
de sorte que les données qui circulent dans le réseau ressemblent toujours 

366
00:20:01,266 --> 00:20:05,010
à ce tableau de 2048 colonnes, chacune d'entre elles ayant 12 000 dimensions.

367
00:20:05,590 --> 00:20:08,633
Cette taille de contexte limite la quantité de texte que le 

368
00:20:08,633 --> 00:20:11,830
transformateur peut incorporer lorsqu'il prédit le mot suivant.

369
00:20:12,370 --> 00:20:15,036
C'est pourquoi les longues conversations avec certains chatbots, 

370
00:20:15,036 --> 00:20:18,276
comme les premières versions de ChatGPT, donnaient souvent l'impression que le 

371
00:20:18,276 --> 00:20:21,434
bot perdait en quelque sorte le fil de la conversation lorsque tu continuais 

372
00:20:21,434 --> 00:20:22,050
trop longtemps.

373
00:20:23,030 --> 00:20:25,631
Nous entrerons dans les détails de l'attention en temps voulu, 

374
00:20:25,631 --> 00:20:28,810
mais en passant, je veux parler un instant de ce qui se passe à la toute fin.

375
00:20:29,450 --> 00:20:32,159
Rappelle-toi que le résultat souhaité est une distribution de 

376
00:20:32,159 --> 00:20:34,870
probabilités sur tous les jetons qui pourraient venir ensuite.

377
00:20:35,170 --> 00:20:37,505
Par exemple, si le tout dernier mot est Professeur, 

378
00:20:37,505 --> 00:20:39,975
et que le contexte inclut des mots comme Harry Potter, 

379
00:20:39,975 --> 00:20:42,984
et qu'immédiatement avant nous voyons le professeur le moins aimé, 

380
00:20:42,984 --> 00:20:46,443
et aussi si tu me laisses une certaine marge de manœuvre en me permettant de 

381
00:20:46,443 --> 00:20:49,542
prétendre que les jetons ressemblent simplement à des mots complets, 

382
00:20:49,542 --> 00:20:52,820
alors un réseau bien entraîné qui a accumulé des connaissances sur Harry 

383
00:20:52,820 --> 00:20:55,830
Potter attribuerait vraisemblablement un nombre élevé au mot Rogue.

384
00:20:56,510 --> 00:20:57,970
Cela implique deux étapes différentes.

385
00:20:58,310 --> 00:21:02,882
La première consiste à utiliser une autre matrice qui fait correspondre le tout dernier 

386
00:21:02,882 --> 00:21:05,687
vecteur de ce contexte à une liste de 50 000 valeurs, 

387
00:21:05,687 --> 00:21:07,610
une pour chaque token du vocabulaire.

388
00:21:08,170 --> 00:21:12,044
Il y a ensuite une fonction qui normalise tout cela en une distribution de probabilité, 

389
00:21:12,044 --> 00:21:15,434
elle s'appelle Softmax et nous en parlerons plus en détail dans une seconde, 

390
00:21:15,434 --> 00:21:18,604
mais avant cela, il peut sembler un peu bizarre de n'utiliser que cette 

391
00:21:18,604 --> 00:21:21,642
dernière intégration pour faire une prédiction, alors qu'après tout, 

392
00:21:21,642 --> 00:21:24,855
dans cette dernière étape, il y a des milliers d'autres vecteurs dans la 

393
00:21:24,855 --> 00:21:28,290
couche qui sont juste là avec leurs propres significations riches en contexte.

394
00:21:28,930 --> 00:21:32,163
Cela s'explique par le fait qu'au cours du processus de formation, 

395
00:21:32,163 --> 00:21:35,830
il s'avère beaucoup plus efficace d'utiliser chacun de ces vecteurs dans la 

396
00:21:35,830 --> 00:21:39,304
couche finale pour faire simultanément une prédiction pour ce qui vient 

397
00:21:39,304 --> 00:21:40,270
immédiatement après.

398
00:21:40,970 --> 00:21:43,307
Il y aura beaucoup plus à dire sur la formation plus tard, 

399
00:21:43,307 --> 00:21:45,090
mais je veux juste le rappeler tout de suite.

400
00:21:45,730 --> 00:21:49,690
Cette matrice est appelée matrice de désencastrement et nous lui donnons l'étiquette WU.

401
00:21:50,210 --> 00:21:52,622
Encore une fois, comme toutes les matrices de poids que nous voyons, 

402
00:21:52,622 --> 00:21:55,455
ses entrées commencent au hasard, mais elles sont apprises au cours du processus 

403
00:21:55,455 --> 00:21:55,910
de formation.

404
00:21:56,470 --> 00:21:58,786
Pour garder le cap sur le nombre total de paramètres, 

405
00:21:58,786 --> 00:22:02,389
cette matrice de désencastrement comporte une ligne pour chaque mot du vocabulaire, 

406
00:22:02,389 --> 00:22:05,650
et chaque ligne a le même nombre d'éléments que la dimension d'encastrement.

407
00:22:06,410 --> 00:22:10,515
Elle ajoute donc 617 millions de paramètres supplémentaires au réseau, 

408
00:22:10,515 --> 00:22:15,372
ce qui signifie que nous en avons compté un peu plus d'un milliard jusqu'à présent, 

409
00:22:15,372 --> 00:22:18,609
une petite fraction, mais pas totalement insignifiante, 

410
00:22:18,609 --> 00:22:21,790
des 175 milliards que nous finirons par avoir au total.

411
00:22:22,550 --> 00:22:24,366
En tant que dernière mini-leçon de ce chapitre, 

412
00:22:24,366 --> 00:22:26,523
je veux parler plus en détail de cette fonction softmax, 

413
00:22:26,523 --> 00:22:29,209
puisqu'elle fait une autre apparition pour nous une fois que nous nous 

414
00:22:29,209 --> 00:22:30,610
plongeons dans les blocs d'attention.

415
00:22:31,430 --> 00:22:35,668
L'idée est que si tu veux qu'une séquence de nombres agisse comme une distribution de 

416
00:22:35,668 --> 00:22:39,661
probabilités, par exemple une distribution sur tous les mots suivants possibles, 

417
00:22:39,661 --> 00:22:42,273
alors chaque valeur doit être comprise entre 0 et 1, 

418
00:22:42,273 --> 00:22:44,590
et il faut aussi que leur somme soit égale à 1.

419
00:22:45,250 --> 00:22:48,484
Cependant, si tu joues le jeu de l'apprentissage où tout ce que tu 

420
00:22:48,484 --> 00:22:51,043
fais ressemble à une multiplication matrice-vecteur, 

421
00:22:51,043 --> 00:22:54,810
les résultats que tu obtiens par défaut ne respectent pas du tout cette règle.

422
00:22:55,330 --> 00:22:57,723
Les valeurs sont souvent négatives, ou beaucoup plus grandes que 1, 

423
00:22:57,723 --> 00:22:59,870
et il est presque certain que leur somme n'est pas égale à 1.

424
00:23:00,510 --> 00:23:04,148
Softmax est le moyen standard de transformer une liste arbitraire de nombres en 

425
00:23:04,148 --> 00:23:07,924
une distribution valide de telle sorte que les plus grandes valeurs se rapprochent 

426
00:23:07,924 --> 00:23:11,290
le plus de 1, et que les plus petites valeurs se rapprochent le plus de 0.

427
00:23:11,830 --> 00:23:13,070
C'est tout ce que tu as besoin de savoir.

428
00:23:13,090 --> 00:23:17,306
Mais si tu es curieux, la méthode consiste d'abord à élever e à la puissance de chacun 

429
00:23:17,306 --> 00:23:21,280
des nombres, ce qui signifie que tu as maintenant une liste de valeurs positives, 

430
00:23:21,280 --> 00:23:25,447
puis tu peux prendre la somme de toutes ces valeurs positives et diviser chaque terme 

431
00:23:25,447 --> 00:23:29,470
par cette somme, ce qui normalise le tout en une liste dont la somme est égale à 1.

432
00:23:30,170 --> 00:23:33,399
Tu remarqueras que si l'un des nombres de l'entrée est significativement 

433
00:23:33,399 --> 00:23:35,479
plus grand que le reste, alors dans la sortie, 

434
00:23:35,479 --> 00:23:39,240
le terme correspondant domine la distribution, de sorte que si tu l'échantillonnais, 

435
00:23:39,240 --> 00:23:42,470
tu choisirais presque certainement l'entrée qui maximise la distribution.

436
00:23:42,990 --> 00:23:46,544
Mais c'est plus doux que de simplement choisir le maximum dans le sens où lorsque 

437
00:23:46,544 --> 00:23:50,358
d'autres valeurs sont aussi importantes, elles ont également un poids significatif dans 

438
00:23:50,358 --> 00:23:54,129
la distribution, et tout change continuellement lorsque tu fais varier continuellement 

439
00:23:54,129 --> 00:23:54,650
les entrées.

440
00:23:55,130 --> 00:23:59,831
Dans certaines situations, comme lorsque ChatGPT utilise cette distribution pour créer 

441
00:23:59,831 --> 00:24:04,424
un mot suivant, il est possible de s'amuser un peu plus en ajoutant un peu de piment 

442
00:24:04,424 --> 00:24:08,910
à cette fonction, avec une constante t jetée dans le dénominateur de ces exposants.

443
00:24:09,550 --> 00:24:13,274
Nous l'appelons la température, car elle ressemble vaguement au rôle de la 

444
00:24:13,274 --> 00:24:16,005
température dans certaines équations thermodynamiques, 

445
00:24:16,005 --> 00:24:20,027
et l'effet est que lorsque t est plus grand, tu donnes plus de poids aux valeurs 

446
00:24:20,027 --> 00:24:23,752
inférieures, ce qui signifie que la distribution est un peu plus uniforme, 

447
00:24:23,752 --> 00:24:28,221
et si t est plus petit, alors les valeurs les plus grandes domineront plus agressivement, 

448
00:24:28,221 --> 00:24:31,995
où à l'extrême, en mettant t égal à zéro signifie que tout le poids va à la 

449
00:24:31,995 --> 00:24:32,790
valeur maximale.

450
00:24:33,470 --> 00:24:38,210
Par exemple, je demanderai à GPT-3 de générer une histoire avec le texte de départ, 

451
00:24:38,210 --> 00:24:42,950
il était une fois A, mais j'utiliserai des températures différentes dans chaque cas.

452
00:24:43,630 --> 00:24:48,413
La température zéro signifie qu'elle va toujours avec le mot le plus prévisible, 

453
00:24:48,413 --> 00:24:52,370
et ce que tu obtiens finit par être un dérivé banal de Boucle d'or.

454
00:24:53,010 --> 00:24:56,674
Une température plus élevée lui donne une chance de choisir des mots moins probables, 

455
00:24:56,674 --> 00:24:57,910
mais cela comporte un risque.

456
00:24:58,230 --> 00:25:01,258
Dans ce cas, l'histoire commence de façon plus originale, 

457
00:25:01,258 --> 00:25:03,816
à propos d'un jeune artiste web de Corée du Sud, 

458
00:25:03,816 --> 00:25:06,010
mais elle dégénère rapidement en non-sens.

459
00:25:06,950 --> 00:25:10,830
Techniquement parlant, l'API ne te permet pas de choisir une température supérieure à 2.

460
00:25:11,170 --> 00:25:15,138
Il n'y a aucune raison mathématique à cela, c'est juste une contrainte arbitraire 

461
00:25:15,138 --> 00:25:19,350
imposée pour éviter que leur outil ne soit vu comme générant des choses trop insensées.

462
00:25:19,870 --> 00:25:23,655
Si tu es curieux, cette animation fonctionne en fait de la façon suivante : 

463
00:25:23,655 --> 00:25:27,241
je prends les 20 prochains jetons les plus probables générés par GPT-3, 

464
00:25:27,241 --> 00:25:29,682
ce qui semble être le maximum qu'ils me donnent, 

465
00:25:29,682 --> 00:25:32,970
puis je modifie les probabilités en fonction d'un exposant de 1,5.

466
00:25:33,130 --> 00:25:37,419
Autre élément de jargon, de la même façon que tu pourrais appeler les composantes de 

467
00:25:37,419 --> 00:25:41,709
la sortie de cette fonction des probabilités, les gens appellent souvent les entrées 

468
00:25:41,709 --> 00:25:46,150
des logits, ou certains disent des logits, d'autres des logits, je vais dire des logits.

469
00:25:46,530 --> 00:25:48,629
Ainsi, par exemple, lorsque tu introduis un texte, 

470
00:25:48,629 --> 00:25:51,510
que tous ces enchâssements de mots circulent dans le réseau et que tu 

471
00:25:51,510 --> 00:25:54,351
fais cette multiplication finale avec la matrice de désencastrement, 

472
00:25:54,351 --> 00:25:57,438
les spécialistes de l'apprentissage automatique se réfèrent aux composants 

473
00:25:57,438 --> 00:26:00,319
de cette sortie brute, non normalisée, comme étant les logits pour la 

474
00:26:00,319 --> 00:26:01,390
prédiction du mot suivant.

475
00:26:03,330 --> 00:26:06,850
L'objectif de ce chapitre était en grande partie de jeter les bases de la 

476
00:26:06,850 --> 00:26:10,370
compréhension du mécanisme de l'attention, style Karate Kid cire sur cire.

477
00:26:10,850 --> 00:26:13,868
Tu vois, si tu as une forte intuition pour l'intégration de mots, 

478
00:26:13,868 --> 00:26:17,344
pour la méthode softmax, pour la façon dont les produits de points mesurent 

479
00:26:17,344 --> 00:26:20,821
la similarité, et aussi le principe sous-jacent selon lequel la plupart des 

480
00:26:20,821 --> 00:26:24,342
calculs doivent ressembler à une multiplication de matrice avec des matrices 

481
00:26:24,342 --> 00:26:28,230
pleines de paramètres réglables, alors la compréhension du mécanisme de l'attention, 

482
00:26:28,230 --> 00:26:32,210
cette pièce maîtresse de tout le boom moderne de l'IA, devrait être relativement aisée.

483
00:26:32,650 --> 00:26:34,510
Pour cela, viens me rejoindre dans le prochain chapitre.

484
00:26:36,390 --> 00:26:38,671
À l'heure où je publie ces lignes, une ébauche de ce prochain 

485
00:26:38,671 --> 00:26:41,210
chapitre est disponible pour examen par les sympathisants de Patreon.

486
00:26:41,770 --> 00:26:44,305
Une version finale devrait être publiée d'ici une semaine ou deux, 

487
00:26:44,305 --> 00:26:47,370
cela dépend généralement de ce que je vais changer en fonction de cette révision.

488
00:26:47,810 --> 00:26:49,970
En attendant, si tu veux te plonger dans l'attention, 

489
00:26:49,970 --> 00:26:52,410
et si tu veux aider un peu la chaîne, elle est là à attendre.

