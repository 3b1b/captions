[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Les initiales GPT signifient Generative Pretrained Transformer (transformateur génératif préformé).",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Ce premier mot est donc assez simple, ce sont des bots qui génèrent de nouveaux textes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Le préfixe indique que le modèle a été soumis à un processus d'apprentissage à partir d'une quantité massive de données, et ce préfixe insinue qu'il y a plus de place pour l'affiner sur des tâches spécifiques avec un entraînement supplémentaire.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Mais le dernier mot, c'est la vraie pièce maîtresse.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Un transformateur est un type spécifique de réseau neuronal, un modèle d'apprentissage automatique, et c'est l'invention centrale qui sous-tend l'essor actuel de l'IA.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Ce que je veux faire avec cette vidéo et les chapitres suivants, c'est expliquer visuellement ce qui se passe réellement à l'intérieur d'un transformateur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Nous allons suivre les données qui y circulent et procéder étape par étape.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Il existe de nombreuses sortes de modèles que tu peux construire à l'aide de transformateurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Certains modèles prennent des données audio et produisent une transcription.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Cette phrase provient d'un modèle qui va dans l'autre sens, en produisant un discours synthétique uniquement à partir d'un texte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Tous ces outils qui ont pris le monde d'assaut en 2022 comme Dolly et Midjourney qui prennent en compte une description textuelle et produisent une image sont basés sur des transformateurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Même si je n'arrive pas à lui faire comprendre ce qu'est censée être la créature Pi, je suis toujours époustouflée de voir que ce genre de chose est possible.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "Et le transformateur original introduit en 2017 par Google a été inventé pour le cas d'utilisation spécifique de la traduction de texte d'une langue à une autre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Mais la variante sur laquelle toi et moi allons nous concentrer, et qui est le type d'outil qui sous-tend des outils comme ChatGPT, sera un modèle qui est entraîné à prendre un morceau de texte, peut-être même avec des images ou des sons qui l'accompagnent, et à produire une prédiction pour ce qui vient ensuite dans le passage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Cette prédiction prend la forme d'une distribution de probabilités sur de nombreux morceaux de texte différents qui pourraient suivre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "À première vue, tu pourrais penser que prédire le mot suivant semble être un objectif très différent de celui de générer un nouveau texte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Mais une fois que tu as un modèle de prédiction comme celui-ci, une façon simple de générer un texte plus long est de lui donner un extrait initial avec lequel travailler, de lui demander de prendre un échantillon aléatoire de la distribution qu'il vient de générer, d'ajouter cet échantillon au texte, puis de relancer tout le processus pour faire une nouvelle prédiction basée sur tout le nouveau texte, y compris ce qu'il vient d'ajouter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Je ne sais pas ce qu'il en est pour toi, mais je n'ai vraiment pas l'impression que cela devrait fonctionner.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Dans cette animation, par exemple, j'exécute GPT-2 sur mon ordinateur portable et je lui demande de prédire et d'échantillonner de façon répétée le prochain morceau de texte pour générer une histoire basée sur le texte de départ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "L'histoire n'a pas vraiment de sens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Mais si je le remplace par des appels d'API à GPT-3, qui est le même modèle de base, juste beaucoup plus grand, soudain, presque par magie, nous obtenons une histoire sensée, qui semble même déduire qu'une créature pi vivrait dans un pays de maths et de calculs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Ce processus de prédiction et d'échantillonnage répétés est essentiellement ce qui se passe lorsque tu interagis avec ChatGPT ou tout autre grand modèle de langage et que tu les vois produire un mot à la fois.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "En fait, une fonction que j'apprécierais beaucoup est la possibilité de voir la distribution sous-jacente pour chaque nouveau mot qu'il choisit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Commençons par un aperçu de très haut niveau de la façon dont les données circulent dans un transformateur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Nous passerons beaucoup plus de temps à motiver, interpréter et développer les détails de chaque étape, mais dans les grandes lignes, lorsqu'un de ces chatbots génère un mot donné, voici ce qui se passe sous le capot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Tout d'abord, l'entrée est décomposée en un tas de petits morceaux.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Ces morceaux sont appelés jetons, et dans le cas d'un texte, il s'agit généralement de mots ou de petits morceaux de mots ou d'autres combinaisons de caractères courantes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "S'il s'agit d'images ou de sons, les jetons peuvent être de petites parties de cette image ou de petits morceaux de ce son.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Chacun de ces jetons est ensuite associé à un vecteur, c'est-à-dire à une liste de nombres, qui est censé coder d'une manière ou d'une autre la signification de cette pièce.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Si tu considères que ces vecteurs donnent des coordonnées dans un espace à très haute dimension, les mots ayant des significations similaires ont tendance à atterrir sur des vecteurs qui sont proches les uns des autres dans cet espace.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Cette séquence de vecteurs passe ensuite par une opération connue sous le nom de bloc d'attention, ce qui permet aux vecteurs de se parler et de se transmettre des informations pour mettre à jour leurs valeurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Par exemple, le sens du mot modèle dans l'expression un modèle d'apprentissage automatique est différent de son sens dans l'expression un modèle de mode.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Le bloc d'attention est chargé de déterminer quels mots du contexte sont pertinents pour mettre à jour la signification de quels autres mots, et comment exactement ces significations doivent être mises à jour.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "Et encore une fois, chaque fois que j'utilise le mot sens, celui-ci est en quelque sorte entièrement codé dans les entrées de ces vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Ensuite, ces vecteurs passent par un autre type d'opération, et selon la source que tu lis, on parlera d'un perceptron multicouche ou peut-être d'une couche d'anticipation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "Et ici, les vecteurs ne communiquent pas entre eux, ils subissent tous la même opération en parallèle.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Et bien que ce bloc soit un peu plus difficile à interpréter, nous verrons plus loin que cette étape revient un peu à poser une longue liste de questions sur chaque vecteur, puis à les mettre à jour en fonction des réponses à ces questions.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Toutes les opérations dans ces deux blocs ressemblent à une pile géante de multiplications de matrices, et notre travail principal va consister à comprendre comment lire les matrices sous-jacentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "J'omets certains détails concernant les étapes de normalisation qui se déroulent entre les deux, mais il s'agit après tout d'un aperçu de haut niveau.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Après cela, le processus se répète essentiellement, tu vas et viens entre les blocs d'attention et les blocs de perceptron multicouche, jusqu'à ce qu'à la toute fin, l'espoir est que toute la signification essentielle du passage a été en quelque sorte incorporée dans le tout dernier vecteur de la séquence.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Nous effectuons ensuite une certaine opération sur ce dernier vecteur qui produit une distribution de probabilité sur tous les tokens possibles, tous les petits morceaux de texte possibles qui pourraient venir ensuite.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "Et comme je l'ai dit, une fois que tu as un outil qui prédit ce qui vient après un extrait de texte, tu peux lui donner un peu de texte de départ et le faire jouer à plusieurs reprises à ce jeu de prédiction de ce qui vient après, d'échantillonnage de la distribution, d'ajout, et de répétition encore et encore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Certains d'entre vous se souviennent peut-être que bien avant que ChatGPT n'entre en scène, voici à quoi ressemblaient les premières démonstrations de GPT-3, qui permettaient de compléter automatiquement des histoires et des essais à partir d'un extrait initial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Pour transformer un outil comme celui-ci en chatbot, le point de départ le plus simple est d'avoir un petit bout de texte qui établit le cadre d'une interaction entre un utilisateur et un assistant IA utile, ce que tu appellerais l'invite du système, puis tu utiliserais la question ou l'invite initiale de l'utilisateur comme premier bout de dialogue, puis tu ferais en sorte qu'il commence à prédire ce qu'un assistant IA aussi utile dirait en réponse.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Il y a plus à dire sur l'étape de formation nécessaire pour que cela fonctionne bien, mais à un niveau élevé, c'est l'idée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "Dans ce chapitre, toi et moi allons nous étendre sur les détails de ce qui se passe au tout début du réseau, à la toute fin du réseau, et je veux aussi passer beaucoup de temps à revoir certains éléments importants de connaissances de base, des choses qui auraient été une seconde nature pour n'importe quel ingénieur en apprentissage automatique au moment où les transformateurs sont apparus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Si tu te sens à l'aise avec ces connaissances de base et que tu es un peu impatient, sens-toi libre de passer au chapitre suivant, qui va se concentrer sur les blocs d'attention, généralement considérés comme le cœur du transformateur.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Après cela, je veux parler davantage de ces blocs de perceptron multicouche, du fonctionnement de l'entraînement et d'un certain nombre d'autres détails qui auront été sautés jusqu'à ce point.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Pour un contexte plus large, ces vidéos sont des ajouts à une mini-série sur l'apprentissage profond, et ce n'est pas grave si tu n'as pas regardé les précédentes, je pense que tu peux le faire dans le désordre, mais avant de plonger dans les transformateurs en particulier, je pense qu'il vaut la peine de s'assurer que nous sommes sur la même longueur d'onde en ce qui concerne le principe de base et la structure de l'apprentissage profond.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Au risque d'énoncer l'évidence, il s'agit d'une approche de l'apprentissage automatique, qui décrit tout modèle dans lequel tu utilises des données pour déterminer d'une manière ou d'une autre le comportement d'un modèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Ce que je veux dire par là, c'est que tu veux une fonction qui prend une image et qui produit une étiquette la décrivant, ou notre exemple de prédiction du mot suivant dans un passage de texte, ou toute autre tâche qui semble nécessiter un certain élément d'intuition et de reconnaissance des formes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Nous considérons presque cela comme acquis de nos jours, mais l'idée de l'apprentissage automatique est qu'au lieu d'essayer de définir explicitement une procédure pour effectuer cette tâche dans le code, ce que les gens auraient fait dans les premiers jours de l'IA, tu mets en place une structure très flexible avec des paramètres réglables, comme un tas de boutons et de cadrans, et ensuite, d'une manière ou d'une autre, tu utilises de nombreux exemples de ce à quoi la sortie devrait ressembler pour une entrée donnée, pour ajuster et régler les valeurs de ces paramètres afin d'imiter ce comportement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Par exemple, la forme la plus simple d'apprentissage automatique est peut-être la régression linéaire, où tes entrées et tes sorties sont chacune des nombres uniques, quelque chose comme la superficie d'une maison et son prix, et ce que tu veux, c'est trouver une ligne de meilleur ajustement à travers ces données, tu sais, pour prédire les prix futurs des maisons.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Cette ligne est décrite par deux paramètres continus, à savoir la pente et l'ordonnée à l'origine, et l'objectif de la régression linéaire est de déterminer ces paramètres pour qu'ils correspondent étroitement aux données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Inutile de dire que les modèles d'apprentissage profond se complexifient énormément.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "Le GPT-3, par exemple, n'a pas deux, mais 175 milliards de paramètres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Mais voilà, il n'est pas évident que tu puisses créer un modèle géant avec un grand nombre de paramètres sans qu'il ne surajoute grossièrement les données d'apprentissage ou qu'il ne soit complètement impossible à former.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "L'apprentissage profond décrit une classe de modèles qui, au cours des deux dernières décennies, se sont révélés remarquablement évolutifs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Ce qui les unit, c'est le même algorithme d'apprentissage, appelé rétropropagation, et le contexte que je veux que tu aies au fur et à mesure que nous avançons, c'est que pour que cet algorithme d'apprentissage fonctionne bien à l'échelle, ces modèles doivent suivre un certain format spécifique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Si tu connais ce format au départ, cela permet d'expliquer un grand nombre des choix de traitement de la langue par un transformateur, qui risquent sinon de sembler arbitraires.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Tout d'abord, quel que soit le modèle que tu fais, l'entrée doit être formatée comme un tableau de nombres réels.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Il peut s'agir d'une liste de nombres, d'un tableau à deux dimensions ou, très souvent, de tableaux à plus haute dimension, pour lesquels le terme général utilisé est tenseur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Tu considères souvent que ces données d'entrée sont progressivement transformées en plusieurs couches distinctes, où chaque couche est toujours structurée comme une sorte de tableau de nombres réels, jusqu'à ce que tu arrives à une couche finale que tu considères comme la sortie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Par exemple, la dernière couche de notre modèle de traitement de texte est une liste de nombres représentant la distribution de probabilité pour tous les prochains tokens possibles.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "Dans l'apprentissage profond, ces paramètres de modèle sont presque toujours appelés poids, et ce parce qu'une caractéristique clé de ces modèles est que la seule façon dont ces paramètres interagissent avec les données traitées est par le biais de sommes pondérées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Tu saupoudres également quelques fonctions non linéaires, mais elles ne dépendent pas de paramètres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Généralement, au lieu de voir les sommes pondérées toutes nues et écrites explicitement comme ceci, tu les trouveras plutôt regroupées sous forme de divers composants dans un produit vectoriel matriciel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Cela revient à dire la même chose, si tu repenses à la façon dont fonctionne la multiplication vectorielle matricielle, chaque composant de la sortie ressemble à une somme pondérée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Pour toi et moi, il est souvent plus simple, d'un point de vue conceptuel, de penser à des matrices remplies de paramètres réglables qui transforment les vecteurs tirés des données en cours de traitement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Par exemple, les 175 milliards de poids du GPT-3 sont organisés en un peu moins de 28 000 matrices distinctes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Ces matrices se répartissent à leur tour en huit catégories différentes, et ce que nous allons faire, toi et moi, c'est passer en revue chacune de ces catégories pour comprendre ce que fait ce type de matrice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Au fur et à mesure que nous avançons, je pense qu'il est amusant de se référer aux chiffres spécifiques de GPT-3 pour compter exactement d'où viennent ces 175 milliards.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Même s'il existe aujourd'hui des modèles plus grands et meilleurs, celui-ci a un certain charme en tant que modèle de grande langue pour vraiment capter l'attention du monde en dehors des communautés ML.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "De plus, d'un point de vue pratique, les entreprises ont tendance à garder les lèvres beaucoup plus serrées sur les chiffres spécifiques des réseaux plus modernes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Je veux juste te montrer que lorsque tu regardes sous le capot pour voir ce qui se passe à l'intérieur d'un outil comme ChatGPT, presque tous les calculs ressemblent à des multiplications de matrices et de vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Il y a un petit risque de se perdre dans la mer de milliards de chiffres, mais tu dois faire une distinction très nette dans ton esprit entre les poids du modèle, que je colorerai toujours en bleu ou en rouge, et les données traitées, que je colorerai toujours en gris.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Les poids sont les cerveaux réels, ce sont les choses apprises pendant l'entraînement, et ils déterminent son comportement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Les données traitées codent simplement l'entrée spécifique introduite dans le modèle pour une exécution donnée, comme un exemple de bout de texte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Avec tout cela comme base, entrons dans la première étape de cet exemple de traitement de texte, qui consiste à diviser l'entrée en petits morceaux et à transformer ces morceaux en vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "J'ai mentionné que ces morceaux sont appelés des jetons, qui peuvent être des morceaux de mots ou de ponctuation, mais de temps en temps, dans ce chapitre et surtout dans le prochain, j'aimerais faire semblant que c'est divisé plus proprement en mots.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Comme nous, les humains, pensons avec des mots, cela facilitera grandement la référence à de petits exemples et la clarification de chaque étape.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Le modèle possède un vocabulaire prédéfini, une liste de tous les mots possibles, disons 50 000 d'entre eux, et la première matrice que nous rencontrerons, appelée matrice d'intégration, comporte une seule colonne pour chacun de ces mots.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Ce sont ces colonnes qui déterminent le vecteur en lequel chaque mot se transforme dans cette première étape.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Nous l'appelons We, et comme toutes les matrices que nous voyons, ses valeurs commencent au hasard, mais elles vont être apprises en fonction des données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Transformer des mots en vecteurs était une pratique courante en apprentissage automatique bien avant les transformateurs, mais c'est un peu bizarre si tu ne l'as jamais vu auparavant, et cela pose les bases de tout ce qui suit, alors prenons un moment pour nous familiariser avec.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Nous appelons souvent cet encastrement un mot, ce qui t'invite à considérer ces vecteurs de façon très géométrique comme des points dans un espace à haute dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Visualiser une liste de trois nombres comme des coordonnées de points dans l'espace 3D ne poserait aucun problème, mais les enchâssements de mots ont tendance à être beaucoup plus dimensionnels.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "Dans GPT-3, il y a 12 288 dimensions, et comme tu le verras, il est important de travailler dans un espace qui a beaucoup de directions distinctes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "De la même façon que tu peux prendre une tranche bidimensionnelle dans un espace 3D et projeter tous les points sur cette tranche, pour animer les enchâssements de mots qu'un modèle simple me donne, je vais faire une chose analogue en choisissant une tranche tridimensionnelle dans cet espace à très haute dimension, et en projetant les vecteurs de mots vers le bas sur cette tranche et en affichant les résultats.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "L'idée principale ici est qu'au fur et à mesure qu'un modèle ajuste et règle ses poids pour déterminer comment les mots sont intégrés sous forme de vecteurs au cours de la formation, il tend à se fixer sur un ensemble d'intégrations où les directions dans l'espace ont une sorte de signification sémantique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Pour le modèle simple mot-vecteur que j'utilise ici, si je lance une recherche pour tous les mots dont l'intégration est la plus proche de celle de tour, tu remarqueras qu'ils semblent tous donner une impression de tour très similaire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Et si tu veux utiliser Python et t'amuser à la maison, voici le modèle spécifique que j'utilise pour faire les animations.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Ce n'est pas un transformateur, mais c'est suffisant pour illustrer l'idée que les directions dans l'espace peuvent être porteuses de sens sémantique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Un exemple très classique de ceci est la différence entre les vecteurs de la femme et de l'homme, quelque chose que tu visualiserais comme un petit vecteur reliant la pointe de l'un à la pointe de l'autre, c'est très similaire à la différence entre le roi et la reine.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Supposons donc que tu ne connaisses pas le mot désignant une femme monarque, tu pourrais le trouver en prenant roi, en ajoutant cette direction femme-homme et en recherchant les encastrements les plus proches de ce point.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Du moins, en quelque sorte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Bien qu'il s'agisse d'un exemple classique pour le modèle avec lequel je joue, l'intégration réelle de la reine est en fait un peu plus éloignée que ce qui est suggéré, probablement parce que la façon dont la reine est utilisée dans les données de formation n'est pas simplement une version féminine du roi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Lorsque je me suis amusé, les relations familiales semblaient illustrer beaucoup mieux l'idée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Le fait est qu'il semble que pendant la formation, le modèle a trouvé avantageux de choisir des encastrements tels qu'une direction dans cet espace encode des informations sur le sexe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Un autre exemple est que si tu prends l'encastrement de l'Italie, que tu soustrais l'encastrement de l'Allemagne, et que tu ajoutes cela à l'encastrement d'Hitler, tu obtiens quelque chose de très proche de l'encastrement de Mussolini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "C'est comme si le modèle avait appris à associer certaines directions à l'italianité, et d'autres aux dirigeants de l'axe de la Seconde Guerre mondiale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Mon exemple préféré dans cette veine est peut-être la façon dont, dans certains modèles, si tu prends la différence entre l'Allemagne et le Japon, et que tu l'ajoutes aux sushis, tu te retrouves très proche de la saucisse bratwurst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "En jouant à ce jeu de recherche des voisins les plus proches, j'ai également été heureuse de voir à quel point Kat était proche à la fois de la bête et du monstre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Une intuition mathématique qu'il est utile d'avoir à l'esprit, en particulier pour le chapitre suivant, est que le produit en points de deux vecteurs peut être considéré comme un moyen de mesurer leur degré d'alignement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "D'un point de vue informatique, les produits points impliquent la multiplication de tous les composants correspondants et l'addition des résultats, ce qui est une bonne chose, puisqu'une grande partie de nos calculs doit ressembler à des sommes pondérées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Géométriquement, le produit du point est positif lorsque les vecteurs pointent dans des directions similaires, il est nul s'ils sont perpendiculaires et il est négatif lorsqu'ils pointent dans des directions opposées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Par exemple, disons que tu joues avec ce modèle, et que tu émets l'hypothèse que l'intégration de chats moins chat pourrait représenter une sorte de direction de la pluralité dans cet espace.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Pour tester cela, je vais prendre ce vecteur et calculer son produit point par rapport aux embeddings de certains noms singuliers, et le comparer aux produits points avec les noms pluriels correspondants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Si tu t'amuses avec ça, tu remarqueras que les pluriels semblent en effet donner systématiquement des valeurs plus élevées que les singuliers, ce qui indique qu'ils s'alignent davantage sur cette direction.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "C'est aussi amusant de voir que si tu fais le produit de ce point avec l'intégration des mots 1, 2, 3, et ainsi de suite, ils donnent des valeurs croissantes, c'est comme si nous pouvions mesurer quantitativement à quel point le modèle trouve un mot donné pluriel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Encore une fois, les spécificités de la façon dont les mots sont intégrés sont apprises à l'aide de données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Cette matrice d'intégration, dont les colonnes nous indiquent ce qu'il advient de chaque mot, constitue la première pile de poids de notre modèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "En utilisant les chiffres du GPT-3, la taille du vocabulaire est spécifiquement de 50 257, et encore une fois, techniquement, il ne s'agit pas de mots en tant que tels, mais de jetons.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "La dimension d'intégration est de 12 288, et la multiplication de ces dimensions nous indique qu'il s'agit d'environ 617 millions de poids.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Allons-y et ajoutons ceci à un décompte en cours, en nous rappelant qu'à la fin, nous devrions compter jusqu'à 175 milliards.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "Dans le cas des transformateurs, tu dois vraiment penser que les vecteurs de cet espace d'intégration ne représentent pas simplement des mots individuels.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "D'une part, ils encodent également des informations sur la position de ce mot, ce dont nous parlerons plus tard, mais surtout, tu dois considérer qu'ils ont la capacité de s'imprégner du contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Un vecteur qui a commencé sa vie comme l'intégration du mot roi, par exemple, pourrait progressivement être tiré par divers blocs de ce réseau, de sorte qu'à la fin, il pointe dans une direction beaucoup plus spécifique et nuancée qui code en quelque sorte qu'il s'agissait d'un roi qui vivait en Écosse, qui avait obtenu son poste après avoir assassiné le roi précédent, et qui est décrit dans la langue de Shakespeare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Réfléchis à ta propre compréhension d'un mot donné.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "La signification de ce mot est clairement influencée par l'environnement, et parfois cela inclut un contexte très éloigné, donc en mettant en place un modèle qui a la capacité de prédire le mot suivant, l'objectif est de le rendre capable d'incorporer le contexte de manière efficace.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Pour être clair, dans cette toute première étape, lorsque tu crées le tableau de vecteurs basé sur le texte d'entrée, chacun d'entre eux est simplement extrait de la matrice d'intégration, de sorte qu'au départ, chacun d'entre eux ne peut encoder que la signification d'un seul mot sans aucune contribution de son environnement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Mais tu dois considérer que l'objectif premier de ce réseau par lequel il passe est de permettre à chacun de ces vecteurs de s'imprégner d'un sens beaucoup plus riche et spécifique que ce que de simples mots individuels pourraient représenter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Le réseau ne peut traiter qu'un nombre fixe de vecteurs à la fois, appelé taille du contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Pour GPT-3, il a été formé avec une taille de contexte de 2048, de sorte que les données qui circulent dans le réseau ressemblent toujours à ce tableau de 2048 colonnes, chacune d'entre elles ayant 12 000 dimensions.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Cette taille de contexte limite la quantité de texte que le transformateur peut incorporer lorsqu'il prédit le mot suivant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "C'est pourquoi les longues conversations avec certains chatbots, comme les premières versions de ChatGPT, donnaient souvent l'impression que le bot perdait en quelque sorte le fil de la conversation lorsque tu continuais trop longtemps.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Nous entrerons dans les détails de l'attention en temps voulu, mais en passant, je veux parler un instant de ce qui se passe à la toute fin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Rappelle-toi que le résultat souhaité est une distribution de probabilités sur tous les jetons qui pourraient venir ensuite.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Par exemple, si le tout dernier mot est Professeur, et que le contexte inclut des mots comme Harry Potter, et qu'immédiatement avant nous voyons le professeur le moins aimé, et aussi si tu me laisses une certaine marge de manœuvre en me permettant de prétendre que les jetons ressemblent simplement à des mots complets, alors un réseau bien entraîné qui a accumulé des connaissances sur Harry Potter attribuerait vraisemblablement un nombre élevé au mot Rogue.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Cela implique deux étapes différentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "La première consiste à utiliser une autre matrice qui fait correspondre le tout dernier vecteur de ce contexte à une liste de 50 000 valeurs, une pour chaque token du vocabulaire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Il y a ensuite une fonction qui normalise tout cela en une distribution de probabilité, elle s'appelle Softmax et nous en parlerons plus en détail dans une seconde, mais avant cela, il peut sembler un peu bizarre de n'utiliser que cette dernière intégration pour faire une prédiction, alors qu'après tout, dans cette dernière étape, il y a des milliers d'autres vecteurs dans la couche qui sont juste là avec leurs propres significations riches en contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Cela s'explique par le fait qu'au cours du processus de formation, il s'avère beaucoup plus efficace d'utiliser chacun de ces vecteurs dans la couche finale pour faire simultanément une prédiction pour ce qui vient immédiatement après.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Il y aura beaucoup plus à dire sur la formation plus tard, mais je veux juste le rappeler tout de suite.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Cette matrice est appelée matrice de désencastrement et nous lui donnons l'étiquette WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Encore une fois, comme toutes les matrices de poids que nous voyons, ses entrées commencent au hasard, mais elles sont apprises au cours du processus de formation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Pour garder le cap sur le nombre total de paramètres, cette matrice de désencastrement comporte une ligne pour chaque mot du vocabulaire, et chaque ligne a le même nombre d'éléments que la dimension d'encastrement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Elle ajoute donc 617 millions de paramètres supplémentaires au réseau, ce qui signifie que nous en avons compté un peu plus d'un milliard jusqu'à présent, une petite fraction, mais pas totalement insignifiante, des 175 milliards que nous finirons par avoir au total.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "En tant que dernière mini-leçon de ce chapitre, je veux parler plus en détail de cette fonction softmax, puisqu'elle fait une autre apparition pour nous une fois que nous nous plongeons dans les blocs d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "L'idée est que si tu veux qu'une séquence de nombres agisse comme une distribution de probabilités, par exemple une distribution sur tous les mots suivants possibles, alors chaque valeur doit être comprise entre 0 et 1, et il faut aussi que leur somme soit égale à 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Cependant, si tu joues le jeu de l'apprentissage où tout ce que tu fais ressemble à une multiplication matrice-vecteur, les résultats que tu obtiens par défaut ne respectent pas du tout cette règle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Les valeurs sont souvent négatives, ou beaucoup plus grandes que 1, et il est presque certain que leur somme n'est pas égale à 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax est le moyen standard de transformer une liste arbitraire de nombres en une distribution valide de telle sorte que les plus grandes valeurs se rapprochent le plus de 1, et que les plus petites valeurs se rapprochent le plus de 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "C'est tout ce que tu as besoin de savoir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Mais si tu es curieux, la méthode consiste d'abord à élever e à la puissance de chacun des nombres, ce qui signifie que tu as maintenant une liste de valeurs positives, puis tu peux prendre la somme de toutes ces valeurs positives et diviser chaque terme par cette somme, ce qui normalise le tout en une liste dont la somme est égale à 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Tu remarqueras que si l'un des nombres de l'entrée est significativement plus grand que le reste, alors dans la sortie, le terme correspondant domine la distribution, de sorte que si tu l'échantillonnais, tu choisirais presque certainement l'entrée qui maximise la distribution.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Mais c'est plus doux que de simplement choisir le maximum dans le sens où lorsque d'autres valeurs sont aussi importantes, elles ont également un poids significatif dans la distribution, et tout change continuellement lorsque tu fais varier continuellement les entrées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "Dans certaines situations, comme lorsque ChatGPT utilise cette distribution pour créer un mot suivant, il est possible de s'amuser un peu plus en ajoutant un peu de piment à cette fonction, avec une constante t jetée dans le dénominateur de ces exposants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Nous l'appelons la température, car elle ressemble vaguement au rôle de la température dans certaines équations thermodynamiques, et l'effet est que lorsque t est plus grand, tu donnes plus de poids aux valeurs inférieures, ce qui signifie que la distribution est un peu plus uniforme, et si t est plus petit, alors les valeurs les plus grandes domineront plus agressivement, où à l'extrême, en mettant t égal à zéro signifie que tout le poids va à la valeur maximale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Par exemple, je demanderai à GPT-3 de générer une histoire avec le texte de départ, il était une fois A, mais j'utiliserai des températures différentes dans chaque cas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "La température zéro signifie qu'elle va toujours avec le mot le plus prévisible, et ce que tu obtiens finit par être un dérivé banal de Boucle d'or.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Une température plus élevée lui donne une chance de choisir des mots moins probables, mais cela comporte un risque.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "Dans ce cas, l'histoire commence de façon plus originale, à propos d'un jeune artiste web de Corée du Sud, mais elle dégénère rapidement en non-sens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Techniquement parlant, l'API ne te permet pas de choisir une température supérieure à 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Il n'y a aucune raison mathématique à cela, c'est juste une contrainte arbitraire imposée pour éviter que leur outil ne soit vu comme générant des choses trop insensées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Si tu es curieux, cette animation fonctionne en fait de la façon suivante : je prends les 20 prochains jetons les plus probables générés par GPT-3, ce qui semble être le maximum qu'ils me donnent, puis je modifie les probabilités en fonction d'un exposant de 1,5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Autre élément de jargon, de la même façon que tu pourrais appeler les composantes de la sortie de cette fonction des probabilités, les gens appellent souvent les entrées des logits, ou certains disent des logits, d'autres des logits, je vais dire des logits.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Ainsi, par exemple, lorsque tu introduis un texte, que tous ces enchâssements de mots circulent dans le réseau et que tu fais cette multiplication finale avec la matrice de désencastrement, les spécialistes de l'apprentissage automatique se réfèrent aux composants de cette sortie brute, non normalisée, comme étant les logits pour la prédiction du mot suivant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "L'objectif de ce chapitre était en grande partie de jeter les bases de la compréhension du mécanisme de l'attention, style Karate Kid cire sur cire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Tu vois, si tu as une forte intuition pour l'intégration de mots, pour la méthode softmax, pour la façon dont les produits de points mesurent la similarité, et aussi le principe sous-jacent selon lequel la plupart des calculs doivent ressembler à une multiplication de matrice avec des matrices pleines de paramètres réglables, alors la compréhension du mécanisme de l'attention, cette pièce maîtresse de tout le boom moderne de l'IA, devrait être relativement aisée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Pour cela, viens me rejoindre dans le prochain chapitre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "À l'heure où je publie ces lignes, une ébauche de ce prochain chapitre est disponible pour examen par les sympathisants de Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Une version finale devrait être publiée d'ici une semaine ou deux, cela dépend généralement de ce que je vais changer en fonction de cette révision.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "En attendant, si tu veux te plonger dans l'attention, et si tu veux aider un peu la chaîne, elle est là à attendre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
