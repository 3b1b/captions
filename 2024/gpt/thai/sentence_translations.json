[
 {
  "translatedText": "ชื่อย่อ GPT ย่อมาจาก Generative Pretrained Transformer",
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "translatedText": "เพื่อให้คำแรกนั้นตรงไปตรงมาเพียงพอ สิ่งเหล่านี้คือบอทที่สร้างข้อความใหม่",
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "translatedText": "การฝึกอบรมล่วงหน้าหมายถึงวิธีที่โมเดลผ่านกระบวนการเรียนรู้จากข้อมูลจำนวนมหาศาล และคำนำหน้าบ่งบอกว่ามีพื้นที่มากขึ้นในการปรับแต่งงานเฉพาะด้วยการฝึกอบรมเพิ่มเติม",
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "translatedText": "แต่คำสุดท้ายนั่นคือส่วนสำคัญที่แท้จริง",
  "input": "But the last word, that's the real key piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "translatedText": "หม้อแปลงไฟฟ้าเป็นโครงข่ายประสาทเทียมชนิดหนึ่ง ซึ่งเป็นโมเดลการเรียนรู้ของเครื่องจักร และเป็นสิ่งประดิษฐ์หลักที่เป็นรากฐานของความเจริญรุ่งเรืองใน AI ในปัจจุบัน",
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "translatedText": "สิ่งที่ฉันต้องการทำกับวิดีโอนี้และบทต่อไปนี้คือคำอธิบายที่ขับเคลื่อนด้วยภาพสำหรับสิ่งที่เกิดขึ้นจริงภายในหม้อแปลงไฟฟ้า",
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "translatedText": "เราจะติดตามข้อมูลที่ไหลผ่านและทีละขั้นตอน",
  "input": "We're going to follow the data that flows through it and go step by step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "translatedText": "มีโมเดลหลายประเภทที่คุณสามารถสร้างโดยใช้หม้อแปลงไฟฟ้า",
  "input": "There are many different kinds of models that you can build using transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "translatedText": "บางรุ่นใช้เสียงและสร้างการถอดเสียง",
  "input": "Some models take in audio and produce a transcript.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "translatedText": "ประโยคนี้มาจากแบบจำลองที่หันไปทางอื่น โดยสร้างคำพูดสังเคราะห์จากข้อความเท่านั้น",
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "translatedText": "เครื่องมือทั้งหมดที่ทำให้โลกต้องตะลึงในปี 2022 เช่น Dolly และ Midjourney ที่ใส่คำอธิบายและสร้างรูปภาพนั้นอิงจากหม้อแปลงไฟฟ้า",
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "translatedText": "แม้ว่าฉันจะไม่เข้าใจว่าสิ่งมีชีวิตพายควรจะเป็นอย่างไร แต่ฉันก็ยังทึ่งที่เรื่องแบบนี้เป็นไปได้แม้จะอยู่ห่างไกลก็ตาม",
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "translatedText": "และหม้อแปลงไฟฟ้าต้นฉบับที่ Google เปิดตัวในปี 2560 ได้รับการประดิษฐ์ขึ้นเพื่อกรณีการใช้งานเฉพาะในการแปลข้อความจากภาษาหนึ่งเป็นอีกภาษาหนึ่ง",
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "translatedText": "แต่รูปแบบที่คุณและฉันจะเน้น ซึ่งเป็นประเภทที่รองรับเครื่องมืออย่าง ChatGPT จะเป็นโมเดลที่ได้รับการฝึกฝนให้รับข้อความ อาจมีภาพหรือเสียงล้อมรอบอยู่ด้วย และสร้างคำทำนายได้ สำหรับสิ่งที่จะเกิดขึ้นต่อไปในข้อนี้",
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "translatedText": "การทำนายนั้นอยู่ในรูปแบบของการแจกแจงความน่าจะเป็นบนข้อความส่วนต่างๆ ที่อาจตามมา",
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "translatedText": "เมื่อมองแวบแรก คุณอาจคิดว่าการคาดเดาคำถัดไปให้ความรู้สึกเหมือนเป็นเป้าหมายที่แตกต่างไปจากการสร้างข้อความใหม่",
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "translatedText": "แต่เมื่อคุณมีโมเดลการทำนายเช่นนี้ สิ่งง่ายๆ ที่คุณสร้างข้อความที่ยาวขึ้นได้คือให้ตัวอย่างเริ่มต้นใช้งานได้ โดยให้มันสุ่มตัวอย่างจากการแจกแจงที่เพิ่งสร้างขึ้น แล้วผนวกตัวอย่างนั้นเข้ากับข้อความ จากนั้นรันกระบวนการทั้งหมดอีกครั้งเพื่อทำการคาดคะเนใหม่ตามข้อความใหม่ทั้งหมด รวมถึงสิ่งที่เพิ่งเพิ่มเข้าไป",
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "translatedText": "ฉันไม่รู้เกี่ยวกับคุณ แต่รู้สึกว่าวิธีนี้ไม่น่าจะได้ผลจริงๆ",
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "translatedText": "ตัวอย่างเช่น ในแอนิเมชันนี้ ฉันใช้ GPT-2 บนแล็ปท็อปของฉัน และให้มันคาดเดาซ้ำๆ และสุ่มตัวอย่างข้อความชิ้นถัดไปเพื่อสร้างเรื่องราวตามข้อความเริ่มต้น",
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "translatedText": "เรื่องราวไม่ได้สมเหตุสมผลขนาดนั้นจริงๆ",
  "input": "The story just doesn't really make that much sense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "translatedText": "แต่ถ้าฉันเปลี่ยนเป็นการเรียก API เป็น GPT-3 แทน ซึ่งเป็นโมเดลพื้นฐานเดียวกัน ซึ่งใหญ่กว่ามาก และเกือบจะน่าอัศจรรย์ เราก็ได้เรื่องราวที่สมเหตุสมผล เรื่องราวที่ดูเหมือนว่าจะอนุมานได้ว่าสิ่งมีชีวิต pi จะอาศัยอยู่ใน ดินแดนแห่งคณิตศาสตร์และการคำนวณ",
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "translatedText": "กระบวนการคาดการณ์และการสุ่มตัวอย่างซ้ำที่นี่เป็นสิ่งที่เกิดขึ้นเมื่อคุณโต้ตอบกับ ChatGPT หรือโมเดลภาษาขนาดใหญ่อื่นๆ เหล่านี้ และคุณเห็นว่าพวกเขาสร้างคำทีละคำ",
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "translatedText": "อันที่จริง คุณลักษณะหนึ่งที่ฉันชอบมากคือความสามารถในการดูการแจกแจงพื้นฐานของคำใหม่แต่ละคำที่เลือก",
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "translatedText": "มาเริ่มกันด้วยการแสดงตัวอย่างระดับสูงว่าข้อมูลไหลผ่านหม้อแปลงอย่างไร",
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "translatedText": "เราจะใช้เวลามากขึ้นในการจูงใจ ตีความ และขยายรายละเอียดของแต่ละขั้นตอน แต่เมื่อแชทบอทตัวใดตัวหนึ่งสร้างคำที่กำหนด ต่อไปนี้คือสิ่งที่เกิดขึ้นภายใต้ประทุน",
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "translatedText": "ขั้นแรก อินพุตจะถูกแบ่งออกเป็นส่วนๆ เล็กๆ น้อยๆ",
  "input": "First, the input is broken up into a bunch of little pieces.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "translatedText": "ชิ้นส่วนเหล่านี้เรียกว่าโทเค็น และในกรณีของข้อความ สิ่งเหล่านี้มักจะเป็นคำหรือคำเล็กๆ น้อยๆ หรือการผสมอักขระทั่วไปอื่นๆ",
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "translatedText": "หากมีภาพหรือเสียงเข้ามาเกี่ยวข้อง โทเค็นอาจเป็นส่วนเล็กๆ ของภาพนั้นหรือส่วนเล็กๆ ของเสียงนั้น",
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "translatedText": "โทเค็นแต่ละอันจะเชื่อมโยงกับเวกเตอร์ ซึ่งหมายถึงรายการตัวเลขซึ่งมีไว้เพื่อเข้ารหัสความหมายของชิ้นส่วนนั้น",
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "translatedText": "หากคุณคิดว่าเวกเตอร์เหล่านี้เป็นการให้พิกัดในพื้นที่มิติที่สูงมาก คำที่มีความหมายคล้ายกันมักจะไปเกาะบนเวกเตอร์ที่อยู่ใกล้กันในพื้นที่นั้น",
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "translatedText": "ลำดับของเวกเตอร์นี้จะผ่านการดำเนินการที่เรียกว่าบล็อกความสนใจ และช่วยให้เวกเตอร์สามารถพูดคุยกันและส่งข้อมูลไปมาเพื่ออัปเดตค่าของพวกเขา",
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "translatedText": "ตัวอย่างเช่น ความหมายของคำว่า model ในวลี a model การเรียนรู้ของเครื่องแตกต่างจากความหมายในวลี a fashion model",
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "translatedText": "บล็อกความสนใจคือสิ่งที่มีหน้าที่ค้นหาว่าคำใดในบริบทที่เกี่ยวข้องกับการอัปเดตความหมายของคำอื่นๆ และควรอัปเดตความหมายเหล่านั้นอย่างไร",
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "translatedText": "และอีกครั้ง, ทุกครั้งที่ผมใช้คำว่า ความหมาย, นี่จะถูกเข้ารหัสทั้งหมด ในรายการของเวกเตอร์เหล่านั้น",
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "translatedText": "หลังจากนั้น เวกเตอร์เหล่านี้จะผ่านการดำเนินการประเภทอื่น และขึ้นอยู่กับแหล่งที่มาที่คุณกำลังอ่านอยู่ เราจะเรียกว่าเพอร์เซพตรอนแบบหลายเลเยอร์หรืออาจเป็นเลเยอร์ฟีดฟอร์เวิร์ด",
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "translatedText": "และตรงนี้เวกเตอร์ไม่คุยกัน, พวกมันทั้งหมดผ่านการดำเนินการแบบขนานกัน",
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "translatedText": "และในขณะที่บล็อกนี้ตีความได้ยากกว่านิดหน่อย แต่ต่อไปเราจะพูดถึงว่าขั้นตอนนั้นเหมือนกับการถามคำถามยาวๆ เกี่ยวกับเวกเตอร์แต่ละตัวอย่างไร จากนั้นจึงอัปเดตคำถามเหล่านั้นตามคำตอบของคำถามเหล่านั้น",
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "translatedText": "การดำเนินการทั้งหมดในบล็อกทั้งสองนี้ดูเหมือนกองคูณเมทริกซ์จำนวนมหาศาล และงานหลักของเราคือ เข้าใจวิธีอ่านเมทริกซ์ที่อยู่ข้างใต้",
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "translatedText": "ฉันกำลังอธิบายรายละเอียดบางอย่างเกี่ยวกับขั้นตอนการทำให้เป็นมาตรฐานที่เกิดขึ้นระหว่างนั้น แต่นี่คือการแสดงตัวอย่างในระดับสูง",
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "translatedText": "หลังจากนั้น กระบวนการจะเกิดซ้ำ โดยคุณกลับไปกลับมาระหว่างบล็อกความสนใจและบล็อกการรับรู้หลายชั้น จนกระทั่งถึงจุดสิ้นสุด ความหวังก็คือว่า ความหมายที่สำคัญทั้งหมดของข้อความนี้ได้ถูกรวมเข้ากับเวกเตอร์สุดท้ายใน ลำดับ",
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "translatedText": "จากนั้นเราดำเนินการบางอย่างกับเวกเตอร์สุดท้ายที่สร้างการแจกแจงความน่าจะเป็นสำหรับโทเค็นที่เป็นไปได้ทั้งหมด ซึ่งเป็นข้อความชิ้นเล็กๆ ที่เป็นไปได้ทั้งหมดที่อาจเกิดขึ้นถัดไป",
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "translatedText": "และอย่างที่ผมบอกไป เมื่อคุณมีเครื่องมือที่คาดเดาสิ่งที่จะเกิดขึ้นถัดไปด้วยตัวอย่างข้อความ คุณสามารถป้อนข้อความเริ่มต้นได้เล็กน้อย และให้มันเล่นเกมนี้ซ้ำๆ ของการทำนายสิ่งที่จะเกิดขึ้นต่อไป โดยสุ่มตัวอย่างจากการแจกแจง ต่อท้าย มันแล้วซ้ำไปซ้ำมา",
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "translatedText": "พวกคุณบางคนที่รู้อาจจำได้ว่านานแค่ไหนก่อนที่ ChatGPT จะเข้ามามีบทบาท นี่คือหน้าตาของการสาธิต GPT-3 ในช่วงแรก คุณจะต้องให้ระบบเติมเรื่องราวและเรียงความอัตโนมัติโดยอิงจากตัวอย่างเริ่มต้น",
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "translatedText": "หากต้องการสร้างเครื่องมือเช่นนี้ให้เป็นแชทบอต จุดเริ่มต้นที่ง่ายที่สุดคือการมีข้อความเล็กน้อยที่กำหนดการตั้งค่าของผู้ใช้โต้ตอบกับผู้ช่วย AI ที่เป็นประโยชน์ คุณจะเรียกว่าข้อความแจ้งของระบบ จากนั้นคุณจะใช้ คำถามหรือข้อความเริ่มต้นของผู้ใช้เป็นบทสนทนาชิ้นแรก จากนั้นคุณก็เริ่มคาดเดาว่าผู้ช่วย AI ที่เป็นประโยชน์จะพูดอะไรเพื่อตอบโต้",
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "translatedText": "ยังมีอีกหลายสิ่งที่จะพูดเกี่ยวกับขั้นตอนการฝึกอบรมที่จำเป็นเพื่อให้งานนี้ออกมาดี แต่ในระดับสูง นี่คือแนวคิด",
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "translatedText": "ในบทนี้ คุณและฉันจะขยายรายละเอียดของสิ่งที่เกิดขึ้นที่จุดเริ่มต้นของเครือข่าย ที่จุดสิ้นสุดของเครือข่าย และฉันต้องการใช้เวลาส่วนใหญ่ในการทบทวนความรู้พื้นฐานบางส่วนที่สำคัญ สิ่งต่าง ๆ ที่จะกลายมาเป็นลักษณะที่สองของวิศวกรแมชชีนเลิร์นนิงเมื่อหม้อแปลงมาถึง",
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "translatedText": "หากคุณพอใจกับความรู้พื้นฐานนั้นและใจร้อนนิดหน่อย คุณสามารถข้ามไปยังบทถัดไปได้เลย ซึ่งจะเน้นไปที่บล็อคความสนใจ ซึ่งโดยทั่วไปถือเป็นหัวใจของหม้อแปลงไฟฟ้า",
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "translatedText": "หลังจากนั้น ฉันต้องการพูดคุยเพิ่มเติมเกี่ยวกับบล็อกเพอร์เซปตรอนแบบหลายชั้น วิธีการทำงานของการฝึก และรายละเอียดอื่นๆ อีกจำนวนหนึ่งที่จะถูกข้ามไปยังจุดนั้น",
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "translatedText": "สำหรับบริบทที่กว้างขึ้น วิดีโอเหล่านี้เป็นส่วนเสริมของมินิซีรีส์เกี่ยวกับการเรียนรู้เชิงลึก และไม่เป็นไรหากคุณยังไม่ได้ดูรายการก่อนหน้านี้ ฉันคิดว่าคุณน่าจะทำมันผิดระเบียบได้ แต่ก่อนที่จะเจาะลึกเรื่อง Transformer โดยเฉพาะ ฉันคิดว่า ควรทำให้แน่ใจว่าเราเข้าใจตรงกันเกี่ยวกับหลักฐานพื้นฐานและโครงสร้างของการเรียนรู้เชิงลึก",
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "translatedText": "ด้วยความเสี่ยงที่จะระบุสิ่งที่ชัดเจน นี่เป็นแนวทางหนึ่งในการเรียนรู้ของเครื่อง ซึ่งจะอธิบายโมเดลใดๆ ก็ตามที่คุณกำลังใช้ข้อมูลเพื่อกำหนดวิธีการทำงานของโมเดล",
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "translatedText": "สิ่งที่ฉันหมายถึงคือ สมมติว่าคุณต้องการฟังก์ชันที่รับรูปภาพและสร้างป้ายกำกับที่อธิบาย หรือตัวอย่างของเราในการทำนายคำถัดไปเมื่อมีข้อความ หรืองานอื่นใดที่ดูเหมือนว่าจะต้องมีองค์ประกอบบางอย่าง ของสัญชาตญาณและการจดจำรูปแบบ",
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "translatedText": "ทุกวันนี้เราแทบจะมองข้ามสิ่งนี้ไป แต่แนวคิดเกี่ยวกับการเรียนรู้ของเครื่องก็คือ แทนที่จะพยายามกำหนดขั้นตอนในการทำงานนั้นอย่างชัดเจนด้วยโค้ด ซึ่งเป็นสิ่งที่ผู้คนจะทำในช่วงแรก ๆ ของ AI แทนคุณ ตั้งค่าโครงสร้างที่ยืดหยุ่นมากด้วยพารามิเตอร์ที่ปรับได้ เช่น ปุ่มหมุนและแป้นหมุนจำนวนหนึ่ง จากนั้นคุณก็ใช้ตัวอย่างมากมายว่าเอาต์พุตควรมีลักษณะอย่างไรสำหรับอินพุตที่กำหนด เพื่อปรับแต่งและปรับแต่งค่าของพารามิเตอร์เหล่านั้นเพื่อเลียนแบบพฤติกรรมนี้",
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "translatedText": "ตัวอย่างเช่น รูปแบบแมชชีนเลิร์นนิงที่ง่ายที่สุดอาจเป็นการถดถอยเชิงเส้น โดยที่อินพุตและเอาท์พุตของคุณเป็นตัวเลขเดี่ยวๆ เช่น พื้นที่เป็นตารางฟุตของบ้านและราคาของบ้าน และสิ่งที่คุณต้องการคือหาเส้นที่เหมาะสมที่สุดกับสิ่งนี้ ข้อมูลเพื่อทำนายราคาบ้านในอนาคต",
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "translatedText": "เส้นดังกล่าวอธิบายด้วยพารามิเตอร์ต่อเนื่องสองตัว เช่น ความชันและค่าตัดแกน y และเป้าหมายของการถดถอยเชิงเส้นคือการกำหนดพารามิเตอร์เหล่านั้นให้ตรงกับข้อมูลอย่างใกล้ชิด",
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "translatedText": "ไม่ต้องพูดอะไรมาก โมเดลการเรียนรู้เชิงลึกมีความซับซ้อนมากขึ้น",
  "input": "Needless to say, deep learning models get much more complicated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "translatedText": "ตัวอย่างเช่น GPT-3 มีพารามิเตอร์ไม่ใช่สองตัว แต่มี 175 พันล้านพารามิเตอร์",
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "translatedText": "แต่สิ่งสำคัญคือ ไม่ใช่ว่าคุณสามารถสร้างโมเดลขนาดยักษ์ที่มีพารามิเตอร์จำนวนมากได้ โดยที่ข้อมูลการฝึกไม่พอดีหรือฝึกยากเกินไป",
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "translatedText": "การเรียนรู้เชิงลึกอธิบายถึงคลาสของโมเดลที่ในช่วงสองสามทศวรรษที่ผ่านมาได้พิสูจน์แล้วว่าสามารถขยายขนาดได้ดีอย่างน่าทึ่ง",
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "translatedText": "สิ่งที่รวมเป็นหนึ่งเดียวคืออัลกอริธึมการฝึกอบรมแบบเดียวกัน ที่เรียกว่า backpropagation และบริบทที่ฉันอยากให้คุณมีในระหว่างดำเนินการก็คือ เพื่อให้อัลกอริธึมการฝึกอบรมนี้ทำงานได้ดีในวงกว้าง โมเดลเหล่านี้จะต้องเป็นไปตามรูปแบบเฉพาะบางอย่าง",
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "translatedText": "หากคุณรู้ว่ารูปแบบนี้กำลังเข้ามา จะช่วยอธิบายตัวเลือกต่างๆ มากมายสำหรับวิธีที่ Transformer ประมวลผลภาษา ซึ่งมิฉะนั้นจะเสี่ยงต่อความรู้สึกตามอำเภอใจ",
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "translatedText": "อันดับแรก ไม่ว่าคุณกำลังสร้างโมเดลใดก็ตาม ข้อมูลเข้าจะต้องถูกจัดรูปแบบเป็นอาร์เรย์ของจำนวนจริง",
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "translatedText": "นี่อาจหมายถึงรายการตัวเลข อาจเป็นอาร์เรย์สองมิติ หรือบ่อยครั้งที่คุณจัดการกับอาร์เรย์มิติที่สูงกว่า โดยที่คำทั่วไปที่ใช้คือเทนเซอร์",
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "translatedText": "คุณมักจะคิดว่าข้อมูลอินพุตนั้นถูกแปลงอย่างต่อเนื่องเป็นเลเยอร์ที่แตกต่างกันจำนวนมาก โดยที่แต่ละเลเยอร์จะมีโครงสร้างเป็นอาร์เรย์ของจำนวนจริงเสมอ จนกว่าคุณจะไปถึงเลเยอร์สุดท้ายที่คุณจะพิจารณาผลลัพธ์",
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "translatedText": "ตัวอย่างเช่น เลเยอร์สุดท้ายในโมเดลการประมวลผลข้อความของเราคือรายการตัวเลขที่แสดงถึงการแจกแจงความน่าจะเป็นสำหรับโทเค็นถัดไปที่เป็นไปได้ทั้งหมด",
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "translatedText": "ในการเรียนรู้เชิงลึก พารามิเตอร์โมเดลเหล่านี้มักถูกเรียกว่าน้ำหนัก และนี่เป็นเพราะคุณลักษณะสำคัญของโมเดลเหล่านี้ก็คือ วิธีเดียวที่พารามิเตอร์เหล่านี้โต้ตอบกับข้อมูลที่กำลังประมวลผลก็คือผ่านผลรวมถ่วงน้ำหนัก",
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "translatedText": "คุณยังโรยฟังก์ชันที่ไม่ใช่เชิงเส้นบางส่วนให้ทั่วด้วย แต่ฟังก์ชันเหล่านี้จะไม่ขึ้นอยู่กับพารามิเตอร์",
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "translatedText": "โดยทั่วไปแล้ว แทนที่จะเห็นผลรวมถ่วงน้ำหนักทั้งหมดเปล่าๆ และเขียนออกมาอย่างชัดเจนแบบนี้ คุณจะพบว่ามันรวมเข้าด้วยกันเป็นส่วนประกอบต่างๆ ในผลคูณเมทริกซ์",
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "translatedText": "มันเหมือนกับพูดสิ่งเดียวกัน หากคุณคิดย้อนกลับไปว่าการคูณเวกเตอร์เมทริกซ์ทำงานอย่างไร แต่ละส่วนประกอบในผลลัพธ์จะดูเหมือนผลรวมถ่วงน้ำหนัก",
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "translatedText": "บ่อยครั้งที่แนวคิดจะสะอาดกว่าสำหรับคุณและฉันที่จะคิดถึงเมทริกซ์ที่เต็มไปด้วยพารามิเตอร์ที่ปรับแต่งได้ซึ่งแปลงเวกเตอร์ที่ดึงมาจากข้อมูลที่กำลังประมวลผล",
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "translatedText": "ตัวอย่างเช่น น้ำหนัก 175 พันล้านน้ำหนักใน GPT-3 ได้รับการจัดเป็นเมทริกซ์ที่แตกต่างกันไม่เกิน 28,000 รายการ",
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "translatedText": "เมทริกซ์เหล่านั้นแบ่งออกเป็นแปดหมวดหมู่ที่แตกต่างกัน และสิ่งที่คุณและฉันจะทำคือการดูแต่ละหมวดหมู่เหล่านั้นเพื่อทำความเข้าใจว่าประเภทนั้นทำอะไร",
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "translatedText": "ขณะที่เราดำเนินการนี้ ฉันคิดว่าเป็นเรื่องสนุกที่จะอ้างอิงตัวเลขเฉพาะจาก GPT-3 เพื่อนับจำนวนที่แน่นอนว่า 175 พันล้านจำนวนนั้นมาจากไหน",
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "translatedText": "แม้ว่าในปัจจุบันจะมีโมเดลที่ใหญ่กว่าและดีกว่า แต่โมเดลนี้มีเสน่ห์บางอย่างในฐานะโมเดลภาษาขนาดใหญ่ที่ดึงดูดความสนใจของโลกภายนอกชุมชน ML",
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "translatedText": "นอกจากนี้ ในทางปฏิบัติแล้ว บริษัทต่างๆ มักจะเข้มงวดกับตัวเลขเฉพาะสำหรับเครือข่ายสมัยใหม่มากขึ้น",
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "translatedText": "ฉันแค่อยากจะจัดฉากว่าเมื่อคุณแอบดูเบื้องหลังเพื่อดูว่าเกิดอะไรขึ้นภายในเครื่องมืออย่าง ChatGPT การคำนวณจริงเกือบทั้งหมดจะดูเหมือนเป็นการคูณเมทริกซ์เวกเตอร์",
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "translatedText": "มีความเสี่ยงเล็กน้อยที่จะสูญหายไปในทะเลตัวเลขนับพันล้าน แต่คุณควรแยกแยะความแตกต่างที่ชัดเจนในใจระหว่างน้ำหนักของแบบจำลอง ซึ่งฉันจะกำหนดเป็นสีน้ำเงินหรือสีแดงเสมอ และข้อมูลที่เป็นอยู่ ประมวลผลแล้ว ซึ่งฉันจะให้เป็นสีเทาเสมอ",
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "translatedText": "ตุ้มน้ำหนักคือสมองที่แท้จริง คือสิ่งที่เรียนรู้ระหว่างการฝึก และเป็นตัวกำหนดว่าจะมีพฤติกรรมอย่างไร",
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "translatedText": "ข้อมูลที่กำลังประมวลผลเพียงเข้ารหัสอินพุตเฉพาะใดๆ ก็ตามที่ถูกป้อนเข้าไปในโมเดลสำหรับการทำงานที่กำหนด เช่น ตัวอย่างข้อความ",
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "translatedText": "เมื่อพิจารณาทั้งหมดนี้เป็นรากฐานแล้ว เรามาเจาะลึกถึงขั้นตอนแรกของตัวอย่างการประมวลผลข้อความนี้กัน ซึ่งก็คือการแบ่งข้อมูลที่ป้อนออกเป็นส่วนเล็กๆ และเปลี่ยนส่วนเหล่านั้นให้เป็นเวกเตอร์",
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "translatedText": "ฉันพูดถึงการที่ชิ้นส่วนเหล่านั้นถูกเรียกว่าโทเค็น ซึ่งอาจเป็นชิ้นส่วนของคำหรือเครื่องหมายวรรคตอน แต่ในบทนี้และโดยเฉพาะอย่างยิ่งในบทถัดไป ฉันอยากจะแกล้งทำเป็นว่ามันถูกแยกออกเป็นคำอย่างหมดจดมากขึ้น",
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "translatedText": "เนื่องจากมนุษย์เราคิดเป็นคำพูด จึงทำให้การอ้างอิงตัวอย่างเล็กๆ น้อยๆ และชี้แจงแต่ละขั้นตอนได้ง่ายขึ้นมาก",
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "translatedText": "แบบจำลองนี้มีคำศัพท์ที่กำหนดไว้ล่วงหน้า รายการคำศัพท์ที่เป็นไปได้ทั้งหมด ประมาณ 50,000 คำ และเมทริกซ์แรกที่เราจะพบ ซึ่งเรียกว่าเมทริกซ์แบบฝัง จะมีคอลัมน์เดียวสำหรับแต่ละคำเหล่านี้",
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "translatedText": "คอลัมน์เหล่านี้คือสิ่งที่กำหนดว่าแต่ละคำจะกลายเป็นเวกเตอร์ใดในขั้นตอนแรก",
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "translatedText": "เราตั้งชื่อมันว่า &quot;เรา&quot; และเช่นเดียวกับเมทริกซ์ทั้งหมดที่เราเห็น ค่าของมันเริ่มต้นแบบสุ่ม แต่จะต้องเรียนรู้จากข้อมูล",
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "translatedText": "การเปลี่ยนคำให้เป็นเวกเตอร์ถือเป็นเรื่องปกติในแมชชีนเลิร์นนิงก่อนมีหม้อแปลงไฟฟ้า แต่จะแปลกนิดหน่อยหากคุณไม่เคยเห็นมาก่อน และสิ่งนี้ได้วางรากฐานสำหรับทุกสิ่งที่ตามมา ดังนั้น เรามาดูกันสักครู่เพื่อทำความคุ้นเคยกับมัน",
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "translatedText": "เรามักเรียกสิ่งนี้ว่าการฝังคำ ซึ่งเชิญชวนให้คุณนึกถึงเวกเตอร์เหล่านี้ในเชิงเรขาคณิตว่าเป็นจุดในพื้นที่มิติสูง",
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "translatedText": "การแสดงรายการตัวเลขสามตัวเป็นพิกัดสำหรับจุดในพื้นที่ 3 มิติจะไม่มีปัญหา แต่การฝังคำมักจะมีมิติที่สูงกว่ามาก",
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "translatedText": "ใน GPT-3 มีขนาด 12,288 มิติ และอย่างที่คุณเห็น การทำงานในพื้นที่ที่มีทิศทางที่แตกต่างกันจำนวนมากเป็นสิ่งสำคัญ",
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "translatedText": "ในลักษณะเดียวกับที่คุณสามารถนำชิ้นสองมิติผ่านพื้นที่ 3 มิติ และฉายจุดทั้งหมดลงบนชิ้นนั้น เพื่อประโยชน์ของการสร้างภาพเคลื่อนไหวการฝังคำที่แบบจำลองง่ายๆ ให้มา ฉันจะทำสิ่งที่คล้ายคลึงกัน โดยการเลือกชิ้นสามมิติผ่านพื้นที่มิติที่สูงมากนี้ และฉายคำว่าเวกเตอร์ลงไปบนนั้นและแสดงผลลัพธ์",
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "translatedText": "แนวคิดสำคัญที่นี่คือเมื่อแบบจำลองปรับแต่งและปรับน้ำหนักเพื่อกำหนดว่าคำต่างๆ จะถูกฝังเป็นเวกเตอร์ในระหว่างการฝึกได้อย่างไร โมเดลนั้นมักจะตั้งอยู่บนชุดของการฝังที่ทิศทางในอวกาศมีความหมายเชิงความหมาย",
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "translatedText": "สำหรับโมเดลคำต่อเวกเตอร์แบบง่ายๆ ที่ฉันใช้อยู่นี้ หากฉันค้นหาคำทั้งหมดที่มีการฝังไว้ใกล้กับหอคอยมากที่สุด คุณจะสังเกตเห็นว่าคำเหล่านั้นให้ความรู้สึกเหมือนหอคอยที่คล้ายกันมาก",
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "translatedText": "และถ้าคุณต้องการดึง Python ขึ้นมาแล้วเล่นที่บ้าน นี่คือโมเดลเฉพาะที่ฉันใช้สร้างแอนิเมชั่น",
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "translatedText": "ไม่ใช่หม้อแปลงไฟฟ้า แต่ก็เพียงพอที่จะแสดงให้เห็นว่าทิศทางในอวกาศสามารถมีความหมายเชิงความหมายได้",
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "translatedText": "ตัวอย่างคลาสสิกของเรื่องนี้คือถ้าคุณหาความแตกต่างระหว่างเวกเตอร์สำหรับผู้หญิงและผู้ชาย สิ่งที่คุณจินตนาการว่าเป็นเวกเตอร์เล็กๆ ที่เชื่อมปลายของอันหนึ่งเข้ากับปลายของอีกอัน มันจะคล้ายกันมากกับความแตกต่างระหว่างราชากับ ราชินี",
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "translatedText": "สมมุติว่าคุณไม่รู้จักคำว่ากษัตริย์หญิง คุณสามารถหาได้โดยการขึ้นเป็นกษัตริย์ เพิ่มทิศทางหญิง-ชาย และค้นหาส่วนที่ฝังใกล้กับจุดนั้นมากที่สุด",
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "translatedText": "อย่างน้อยก็ชนิดของ",
  "input": "At least, kind of.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "translatedText": "แม้ว่านี่จะเป็นตัวอย่างคลาสสิกสำหรับโมเดลที่ฉันเล่นด้วย แต่จริงๆ แล้วการฝังราชินีที่แท้จริงนั้นไกลเกินกว่าที่จะแนะนำเล็กน้อย อาจเป็นเพราะวิธีที่ราชินีใช้ในข้อมูลการฝึกอบรมไม่ได้เป็นเพียงราชาในเวอร์ชันผู้หญิงเท่านั้น",
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "translatedText": "เมื่อฉันลองเล่นดู ความสัมพันธ์ในครอบครัวดูเหมือนจะอธิบายแนวคิดได้ดีขึ้นมาก",
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "translatedText": "ประเด็นก็คือ ดูเหมือนว่าในระหว่างการฝึกอบรม โมเดลพบว่ามีข้อดีในการเลือกการฝัง โดยทิศทางเดียวในพื้นที่นี้จะเข้ารหัสข้อมูลเพศ",
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "translatedText": "อีกตัวอย่างหนึ่งก็คือ ถ้าคุณเอาการฝังของอิตาลี และลบการฝังของเยอรมนี แล้วบวกเข้ากับการฝังของฮิตเลอร์ คุณจะได้บางอย่างที่ใกล้เคียงกับการฝังของมุสโสลินีมาก",
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "translatedText": "ราวกับว่าโมเดลเรียนรู้ที่จะเชื่อมโยงทิศทางบางอย่างกับความเป็นอิตาลี และทิศทางอื่นๆ กับผู้นำแกนนำในสงครามโลกครั้งที่สอง",
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "translatedText": "บางทีตัวอย่างที่ฉันชื่นชอบในแนวทางนี้คือ ในบางรุ่น ถ้าคุณคำนึงถึงความแตกต่างระหว่างเยอรมนีและญี่ปุ่น แล้วเพิ่มลงในซูชิ คุณจะเข้าใกล้บราทเวิร์สอย่างมาก",
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "translatedText": "นอกจากนี้ ในการเล่นเกมค้นหาเพื่อนบ้านที่ใกล้ที่สุดนี้ ฉันดีใจที่ได้เห็นว่า Kat อยู่ใกล้ทั้งสัตว์ร้ายและสัตว์ประหลาดมากแค่ไหน",
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "translatedText": "สัญชาตญาณทางคณิตศาสตร์เล็กๆ น้อยๆ ที่เป็นประโยชน์ในใจ โดยเฉพาะบทถัดไป คือ การพิจารณาว่าดอทโปรดัคของเวกเตอร์สองตัวเป็นวิธีหนึ่งในการวัดว่าเวกเตอร์สองตัวอยู่ในแนวเดียวกันได้ดีเพียงใด",
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "translatedText": "ในการคำนวณ ผลิตภัณฑ์ดอทเกี่ยวข้องกับการคูณส่วนประกอบที่เกี่ยวข้องทั้งหมดแล้วบวกผลลัพธ์ ซึ่งถือว่าดี เนื่องจากการคำนวณส่วนใหญ่ของเราต้องมีลักษณะเหมือนผลรวมถ่วงน้ำหนัก",
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "translatedText": "ในเชิงเรขาคณิต ผลคูณดอทจะเป็นค่าบวกเมื่อเวกเตอร์ชี้ไปในทิศทางที่คล้ายกัน ถ้าเวกเตอร์ชี้ไปในทิศทางเดียวกัน มันจะเป็นศูนย์หากตั้งฉากกัน และจะเป็นลบทุกครั้งที่ชี้ไปในทิศทางตรงกันข้าม",
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "translatedText": "ตัวอย่างเช่น สมมติว่าคุณกำลังเล่นกับโมเดลนี้ และคุณตั้งสมมุติฐานว่าการฝังแมวลบด้วย cat อาจแสดงถึงทิศทางแบบพหูพจน์ในพื้นที่นี้",
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "translatedText": "เพื่อทดสอบสิ่งนี้ ฉันจะใช้เวกเตอร์นี้และคำนวณผลคูณดอทของมันเทียบกับการฝังคำนามเอกพจน์บางคำ แล้วเปรียบเทียบกับผลคูณดอทที่มีคำนามพหูพจน์ที่สอดคล้องกัน",
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "translatedText": "หากคุณลองเล่นดู คุณจะสังเกตเห็นว่าค่าพหูพจน์ดูเหมือนจะให้ค่าที่สูงกว่าค่าเอกพจน์อย่างสม่ำเสมอ ซึ่งบ่งชี้ว่าพวกมันสอดคล้องกับทิศทางนี้มากกว่า",
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "translatedText": "นอกจากนี้ ยังน่าสนุกที่ถ้าคุณนำดอทโปรดัคนี้ฝังคำ 1, 2, 3 และอื่นๆ เข้าด้วยกัน พวกมันจะให้ค่าที่เพิ่มขึ้น เหมือนกับว่าเราสามารถวัดในเชิงปริมาณได้ว่าแบบจำลองพหูพจน์ค้นหาคำที่กำหนดได้อย่างไร",
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "translatedText": "อีกครั้ง เฉพาะเจาะจงสำหรับวิธีการฝังคำศัพท์นั้นเรียนรู้โดยใช้ข้อมูล",
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "translatedText": "เมทริกซ์แบบฝังซึ่งมีคอลัมน์บอกเราว่าเกิดอะไรขึ้นกับแต่ละคำ ถือเป็นกองน้ำหนักชุดแรกในแบบจำลองของเรา",
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "translatedText": "เมื่อใช้ตัวเลข GPT-3 ขนาดคำศัพท์โดยเฉพาะคือ 50,257 และอีกครั้ง ในทางเทคนิคแล้ว สิ่งนี้ไม่ได้ประกอบด้วยคำต่อตัว แต่เป็นโทเค็น",
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "translatedText": "มิติข้อมูลการฝังคือ 12,288 และการคูณสิ่งเหล่านี้บอกเราว่าสิ่งนี้ประกอบด้วยน้ำหนักประมาณ 617 ล้าน",
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "translatedText": "เรามาเพิ่มตัวเลขนี้เข้าในการนับอย่างต่อเนื่อง โดยจำไว้ว่าในตอนท้ายเราควรนับได้ถึง 175 พันล้าน",
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "translatedText": "ในกรณีของหม้อแปลงไฟฟ้า คุณต้องคิดถึงเวกเตอร์ในพื้นที่ฝังนี้จริงๆ ไม่ใช่แค่เป็นตัวแทนของคำแต่ละคำเท่านั้น",
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "translatedText": "ประการหนึ่ง พวกเขายังเข้ารหัสข้อมูลเกี่ยวกับตำแหน่งของคำนั้น ซึ่งเราจะพูดถึงในภายหลัง แต่ที่สำคัญกว่านั้น คุณควรคิดว่าคำเหล่านั้นมีความสามารถในการซึมซับบริบท",
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "translatedText": "เวกเตอร์ที่เริ่มต้นชีวิตด้วยการฝังคำว่า king อาจถูกดึงและดึงโดยบล็อกต่างๆ ในเครือข่ายนี้อย่างต่อเนื่อง เพื่อว่าในตอนท้ายจะชี้ไปในทิศทางที่เฉพาะเจาะจงและเหมาะสมยิ่งขึ้นมากซึ่งเข้ารหัสด้วยวิธีใดวิธีหนึ่ง เป็นกษัตริย์ที่อาศัยอยู่ในสกอตแลนด์ และประสบความสำเร็จในตำแหน่งของเขาหลังจากสังหารกษัตริย์องค์ก่อน และได้รับการบรรยายเป็นภาษาเช็คสเปียร์",
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "translatedText": "คิดถึงความเข้าใจของคุณเองเกี่ยวกับคำที่กำหนด",
  "input": "Think about your own understanding of a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "translatedText": "ความหมายของคำนั้นได้รับการบอกเล่าจากสิ่งรอบข้างอย่างชัดเจน และบางครั้งอาจรวมบริบทจากระยะไกลด้วย ดังนั้นในการรวบรวมแบบจำลองที่มีความสามารถในการคาดเดาคำถัดไปได้ เป้าหมายคือการเสริมพลังให้รวมบริบทเข้าไปด้วย อย่างมีประสิทธิภาพ",
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "translatedText": "เพื่อให้ชัดเจน ในขั้นตอนแรกนั้น เมื่อคุณสร้างอาร์เรย์ของเวกเตอร์ตามข้อความที่ป้อน แต่ละเวกเตอร์จะถูกดึงออกจากเมทริกซ์ที่ฝัง ดังนั้นในขั้นต้นแต่ละเวกเตอร์สามารถเข้ารหัสความหมายของคำเดียวเท่านั้นโดยไม่ต้อง อินพุตใดๆ จากสิ่งรอบตัว",
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "translatedText": "แต่คุณควรคิดถึงเป้าหมายหลักของเครือข่ายนี้ที่เครือข่ายนี้ไหลผ่านคือการทำให้เวกเตอร์แต่ละตัวซึมซับความหมายที่เข้มข้นและเฉพาะเจาะจงมากกว่าที่คำแต่ละคำจะเป็นตัวแทนได้",
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "translatedText": "เครือข่ายสามารถประมวลผลเวกเตอร์ได้ครั้งละจำนวนคงที่เท่านั้น ซึ่งเรียกว่าขนาดบริบท",
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "translatedText": "สำหรับ GPT-3 ได้รับการฝึกฝนด้วยขนาดบริบท 2048 ดังนั้นข้อมูลที่ไหลผ่านเครือข่ายจะมีลักษณะเหมือนอาร์เรย์ 2048 คอลัมน์นี้เสมอ โดยแต่ละคอลัมน์จะมี 12,000 มิติ",
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "translatedText": "ขนาดบริบทนี้จำกัดจำนวนข้อความที่ Transformer สามารถรวมไว้เมื่อทำการคาดเดาคำถัดไป",
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "translatedText": "นี่คือสาเหตุที่การสนทนาระยะยาวกับแชทบอตบางประเภท เช่น ChatGPT เวอร์ชันแรกๆ มักจะทำให้บอทสูญเสียหัวข้อการสนทนาเมื่อคุณสนทนาต่อนานเกินไป",
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "translatedText": "เราจะลงรายละเอียดความสนใจในเวลาที่กำหนด แต่ข้ามไปข้างหน้าฉันต้องการพูดคุยสักครู่เกี่ยวกับสิ่งที่เกิดขึ้นในตอนท้ายสุด",
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "translatedText": "โปรดจำไว้ว่า ผลลัพธ์ที่ต้องการคือการแจกแจงความน่าจะเป็นของโทเค็นทั้งหมดที่อาจเกิดขึ้นถัดไป",
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "translatedText": "ตัวอย่างเช่น หากคำสุดท้ายคือศาสตราจารย์ และบริบทมีคำอย่างแฮร์รี่ พอตเตอร์ด้วย และก่อนหน้านั้นเราเห็นครูคนโปรดน้อยที่สุด และถ้าคุณให้โอกาสฉันบ้างโดยให้ฉันแกล้งทำเป็นว่าโทเค็นนั้นดูเหมือนเป็นคำเต็ม ดังนั้น เครือข่ายที่ได้รับการฝึกฝนมาอย่างดีซึ่งสั่งสมความรู้เกี่ยวกับแฮร์รี่ พอตเตอร์ คงจะกำหนดให้คำว่าสเนปมีจำนวนมาก",
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "translatedText": "สิ่งนี้เกี่ยวข้องกับสองขั้นตอนที่แตกต่างกัน",
  "input": "This involves two different steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "translatedText": "ประการแรกคือการใช้เมทริกซ์อื่นที่แมปเวกเตอร์สุดท้ายในบริบทนั้นกับรายการค่า 50,000 ค่า หนึ่งค่าสำหรับแต่ละโทเค็นในคำศัพท์",
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "translatedText": "จากนั้น มีฟังก์ชันที่ทำให้สิ่งนี้เป็นมาตรฐานในการแจกแจงความน่าจะเป็น เรียกว่า Softmax และเราจะพูดถึงมันเพิ่มเติมในอีกสักครู่ แต่ก่อนหน้านั้น อาจดูแปลกนิดหน่อยที่จะใช้เฉพาะการฝังครั้งล่าสุดนี้เพื่อทำนาย เมื่อ ในขั้นตอนสุดท้ายนั้น มีเวกเตอร์อื่นๆ อีกหลายพันตัวในเลเยอร์ที่นั่งอยู่ตรงนั้นพร้อมความหมายที่หลากหลายตามบริบทของตัวมันเอง",
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "translatedText": "สิ่งนี้เกี่ยวข้องกับความจริงที่ว่าในกระบวนการฝึกอบรมจะมีประสิทธิภาพมากขึ้นหากคุณใช้เวกเตอร์แต่ละตัวในเลเยอร์สุดท้ายเพื่อคาดการณ์สิ่งที่จะเกิดขึ้นหลังจากนั้นพร้อมกัน",
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "translatedText": "มีอะไรอีกมากมายที่จะพูดเกี่ยวกับการฝึกซ้อมในภายหลัง แต่ฉันแค่อยากจะพูดถึงตอนนี้",
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "translatedText": "เมทริกซ์นี้เรียกว่าเมทริกซ์ Unembedding และเราให้ป้ายกำกับ WU",
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "translatedText": "เช่นเดียวกับเมทริกซ์น้ำหนักทั้งหมดที่เราเห็น รายการต่างๆ เริ่มต้นจากการสุ่ม แต่จะเรียนรู้ในระหว่างกระบวนการฝึกอบรม",
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "translatedText": "เพื่อรักษาคะแนนในการนับพารามิเตอร์ทั้งหมดของเรา เมทริกซ์ Unembedding นี้มีหนึ่งแถวสำหรับแต่ละคำในคำศัพท์ และแต่ละแถวมีจำนวนองค์ประกอบเท่ากันกับมิติข้อมูลการฝัง",
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "translatedText": "มันคล้ายกับเมทริกซ์การฝังอย่างมาก เพียงแค่สลับลำดับกัน ดังนั้นจึงเพิ่มพารามิเตอร์อีก 617 ล้านพารามิเตอร์ให้กับเครือข่าย ซึ่งหมายความว่าการนับของเราจนถึงตอนนี้มีมากกว่าพันล้านเพียงเล็กน้อย ซึ่งเป็นเศษเพียงเล็กน้อยแต่ไม่มีนัยสำคัญทั้งหมดของ 175 พันล้านที่เรา จะลงเอยด้วยยอดรวม",
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "translatedText": "เนื่องจากเป็นบทเรียนเล็กๆ สุดท้ายสำหรับบทนี้ ฉันอยากจะพูดเพิ่มเติมเกี่ยวกับฟังก์ชัน softmax นี้ เนื่องจากฟังก์ชันนี้จะปรากฏให้เห็นอีกครั้งเมื่อเราดำดิ่งลงไปในบล็อคความสนใจ",
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "translatedText": "แนวคิดก็คือถ้าคุณต้องการให้ลำดับของตัวเลขทำหน้าที่เป็นการแจกแจงความน่าจะเป็น พูดการแจกแจงของคำถัดไปที่เป็นไปได้ทั้งหมด แต่ละค่าจะต้องอยู่ระหว่าง 0 ถึง 1 และคุณยังต้องให้ค่าทั้งหมดรวมกันได้ 1 .",
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "translatedText": "อย่างไรก็ตาม หากคุณกำลังเล่นเกมการเรียนรู้ที่ทุกสิ่งที่คุณทำดูเหมือนการคูณเมทริกซ์-เวกเตอร์ ผลลัพธ์ที่คุณได้รับตามค่าเริ่มต้นจะไม่เป็นไปตามสิ่งนี้เลย",
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "translatedText": "ค่าต่างๆ มักจะเป็นลบ หรือมากกว่า 1 มากและแทบจะรวมกันไม่ได้ 1 เลยด้วยซ้ำ",
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "translatedText": "Softmax เป็นวิธีมาตรฐานในการเปลี่ยนรายการตัวเลขใดๆ เป็นการแจกแจงที่ถูกต้อง โดยค่าที่มากที่สุดจะเข้าใกล้ 1 มากที่สุด และค่าที่น้อยกว่าจะเข้าใกล้ 0 มาก",
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "translatedText": "นั่นคือทั้งหมดที่คุณต้องรู้จริงๆ",
  "input": "That's all you really need to know.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "translatedText": "แต่ถ้าคุณสงสัย วิธีการทำงานคือยก e ยกกำลังของแต่ละจำนวนก่อน ซึ่งหมายความว่าตอนนี้คุณมีรายการค่าบวกแล้ว จากนั้นคุณก็สามารถหาผลรวมของค่าบวกเหล่านั้นแล้วหารได้ แต่ละเทอมด้วยผลรวมนั้น ซึ่งจะทำให้คำนั้นกลายเป็นรายการที่รวมกันได้ 1",
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "translatedText": "คุณจะสังเกตเห็นว่าหากตัวเลขตัวใดตัวหนึ่งในอินพุตมีความหมายมากกว่าตัวเลขที่เหลือ ดังนั้นในเอาต์พุต คำที่เกี่ยวข้องจะมีอิทธิพลเหนือการกระจายตัว ดังนั้นหากคุณสุ่มตัวอย่างจากตัวเลขดังกล่าว คุณแทบจะเลือกอินพุตที่ขยายใหญ่สุดอย่างแน่นอน",
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "translatedText": "แต่มันเบากว่าการเลือกค่าสูงสุดในแง่ที่ว่าเมื่อค่าอื่นๆ มีขนาดใหญ่พอๆ กัน ค่าเหล่านั้นก็จะได้น้ำหนักที่มีความหมายในการแจกแจงด้วย และทุกอย่างจะเปลี่ยนแปลงอย่างต่อเนื่องเมื่อคุณเปลี่ยนอินพุตอย่างต่อเนื่อง",
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "translatedText": "ในบางสถานการณ์ เช่น เมื่อ ChatGPT ใช้การแจกแจงนี้เพื่อสร้างคำถัดไป ก็มีพื้นที่ให้สนุกเพิ่มขึ้นอีกเล็กน้อยโดยการเพิ่มเครื่องเทศเข้าไปเล็กน้อยในฟังก์ชันนี้ โดยใส่ค่าคงที่ t เข้าไปในตัวส่วนของเลขชี้กำลังเหล่านั้น",
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "translatedText": "เราเรียกมันว่าอุณหภูมิ เนื่องจากมันดูคลุมเครือกับบทบาทของอุณหภูมิในสมการทางอุณหพลศาสตร์บางสมการ และผลที่ได้ก็คือเมื่อ t มากขึ้น คุณจะให้น้ำหนักกับค่าที่ต่ำกว่ามากขึ้น ซึ่งหมายความว่าการกระจายตัวจะสม่ำเสมอมากขึ้นเล็กน้อย และถ้า t มีค่าน้อยกว่า ดังนั้นค่าที่มากกว่าจะมีอำนาจเหนือกว่า โดยที่ค่า t เท่ากับศูนย์หมายความว่าน้ำหนักทั้งหมดไปที่ค่าสูงสุด",
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "translatedText": "ตัวอย่างเช่น ฉันจะให้ GPT-3 สร้างเรื่องราวด้วยข้อความเริ่มต้น กาลครั้งหนึ่งมี A แต่ฉันจะใช้อุณหภูมิที่แตกต่างกันในแต่ละกรณี",
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "translatedText": "อุณหภูมิเป็นศูนย์หมายความว่าคำนั้นจะมาพร้อมกับคำที่คาดเดาได้มากที่สุดเสมอ และสิ่งที่คุณได้รับจะกลายเป็นอนุพันธ์ของ Goldilocks ซ้ำซาก",
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "translatedText": "อุณหภูมิที่สูงขึ้นทำให้มีโอกาสเลือกคำที่มีแนวโน้มน้อยลง แต่ก็มาพร้อมกับความเสี่ยง",
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "translatedText": "ในกรณีนี้ เรื่องราวเริ่มต้นจากเดิมมากขึ้นเกี่ยวกับศิลปินเว็บหนุ่มจากเกาหลีใต้ แต่กลับกลายเป็นเรื่องไร้สาระอย่างรวดเร็ว",
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "translatedText": "ในทางเทคนิคแล้ว API จะไม่ยอมให้คุณเลือกอุณหภูมิที่ใหญ่กว่า 2 จริงๆ",
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "translatedText": "ไม่มีเหตุผลทางคณิตศาสตร์สำหรับสิ่งนี้ มันเป็นเพียงข้อจำกัดตามอำเภอใจที่กำหนดเพื่อไม่ให้มีคนเห็นว่าเครื่องมือของพวกเขาสร้างสิ่งที่ไร้สาระเกินไป",
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "translatedText": "ดังนั้น หากคุณสงสัย วิธีการทำงานของแอนิเมชั่นนี้ก็คือ ฉันกำลังรับโทเค็นถัดไปที่เป็นไปได้มากที่สุด 20 อันที่ GPT-3 สร้างขึ้น ซึ่งดูเหมือนว่าจะเป็นจำนวนเงินสูงสุดที่พวกเขาจะให้ฉัน จากนั้นฉันจะปรับแต่งความน่าจะเป็นตาม บนเลขชี้กำลัง 1 5",
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "translatedText": "ศัพท์เฉพาะอีกประการหนึ่ง ในลักษณะเดียวกับที่คุณเรียกส่วนประกอบของผลลัพธ์ของความน่าจะเป็นของฟังก์ชันนี้ ผู้คนมักเรียกอินพุตว่า logits หรือบางคนบอกว่า logits บางคนบอกว่า logits ฉันจะบอกว่า logits .",
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "translatedText": "ตัวอย่างเช่น เมื่อคุณป้อนข้อความ คุณจะมีคำที่ฝังอยู่ทั้งหมดไหลผ่านเครือข่าย และคุณทำการคูณครั้งสุดท้ายด้วยเมทริกซ์ที่ไม่มีการฝัง ผู้คนที่เรียนรู้เกี่ยวกับเครื่องจักรจะอ้างถึงส่วนประกอบต่างๆ ในเอาต์พุตดิบที่ไม่ปกตินั้นว่าเป็นบันทึก สำหรับการทำนายคำถัดไป",
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "translatedText": "เป้าหมายมากมายในบทนี้คือการวางรากฐานสำหรับการทำความเข้าใจกลไกความสนใจ สไตล์คาราเต้คิดแบบแวกซ์ออนแวกซ์",
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "translatedText": "คุณจะเห็นว่าถ้าคุณมีสัญชาตญาณที่ชัดเจนในการฝังคำ สำหรับ softmax ว่า dot product วัดความคล้ายคลึงกันอย่างไร และยังมีหลักฐานพื้นฐานที่ว่าการคำนวณส่วนใหญ่ต้องมีลักษณะเหมือนการคูณเมทริกซ์ด้วยเมทริกซ์ที่เต็มไปด้วยพารามิเตอร์ที่ปรับได้ จากนั้นจึงทำความเข้าใจความสนใจ กลไกซึ่งเป็นรากฐานสำคัญนี้ในความเจริญรุ่งเรืองสมัยใหม่ใน AI ควรจะค่อนข้างราบรื่น",
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "translatedText": "เพื่อสิ่งนั้น มาร่วมกับฉันในบทต่อไป",
  "input": "For that, come join me in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "translatedText": "ในขณะที่ฉันกำลังเผยแพร่เนื้อหานี้ ผู้สนับสนุน Patreon จะสามารถอ่านฉบับร่างของบทถัดไปได้",
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "translatedText": "เวอร์ชันสุดท้ายควรเผยแพร่สู่สาธารณะภายในหนึ่งหรือสองสัปดาห์ โดยปกติแล้วจะขึ้นอยู่กับว่าฉันต้องเปลี่ยนแปลงไปมากน้อยเพียงใดตามรีวิวนั้น",
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "translatedText": "ในระหว่างนี้ หากคุณต้องการเจาะลึกความสนใจ และหากคุณต้องการช่วยช่องอีกสักหน่อย คุณก็รออยู่",
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]