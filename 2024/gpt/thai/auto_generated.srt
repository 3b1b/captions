1
00:00:00,000 --> 00:00:04,560
ชื่อย่อ GPT ย่อมาจาก Generative Pretrained Transformer

2
00:00:05,220 --> 00:00:09,020
เพื่อให้คำแรกนั้นตรงไปตรงมาเพียงพอ สิ่งเหล่านี้คือบอทที่สร้างข้อความใหม่

3
00:00:09,800 --> 00:00:14,887
การฝึกอบรมล่วงหน้าหมายถึงวิธีที่โมเดลผ่านกระบวนการเรียนรู้จากข้อมูลจำนวนมหาศาล 

4
00:00:14,887 --> 00:00:20,040
และคำนำหน้าบ่งบอกว่ามีพื้นที่มากขึ้นในการปรับแต่งงานเฉพาะด้วยการฝึกอบรมเพิ่มเติม

5
00:00:20,720 --> 00:00:22,900
แต่คำสุดท้ายนั่นคือส่วนสำคัญที่แท้จริง

6
00:00:23,380 --> 00:00:27,479
หม้อแปลงไฟฟ้าเป็นโครงข่ายประสาทเทียมชนิดหนึ่ง ซึ่งเป็นโมเดลการเรียนรู้ของเครื่องจักร 

7
00:00:27,479 --> 00:00:31,000
และเป็นสิ่งประดิษฐ์หลักที่เป็นรากฐานของความเจริญรุ่งเรืองใน AI ในปัจจุบัน

8
00:00:31,740 --> 00:00:35,430
สิ่งที่ฉันต้องการทำกับวิดีโอนี้และบทต่อไปนี้คือคำอธิบายที่ข

9
00:00:35,430 --> 00:00:39,120
ับเคลื่อนด้วยภาพสำหรับสิ่งที่เกิดขึ้นจริงภายในหม้อแปลงไฟฟ้า

10
00:00:39,700 --> 00:00:42,820
เราจะติดตามข้อมูลที่ไหลผ่านและทีละขั้นตอน

11
00:00:43,440 --> 00:00:47,380
มีโมเดลหลายประเภทที่คุณสามารถสร้างโดยใช้หม้อแปลงไฟฟ้า

12
00:00:47,800 --> 00:00:50,800
บางรุ่นใช้เสียงและสร้างการถอดเสียง

13
00:00:51,340 --> 00:00:56,220
ประโยคนี้มาจากแบบจำลองที่หันไปทางอื่น โดยสร้างคำพูดสังเคราะห์จากข้อความเท่านั้น

14
00:00:56,660 --> 00:01:01,089
เครื่องมือทั้งหมดที่ทำให้โลกต้องตะลึงในปี 2022 เช่น Dolly และ 

15
00:01:01,089 --> 00:01:05,519
Midjourney ที่ใส่คำอธิบายและสร้างรูปภาพนั้นอิงจากหม้อแปลงไฟฟ้า

16
00:01:06,000 --> 00:01:09,363
แม้ว่าฉันจะไม่เข้าใจว่าสิ่งมีชีวิตพายควรจะเป็นอย่างไร 

17
00:01:09,363 --> 00:01:13,100
แต่ฉันก็ยังทึ่งที่เรื่องแบบนี้เป็นไปได้แม้จะอยู่ห่างไกลก็ตาม

18
00:01:13,900 --> 00:01:16,930
และหม้อแปลงไฟฟ้าต้นฉบับที่ Google เปิดตัวในปี 2560 

19
00:01:16,930 --> 00:01:22,100
ได้รับการประดิษฐ์ขึ้นเพื่อกรณีการใช้งานเฉพาะในการแปลข้อความจากภาษาหนึ่งเป็นอีกภาษาหนึ่ง

20
00:01:22,660 --> 00:01:27,758
แต่รูปแบบที่คุณและฉันจะเน้น ซึ่งเป็นประเภทที่รองรับเครื่องมืออย่าง 

21
00:01:27,758 --> 00:01:34,074
ChatGPT จะเป็นโมเดลที่ได้รับการฝึกฝนให้รับข้อความ อาจมีภาพหรือเสียงล้อมรอบอยู่ด้วย 

22
00:01:34,074 --> 00:01:38,260
และสร้างคำทำนายได้ สำหรับสิ่งที่จะเกิดขึ้นต่อไปในข้อนี้

23
00:01:38,600 --> 00:01:43,800
การทำนายนั้นอยู่ในรูปแบบของการแจกแจงความน่าจะเป็นบนข้อความส่วนต่างๆ ที่อาจตามมา

24
00:01:45,040 --> 00:01:47,490
เมื่อมองแวบแรก คุณอาจคิดว่าการคาดเดาคำถัดไปให้ความรู้ส

25
00:01:47,490 --> 00:01:49,940
ึกเหมือนเป็นเป้าหมายที่แตกต่างไปจากการสร้างข้อความใหม่

26
00:01:50,180 --> 00:01:54,971
แต่เมื่อคุณมีโมเดลการทำนายเช่นนี้ สิ่งง่ายๆ ที่คุณสร้างข้อความที่ยาวขึ้นได

27
00:01:54,971 --> 00:02:00,410
้คือให้ตัวอย่างเริ่มต้นใช้งานได้ โดยให้มันสุ่มตัวอย่างจากการแจกแจงที่เพิ่งสร้างขึ้น 

28
00:02:00,410 --> 00:02:05,201
แล้วผนวกตัวอย่างนั้นเข้ากับข้อความ จากนั้นรันกระบวนการทั้งหมดอีกครั้งเพื่อ

29
00:02:05,201 --> 00:02:09,539
ทำการคาดคะเนใหม่ตามข้อความใหม่ทั้งหมด รวมถึงสิ่งที่เพิ่งเพิ่มเข้าไป

30
00:02:10,100 --> 00:02:13,000
ฉันไม่รู้เกี่ยวกับคุณ แต่รู้สึกว่าวิธีนี้ไม่น่าจะได้ผลจริงๆ

31
00:02:13,420 --> 00:02:18,228
ตัวอย่างเช่น ในแอนิเมชันนี้ ฉันใช้ GPT-2 บนแล็ปท็อปของฉัน และให้มันคาดเดาซ้ำๆ 

32
00:02:18,228 --> 00:02:22,420
และสุ่มตัวอย่างข้อความชิ้นถัดไปเพื่อสร้างเรื่องราวตามข้อความเริ่มต้น

33
00:02:22,420 --> 00:02:26,120
เรื่องราวไม่ได้สมเหตุสมผลขนาดนั้นจริงๆ

34
00:02:26,500 --> 00:02:31,056
แต่ถ้าฉันเปลี่ยนเป็นการเรียก API เป็น GPT-3 แทน ซึ่งเป็นโมเดลพื้นฐานเดียวกัน 

35
00:02:31,056 --> 00:02:35,139
ซึ่งใหญ่กว่ามาก และเกือบจะน่าอัศจรรย์ เราก็ได้เรื่องราวที่สมเหตุสมผล 

36
00:02:35,139 --> 00:02:39,045
เรื่องราวที่ดูเหมือนว่าจะอนุมานได้ว่าสิ่งมีชีวิต pi จะอาศัยอยู่ใน 

37
00:02:39,045 --> 00:02:40,880
ดินแดนแห่งคณิตศาสตร์และการคำนวณ

38
00:02:41,580 --> 00:02:46,894
กระบวนการคาดการณ์และการสุ่มตัวอย่างซ้ำที่นี่เป็นสิ่งที่เกิดขึ้นเมื่อคุณโต้ตอบกับ 

39
00:02:46,894 --> 00:02:51,880
ChatGPT หรือโมเดลภาษาขนาดใหญ่อื่นๆ เหล่านี้ และคุณเห็นว่าพวกเขาสร้างคำทีละคำ

40
00:02:52,480 --> 00:02:55,815
อันที่จริง คุณลักษณะหนึ่งที่ฉันชอบมากคือความสามา

41
00:02:55,815 --> 00:02:59,220
รถในการดูการแจกแจงพื้นฐานของคำใหม่แต่ละคำที่เลือก

42
00:03:03,820 --> 00:03:08,180
มาเริ่มกันด้วยการแสดงตัวอย่างระดับสูงว่าข้อมูลไหลผ่านหม้อแปลงอย่างไร

43
00:03:08,640 --> 00:03:13,285
เราจะใช้เวลามากขึ้นในการจูงใจ ตีความ และขยายรายละเอียดของแต่ละขั้นตอน 

44
00:03:13,285 --> 00:03:18,660
แต่เมื่อแชทบอทตัวใดตัวหนึ่งสร้างคำที่กำหนด ต่อไปนี้คือสิ่งที่เกิดขึ้นภายใต้ประทุน

45
00:03:19,080 --> 00:03:22,040
ขั้นแรก อินพุตจะถูกแบ่งออกเป็นส่วนๆ เล็กๆ น้อยๆ

46
00:03:22,620 --> 00:03:25,705
ชิ้นส่วนเหล่านี้เรียกว่าโทเค็น และในกรณีของข้อความ 

47
00:03:25,705 --> 00:03:29,820
สิ่งเหล่านี้มักจะเป็นคำหรือคำเล็กๆ น้อยๆ หรือการผสมอักขระทั่วไปอื่นๆ

48
00:03:30,740 --> 00:03:34,625
หากมีภาพหรือเสียงเข้ามาเกี่ยวข้อง โทเค็นอาจเป็นส่วนเล็กๆ 

49
00:03:34,625 --> 00:03:37,080
ของภาพนั้นหรือส่วนเล็กๆ ของเสียงนั้น

50
00:03:37,580 --> 00:03:41,432
โทเค็นแต่ละอันจะเชื่อมโยงกับเวกเตอร์ ซึ่งหมายถึงรายก

51
00:03:41,432 --> 00:03:45,360
ารตัวเลขซึ่งมีไว้เพื่อเข้ารหัสความหมายของชิ้นส่วนนั้น

52
00:03:45,880 --> 00:03:50,119
หากคุณคิดว่าเวกเตอร์เหล่านี้เป็นการให้พิกัดในพื้นที่มิติที่สูงมาก 

53
00:03:50,119 --> 00:03:54,680
คำที่มีความหมายคล้ายกันมักจะไปเกาะบนเวกเตอร์ที่อยู่ใกล้กันในพื้นที่นั้น

54
00:03:55,280 --> 00:03:59,545
ลำดับของเวกเตอร์นี้จะผ่านการดำเนินการที่เรียกว่าบล็อกความสนใจ 

55
00:03:59,545 --> 00:04:04,500
และช่วยให้เวกเตอร์สามารถพูดคุยกันและส่งข้อมูลไปมาเพื่ออัปเดตค่าของพวกเขา

56
00:04:04,880 --> 00:04:08,025
ตัวอย่างเช่น ความหมายของคำว่า model ในวลี a model 

57
00:04:08,025 --> 00:04:11,800
การเรียนรู้ของเครื่องแตกต่างจากความหมายในวลี a fashion model

58
00:04:12,260 --> 00:04:17,072
บล็อกความสนใจคือสิ่งที่มีหน้าที่ค้นหาว่าคำใดในบริบทที่เกี่ยวข้องก

59
00:04:17,072 --> 00:04:21,959
ับการอัปเดตความหมายของคำอื่นๆ และควรอัปเดตความหมายเหล่านั้นอย่างไร

60
00:04:22,500 --> 00:04:26,440
และอีกครั้ง, ทุกครั้งที่ผมใช้คำว่า ความหมาย, นี่จะถูกเข้ารหัสทั้งหมด 

61
00:04:26,440 --> 00:04:28,040
ในรายการของเวกเตอร์เหล่านั้น

62
00:04:29,180 --> 00:04:32,186
หลังจากนั้น เวกเตอร์เหล่านี้จะผ่านการดำเนินการประเภทอื่น 

63
00:04:32,186 --> 00:04:36,670
และขึ้นอยู่กับแหล่งที่มาที่คุณกำลังอ่านอยู่ เราจะเรียกว่าเพอร์เซพตรอนแบบหลายเลเยอร์หร

64
00:04:36,670 --> 00:04:38,200
ืออาจเป็นเลเยอร์ฟีดฟอร์เวิร์ด

65
00:04:38,580 --> 00:04:42,660
และตรงนี้เวกเตอร์ไม่คุยกัน, พวกมันทั้งหมดผ่านการดำเนินการแบบขนานกัน

66
00:04:43,060 --> 00:04:46,687
และในขณะที่บล็อกนี้ตีความได้ยากกว่านิดหน่อย แต่ต่อไปเราจะพูดถึง

67
00:04:46,687 --> 00:04:50,833
ว่าขั้นตอนนั้นเหมือนกับการถามคำถามยาวๆ เกี่ยวกับเวกเตอร์แต่ละตัวอย่างไร 

68
00:04:50,833 --> 00:04:54,000
จากนั้นจึงอัปเดตคำถามเหล่านั้นตามคำตอบของคำถามเหล่านั้น

69
00:04:54,900 --> 00:05:00,688
การดำเนินการทั้งหมดในบล็อกทั้งสองนี้ดูเหมือนกองคูณเมทริกซ์จำนวนมหาศาล 

70
00:05:00,688 --> 00:05:05,320
และงานหลักของเราคือ เข้าใจวิธีอ่านเมทริกซ์ที่อยู่ข้างใต้

71
00:05:06,980 --> 00:05:11,334
ฉันกำลังอธิบายรายละเอียดบางอย่างเกี่ยวกับขั้นตอนการทำให้เป็นมาตรฐานที่เกิดขึ้นระหว่างนั้น 

72
00:05:11,334 --> 00:05:12,980
แต่นี่คือการแสดงตัวอย่างในระดับสูง

73
00:05:13,680 --> 00:05:18,620
หลังจากนั้น กระบวนการจะเกิดซ้ำ โดยคุณกลับไปกลับมาระหว่างบล็อกความสนใจแ

74
00:05:18,620 --> 00:05:23,207
ละบล็อกการรับรู้หลายชั้น จนกระทั่งถึงจุดสิ้นสุด ความหวังก็คือว่า 

75
00:05:23,207 --> 00:05:28,500
ความหมายที่สำคัญทั้งหมดของข้อความนี้ได้ถูกรวมเข้ากับเวกเตอร์สุดท้ายใน ลำดับ

76
00:05:28,920 --> 00:05:33,670
จากนั้นเราดำเนินการบางอย่างกับเวกเตอร์สุดท้ายที่สร้างการแจกแจงความน่าจะเป็นสำหรับโทเค

77
00:05:33,670 --> 00:05:38,420
็นที่เป็นไปได้ทั้งหมด ซึ่งเป็นข้อความชิ้นเล็กๆ ที่เป็นไปได้ทั้งหมดที่อาจเกิดขึ้นถัดไป

78
00:05:38,980 --> 00:05:44,245
และอย่างที่ผมบอกไป เมื่อคุณมีเครื่องมือที่คาดเดาสิ่งที่จะเกิดขึ้นถัดไปด้วยตัวอย่างข้อความ 

79
00:05:44,245 --> 00:05:47,989
คุณสามารถป้อนข้อความเริ่มต้นได้เล็กน้อย และให้มันเล่นเกมนี้ซ้ำๆ 

80
00:05:47,989 --> 00:05:53,080
ของการทำนายสิ่งที่จะเกิดขึ้นต่อไป โดยสุ่มตัวอย่างจากการแจกแจง ต่อท้าย มันแล้วซ้ำไปซ้ำมา

81
00:05:53,640 --> 00:05:56,808
พวกคุณบางคนที่รู้อาจจำได้ว่านานแค่ไหนก่อนที่ ChatGPT 

82
00:05:56,808 --> 00:06:00,156
จะเข้ามามีบทบาท นี่คือหน้าตาของการสาธิต GPT-3 ในช่วงแรก 

83
00:06:00,156 --> 00:06:04,640
คุณจะต้องให้ระบบเติมเรื่องราวและเรียงความอัตโนมัติโดยอิงจากตัวอย่างเริ่มต้น

84
00:06:05,580 --> 00:06:08,575
หากต้องการสร้างเครื่องมือเช่นนี้ให้เป็นแชทบอต 

85
00:06:08,575 --> 00:06:13,915
จุดเริ่มต้นที่ง่ายที่สุดคือการมีข้อความเล็กน้อยที่กำหนดการตั้งค่าของผู้ใช้โต้ตอบกั

86
00:06:13,915 --> 00:06:18,864
บผู้ช่วย AI ที่เป็นประโยชน์ คุณจะเรียกว่าข้อความแจ้งของระบบ จากนั้นคุณจะใช้ 

87
00:06:18,864 --> 00:06:24,465
คำถามหรือข้อความเริ่มต้นของผู้ใช้เป็นบทสนทนาชิ้นแรก จากนั้นคุณก็เริ่มคาดเดาว่าผู้ช่วย 

88
00:06:24,465 --> 00:06:26,940
AI ที่เป็นประโยชน์จะพูดอะไรเพื่อตอบโต้

89
00:06:27,720 --> 00:06:32,428
ยังมีอีกหลายสิ่งที่จะพูดเกี่ยวกับขั้นตอนการฝึกอบรมที่จำเป็นเพื่อให้งานนี้ออกมาดี 

90
00:06:32,428 --> 00:06:33,940
แต่ในระดับสูง นี่คือแนวคิด

91
00:06:35,720 --> 00:06:40,944
ในบทนี้ คุณและฉันจะขยายรายละเอียดของสิ่งที่เกิดขึ้นที่จุดเริ่มต้นของเครือข่าย 

92
00:06:40,944 --> 00:06:46,571
ที่จุดสิ้นสุดของเครือข่าย และฉันต้องการใช้เวลาส่วนใหญ่ในการทบทวนความรู้พื้นฐานบางส่ว

93
00:06:46,571 --> 00:06:52,600
นที่สำคัญ สิ่งต่าง ๆ ที่จะกลายมาเป็นลักษณะที่สองของวิศวกรแมชชีนเลิร์นนิงเมื่อหม้อแปลงมาถึง

94
00:06:53,060 --> 00:06:58,274
หากคุณพอใจกับความรู้พื้นฐานนั้นและใจร้อนนิดหน่อย คุณสามารถข้ามไปยังบทถัดไปได้เลย 

95
00:06:58,274 --> 00:07:02,780
ซึ่งจะเน้นไปที่บล็อคความสนใจ ซึ่งโดยทั่วไปถือเป็นหัวใจของหม้อแปลงไฟฟ้า

96
00:07:03,360 --> 00:07:07,438
หลังจากนั้น ฉันต้องการพูดคุยเพิ่มเติมเกี่ยวกับบล็อกเพอร์เซปตรอนแบบหลายชั้น 

97
00:07:07,438 --> 00:07:11,680
วิธีการทำงานของการฝึก และรายละเอียดอื่นๆ อีกจำนวนหนึ่งที่จะถูกข้ามไปยังจุดนั้น

98
00:07:12,180 --> 00:07:16,226
สำหรับบริบทที่กว้างขึ้น วิดีโอเหล่านี้เป็นส่วนเสริมของมินิซีรีส์เกี่ยวกับการเรี

99
00:07:16,226 --> 00:07:19,299
ยนรู้เชิงลึก และไม่เป็นไรหากคุณยังไม่ได้ดูรายการก่อนหน้านี้ 

100
00:07:19,299 --> 00:07:23,551
ฉันคิดว่าคุณน่าจะทำมันผิดระเบียบได้ แต่ก่อนที่จะเจาะลึกเรื่อง Transformer โดยเฉพาะ 

101
00:07:23,551 --> 00:07:27,597
ฉันคิดว่า ควรทำให้แน่ใจว่าเราเข้าใจตรงกันเกี่ยวกับหลักฐานพื้นฐานและโครงสร้างของ

102
00:07:27,597 --> 00:07:28,520
การเรียนรู้เชิงลึก

103
00:07:29,020 --> 00:07:33,749
ด้วยความเสี่ยงที่จะระบุสิ่งที่ชัดเจน นี่เป็นแนวทางหนึ่งในการเรียนรู้ของเครื่อง 

104
00:07:33,749 --> 00:07:38,300
ซึ่งจะอธิบายโมเดลใดๆ ก็ตามที่คุณกำลังใช้ข้อมูลเพื่อกำหนดวิธีการทำงานของโมเดล

105
00:07:39,140 --> 00:07:44,520
สิ่งที่ฉันหมายถึงคือ สมมติว่าคุณต้องการฟังก์ชันที่รับรูปภาพและสร้างป้ายกำกับที่อธิบาย 

106
00:07:44,520 --> 00:07:47,649
หรือตัวอย่างของเราในการทำนายคำถัดไปเมื่อมีข้อความ 

107
00:07:47,649 --> 00:07:52,780
หรืองานอื่นใดที่ดูเหมือนว่าจะต้องมีองค์ประกอบบางอย่าง ของสัญชาตญาณและการจดจำรูปแบบ

108
00:07:53,200 --> 00:07:58,034
ทุกวันนี้เราแทบจะมองข้ามสิ่งนี้ไป แต่แนวคิดเกี่ยวกับการเรียนรู้ของเครื่องก็คือ 

109
00:07:58,034 --> 00:08:01,706
แทนที่จะพยายามกำหนดขั้นตอนในการทำงานนั้นอย่างชัดเจนด้วยโค้ด 

110
00:08:01,706 --> 00:08:04,766
ซึ่งเป็นสิ่งที่ผู้คนจะทำในช่วงแรก ๆ ของ AI แทนคุณ 

111
00:08:04,766 --> 00:08:08,500
ตั้งค่าโครงสร้างที่ยืดหยุ่นมากด้วยพารามิเตอร์ที่ปรับได้ เช่น 

112
00:08:08,500 --> 00:08:13,763
ปุ่มหมุนและแป้นหมุนจำนวนหนึ่ง จากนั้นคุณก็ใช้ตัวอย่างมากมายว่าเอาต์พุตควรมีลักษณะอย่าง

113
00:08:13,763 --> 00:08:19,026
ไรสำหรับอินพุตที่กำหนด เพื่อปรับแต่งและปรับแต่งค่าของพารามิเตอร์เหล่านั้นเพื่อเลียนแบบ

114
00:08:19,026 --> 00:08:19,700
พฤติกรรมนี้

115
00:08:19,700 --> 00:08:24,537
ตัวอย่างเช่น รูปแบบแมชชีนเลิร์นนิงที่ง่ายที่สุดอาจเป็นการถดถอยเชิงเส้น 

116
00:08:24,537 --> 00:08:28,079
โดยที่อินพุตและเอาท์พุตของคุณเป็นตัวเลขเดี่ยวๆ เช่น 

117
00:08:28,079 --> 00:08:33,734
พื้นที่เป็นตารางฟุตของบ้านและราคาของบ้าน และสิ่งที่คุณต้องการคือหาเส้นที่เหมาะสมที่

118
00:08:33,734 --> 00:08:36,799
สุดกับสิ่งนี้ ข้อมูลเพื่อทำนายราคาบ้านในอนาคต

119
00:08:37,440 --> 00:08:42,407
เส้นดังกล่าวอธิบายด้วยพารามิเตอร์ต่อเนื่องสองตัว เช่น ความชันและค่าตัดแกน y 

120
00:08:42,407 --> 00:08:48,160
และเป้าหมายของการถดถอยเชิงเส้นคือการกำหนดพารามิเตอร์เหล่านั้นให้ตรงกับข้อมูลอย่างใกล้ชิด

121
00:08:48,880 --> 00:08:52,100
ไม่ต้องพูดอะไรมาก โมเดลการเรียนรู้เชิงลึกมีความซับซ้อนมากขึ้น

122
00:08:52,620 --> 00:08:57,660
ตัวอย่างเช่น GPT-3 มีพารามิเตอร์ไม่ใช่สองตัว แต่มี 175 พันล้านพารามิเตอร์

123
00:08:58,120 --> 00:09:05,715
แต่สิ่งสำคัญคือ ไม่ใช่ว่าคุณสามารถสร้างโมเดลขนาดยักษ์ที่มีพารามิเตอร์จำนวนมากได้ 

124
00:09:05,715 --> 00:09:09,560
โดยที่ข้อมูลการฝึกไม่พอดีหรือฝึกยากเกินไป

125
00:09:10,260 --> 00:09:13,194
การเรียนรู้เชิงลึกอธิบายถึงคลาสของโมเดลที่ในช่วงสองสามทศวร

126
00:09:13,194 --> 00:09:16,180
รษที่ผ่านมาได้พิสูจน์แล้วว่าสามารถขยายขนาดได้ดีอย่างน่าทึ่ง

127
00:09:16,480 --> 00:09:20,951
สิ่งที่รวมเป็นหนึ่งเดียวคืออัลกอริธึมการฝึกอบรมแบบเดียวกัน ที่เรียกว่า 

128
00:09:20,951 --> 00:09:25,108
backpropagation และบริบทที่ฉันอยากให้คุณมีในระหว่างดำเนินการก็คือ 

129
00:09:25,108 --> 00:09:28,319
เพื่อให้อัลกอริธึมการฝึกอบรมนี้ทำงานได้ดีในวงกว้าง 

130
00:09:28,319 --> 00:09:31,280
โมเดลเหล่านี้จะต้องเป็นไปตามรูปแบบเฉพาะบางอย่าง

131
00:09:31,800 --> 00:09:36,421
หากคุณรู้ว่ารูปแบบนี้กำลังเข้ามา จะช่วยอธิบายตัวเลือกต่างๆ มากมายสำหรับวิธีที่ 

132
00:09:36,421 --> 00:09:40,400
Transformer ประมวลผลภาษา ซึ่งมิฉะนั้นจะเสี่ยงต่อความรู้สึกตามอำเภอใจ

133
00:09:41,440 --> 00:09:44,090
อันดับแรก ไม่ว่าคุณกำลังสร้างโมเดลใดก็ตาม ข้อมู

134
00:09:44,090 --> 00:09:46,740
ลเข้าจะต้องถูกจัดรูปแบบเป็นอาร์เรย์ของจำนวนจริง

135
00:09:46,740 --> 00:09:50,203
นี่อาจหมายถึงรายการตัวเลข อาจเป็นอาร์เรย์สองมิติ 

136
00:09:50,203 --> 00:09:56,000
หรือบ่อยครั้งที่คุณจัดการกับอาร์เรย์มิติที่สูงกว่า โดยที่คำทั่วไปที่ใช้คือเทนเซอร์

137
00:09:56,560 --> 00:10:01,754
คุณมักจะคิดว่าข้อมูลอินพุตนั้นถูกแปลงอย่างต่อเนื่องเป็นเลเยอร์ที่แตกต่างกันจำนวนมาก 

138
00:10:01,754 --> 00:10:05,464
โดยที่แต่ละเลเยอร์จะมีโครงสร้างเป็นอาร์เรย์ของจำนวนจริงเสมอ 

139
00:10:05,464 --> 00:10:08,680
จนกว่าคุณจะไปถึงเลเยอร์สุดท้ายที่คุณจะพิจารณาผลลัพธ์

140
00:10:09,280 --> 00:10:13,142
ตัวอย่างเช่น เลเยอร์สุดท้ายในโมเดลการประมวลผลข้อความของเราคือรายการตัว

141
00:10:13,142 --> 00:10:17,060
เลขที่แสดงถึงการแจกแจงความน่าจะเป็นสำหรับโทเค็นถัดไปที่เป็นไปได้ทั้งหมด

142
00:10:17,820 --> 00:10:21,768
ในการเรียนรู้เชิงลึก พารามิเตอร์โมเดลเหล่านี้มักถูกเรียกว่าน้ำหนัก 

143
00:10:21,768 --> 00:10:24,773
และนี่เป็นเพราะคุณลักษณะสำคัญของโมเดลเหล่านี้ก็คือ 

144
00:10:24,773 --> 00:10:29,900
วิธีเดียวที่พารามิเตอร์เหล่านี้โต้ตอบกับข้อมูลที่กำลังประมวลผลก็คือผ่านผลรวมถ่วงน้ำหนัก

145
00:10:30,340 --> 00:10:32,492
คุณยังโรยฟังก์ชันที่ไม่ใช่เชิงเส้นบางส่วนให้ทั่วด้วย 

146
00:10:32,492 --> 00:10:34,360
แต่ฟังก์ชันเหล่านี้จะไม่ขึ้นอยู่กับพารามิเตอร์

147
00:10:35,200 --> 00:10:41,284
โดยทั่วไปแล้ว แทนที่จะเห็นผลรวมถ่วงน้ำหนักทั้งหมดเปล่าๆ และเขียนออกมาอย่างชัดเจนแบบนี้ 

148
00:10:41,284 --> 00:10:45,620
คุณจะพบว่ามันรวมเข้าด้วยกันเป็นส่วนประกอบต่างๆ ในผลคูณเมทริกซ์

149
00:10:46,740 --> 00:10:51,462
มันเหมือนกับพูดสิ่งเดียวกัน หากคุณคิดย้อนกลับไปว่าการคูณเวกเตอร์เมทริกซ์ทำงานอย่างไร 

150
00:10:51,462 --> 00:10:54,240
แต่ละส่วนประกอบในผลลัพธ์จะดูเหมือนผลรวมถ่วงน้ำหนัก

151
00:10:54,780 --> 00:11:00,099
บ่อยครั้งที่แนวคิดจะสะอาดกว่าสำหรับคุณและฉันที่จะคิดถึงเมทริกซ์ที่เต็มไปด้ว

152
00:11:00,099 --> 00:11:05,420
ยพารามิเตอร์ที่ปรับแต่งได้ซึ่งแปลงเวกเตอร์ที่ดึงมาจากข้อมูลที่กำลังประมวลผล

153
00:11:06,340 --> 00:11:09,881
ตัวอย่างเช่น น้ำหนัก 175 พันล้านน้ำหนักใน GPT-3 

154
00:11:09,881 --> 00:11:14,160
ได้รับการจัดเป็นเมทริกซ์ที่แตกต่างกันไม่เกิน 28,000 รายการ

155
00:11:14,660 --> 00:11:17,660
เมทริกซ์เหล่านั้นแบ่งออกเป็นแปดหมวดหมู่ที่แตกต่างกัน 

156
00:11:17,660 --> 00:11:22,700
และสิ่งที่คุณและฉันจะทำคือการดูแต่ละหมวดหมู่เหล่านั้นเพื่อทำความเข้าใจว่าประเภทนั้นทำอะไร

157
00:11:23,160 --> 00:11:27,632
ขณะที่เราดำเนินการนี้ ฉันคิดว่าเป็นเรื่องสนุกที่จะอ้างอิงตัวเลขเฉพาะจาก 

158
00:11:27,632 --> 00:11:31,360
GPT-3 เพื่อนับจำนวนที่แน่นอนว่า 175 พันล้านจำนวนนั้นมาจากไหน

159
00:11:31,880 --> 00:11:34,944
แม้ว่าในปัจจุบันจะมีโมเดลที่ใหญ่กว่าและดีกว่า 

160
00:11:34,944 --> 00:11:40,740
แต่โมเดลนี้มีเสน่ห์บางอย่างในฐานะโมเดลภาษาขนาดใหญ่ที่ดึงดูดความสนใจของโลกภายนอกชุมชน ML

161
00:11:41,440 --> 00:11:44,062
นอกจากนี้ ในทางปฏิบัติแล้ว บริษัทต่างๆ มักจะเข้

162
00:11:44,062 --> 00:11:46,740
มงวดกับตัวเลขเฉพาะสำหรับเครือข่ายสมัยใหม่มากขึ้น

163
00:11:47,360 --> 00:11:53,017
ฉันแค่อยากจะจัดฉากว่าเมื่อคุณแอบดูเบื้องหลังเพื่อดูว่าเกิดอะไรขึ้นภายในเครื่องมืออย่าง 

164
00:11:53,017 --> 00:11:57,440
ChatGPT การคำนวณจริงเกือบทั้งหมดจะดูเหมือนเป็นการคูณเมทริกซ์เวกเตอร์

165
00:11:57,900 --> 00:12:01,432
มีความเสี่ยงเล็กน้อยที่จะสูญหายไปในทะเลตัวเลขนับพันล้าน 

166
00:12:01,432 --> 00:12:05,532
แต่คุณควรแยกแยะความแตกต่างที่ชัดเจนในใจระหว่างน้ำหนักของแบบจำลอง 

167
00:12:05,532 --> 00:12:10,263
ซึ่งฉันจะกำหนดเป็นสีน้ำเงินหรือสีแดงเสมอ และข้อมูลที่เป็นอยู่ ประมวลผลแล้ว 

168
00:12:10,263 --> 00:12:11,840
ซึ่งฉันจะให้เป็นสีเทาเสมอ

169
00:12:12,180 --> 00:12:15,752
ตุ้มน้ำหนักคือสมองที่แท้จริง คือสิ่งที่เรียนรู้ระหว่างการฝึก 

170
00:12:15,752 --> 00:12:17,920
และเป็นตัวกำหนดว่าจะมีพฤติกรรมอย่างไร

171
00:12:18,280 --> 00:12:21,676
ข้อมูลที่กำลังประมวลผลเพียงเข้ารหัสอินพุตเฉพาะใดๆ 

172
00:12:21,676 --> 00:12:26,500
ก็ตามที่ถูกป้อนเข้าไปในโมเดลสำหรับการทำงานที่กำหนด เช่น ตัวอย่างข้อความ

173
00:12:27,480 --> 00:12:30,427
เมื่อพิจารณาทั้งหมดนี้เป็นรากฐานแล้ว เรามาเจาะลึกถึงขั้นตอนแ

174
00:12:30,427 --> 00:12:34,553
รกของตัวอย่างการประมวลผลข้อความนี้กัน ซึ่งก็คือการแบ่งข้อมูลที่ป้อนออกเป็นส่วนเล็กๆ 

175
00:12:34,553 --> 00:12:36,420
และเปลี่ยนส่วนเหล่านั้นให้เป็นเวกเตอร์

176
00:12:37,020 --> 00:12:39,855
ฉันพูดถึงการที่ชิ้นส่วนเหล่านั้นถูกเรียกว่าโทเค็น 

177
00:12:39,855 --> 00:12:42,521
ซึ่งอาจเป็นชิ้นส่วนของคำหรือเครื่องหมายวรรคตอน 

178
00:12:42,521 --> 00:12:46,208
แต่ในบทนี้และโดยเฉพาะอย่างยิ่งในบทถัดไป ฉันอยากจะแกล้งทำเป็นว่ามั

179
00:12:46,208 --> 00:12:48,080
นถูกแยกออกเป็นคำอย่างหมดจดมากขึ้น

180
00:12:48,600 --> 00:12:51,919
เนื่องจากมนุษย์เราคิดเป็นคำพูด จึงทำให้การอ้างอิงตัวอย่างเล็กๆ 

181
00:12:51,919 --> 00:12:54,080
น้อยๆ และชี้แจงแต่ละขั้นตอนได้ง่ายขึ้นมาก

182
00:12:55,260 --> 00:13:01,565
แบบจำลองนี้มีคำศัพท์ที่กำหนดไว้ล่วงหน้า รายการคำศัพท์ที่เป็นไปได้ทั้งหมด ประมาณ 50,000 คำ 

183
00:13:01,565 --> 00:13:07,800
และเมทริกซ์แรกที่เราจะพบ ซึ่งเรียกว่าเมทริกซ์แบบฝัง จะมีคอลัมน์เดียวสำหรับแต่ละคำเหล่านี้

184
00:13:08,940 --> 00:13:13,760
คอลัมน์เหล่านี้คือสิ่งที่กำหนดว่าแต่ละคำจะกลายเป็นเวกเตอร์ใดในขั้นตอนแรก

185
00:13:15,100 --> 00:13:19,421
เราตั้งชื่อมันว่า &quot;เรา&quot; และเช่นเดียวกับเมทริกซ์ทั้งหมดที่เราเห็น 

186
00:13:19,421 --> 00:13:22,360
ค่าของมันเริ่มต้นแบบสุ่ม แต่จะต้องเรียนรู้จากข้อมูล

187
00:13:23,620 --> 00:13:28,215
การเปลี่ยนคำให้เป็นเวกเตอร์ถือเป็นเรื่องปกติในแมชชีนเลิร์นนิงก่อนมีหม้อแปลงไฟฟ้า 

188
00:13:28,215 --> 00:13:32,980
แต่จะแปลกนิดหน่อยหากคุณไม่เคยเห็นมาก่อน และสิ่งนี้ได้วางรากฐานสำหรับทุกสิ่งที่ตามมา 

189
00:13:32,980 --> 00:13:35,760
ดังนั้น เรามาดูกันสักครู่เพื่อทำความคุ้นเคยกับมัน

190
00:13:36,040 --> 00:13:39,795
เรามักเรียกสิ่งนี้ว่าการฝังคำ ซึ่งเชิญชวนให้คุณนึกถึงเ

191
00:13:39,795 --> 00:13:43,620
วกเตอร์เหล่านี้ในเชิงเรขาคณิตว่าเป็นจุดในพื้นที่มิติสูง

192
00:13:44,180 --> 00:13:47,944
การแสดงรายการตัวเลขสามตัวเป็นพิกัดสำหรับจุดในพื้นที่ 

193
00:13:47,944 --> 00:13:51,780
3 มิติจะไม่มีปัญหา แต่การฝังคำมักจะมีมิติที่สูงกว่ามาก

194
00:13:52,280 --> 00:13:55,798
ใน GPT-3 มีขนาด 12,288 มิติ และอย่างที่คุณเห็น 

195
00:13:55,798 --> 00:14:00,440
การทำงานในพื้นที่ที่มีทิศทางที่แตกต่างกันจำนวนมากเป็นสิ่งสำคัญ

196
00:14:01,180 --> 00:14:05,360
ในลักษณะเดียวกับที่คุณสามารถนำชิ้นสองมิติผ่านพื้นที่ 3 มิติ 

197
00:14:05,360 --> 00:14:10,168
และฉายจุดทั้งหมดลงบนชิ้นนั้น เพื่อประโยชน์ของการสร้างภาพเคลื่อนไหวการ

198
00:14:10,168 --> 00:14:14,000
ฝังคำที่แบบจำลองง่ายๆ ให้มา ฉันจะทำสิ่งที่คล้ายคลึงกัน 

199
00:14:14,000 --> 00:14:17,483
โดยการเลือกชิ้นสามมิติผ่านพื้นที่มิติที่สูงมากนี้ 

200
00:14:17,483 --> 00:14:20,480
และฉายคำว่าเวกเตอร์ลงไปบนนั้นและแสดงผลลัพธ์

201
00:14:21,280 --> 00:14:26,382
แนวคิดสำคัญที่นี่คือเมื่อแบบจำลองปรับแต่งและปรับน้ำหนักเพื่อกำหนดว่าคำต่างๆ 

202
00:14:26,382 --> 00:14:29,471
จะถูกฝังเป็นเวกเตอร์ในระหว่างการฝึกได้อย่างไร 

203
00:14:29,471 --> 00:14:34,440
โมเดลนั้นมักจะตั้งอยู่บนชุดของการฝังที่ทิศทางในอวกาศมีความหมายเชิงความหมาย

204
00:14:34,980 --> 00:14:38,136
สำหรับโมเดลคำต่อเวกเตอร์แบบง่ายๆ ที่ฉันใช้อยู่นี้ 

205
00:14:38,136 --> 00:14:41,670
หากฉันค้นหาคำทั้งหมดที่มีการฝังไว้ใกล้กับหอคอยมากที่สุด 

206
00:14:41,670 --> 00:14:45,900
คุณจะสังเกตเห็นว่าคำเหล่านั้นให้ความรู้สึกเหมือนหอคอยที่คล้ายกันมาก

207
00:14:46,340 --> 00:14:51,380
และถ้าคุณต้องการดึง Python ขึ้นมาแล้วเล่นที่บ้าน นี่คือโมเดลเฉพาะที่ฉันใช้สร้างแอนิเมชั่น

208
00:14:51,620 --> 00:14:54,578
ไม่ใช่หม้อแปลงไฟฟ้า แต่ก็เพียงพอที่จะแสดงให้เห็

209
00:14:54,578 --> 00:14:57,600
นว่าทิศทางในอวกาศสามารถมีความหมายเชิงความหมายได้

210
00:14:58,300 --> 00:15:04,113
ตัวอย่างคลาสสิกของเรื่องนี้คือถ้าคุณหาความแตกต่างระหว่างเวกเตอร์สำหรับผู้หญิงและผู้ชาย 

211
00:15:04,113 --> 00:15:09,792
สิ่งที่คุณจินตนาการว่าเป็นเวกเตอร์เล็กๆ ที่เชื่อมปลายของอันหนึ่งเข้ากับปลายของอีกอัน 

212
00:15:09,792 --> 00:15:13,200
มันจะคล้ายกันมากกับความแตกต่างระหว่างราชากับ ราชินี

213
00:15:15,080 --> 00:15:20,838
สมมุติว่าคุณไม่รู้จักคำว่ากษัตริย์หญิง คุณสามารถหาได้โดยการขึ้นเป็นกษัตริย์ 

214
00:15:20,838 --> 00:15:25,460
เพิ่มทิศทางหญิง-ชาย และค้นหาส่วนที่ฝังใกล้กับจุดนั้นมากที่สุด

215
00:15:27,000 --> 00:15:28,200
อย่างน้อยก็ชนิดของ

216
00:15:28,480 --> 00:15:32,198
แม้ว่านี่จะเป็นตัวอย่างคลาสสิกสำหรับโมเดลที่ฉันเล่นด้วย แต่จริงๆ 

217
00:15:32,198 --> 00:15:35,631
แล้วการฝังราชินีที่แท้จริงนั้นไกลเกินกว่าที่จะแนะนำเล็กน้อย 

218
00:15:35,631 --> 00:15:40,780
อาจเป็นเพราะวิธีที่ราชินีใช้ในข้อมูลการฝึกอบรมไม่ได้เป็นเพียงราชาในเวอร์ชันผู้หญิงเท่านั้น

219
00:15:41,620 --> 00:15:45,260
เมื่อฉันลองเล่นดู ความสัมพันธ์ในครอบครัวดูเหมือนจะอธิบายแนวคิดได้ดีขึ้นมาก

220
00:15:46,340 --> 00:15:51,768
ประเด็นก็คือ ดูเหมือนว่าในระหว่างการฝึกอบรม โมเดลพบว่ามีข้อดีในการเลือกการฝัง 

221
00:15:51,768 --> 00:15:54,900
โดยทิศทางเดียวในพื้นที่นี้จะเข้ารหัสข้อมูลเพศ

222
00:15:56,800 --> 00:16:01,891
อีกตัวอย่างหนึ่งก็คือ ถ้าคุณเอาการฝังของอิตาลี และลบการฝังของเยอรมนี 

223
00:16:01,891 --> 00:16:08,090
แล้วบวกเข้ากับการฝังของฮิตเลอร์ คุณจะได้บางอย่างที่ใกล้เคียงกับการฝังของมุสโสลินีมาก

224
00:16:08,570 --> 00:16:12,661
ราวกับว่าโมเดลเรียนรู้ที่จะเชื่อมโยงทิศทางบางอย่างกับความเป็นอิตาลี 

225
00:16:12,661 --> 00:16:15,670
และทิศทางอื่นๆ กับผู้นำแกนนำในสงครามโลกครั้งที่สอง

226
00:16:16,470 --> 00:16:19,788
บางทีตัวอย่างที่ฉันชื่นชอบในแนวทางนี้คือ ในบางรุ่น 

227
00:16:19,788 --> 00:16:24,212
ถ้าคุณคำนึงถึงความแตกต่างระหว่างเยอรมนีและญี่ปุ่น แล้วเพิ่มลงในซูชิ 

228
00:16:24,212 --> 00:16:26,230
คุณจะเข้าใกล้บราทเวิร์สอย่างมาก

229
00:16:27,350 --> 00:16:30,180
นอกจากนี้ ในการเล่นเกมค้นหาเพื่อนบ้านที่ใกล้ที่สุดนี้ 

230
00:16:30,180 --> 00:16:33,850
ฉันดีใจที่ได้เห็นว่า Kat อยู่ใกล้ทั้งสัตว์ร้ายและสัตว์ประหลาดมากแค่ไหน

231
00:16:34,690 --> 00:16:38,455
สัญชาตญาณทางคณิตศาสตร์เล็กๆ น้อยๆ ที่เป็นประโยชน์ในใจ โดยเฉพาะบทถัดไป คือ 

232
00:16:38,455 --> 00:16:43,035
การพิจารณาว่าดอทโปรดัคของเวกเตอร์สองตัวเป็นวิธีหนึ่งในการวัดว่าเวกเตอร์สองตัวอยู่ในแนวเดีย

233
00:16:43,035 --> 00:16:43,850
วกันได้ดีเพียงใด

234
00:16:44,870 --> 00:16:49,858
ในการคำนวณ ผลิตภัณฑ์ดอทเกี่ยวข้องกับการคูณส่วนประกอบที่เกี่ยวข้องทั้งหมดแล้วบวกผลลัพธ์ 

235
00:16:49,858 --> 00:16:54,330
ซึ่งถือว่าดี เนื่องจากการคำนวณส่วนใหญ่ของเราต้องมีลักษณะเหมือนผลรวมถ่วงน้ำหนัก

236
00:16:55,190 --> 00:16:59,439
ในเชิงเรขาคณิต ผลคูณดอทจะเป็นค่าบวกเมื่อเวกเตอร์ชี้ไปในทิศทางที่คล้ายกัน 

237
00:16:59,439 --> 00:17:02,990
ถ้าเวกเตอร์ชี้ไปในทิศทางเดียวกัน มันจะเป็นศูนย์หากตั้งฉากกัน 

238
00:17:02,990 --> 00:17:05,609
และจะเป็นลบทุกครั้งที่ชี้ไปในทิศทางตรงกันข้าม

239
00:17:06,550 --> 00:17:13,495
ตัวอย่างเช่น สมมติว่าคุณกำลังเล่นกับโมเดลนี้ และคุณตั้งสมมุติฐานว่าการฝังแมวลบด้วย 

240
00:17:13,495 --> 00:17:17,010
cat อาจแสดงถึงทิศทางแบบพหูพจน์ในพื้นที่นี้

241
00:17:17,430 --> 00:17:22,240
เพื่อทดสอบสิ่งนี้ ฉันจะใช้เวกเตอร์นี้และคำนวณผลคูณดอทของมันเทียบกับการฝังค

242
00:17:22,240 --> 00:17:27,050
ำนามเอกพจน์บางคำ แล้วเปรียบเทียบกับผลคูณดอทที่มีคำนามพหูพจน์ที่สอดคล้องกัน

243
00:17:27,270 --> 00:17:31,670
หากคุณลองเล่นดู คุณจะสังเกตเห็นว่าค่าพหูพจน์ดูเหมือนจะให้ค่าที่สูงกว่า

244
00:17:31,670 --> 00:17:36,070
ค่าเอกพจน์อย่างสม่ำเสมอ ซึ่งบ่งชี้ว่าพวกมันสอดคล้องกับทิศทางนี้มากกว่า

245
00:17:37,070 --> 00:17:40,587
นอกจากนี้ ยังน่าสนุกที่ถ้าคุณนำดอทโปรดัคนี้ฝังคำ 1, 2, 

246
00:17:40,587 --> 00:17:43,785
3 และอื่นๆ เข้าด้วยกัน พวกมันจะให้ค่าที่เพิ่มขึ้น 

247
00:17:43,785 --> 00:17:49,030
เหมือนกับว่าเราสามารถวัดในเชิงปริมาณได้ว่าแบบจำลองพหูพจน์ค้นหาคำที่กำหนดได้อย่างไร

248
00:17:50,250 --> 00:17:53,570
อีกครั้ง เฉพาะเจาะจงสำหรับวิธีการฝังคำศัพท์นั้นเรียนรู้โดยใช้ข้อมูล

249
00:17:54,050 --> 00:17:57,361
เมทริกซ์แบบฝังซึ่งมีคอลัมน์บอกเราว่าเกิดอะไรขึ้นกับแต่ละคำ 

250
00:17:57,361 --> 00:17:59,550
ถือเป็นกองน้ำหนักชุดแรกในแบบจำลองของเรา

251
00:18:00,030 --> 00:18:04,978
เมื่อใช้ตัวเลข GPT-3 ขนาดคำศัพท์โดยเฉพาะคือ 50,257 และอีกครั้ง 

252
00:18:04,978 --> 00:18:09,770
ในทางเทคนิคแล้ว สิ่งนี้ไม่ได้ประกอบด้วยคำต่อตัว แต่เป็นโทเค็น

253
00:18:10,630 --> 00:18:17,193
มิติข้อมูลการฝังคือ 12,288 และการคูณสิ่งเหล่านี้บอกเราว่าสิ่งนี้ประกอบด้วยน้ำหนักประมาณ 

254
00:18:17,193 --> 00:18:17,790
617 ล้าน

255
00:18:18,250 --> 00:18:21,000
เรามาเพิ่มตัวเลขนี้เข้าในการนับอย่างต่อเนื่อง 

256
00:18:21,000 --> 00:18:23,810
โดยจำไว้ว่าในตอนท้ายเราควรนับได้ถึง 175 พันล้าน

257
00:18:25,430 --> 00:18:29,617
ในกรณีของหม้อแปลงไฟฟ้า คุณต้องคิดถึงเวกเตอร์ในพื้นที่ฝังนี้จริงๆ 

258
00:18:29,617 --> 00:18:32,130
ไม่ใช่แค่เป็นตัวแทนของคำแต่ละคำเท่านั้น

259
00:18:32,550 --> 00:18:38,148
ประการหนึ่ง พวกเขายังเข้ารหัสข้อมูลเกี่ยวกับตำแหน่งของคำนั้น ซึ่งเราจะพูดถึงในภายหลัง 

260
00:18:38,148 --> 00:18:42,770
แต่ที่สำคัญกว่านั้น คุณควรคิดว่าคำเหล่านั้นมีความสามารถในการซึมซับบริบท

261
00:18:43,350 --> 00:18:48,030
เวกเตอร์ที่เริ่มต้นชีวิตด้วยการฝังคำว่า king อาจถูกดึงและดึงโดยบล็อกต่างๆ 

262
00:18:48,030 --> 00:18:53,344
ในเครือข่ายนี้อย่างต่อเนื่อง เพื่อว่าในตอนท้ายจะชี้ไปในทิศทางที่เฉพาะเจาะจงและเหมาะส

263
00:18:53,344 --> 00:18:58,404
มยิ่งขึ้นมากซึ่งเข้ารหัสด้วยวิธีใดวิธีหนึ่ง เป็นกษัตริย์ที่อาศัยอยู่ในสกอตแลนด์ 

264
00:18:58,404 --> 00:19:02,389
และประสบความสำเร็จในตำแหน่งของเขาหลังจากสังหารกษัตริย์องค์ก่อน 

265
00:19:02,389 --> 00:19:04,730
และได้รับการบรรยายเป็นภาษาเช็คสเปียร์

266
00:19:05,210 --> 00:19:07,790
คิดถึงความเข้าใจของคุณเองเกี่ยวกับคำที่กำหนด

267
00:19:08,250 --> 00:19:12,273
ความหมายของคำนั้นได้รับการบอกเล่าจากสิ่งรอบข้างอย่างชัดเจน 

268
00:19:12,273 --> 00:19:17,320
และบางครั้งอาจรวมบริบทจากระยะไกลด้วย ดังนั้นในการรวบรวมแบบจำลองที่มีความสา

269
00:19:17,320 --> 00:19:23,390
มารถในการคาดเดาคำถัดไปได้ เป้าหมายคือการเสริมพลังให้รวมบริบทเข้าไปด้วย อย่างมีประสิทธิภาพ

270
00:19:24,050 --> 00:19:28,644
เพื่อให้ชัดเจน ในขั้นตอนแรกนั้น เมื่อคุณสร้างอาร์เรย์ของเวกเตอร์ตามข้อความที่ป้อน 

271
00:19:28,644 --> 00:19:32,847
แต่ละเวกเตอร์จะถูกดึงออกจากเมทริกซ์ที่ฝัง ดังนั้นในขั้นต้นแต่ละเวกเตอร์สามา

272
00:19:32,847 --> 00:19:36,770
รถเข้ารหัสความหมายของคำเดียวเท่านั้นโดยไม่ต้อง อินพุตใดๆ จากสิ่งรอบตัว

273
00:19:37,710 --> 00:19:43,305
แต่คุณควรคิดถึงเป้าหมายหลักของเครือข่ายนี้ที่เครือข่ายนี้ไหลผ่านคือการทำให้เวกเตอ

274
00:19:43,305 --> 00:19:48,970
ร์แต่ละตัวซึมซับความหมายที่เข้มข้นและเฉพาะเจาะจงมากกว่าที่คำแต่ละคำจะเป็นตัวแทนได้

275
00:19:49,510 --> 00:19:54,170
เครือข่ายสามารถประมวลผลเวกเตอร์ได้ครั้งละจำนวนคงที่เท่านั้น ซึ่งเรียกว่าขนาดบริบท

276
00:19:54,510 --> 00:19:57,646
สำหรับ GPT-3 ได้รับการฝึกฝนด้วยขนาดบริบท 2048 

277
00:19:57,646 --> 00:20:02,896
ดังนั้นข้อมูลที่ไหลผ่านเครือข่ายจะมีลักษณะเหมือนอาร์เรย์ 2048 คอลัมน์นี้เสมอ 

278
00:20:02,896 --> 00:20:05,010
โดยแต่ละคอลัมน์จะมี 12,000 มิติ

279
00:20:05,590 --> 00:20:11,830
ขนาดบริบทนี้จำกัดจำนวนข้อความที่ Transformer สามารถรวมไว้เมื่อทำการคาดเดาคำถัดไป

280
00:20:12,370 --> 00:20:16,848
นี่คือสาเหตุที่การสนทนาระยะยาวกับแชทบอตบางประเภท เช่น ChatGPT 

281
00:20:16,848 --> 00:20:22,050
เวอร์ชันแรกๆ มักจะทำให้บอทสูญเสียหัวข้อการสนทนาเมื่อคุณสนทนาต่อนานเกินไป

282
00:20:23,030 --> 00:20:25,920
เราจะลงรายละเอียดความสนใจในเวลาที่กำหนด แต่ข้ามไปข้างหน้าฉ

283
00:20:25,920 --> 00:20:28,810
ันต้องการพูดคุยสักครู่เกี่ยวกับสิ่งที่เกิดขึ้นในตอนท้ายสุด

284
00:20:29,450 --> 00:20:34,870
โปรดจำไว้ว่า ผลลัพธ์ที่ต้องการคือการแจกแจงความน่าจะเป็นของโทเค็นทั้งหมดที่อาจเกิดขึ้นถัดไป

285
00:20:35,170 --> 00:20:40,318
ตัวอย่างเช่น หากคำสุดท้ายคือศาสตราจารย์ และบริบทมีคำอย่างแฮร์รี่ พอตเตอร์ด้วย 

286
00:20:40,318 --> 00:20:45,466
และก่อนหน้านั้นเราเห็นครูคนโปรดน้อยที่สุด และถ้าคุณให้โอกาสฉันบ้างโดยให้ฉันแกล

287
00:20:45,466 --> 00:20:48,635
้งทำเป็นว่าโทเค็นนั้นดูเหมือนเป็นคำเต็ม ดังนั้น 

288
00:20:48,635 --> 00:20:53,783
เครือข่ายที่ได้รับการฝึกฝนมาอย่างดีซึ่งสั่งสมความรู้เกี่ยวกับแฮร์รี่ พอตเตอร์ 

289
00:20:53,783 --> 00:20:55,830
คงจะกำหนดให้คำว่าสเนปมีจำนวนมาก

290
00:20:56,510 --> 00:20:57,970
สิ่งนี้เกี่ยวข้องกับสองขั้นตอนที่แตกต่างกัน

291
00:20:58,310 --> 00:21:04,122
ประการแรกคือการใช้เมทริกซ์อื่นที่แมปเวกเตอร์สุดท้ายในบริบทนั้นกับรายการค่า 

292
00:21:04,122 --> 00:21:07,610
50,000 ค่า หนึ่งค่าสำหรับแต่ละโทเค็นในคำศัพท์

293
00:21:08,170 --> 00:21:12,940
จากนั้น มีฟังก์ชันที่ทำให้สิ่งนี้เป็นมาตรฐานในการแจกแจงความน่าจะเป็น เรียกว่า 

294
00:21:12,940 --> 00:21:16,792
Softmax และเราจะพูดถึงมันเพิ่มเติมในอีกสักครู่ แต่ก่อนหน้านั้น 

295
00:21:16,792 --> 00:21:22,174
อาจดูแปลกนิดหน่อยที่จะใช้เฉพาะการฝังครั้งล่าสุดนี้เพื่อทำนาย เมื่อ ในขั้นตอนสุดท้ายนั้น 

296
00:21:22,174 --> 00:21:27,189
มีเวกเตอร์อื่นๆ อีกหลายพันตัวในเลเยอร์ที่นั่งอยู่ตรงนั้นพร้อมความหมายที่หลากหลายตา

297
00:21:27,189 --> 00:21:28,290
มบริบทของตัวมันเอง

298
00:21:28,930 --> 00:21:34,600
สิ่งนี้เกี่ยวข้องกับความจริงที่ว่าในกระบวนการฝึกอบรมจะมีประสิทธิภาพมากขึ้นหากคุณใช

299
00:21:34,600 --> 00:21:40,270
้เวกเตอร์แต่ละตัวในเลเยอร์สุดท้ายเพื่อคาดการณ์สิ่งที่จะเกิดขึ้นหลังจากนั้นพร้อมกัน

300
00:21:40,970 --> 00:21:45,090
มีอะไรอีกมากมายที่จะพูดเกี่ยวกับการฝึกซ้อมในภายหลัง แต่ฉันแค่อยากจะพูดถึงตอนนี้

301
00:21:45,730 --> 00:21:49,690
เมทริกซ์นี้เรียกว่าเมทริกซ์ Unembedding และเราให้ป้ายกำกับ WU

302
00:21:50,210 --> 00:21:53,060
เช่นเดียวกับเมทริกซ์น้ำหนักทั้งหมดที่เราเห็น รายการต่างๆ 

303
00:21:53,060 --> 00:21:55,910
เริ่มต้นจากการสุ่ม แต่จะเรียนรู้ในระหว่างกระบวนการฝึกอบรม

304
00:21:56,470 --> 00:22:00,453
เพื่อรักษาคะแนนในการนับพารามิเตอร์ทั้งหมดของเรา เมทริกซ์ Unembedding 

305
00:22:00,453 --> 00:22:05,650
นี้มีหนึ่งแถวสำหรับแต่ละคำในคำศัพท์ และแต่ละแถวมีจำนวนองค์ประกอบเท่ากันกับมิติข้อมูลการฝัง

306
00:22:06,410 --> 00:22:09,497
มันคล้ายกับเมทริกซ์การฝังอย่างมาก เพียงแค่สลับลำดับกัน 

307
00:22:09,497 --> 00:22:13,145
ดังนั้นจึงเพิ่มพารามิเตอร์อีก 617 ล้านพารามิเตอร์ให้กับเครือข่าย 

308
00:22:13,145 --> 00:22:16,962
ซึ่งหมายความว่าการนับของเราจนถึงตอนนี้มีมากกว่าพันล้านเพียงเล็กน้อย 

309
00:22:16,962 --> 00:22:21,790
ซึ่งเป็นเศษเพียงเล็กน้อยแต่ไม่มีนัยสำคัญทั้งหมดของ 175 พันล้านที่เรา จะลงเอยด้วยยอดรวม

310
00:22:22,550 --> 00:22:26,509
เนื่องจากเป็นบทเรียนเล็กๆ สุดท้ายสำหรับบทนี้ ฉันอยากจะพูดเพิ่มเติมเกี่ยวกับฟังก์ชัน 

311
00:22:26,509 --> 00:22:30,610
softmax นี้ เนื่องจากฟังก์ชันนี้จะปรากฏให้เห็นอีกครั้งเมื่อเราดำดิ่งลงไปในบล็อคความสนใจ

312
00:22:31,430 --> 00:22:36,707
แนวคิดก็คือถ้าคุณต้องการให้ลำดับของตัวเลขทำหน้าที่เป็นการแจกแจงความน่าจะเป็น 

313
00:22:36,707 --> 00:22:41,368
พูดการแจกแจงของคำถัดไปที่เป็นไปได้ทั้งหมด แต่ละค่าจะต้องอยู่ระหว่าง 

314
00:22:41,368 --> 00:22:44,590
0 ถึง 1 และคุณยังต้องให้ค่าทั้งหมดรวมกันได้ 1 .

315
00:22:45,250 --> 00:22:49,997
อย่างไรก็ตาม หากคุณกำลังเล่นเกมการเรียนรู้ที่ทุกสิ่งที่คุณทำดูเหมือนการคูณ

316
00:22:49,997 --> 00:22:54,810
เมทริกซ์-เวกเตอร์ ผลลัพธ์ที่คุณได้รับตามค่าเริ่มต้นจะไม่เป็นไปตามสิ่งนี้เลย

317
00:22:55,330 --> 00:22:59,870
ค่าต่างๆ มักจะเป็นลบ หรือมากกว่า 1 มากและแทบจะรวมกันไม่ได้ 1 เลยด้วยซ้ำ

318
00:23:00,510 --> 00:23:05,936
Softmax เป็นวิธีมาตรฐานในการเปลี่ยนรายการตัวเลขใดๆ เป็นการแจกแจงที่ถูกต้อง 

319
00:23:05,936 --> 00:23:11,290
โดยค่าที่มากที่สุดจะเข้าใกล้ 1 มากที่สุด และค่าที่น้อยกว่าจะเข้าใกล้ 0 มาก

320
00:23:11,830 --> 00:23:13,070
นั่นคือทั้งหมดที่คุณต้องรู้จริงๆ

321
00:23:13,090 --> 00:23:17,458
แต่ถ้าคุณสงสัย วิธีการทำงานคือยก e ยกกำลังของแต่ละจำนวนก่อน 

322
00:23:17,458 --> 00:23:22,918
ซึ่งหมายความว่าตอนนี้คุณมีรายการค่าบวกแล้ว จากนั้นคุณก็สามารถหาผลรวมของค่าบ

323
00:23:22,918 --> 00:23:29,470
วกเหล่านั้นแล้วหารได้ แต่ละเทอมด้วยผลรวมนั้น ซึ่งจะทำให้คำนั้นกลายเป็นรายการที่รวมกันได้ 1

324
00:23:30,170 --> 00:23:34,466
คุณจะสังเกตเห็นว่าหากตัวเลขตัวใดตัวหนึ่งในอินพุตมีความหมายมากกว่าตัวเลขที่เหลือ 

325
00:23:34,466 --> 00:23:37,797
ดังนั้นในเอาต์พุต คำที่เกี่ยวข้องจะมีอิทธิพลเหนือการกระจายตัว 

326
00:23:37,797 --> 00:23:42,470
ดังนั้นหากคุณสุ่มตัวอย่างจากตัวเลขดังกล่าว คุณแทบจะเลือกอินพุตที่ขยายใหญ่สุดอย่างแน่นอน

327
00:23:42,990 --> 00:23:46,992
แต่มันเบากว่าการเลือกค่าสูงสุดในแง่ที่ว่าเมื่อค่าอื่นๆ มีขนาดใหญ่พอๆ 

328
00:23:46,992 --> 00:23:50,415
กัน ค่าเหล่านั้นก็จะได้น้ำหนักที่มีความหมายในการแจกแจงด้วย 

329
00:23:50,415 --> 00:23:54,650
และทุกอย่างจะเปลี่ยนแปลงอย่างต่อเนื่องเมื่อคุณเปลี่ยนอินพุตอย่างต่อเนื่อง

330
00:23:55,130 --> 00:23:59,547
ในบางสถานการณ์ เช่น เมื่อ ChatGPT ใช้การแจกแจงนี้เพื่อสร้างคำถัดไป 

331
00:23:59,547 --> 00:24:05,283
ก็มีพื้นที่ให้สนุกเพิ่มขึ้นอีกเล็กน้อยโดยการเพิ่มเครื่องเทศเข้าไปเล็กน้อยในฟังก์ชันนี้ 

332
00:24:05,283 --> 00:24:08,910
โดยใส่ค่าคงที่ t เข้าไปในตัวส่วนของเลขชี้กำลังเหล่านั้น

333
00:24:09,550 --> 00:24:15,343
เราเรียกมันว่าอุณหภูมิ เนื่องจากมันดูคลุมเครือกับบทบาทของอุณหภูมิในสมการทางอุณหพลศาสตร

334
00:24:15,343 --> 00:24:20,799
์บางสมการ และผลที่ได้ก็คือเมื่อ t มากขึ้น คุณจะให้น้ำหนักกับค่าที่ต่ำกว่ามากขึ้น 

335
00:24:20,799 --> 00:24:25,919
ซึ่งหมายความว่าการกระจายตัวจะสม่ำเสมอมากขึ้นเล็กน้อย และถ้า t มีค่าน้อยกว่า 

336
00:24:25,919 --> 00:24:29,354
ดังนั้นค่าที่มากกว่าจะมีอำนาจเหนือกว่า โดยที่ค่า t 

337
00:24:29,354 --> 00:24:32,790
เท่ากับศูนย์หมายความว่าน้ำหนักทั้งหมดไปที่ค่าสูงสุด

338
00:24:33,470 --> 00:24:38,248
ตัวอย่างเช่น ฉันจะให้ GPT-3 สร้างเรื่องราวด้วยข้อความเริ่มต้น 

339
00:24:38,248 --> 00:24:42,950
กาลครั้งหนึ่งมี A แต่ฉันจะใช้อุณหภูมิที่แตกต่างกันในแต่ละกรณี

340
00:24:43,630 --> 00:24:48,529
อุณหภูมิเป็นศูนย์หมายความว่าคำนั้นจะมาพร้อมกับคำที่คาดเดาได้มากที่สุดเสมอ 

341
00:24:48,529 --> 00:24:52,370
และสิ่งที่คุณได้รับจะกลายเป็นอนุพันธ์ของ Goldilocks ซ้ำซาก

342
00:24:53,010 --> 00:24:57,910
อุณหภูมิที่สูงขึ้นทำให้มีโอกาสเลือกคำที่มีแนวโน้มน้อยลง แต่ก็มาพร้อมกับความเสี่ยง

343
00:24:58,230 --> 00:25:03,372
ในกรณีนี้ เรื่องราวเริ่มต้นจากเดิมมากขึ้นเกี่ยวกับศิลปินเว็บหนุ่มจากเกาหลีใต้ 

344
00:25:03,372 --> 00:25:06,010
แต่กลับกลายเป็นเรื่องไร้สาระอย่างรวดเร็ว

345
00:25:06,950 --> 00:25:10,830
ในทางเทคนิคแล้ว API จะไม่ยอมให้คุณเลือกอุณหภูมิที่ใหญ่กว่า 2 จริงๆ

346
00:25:11,170 --> 00:25:15,260
ไม่มีเหตุผลทางคณิตศาสตร์สำหรับสิ่งนี้ มันเป็นเพียงข้อจำกัดตามอำเภอใจที่

347
00:25:15,260 --> 00:25:19,350
กำหนดเพื่อไม่ให้มีคนเห็นว่าเครื่องมือของพวกเขาสร้างสิ่งที่ไร้สาระเกินไป

348
00:25:19,870 --> 00:25:22,919
ดังนั้น หากคุณสงสัย วิธีการทำงานของแอนิเมชั่นนี้ก็คือ 

349
00:25:22,919 --> 00:25:26,871
ฉันกำลังรับโทเค็นถัดไปที่เป็นไปได้มากที่สุด 20 อันที่ GPT-3 สร้างขึ้น 

350
00:25:26,871 --> 00:25:29,920
ซึ่งดูเหมือนว่าจะเป็นจำนวนเงินสูงสุดที่พวกเขาจะให้ฉัน 

351
00:25:29,920 --> 00:25:32,970
จากนั้นฉันจะปรับแต่งความน่าจะเป็นตาม บนเลขชี้กำลัง 1 5

352
00:25:33,130 --> 00:25:37,470
ศัพท์เฉพาะอีกประการหนึ่ง ในลักษณะเดียวกับที่คุณเรียกส่วนประกอบของ

353
00:25:37,470 --> 00:25:42,010
ผลลัพธ์ของความน่าจะเป็นของฟังก์ชันนี้ ผู้คนมักเรียกอินพุตว่า logits 

354
00:25:42,010 --> 00:25:46,150
หรือบางคนบอกว่า logits บางคนบอกว่า logits ฉันจะบอกว่า logits .

355
00:25:46,530 --> 00:25:51,011
ตัวอย่างเช่น เมื่อคุณป้อนข้อความ คุณจะมีคำที่ฝังอยู่ทั้งหมดไหลผ่านเครือข่าย 

356
00:25:51,011 --> 00:25:54,136
และคุณทำการคูณครั้งสุดท้ายด้วยเมทริกซ์ที่ไม่มีการฝัง 

357
00:25:54,136 --> 00:25:57,733
ผู้คนที่เรียนรู้เกี่ยวกับเครื่องจักรจะอ้างถึงส่วนประกอบต่างๆ 

358
00:25:57,733 --> 00:26:01,390
ในเอาต์พุตดิบที่ไม่ปกตินั้นว่าเป็นบันทึก สำหรับการทำนายคำถัดไป

359
00:26:03,330 --> 00:26:08,209
เป้าหมายมากมายในบทนี้คือการวางรากฐานสำหรับการทำความเข้าใจกลไกความสนใจ 

360
00:26:08,209 --> 00:26:10,370
สไตล์คาราเต้คิดแบบแวกซ์ออนแวกซ์

361
00:26:10,850 --> 00:26:15,861
คุณจะเห็นว่าถ้าคุณมีสัญชาตญาณที่ชัดเจนในการฝังคำ สำหรับ softmax ว่า dot product 

362
00:26:15,861 --> 00:26:21,185
วัดความคล้ายคลึงกันอย่างไร และยังมีหลักฐานพื้นฐานที่ว่าการคำนวณส่วนใหญ่ต้องมีลักษณะเห

363
00:26:21,185 --> 00:26:25,257
มือนการคูณเมทริกซ์ด้วยเมทริกซ์ที่เต็มไปด้วยพารามิเตอร์ที่ปรับได้ 

364
00:26:25,257 --> 00:26:30,769
จากนั้นจึงทำความเข้าใจความสนใจ กลไกซึ่งเป็นรากฐานสำคัญนี้ในความเจริญรุ่งเรืองสมัยใหม่ใน 

365
00:26:30,769 --> 00:26:32,210
AI ควรจะค่อนข้างราบรื่น

366
00:26:32,650 --> 00:26:34,510
เพื่อสิ่งนั้น มาร่วมกับฉันในบทต่อไป

367
00:26:36,390 --> 00:26:41,210
ในขณะที่ฉันกำลังเผยแพร่เนื้อหานี้ ผู้สนับสนุน Patreon จะสามารถอ่านฉบับร่างของบทถัดไปได้

368
00:26:41,770 --> 00:26:44,296
เวอร์ชันสุดท้ายควรเผยแพร่สู่สาธารณะภายในหนึ่งหรือสองสัปดาห์ 

369
00:26:44,296 --> 00:26:47,370
โดยปกติแล้วจะขึ้นอยู่กับว่าฉันต้องเปลี่ยนแปลงไปมากน้อยเพียงใดตามรีวิวนั้น

370
00:26:47,810 --> 00:26:52,410
ในระหว่างนี้ หากคุณต้องการเจาะลึกความสนใจ และหากคุณต้องการช่วยช่องอีกสักหน่อย คุณก็รออยู่

