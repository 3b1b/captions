1
00:00:00,000 --> 00:00:04,560
الأحرف الأولى من GPT تعني المحول التوليدي المُدرب مسبقًا.

2
00:00:05,220 --> 00:00:09,020
لذا فإن الكلمة الأولى واضحة بما فيه الكفاية، فهي روبوتات تولد نصًا جديدًا.

3
00:00:09,800 --> 00:00:14,657
يشير التدريب المسبق إلى كيفية خضوع النموذج لعملية التعلم من كمية هائلة من 

4
00:00:14,657 --> 00:00:20,040
البيانات، وتشير البادئة إلى أن هناك مساحة أكبر لضبطه في مهام محددة مع تدريب إضافي.

5
00:00:20,720 --> 00:00:22,900
لكن الكلمة الأخيرة، هذه هي القطعة الرئيسية الحقيقية.

6
00:00:23,380 --> 00:00:27,044
المحول هو نوع محدد من الشبكات العصبية، وهو نموذج للتعلم الآلي، 

7
00:00:27,044 --> 00:00:31,000
وهو الاختراع الأساسي الكامن وراء الطفرة الحالية في الذكاء الاصطناعي.

8
00:00:31,740 --> 00:00:35,430
ما أريد أن أفعله بهذا الفيديو والفصول التالية 

9
00:00:35,430 --> 00:00:39,120
هو تقديم شرح بصري لما يحدث بالفعل داخل المحول.

10
00:00:39,700 --> 00:00:42,820
سنقوم بمتابعة البيانات التي تتدفق من خلاله ونتحرك خطوة بخطوة.

11
00:00:43,440 --> 00:00:47,380
هناك العديد من أنواع النماذج المختلفة التي يمكنك بنائها باستخدام المحولات.

12
00:00:47,800 --> 00:00:50,800
تأخذ بعض النماذج الصوت وتنتج نصًا.

13
00:00:51,340 --> 00:00:56,220
تأتي هذه الجملة من نموذج يسير في الاتجاه المعاكس، وينتج خطابًا تركيبيًا من النص فقط.

14
00:00:56,660 --> 00:01:00,794
كل تلك الأدوات التي اجتاحت العالم في عام 2022 مثل Dolly 

15
00:01:00,794 --> 00:01:05,519
وMidjourney التي تأخذ وصفًا نصيًا وتنتج صورة تعتمد على المحولات.

16
00:01:06,000 --> 00:01:09,491
حتى لو لم أتمكن من فهم ما يفترض أن يكون عليه مخلوق الفطيرة، 

17
00:01:09,491 --> 00:01:13,100
ما زلت مندهشًا من أن هذا النوع من الأشياء ممكن حتى ولو عن بعد.

18
00:01:13,900 --> 00:01:18,114
وتم اختراع المحول الأصلي الذي قدمته Google في عام 2017 

19
00:01:18,114 --> 00:01:22,100
لحالة الاستخدام المحددة لترجمة النص من لغة إلى أخرى.

20
00:01:22,660 --> 00:01:27,789
لكن المتغير الذي سنركز عليه أنا وأنت، وهو النوع الذي يكمن وراء أدوات مثل 

21
00:01:27,789 --> 00:01:32,989
ChatGPT، سيكون نموذجًا تم تدريبه على استيعاب جزء من النص، ربما حتى مع بعض 

22
00:01:32,989 --> 00:01:38,260
الصور المحيطة أو الصوت المصاحب له، وإنتاج تنبؤ لما سيأتي بعد ذلك في المقطع.

23
00:01:38,600 --> 00:01:43,800
يأخذ هذا التنبؤ شكل توزيع احتمالي على العديد من أجزاء النص المختلفة التي قد تتبعها.

24
00:01:45,040 --> 00:01:47,618
للوهلة الأولى، قد تعتقد أن التنبؤ بالكلمة التالية 

25
00:01:47,618 --> 00:01:49,940
يبدو وكأنه هدف مختلف تمامًا عن إنشاء نص جديد.

26
00:01:50,180 --> 00:01:55,034
ولكن بمجرد أن يكون لديك نموذج تنبؤ مثل هذا، فإن الشيء البسيط الذي يمكنك إنشاء جزء 

27
00:01:55,034 --> 00:02:00,008
أطول من النص هو إعطائه مقتطفًا أوليًا للعمل معه، وجعله يأخذ عينة عشوائية من التوزيع 

28
00:02:00,008 --> 00:02:04,744
الذي أنشأه للتو، وإلحاق تلك العينة بالنص ، ثم قم بتشغيل العملية برمتها مرة أخرى 

29
00:02:04,744 --> 00:02:09,539
لإجراء تنبؤ جديد استنادًا إلى النص الجديد بالكامل، بما في ذلك ما تمت إضافته للتو.

30
00:02:10,100 --> 00:02:13,000
لا أعرف عنك، لكن يبدو أن هذا لا ينبغي أن ينجح حقًا.

31
00:02:13,420 --> 00:02:16,388
في هذه الرسوم المتحركة، على سبيل المثال، أقوم بتشغيل GPT-2 على 

32
00:02:16,388 --> 00:02:19,451
جهاز الكمبيوتر المحمول الخاص بي وأطلب منه التنبؤ بشكل متكرر وأخذ 

33
00:02:19,451 --> 00:02:22,420
عينات من الجزء التالي من النص لإنشاء قصة بناءً على النص الأولي.

34
00:02:22,420 --> 00:02:26,120
القصة ليس لها معنى كبير حقًا.

35
00:02:26,500 --> 00:02:31,216
ولكن إذا قمت باستبدالها باستدعاءات واجهة برمجة التطبيقات (API) إلى GPT-3 بدلاً من 

36
00:02:31,216 --> 00:02:35,933
ذلك، وهو نفس النموذج الأساسي، ولكنه أكبر بكثير، فسنحصل فجأة وبطريقة سحرية تقريبًا 

37
00:02:35,933 --> 00:02:40,880
على قصة معقولة، قصة يبدو أنها تستنتج أن مخلوق باي سيعيش في عالم أرض الرياضيات والحساب.

38
00:02:41,580 --> 00:02:46,730
هذه العملية هنا من التنبؤ المتكرر وأخذ العينات هي في الأساس ما يحدث عندما تتفاعل مع 

39
00:02:46,730 --> 00:02:51,880
ChatGPT أو أي من نماذج اللغات الكبيرة الأخرى هذه وتراهم ينتجون كلمة واحدة في كل مرة.

40
00:02:52,480 --> 00:02:55,722
في الواقع، إحدى الميزات التي سأستمتع بها كثيرًا هي 

41
00:02:55,722 --> 00:02:59,220
القدرة على رؤية التوزيع الأساسي لكل كلمة جديدة تختارها.

42
00:03:03,820 --> 00:03:08,180
دعونا نبدأ الأمور بمعاينة عالية المستوى لكيفية تدفق البيانات عبر المحول.

43
00:03:08,640 --> 00:03:13,526
سنقضي المزيد من الوقت في التحفيز والتفسير والتوسع في تفاصيل كل خطوة، ولكن بشكل 

44
00:03:13,526 --> 00:03:18,660
عام، عندما يقوم أحد روبوتات الدردشة هذه بإنشاء كلمة معينة، إليك ما يحدث تحت الغطاء.

45
00:03:19,080 --> 00:03:22,040
أولاً، يتم تقسيم المدخلات إلى مجموعة من القطع الصغيرة.

46
00:03:22,620 --> 00:03:26,220
تسمى هذه القطع بالرموز، وفي حالة النص، تميل هذه إلى أن تكون 

47
00:03:26,220 --> 00:03:29,820
كلمات أو أجزاء صغيرة من الكلمات أو مجموعات أحرف مشتركة أخرى.

48
00:03:30,740 --> 00:03:33,910
إذا كانت الصور أو الصوت متضمنة، فيمكن أن تكون الرموز المميزة 

49
00:03:33,910 --> 00:03:37,080
عبارة عن بقع صغيرة من تلك الصورة أو أجزاء صغيرة من هذا الصوت.

50
00:03:37,580 --> 00:03:41,409
يتم بعد ذلك ربط كل واحدة من هذه الرموز المميزة بمتجه، مما يعني 

51
00:03:41,409 --> 00:03:45,360
قائمة من الأرقام، والتي تهدف إلى تشفير معنى تلك القطعة بطريقة ما.

52
00:03:45,880 --> 00:03:50,357
إذا كنت تعتقد أن هذه المتجهات تعطي إحداثيات في مساحة ذات أبعاد عالية جدًا، فإن الكلمات 

53
00:03:50,357 --> 00:03:54,680
ذات المعاني المتشابهة تميل إلى الهبوط على ناقلات قريبة من بعضها البعض في ذلك الفضاء.

54
00:03:55,280 --> 00:03:59,637
يمر تسلسل المتجهات هذا عبر عملية تُعرف باسم كتلة الانتباه، وهذا يسمح 

55
00:03:59,637 --> 00:04:04,500
للمتجهات بالتحدث مع بعضها البعض وتمرير المعلومات ذهابًا وإيابًا لتحديث قيمها.

56
00:04:04,880 --> 00:04:08,269
على سبيل المثال، يختلف معنى كلمة نموذج في عبارة 

57
00:04:08,269 --> 00:04:11,800
نموذج التعلم الآلي عن معناها في عبارة نموذج أزياء.

58
00:04:12,260 --> 00:04:17,222
إن كتلة الانتباه هي المسؤولة عن معرفة الكلمات في السياق ذات الصلة 

59
00:04:17,222 --> 00:04:21,959
بتحديث معاني الكلمات الأخرى، وكيف يجب تحديث هذه المعاني بالضبط.

60
00:04:22,500 --> 00:04:25,327
ومرة أخرى، كلما استخدمت معنى الكلمة، يتم تشفيرها 

61
00:04:25,327 --> 00:04:28,040
بالكامل بطريقة أو بأخرى في مدخلات تلك المتجهات.

62
00:04:29,180 --> 00:04:33,662
بعد ذلك، تمر هذه المتجهات من خلال نوع مختلف من العمليات، واعتمادًا على المصدر الذي 

63
00:04:33,662 --> 00:04:38,200
تقرأه، سيشار إلى ذلك باسم الإدراك الحسي متعدد الطبقات أو ربما طبقة التغذية الأمامية.

64
00:04:38,580 --> 00:04:42,660
وهنا لا تتحدث المتجهات مع بعضها البعض، بل تمر جميعها بنفس العملية بالتوازي.

65
00:04:43,060 --> 00:04:48,466
وعلى الرغم من صعوبة تفسير هذه الكتلة قليلًا، سنتحدث لاحقًا عن كيف أن الخطوة تشبه إلى 

66
00:04:48,466 --> 00:04:54,000
حدٍ ما طرح قائمة طويلة من الأسئلة حول كل متجه، ثم تحديثها بناءً على إجابات تلك الأسئلة.

67
00:04:54,900 --> 00:04:59,803
تبدو جميع العمليات في كلتا الكتلتين وكأنها كومة ضخمة من مضاعفات 

68
00:04:59,803 --> 00:05:05,320
المصفوفات، وستكون مهمتنا الأساسية هي فهم كيفية قراءة المصفوفات الأساسية.

69
00:05:06,980 --> 00:05:09,923
أقوم بتغطية بعض التفاصيل حول بعض خطوات التطبيع التي 

70
00:05:09,923 --> 00:05:12,980
تحدث بينهما، ولكن هذه في النهاية معاينة عالية المستوى.

71
00:05:13,680 --> 00:05:18,828
بعد ذلك، تتكرر العملية بشكل أساسي، وتتنقل ذهابًا وإيابًا بين كتل الانتباه 

72
00:05:18,828 --> 00:05:23,560
وكتل الإدراك الحسي متعددة الطبقات، حتى النهاية، يكون الأمل هو أن كل 

73
00:05:23,560 --> 00:05:28,500
المعنى الأساسي للمقطع قد تم بطريقة ما خبزه في المتجه الأخير في الترتيب.

74
00:05:28,920 --> 00:05:33,725
نقوم بعد ذلك بإجراء عملية معينة على المتجه الأخير الذي ينتج توزيعًا احتماليًا على جميع 

75
00:05:33,725 --> 00:05:38,420
الرموز المميزة المحتملة، وجميع الأجزاء الصغيرة المحتملة من النص التي قد تأتي بعد ذلك.

76
00:05:38,980 --> 00:05:43,496
وكما قلت، بمجرد أن يكون لديك أداة تتنبأ بما سيأتي بعد ذلك في ضوء مقتطف من 

77
00:05:43,496 --> 00:05:48,135
النص، يمكنك تغذيتها بقليل من النص الأولي وجعلها تلعب بشكل متكرر لعبة التنبؤ 

78
00:05:48,135 --> 00:05:53,080
بما سيأتي بعد ذلك، وأخذ عينات من التوزيع، والإلحاق ذلك، ثم تكرره مراراً وتكراراً.

79
00:05:53,640 --> 00:05:57,271
ربما يتذكر البعض منكم من ذوي الخبرة المدة التي سبقت ظهور ChatGPT في 

80
00:05:57,271 --> 00:06:00,955
المشهد، هذا هو الشكل الذي كانت تبدو عليه العروض التوضيحية المبكرة لـ 

81
00:06:00,955 --> 00:06:04,640
GPT-3، حيث يمكنك إكمال القصص والمقالات تلقائيًا بناءً على مقتطف أولي.

82
00:06:05,580 --> 00:06:10,860
لتحويل أداة كهذه إلى روبوت دردشة، فإن أسهل نقطة بداية هي الحصول على القليل من النص الذي 

83
00:06:10,860 --> 00:06:16,200
يحدد إعدادات المستخدم الذي يتفاعل مع مساعد الذكاء الاصطناعي المفيد، وهو ما يمكن أن تسميه 

84
00:06:16,200 --> 00:06:21,360
موجه النظام، وبعد ذلك ستستخدم السؤال الأولي للمستخدم أو المطالبة به هو الجزء الأول من 

85
00:06:21,360 --> 00:06:26,700
الحوار، وبعد ذلك يمكنك البدء في التنبؤ بما سيقوله مساعد الذكاء الاصطناعي المفيد ردًا على 

86
00:06:26,700 --> 00:06:26,940
ذلك.

87
00:06:27,720 --> 00:06:30,770
هناك الكثير مما يمكن قوله عن خطوة التدريب المطلوبة 

88
00:06:30,770 --> 00:06:33,940
لإنجاح هذا الأمر، ولكن على مستوى عالٍ، هذه هي الفكرة.

89
00:06:35,720 --> 00:06:41,298
في هذا الفصل، سنتوسع أنا وأنت في تفاصيل ما يحدث في بداية الشبكة، وفي نهايتها، 

90
00:06:41,298 --> 00:06:47,164
وأريد أيضًا قضاء الكثير من الوقت في مراجعة بعض الأجزاء المهمة من المعرفة الأساسية 

91
00:06:47,164 --> 00:06:52,600
أشياء كانت ستصبح طبيعة أي مهندس تعلم آلي بحلول الوقت الذي ظهرت فيه المحولات.

92
00:06:53,060 --> 00:06:57,951
إذا كنت مرتاحًا لهذه المعرفة الأساسية وقليل الصبر، فلا تتردد في الانتقال إلى 

93
00:06:57,951 --> 00:07:02,780
الفصل التالي، والذي سيركز على كتل الانتباه، والتي تعتبر بشكل عام قلب المحول.

94
00:07:03,360 --> 00:07:07,579
بعد ذلك أريد أن أتحدث أكثر عن كتل الإدراك الحسي متعددة الطبقات، وكيفية 

95
00:07:07,579 --> 00:07:11,680
عمل التدريب، وعدد من التفاصيل الأخرى التي سيتم تخطيها حتى تلك النقطة.

96
00:07:12,180 --> 00:07:16,304
للحصول على سياق أوسع، تعد مقاطع الفيديو هذه إضافات إلى سلسلة مصغرة حول التعلم 

97
00:07:16,304 --> 00:07:20,270
العميق، ولا بأس إذا لم تكن قد شاهدت مقاطع الفيديو السابقة، أعتقد أنه يمكنك 

98
00:07:20,270 --> 00:07:24,395
القيام بذلك خارج النظام، ولكن قبل الغوص في المحولات على وجه التحديد، أعتقد من 

99
00:07:24,395 --> 00:07:28,520
الجدير التأكد من أننا على نفس الصفحة حول الفرضية الأساسية وبنية التعلم العميق.

100
00:07:29,020 --> 00:07:33,450
على الرغم من المخاطرة بتوضيح ما هو واضح، فهذا هو أحد أساليب التعلم الآلي، 

101
00:07:33,450 --> 00:07:38,300
والذي يصف أي نموذج تستخدم فيه البيانات لتحديد كيفية تصرف النموذج بطريقة أو بأخرى.

102
00:07:39,140 --> 00:07:43,569
ما أعنيه بذلك هو، لنفترض أنك تريد وظيفة تلتقط صورة وتنتج علامة 

103
00:07:43,569 --> 00:07:48,069
تصفها، أو مثالنا للتنبؤ بالكلمة التالية في ضوء مقطع من النص، أو 

104
00:07:48,069 --> 00:07:52,780
أي مهمة أخرى يبدو أنها تتطلب بعض العناصر الحدس والتعرف على الأنماط.

105
00:07:53,200 --> 00:07:57,645
نحن نعتبر هذا الأمر أمرا مفروغا منه هذه الأيام، ولكن الفكرة في التعلم الآلي 

106
00:07:57,645 --> 00:08:02,208
هي أنه بدلا من محاولة تحديد إجراء واضح لكيفية القيام بهذه المهمة في التعليمات 

107
00:08:02,208 --> 00:08:06,537
البرمجية، وهو ما كان سيفعله الناس في الأيام الأولى للذكاء الاصطناعي، بدلا 

108
00:08:06,537 --> 00:08:10,691
من ذلك قم بإعداد بنية مرنة للغاية مع معلمات قابلة للضبط، مثل مجموعة من 

109
00:08:10,691 --> 00:08:15,078
المقابض والأقراص، ثم تستخدم بطريقة ما العديد من الأمثلة حول الشكل الذي يجب 

110
00:08:15,078 --> 00:08:19,700
أن يبدو عليه الإخراج لمدخل معين لتعديل وضبط قيم تلك المعلمات لتقليد هذا السلوك.

111
00:08:19,700 --> 00:08:24,100
على سبيل المثال، ربما يكون أبسط شكل من أشكال التعلم الآلي هو الانحدار 

112
00:08:24,100 --> 00:08:28,187
الخطي، حيث تكون المدخلات والمخرجات عبارة عن أرقام فردية، شيء مثل 

113
00:08:28,187 --> 00:08:32,525
اللقطات المربعة للمنزل وسعره، وما تريده هو العثور على خط أفضل ملاءمة 

114
00:08:32,525 --> 00:08:36,799
من خلال هذا البيانات، كما تعلمون، للتنبؤ بأسعار المنازل في المستقبل.

115
00:08:37,440 --> 00:08:42,651
يتم وصف هذا الخط بمعلمتين مستمرتين، على سبيل المثال الميل والتقاطع y، 

116
00:08:42,651 --> 00:08:48,160
والهدف من الانحدار الخطي هو تحديد تلك المعلمات لمطابقة البيانات بشكل وثيق.

117
00:08:48,880 --> 00:08:52,100
وغني عن القول أن نماذج التعلم العميق تصبح أكثر تعقيدًا.

118
00:08:52,620 --> 00:08:57,660
GPT-3، على سبيل المثال، لا يحتوي على اثنين، بل 175 مليار معلمة.

119
00:08:58,120 --> 00:09:03,647
ولكن هذا هو الأمر، ليس من المسلم به أنه يمكنك إنشاء نموذج عملاق يحتوي على عدد كبير من 

120
00:09:03,647 --> 00:09:09,110
المعلمات دون الحاجة إلى الإفراط في تجهيز بيانات التدريب بشكل كبير أو استعصاء التدريب 

121
00:09:09,110 --> 00:09:09,560
تمامًا.

122
00:09:10,260 --> 00:09:13,251
يصف التعلم العميق فئة من النماذج التي أثبتت في 

123
00:09:13,251 --> 00:09:16,180
العقدين الماضيين أنها قابلة للتوسع بشكل ملحوظ.

124
00:09:16,480 --> 00:09:21,321
ما يوحدهم هو نفس خوارزمية التدريب، التي تسمى الانتشار العكسي، والسياق 

125
00:09:21,321 --> 00:09:26,162
الذي أريدك أن تحصل عليه أثناء تقدمنا هو أنه لكي تعمل خوارزمية التدريب 

126
00:09:26,162 --> 00:09:31,280
هذه بشكل جيد على نطاق واسع، يجب أن تتبع هذه النماذج تنسيقًا محددًا معينًا.

127
00:09:31,800 --> 00:09:36,004
إذا كنت تعرف هذا التنسيق، فمن المفيد أن تشرح العديد من الاختيارات 

128
00:09:36,004 --> 00:09:40,400
الخاصة بكيفية معالجة المحول للغة، والتي قد تتعرض لخطر الشعور بالتعسف.

129
00:09:41,440 --> 00:09:46,740
أولاً، أيًا كان النموذج الذي تقوم بإنشائه، يجب تنسيق الإدخال كمصفوفة من الأرقام الحقيقية.

130
00:09:46,740 --> 00:09:51,312
قد يعني هذا قائمة من الأرقام، أو يمكن أن تكون مصفوفة ثنائية الأبعاد، أو في كثير 

131
00:09:51,312 --> 00:09:56,000
من الأحيان تتعامل مع مصفوفات ذات أبعاد أعلى، حيث المصطلح العام المستخدم هو الموتر.

132
00:09:56,560 --> 00:10:00,599
غالبًا ما تفكر في أن بيانات الإدخال يتم تحويلها تدريجيًا إلى العديد 

133
00:10:00,599 --> 00:10:04,640
من الطبقات المتميزة، حيث يتم تنظيم كل طبقة دائمًا كنوع من مجموعة من 

134
00:10:04,640 --> 00:10:08,680
الأرقام الحقيقية، حتى تصل إلى الطبقة النهائية التي تعتبرها المخرجات.

135
00:10:09,280 --> 00:10:13,115
على سبيل المثال، الطبقة الأخيرة في نموذج معالجة النص لدينا هي قائمة من 

136
00:10:13,115 --> 00:10:17,060
الأرقام التي تمثل التوزيع الاحتمالي لجميع الرموز المميزة التالية الممكنة.

137
00:10:17,820 --> 00:10:21,966
في التعلم العميق، يُشار دائمًا إلى معلمات النموذج هذه بالأوزان، وذلك 

138
00:10:21,966 --> 00:10:25,933
لأن الميزة الرئيسية لهذه النماذج هي أن الطريقة الوحيدة لتفاعل هذه 

139
00:10:25,933 --> 00:10:29,900
المعلمات مع البيانات التي تتم معالجتها هي من خلال المبالغ المرجحة.

140
00:10:30,340 --> 00:10:34,360
يمكنك أيضًا رش بعض الوظائف غير الخطية طوال الوقت، لكنها لن تعتمد على المعلمات.

141
00:10:35,200 --> 00:10:40,333
عادةً، بدلًا من رؤية المجاميع المرجحة كلها عارية ومكتوبة بشكل واضح 

142
00:10:40,333 --> 00:10:45,620
بهذه الطريقة، ستجدها مجمعة معًا كمكونات مختلفة في منتج متجه المصفوفة.

143
00:10:46,740 --> 00:10:50,490
إنه يعني قول الشيء نفسه، إذا فكرت مرة أخرى في كيفية عمل ضرب 

144
00:10:50,490 --> 00:10:54,240
متجه المصفوفة، فإن كل مكون في الإخراج يبدو وكأنه مجموع مرجح.

145
00:10:54,780 --> 00:10:58,170
غالبًا ما يكون من الأنظف من الناحية المفاهيمية بالنسبة لي 

146
00:10:58,170 --> 00:11:01,736
ولكم التفكير في المصفوفات المملوءة بمعلمات قابلة للضبط والتي 

147
00:11:01,736 --> 00:11:05,420
تحول المتجهات التي يتم استخلاصها من البيانات التي تتم معالجتها.

148
00:11:06,340 --> 00:11:10,180
على سبيل المثال، تم تنظيم تلك الأوزان البالغ عددها 175 

149
00:11:10,180 --> 00:11:14,160
مليارًا في GPT-3 في ما يقل قليلاً عن 28000 مصفوفة متميزة.

150
00:11:14,660 --> 00:11:18,649
تنقسم هذه المصفوفات بدورها إلى ثماني فئات مختلفة، وما سنفعله أنا 

151
00:11:18,649 --> 00:11:22,700
وأنت هو المرور عبر كل واحدة من هذه الفئات لفهم ما يفعله هذا النوع.

152
00:11:23,160 --> 00:11:27,333
بينما نمضي قدمًا، أعتقد أنه من الممتع الرجوع إلى الأرقام 

153
00:11:27,333 --> 00:11:31,360
المحددة من GPT-3 لحساب مصدر تلك الـ 175 مليارًا بالضبط.

154
00:11:31,880 --> 00:11:36,338
حتى لو كانت هناك نماذج أكبر وأفضل في الوقت الحاضر، فإن هذا النموذج يتمتع بسحر 

155
00:11:36,338 --> 00:11:40,740
معين باعتباره نموذج اللغة الكبيرة لجذب انتباه العالم خارج مجتمعات تعلم الآلة.

156
00:11:41,440 --> 00:11:46,740
ومن الناحية العملية أيضًا، تميل الشركات إلى الالتزام بأرقام محددة للشبكات الأكثر حداثة.

157
00:11:47,360 --> 00:11:52,431
أريد فقط أن أبدأ المشهد، فبينما تنظر إلى أسفل الغطاء لترى ما يحدث داخل أداة مثل 

158
00:11:52,431 --> 00:11:57,440
ChatGPT، تبدو كل العمليات الحسابية الفعلية تقريبًا مثل مضاعفة متجهات المصفوفات.

159
00:11:57,900 --> 00:12:02,434
هناك القليل من المخاطرة بالضياع في بحر مليارات الأرقام، ولكن يجب أن ترسم تمييزًا 

160
00:12:02,434 --> 00:12:07,249
حادًا للغاية في عقلك بين أوزان النموذج، والتي سألونها دائمًا باللون الأزرق أو الأحمر، 

161
00:12:07,249 --> 00:12:11,840
والبيانات التي يتم الحصول عليها تمت معالجتها، والتي سألونها دائمًا باللون الرمادي.

162
00:12:12,180 --> 00:12:15,050
الأوزان هي العقول الفعلية، وهي الأشياء التي يتم 

163
00:12:15,050 --> 00:12:17,920
تعلمها أثناء التدريب، وهي التي تحدد كيفية تصرفه.

164
00:12:18,280 --> 00:12:22,248
تقوم البيانات التي تتم معالجتها ببساطة بتشفير أي مدخلات 

165
00:12:22,248 --> 00:12:26,500
محددة يتم إدخالها في النموذج لتشغيل معين، مثل مقتطف من النص.

166
00:12:27,480 --> 00:12:31,818
مع كل ذلك كأساس، دعونا نتعمق في الخطوة الأولى من مثال معالجة النص 

167
00:12:31,818 --> 00:12:36,420
هذا، وهو تقسيم المدخلات إلى أجزاء صغيرة وتحويل تلك الأجزاء إلى متجهات.

168
00:12:37,020 --> 00:12:40,743
لقد ذكرت كيف تسمى هذه القطع بالرموز، والتي قد تكون أجزاء من الكلمات 

169
00:12:40,743 --> 00:12:44,521
أو علامات الترقيم، ولكن بين الحين والآخر في هذا الفصل وخاصة في الفصل 

170
00:12:44,521 --> 00:12:48,080
التالي، أود فقط أن أتظاهر بأنها مقسمة بشكل أكثر وضوحًا إلى كلمات.

171
00:12:48,600 --> 00:12:51,261
نظرًا لأننا نحن البشر نفكر بالكلمات، فإن هذا سيجعل 

172
00:12:51,261 --> 00:12:54,080
من الأسهل بكثير الرجوع إلى أمثلة صغيرة وتوضيح كل خطوة.

173
00:12:55,260 --> 00:12:59,501
يحتوي النموذج على مفردات محددة مسبقًا، وقائمة من كل الكلمات الممكنة، 

174
00:12:59,501 --> 00:13:03,804
على سبيل المثال 50000 منها، والمصفوفة الأولى التي سنواجهها، والمعروفة 

175
00:13:03,804 --> 00:13:07,800
باسم مصفوفة التضمين، تحتوي على عمود واحد لكل كلمة من هذه الكلمات.

176
00:13:08,940 --> 00:13:13,760
هذه الأعمدة هي التي تحدد المتجه الذي تتحول إليه كل كلمة في تلك الخطوة الأولى.

177
00:13:15,100 --> 00:13:18,872
نسميها نحن، ومثل كل المصفوفات التي نراها، تبدأ قيمها 

178
00:13:18,872 --> 00:13:22,360
بشكل عشوائي، ولكن سيتم تعلمها بناءً على البيانات.

179
00:13:23,620 --> 00:13:27,666
كان تحويل الكلمات إلى متجهات ممارسة شائعة في التعلم الآلي قبل فترة 

180
00:13:27,666 --> 00:13:31,773
طويلة من المحولات، ولكنه أمر غريب بعض الشيء إذا لم يسبق لك رؤيته من 

181
00:13:31,773 --> 00:13:35,760
قبل، وهو يضع الأساس لكل ما يلي، لذلك دعونا نتوقف لحظة للتعرف عليه.

182
00:13:36,040 --> 00:13:39,892
غالبًا ما نطلق على هذا التضمين كلمة، مما يدعوك إلى التفكير في 

183
00:13:39,892 --> 00:13:43,620
هذه المتجهات بشكل هندسي للغاية كنقاط في مساحة عالية الأبعاد.

184
00:13:44,180 --> 00:13:47,868
لن يكون تصور قائمة من ثلاثة أرقام كإحداثيات لنقاط في مساحة ثلاثية 

185
00:13:47,868 --> 00:13:51,780
الأبعاد مشكلة، لكن تضمين الكلمات يميل إلى أن يكون ذو أبعاد أعلى بكثير.

186
00:13:52,280 --> 00:13:56,486
في GPT-3 لديهم 12288 بُعدًا، وكما سترون، من المهم 

187
00:13:56,486 --> 00:14:00,440
العمل في مساحة بها الكثير من الاتجاهات المميزة.

188
00:14:01,180 --> 00:14:05,866
بنفس الطريقة التي يمكنك من خلالها أخذ شريحة ثنائية الأبعاد عبر مساحة ثلاثية 

189
00:14:05,866 --> 00:14:10,675
الأبعاد وإسقاط جميع النقاط على تلك الشريحة، من أجل تحريك تضمينات الكلمات التي 

190
00:14:10,675 --> 00:14:15,485
يقدمها لي نموذج بسيط، سأفعل شيئًا مشابهًا عن طريق اختيار شريحة ثلاثية الأبعاد 

191
00:14:15,485 --> 00:14:20,480
عبر هذا الفضاء ذي الأبعاد العالية جدًا، وإسقاط متجهات الكلمات عليها وعرض النتائج.

192
00:14:21,280 --> 00:14:25,708
الفكرة الكبيرة هنا هي أنه عندما يقوم النموذج بتعديل وضبط أوزانه لتحديد 

193
00:14:25,708 --> 00:14:30,011
كيفية دمج الكلمات كمتجهات أثناء التدريب، فإنه يميل إلى الاستقرار على 

194
00:14:30,011 --> 00:14:34,440
مجموعة من التضمينات حيث يكون للاتجاهات في الفضاء نوع من المعنى الدلالي.

195
00:14:34,980 --> 00:14:38,657
بالنسبة لنموذج تحويل الكلمة إلى ناقل البسيط الذي أستخدمه هنا، إذا 

196
00:14:38,657 --> 00:14:42,278
قمت بإجراء بحث عن جميع الكلمات التي تكون تضميناتها أقرب إلى كلمة 

197
00:14:42,278 --> 00:14:45,900
برج، ستلاحظ كيف تبدو جميعها وكأنها تعطي مشاعر برجية متشابهة جدًا.

198
00:14:46,340 --> 00:14:48,837
وإذا كنت تريد تعلم بعض لغة بايثون واللعب بها في المنزل، 

199
00:14:48,837 --> 00:14:51,380
فهذا هو النموذج المحدد الذي أستخدمه لصنع الرسوم المتحركة.

200
00:14:51,620 --> 00:14:57,600
إنه ليس محولاً، لكنه يكفي لتوضيح فكرة أن الاتجاهات في الفضاء يمكن أن تحمل معنى دلاليًا.

201
00:14:58,300 --> 00:15:03,369
أحد الأمثلة الكلاسيكية على ذلك هو أنه إذا أخذت الفرق بين المتجهات 

202
00:15:03,369 --> 00:15:08,130
الخاصة بالمرأة والرجل، وهو شيء يمكن أن تتخيله كمتجه صغير يربط 

203
00:15:08,130 --> 00:15:13,200
طرف أحدهما بطرف الآخر، فهو مشابه جدًا للفرق بين الملك والرجل ملكة.

204
00:15:15,080 --> 00:15:20,416
لنفترض أنك لا تعرف كلمة ملكة أنثى، يمكنك العثور عليها عن طريق أخذ الملك، 

205
00:15:20,416 --> 00:15:25,460
وإضافة اتجاه المرأة-الرجل، والبحث عن التضمينات الأقرب إلى تلك النقطة.

206
00:15:27,000 --> 00:15:28,200
على الأقل نوعا ما.

207
00:15:28,480 --> 00:15:32,727
على الرغم من كونه مثالًا كلاسيكيًا للنموذج الذي ألعب به، فإن التضمين الحقيقي 

208
00:15:32,727 --> 00:15:36,808
للملكة هو في الواقع أبعد قليلاً عما قد يوحي به هذا، ربما لأن الطريقة التي 

209
00:15:36,808 --> 00:15:40,780
يتم بها استخدام الملكة في بيانات التدريب ليست مجرد نسخة أنثوية من الملك.

210
00:15:41,620 --> 00:15:45,260
وعندما تجولت في الأمر، بدا أن العلاقات الأسرية توضح الفكرة بشكل أفضل بكثير.

211
00:15:46,340 --> 00:15:50,564
النقطة المهمة هي أنه يبدو أثناء التدريب أن النموذج وجد أنه من المفيد اختيار 

212
00:15:50,564 --> 00:15:54,900
التضمينات بحيث يقوم اتجاه واحد في هذا الفضاء بتشفير المعلومات المتعلقة بالجنس.

213
00:15:56,800 --> 00:16:02,618
مثال آخر هو أنك إذا أخذت تضمين إيطاليا، وطرحت تضمين ألمانيا، وأضفت 

214
00:16:02,618 --> 00:16:08,090
ذلك إلى تضمين هتلر، فستحصل على شيء قريب جدًا من تضمين موسوليني.

215
00:16:08,570 --> 00:16:11,896
يبدو الأمر كما لو أن النموذج تعلم ربط بعض الاتجاهات 

216
00:16:11,896 --> 00:16:15,670
بالهوية الإيطالية، وأخرى بقادة محور الحرب العالمية الثانية.

217
00:16:16,470 --> 00:16:21,289
ربما المثال المفضل لدي في هذا السياق هو كيف أنه في بعض النماذج، إذا أخذت الفرق 

218
00:16:21,289 --> 00:16:26,230
بين ألمانيا واليابان، وأضفته إلى السوشي، فسينتهي بك الأمر قريبًا جدًا من النقانق.

219
00:16:27,350 --> 00:16:33,850
أيضًا أثناء لعب لعبة العثور على أقرب الجيران، سررت برؤية مدى قرب كات من الوحش والوحش.

220
00:16:34,690 --> 00:16:39,122
أحد الأمور الرياضية البديهية التي من المفيد أن نأخذها في الاعتبار، خاصة في 

221
00:16:39,122 --> 00:16:43,850
الفصل التالي، هو كيف يمكن اعتبار المنتج النقطي لمتجهين وسيلة لقياس مدى توافقهما.

222
00:16:44,870 --> 00:16:49,541
من الناحية الحسابية، تتضمن المنتجات النقطية ضرب جميع المكونات المقابلة ثم إضافة 

223
00:16:49,541 --> 00:16:54,330
النتائج، وهو أمر جيد، نظرًا لأن الكثير من حساباتنا يجب أن تبدو وكأنها مبالغ مرجحة.

224
00:16:55,190 --> 00:17:00,602
هندسيًا، يكون حاصل الضرب النقطي موجبًا عندما تشير المتجهات إلى اتجاهات متشابهة، 

225
00:17:00,602 --> 00:17:05,609
ويكون صفرًا إذا كانت متعامدة، ويكون سالبًا عندما تشير إلى اتجاهات متعاكسة.

226
00:17:06,550 --> 00:17:11,940
على سبيل المثال، لنفترض أنك كنت تلعب بهذا النموذج، وتفترض أن تضمين 

227
00:17:11,940 --> 00:17:17,010
القطط ناقص القطة قد يمثل نوعًا من اتجاه التعددية في هذا الفضاء.

228
00:17:17,430 --> 00:17:22,173
لاختبار ذلك، سأأخذ هذا المتجه وأحسب حاصل ضربه النقطي مقابل تضمينات بعض 

229
00:17:22,173 --> 00:17:27,050
الأسماء المفردة، ومقارنته مع نواتج الضرب النقطية مع أسماء الجمع المقابلة.

230
00:17:27,270 --> 00:17:31,738
إذا تلاعبت بهذا، ستلاحظ أن الجمع يبدو أنه يعطي دائمًا قيمًا أعلى 

231
00:17:31,738 --> 00:17:36,070
من القيم المفردة، مما يشير إلى أنها تتماشى أكثر مع هذا الاتجاه.

232
00:17:37,070 --> 00:17:40,783
ومن الممتع أيضًا أنه إذا أخذت هذا المنتج النقطي مع تضمينات 

233
00:17:40,783 --> 00:17:44,875
الكلمات 1، 2، 3، وما إلى ذلك، فإنها تعطي قيمًا متزايدة، لذا يبدو 

234
00:17:44,875 --> 00:17:49,030
الأمر كما لو أننا نستطيع قياس كمي مدى عثور النموذج على كلمة معينة.

235
00:17:50,250 --> 00:17:53,570
مرة أخرى، يتم تعلم تفاصيل كيفية تضمين الكلمات باستخدام البيانات.

236
00:17:54,050 --> 00:17:56,742
إن مصفوفة التضمين هذه، التي تخبرنا أعمدتها بما 

237
00:17:56,742 --> 00:17:59,550
يحدث لكل كلمة، هي أول كومة من الأوزان في نموذجنا.

238
00:18:00,030 --> 00:18:04,900
باستخدام أرقام GPT-3، يبلغ حجم المفردات على وجه التحديد 50257، ومرة أخرى، 

239
00:18:04,900 --> 00:18:09,770
لا يتكون هذا من الناحية الفنية من كلمات في حد ذاتها، بل من الرموز المميزة.

240
00:18:10,630 --> 00:18:17,790
بُعد التضمين هو 12,288، وبضرب ذلك يخبرنا أن هذا يتكون من حوالي 617 مليون وزن.

241
00:18:18,250 --> 00:18:21,084
دعونا نمضي قدمًا ونضيف هذا إلى حصيلة جارية، متذكرين 

242
00:18:21,084 --> 00:18:23,810
أنه في النهاية يجب أن نحصي ما يصل إلى 175 مليارًا.

243
00:18:25,430 --> 00:18:28,953
في حالة المحولات، أنت تريد حقًا أن تفكر في المتجهات الموجودة 

244
00:18:28,953 --> 00:18:32,130
في مساحة التضمين هذه على أنها لا تمثل مجرد كلمات فردية.

245
00:18:32,550 --> 00:18:37,507
لسبب واحد، أنها تقوم أيضًا بتشفير معلومات حول موضع تلك الكلمة، وهو ما سنتحدث عنه 

246
00:18:37,507 --> 00:18:42,770
لاحقًا، ولكن الأهم من ذلك، يجب أن تفكر فيها على أنها تتمتع بالقدرة على استيعاب السياق.

247
00:18:43,350 --> 00:18:49,458
على سبيل المثال، قد يتم سحب وسحب المتجه الذي بدأ حياته كدمج لكلمة &quot;ملك&quot; 

248
00:18:49,458 --> 00:18:54,598
بواسطة كتل مختلفة في هذه الشبكة، بحيث يشير في النهاية إلى اتجاه أكثر 

249
00:18:54,598 --> 00:18:59,962
تحديدًا ودقة والذي يشفر بطريقة أو بأخرى. كان ملكًا عاش في اسكتلندا، وقد 

250
00:18:59,962 --> 00:19:04,730
وصل إلى منصبه بعد قتل الملك السابق، ويتم وصفه باللغة الشكسبيرية.

251
00:19:05,210 --> 00:19:07,790
فكر في فهمك لكلمة معينة.

252
00:19:08,250 --> 00:19:13,052
يتم تحديد معنى هذه الكلمة بوضوح من خلال البيئة المحيطة، وفي بعض الأحيان 

253
00:19:13,052 --> 00:19:18,187
يتضمن ذلك السياق من مسافة بعيدة، لذلك عند تجميع نموذج لديه القدرة على التنبؤ 

254
00:19:18,187 --> 00:19:23,390
بالكلمة التي تأتي بعد ذلك، فإن الهدف هو تمكينه بطريقة ما من دمج السياق بكفاءة.

255
00:19:24,050 --> 00:19:28,290
لكي نكون واضحين، في تلك الخطوة الأولى، عندما تقوم بإنشاء مجموعة من المتجهات 

256
00:19:28,290 --> 00:19:32,585
بناءً على نص الإدخال، يتم انتزاع كل واحد منها ببساطة من مصفوفة التضمين، لذلك 

257
00:19:32,585 --> 00:19:36,770
في البداية يمكن لكل واحد فقط تشفير معنى كلمة واحدة بدون أي مدخلات من محيطه.

258
00:19:37,710 --> 00:19:43,372
لكن يجب أن تفكر في الهدف الأساسي لهذه الشبكة التي تتدفق من خلالها على أنه تمكين كل واحد 

259
00:19:43,372 --> 00:19:48,970
من تلك المتجهات من استيعاب معنى أكثر ثراءً وتحديدًا مما يمكن أن تمثله مجرد كلمات فردية.

260
00:19:49,510 --> 00:19:51,767
يمكن للشبكة معالجة عدد ثابت فقط من المتجهات في 

261
00:19:51,767 --> 00:19:54,170
المرة الواحدة، وهو ما يُعرف بحجم السياق الخاص بها.

262
00:19:54,510 --> 00:19:59,550
بالنسبة لـ GPT-3، تم تدريبه بحجم سياق يبلغ 2048، وبالتالي فإن البيانات المتدفقة عبر 

263
00:19:59,550 --> 00:20:04,590
الشبكة تبدو دائمًا مثل هذه المجموعة المكونة من 2048 عمودًا، يحتوي كل منها على 12000 

264
00:20:04,590 --> 00:20:05,010
بُعدًا.

265
00:20:05,590 --> 00:20:11,830
يحد حجم السياق هذا من مقدار النص الذي يمكن للمحول دمجه عند التنبؤ بالكلمة التالية.

266
00:20:12,370 --> 00:20:15,525
وهذا هو السبب في أن المحادثات الطويلة مع بعض برامج الدردشة 

267
00:20:15,525 --> 00:20:18,894
الآلية، مثل الإصدارات الأولى من ChatGPT، غالبًا ما أعطت شعورًا 

268
00:20:18,894 --> 00:20:22,050
بأن الروبوت يفقد خيط المحادثة مع استمرارك لفترة طويلة جدًا.

269
00:20:23,030 --> 00:20:25,973
سنتناول تفاصيل الاهتمام في الوقت المناسب، ولكن بالتخطي 

270
00:20:25,973 --> 00:20:28,810
للأمام، أريد أن أتحدث لمدة دقيقة عما يحدث في النهاية.

271
00:20:29,450 --> 00:20:34,870
تذكر أن الناتج المطلوب هو توزيع احتمالي على جميع الرموز المميزة التي قد تأتي بعد ذلك.

272
00:20:35,170 --> 00:20:39,210
على سبيل المثال، إذا كانت الكلمة الأخيرة هي &quot;بروفيسور&quot;، وكان 

273
00:20:39,210 --> 00:20:43,365
السياق يتضمن كلمات مثل &quot;هاري بوتر&quot;، وقبل ذلك مباشرة نرى المعلم 

274
00:20:43,365 --> 00:20:47,463
الأقل تفضيلًا، وأيضًا إذا أعطيتني بعض الحرية من خلال السماح لي بالتظاهر 

275
00:20:47,463 --> 00:20:51,618
بأن الرموز تبدو ببساطة وكأنها كلمات كاملة، إذن من المفترض أن تقوم الشبكة 

276
00:20:51,618 --> 00:20:55,830
المدربة جيدًا والتي اكتسبت المعرفة بهاري بوتر بتخصيص رقم كبير لكلمة Snape.

277
00:20:56,510 --> 00:20:57,970
وهذا ينطوي على خطوتين مختلفتين.

278
00:20:58,310 --> 00:21:02,711
الأول هو استخدام مصفوفة أخرى تقوم بتعيين المتجه الأخير في هذا 

279
00:21:02,711 --> 00:21:07,610
السياق إلى قائمة مكونة من 50000 قيمة، واحدة لكل رمز مميز في المفردات.

280
00:21:08,170 --> 00:21:13,283
ثم هناك دالة تعمل على تطبيع هذا إلى توزيع احتمالي، تسمى Softmax وسنتحدث عنها 

281
00:21:13,283 --> 00:21:18,130
أكثر خلال ثانية واحدة فقط، ولكن قبل ذلك قد يبدو غريبًا بعض الشيء استخدام 

282
00:21:18,130 --> 00:21:23,243
هذا التضمين الأخير فقط للتنبؤ، عندما بعد كل شيء، في تلك الخطوة الأخيرة، هناك 

283
00:21:23,243 --> 00:21:28,290
الآلاف من المتجهات الأخرى في الطبقة الموجودة هناك مع معانيها الغنية بالسياق.

284
00:21:28,930 --> 00:21:34,494
يتعلق هذا بحقيقة أنه في عملية التدريب يتبين أن الأمر أكثر كفاءة إذا استخدمت كل 

285
00:21:34,494 --> 00:21:40,270
واحد من تلك المتجهات في الطبقة النهائية للتنبؤ في نفس الوقت بما سيأتي بعده مباشرة.

286
00:21:40,970 --> 00:21:45,090
هناك الكثير مما يمكن قوله عن التدريب لاحقًا، لكني أريد فقط أن أذكر ذلك الآن.

287
00:21:45,730 --> 00:21:49,690
تسمى هذه المصفوفة بمصفوفة Unembedding ونعطيها التسمية WU.

288
00:21:50,210 --> 00:21:52,824
مرة أخرى، مثل جميع مصفوفات الوزن التي نراها، تبدأ 

289
00:21:52,824 --> 00:21:55,910
إدخالاتها بشكل عشوائي، ولكن يتم تعلمها أثناء عملية التدريب.

290
00:21:56,470 --> 00:22:00,948
للحفاظ على النتيجة في إجمالي عدد المعلمات لدينا، تحتوي مصفوفة إلغاء التضمين هذه 

291
00:22:00,948 --> 00:22:05,650
على صف واحد لكل كلمة في المفردات، وكل صف يحتوي على نفس عدد العناصر مثل بُعد التضمين.

292
00:22:06,410 --> 00:22:11,641
إنها تشبه إلى حد كبير مصفوفة التضمين، فقط مع تبديل الترتيب، لذا فهي تضيف 617 مليون 

293
00:22:11,641 --> 00:22:16,684
معلمة أخرى إلى الشبكة، مما يعني أن عددنا حتى الآن يزيد قليلاً عن مليار، وهو جزء 

294
00:22:16,684 --> 00:22:21,790
صغير ولكنه ليس ضئيلًا تمامًا من الـ 175 مليارًا التي لدينا. سوف ينتهي في المجموع.

295
00:22:22,550 --> 00:22:26,346
كدرس صغير أخير في هذا الفصل، أريد أن أتحدث أكثر عن وظيفة 

296
00:22:26,346 --> 00:22:30,610
softmax هذه، لأنها تظهر لنا مرة أخرى عندما نغوص في كتل الانتباه.

297
00:22:31,430 --> 00:22:35,672
الفكرة هي أنه إذا كنت تريد أن تعمل سلسلة من الأرقام كتوزيع 

298
00:22:35,672 --> 00:22:40,059
احتمالي، مثل التوزيع على جميع الكلمات التالية المحتملة، فيجب 

299
00:22:40,059 --> 00:22:44,590
أن تكون كل قيمة بين 0 و1، وتحتاج أيضًا إلى جمعها جميعًا حتى 1 .

300
00:22:45,250 --> 00:22:50,095
ومع ذلك، إذا كنت تلعب لعبة تعليمية حيث يبدو كل ما تفعله مثل الضرب بمصفوفة 

301
00:22:50,095 --> 00:22:54,810
ومتجه، فإن المخرجات التي تحصل عليها افتراضيًا لا تلتزم بهذا على الإطلاق.

302
00:22:55,330 --> 00:22:59,870
غالبًا ما تكون القيم سالبة، أو أكبر بكثير من 1، ومن المؤكد تقريبًا ألا يكون مجموعها 1.

303
00:23:00,510 --> 00:23:05,692
Softmax هي الطريقة القياسية لتحويل قائمة عشوائية من الأرقام إلى توزيع صالح 

304
00:23:05,692 --> 00:23:11,290
بطريقة تجعل القيم الأكبر تنتهي الأقرب إلى 1، والقيم الأصغر تنتهي قريبة جدًا من 0.

305
00:23:11,830 --> 00:23:13,070
هذا كل ما تحتاج إلى معرفته حقًا.

306
00:23:13,090 --> 00:23:18,528
لكن إذا كنت فضوليًا، فإن الطريقة التي يتم بها الأمر هي أولاً رفع e إلى قوة كل رقم، 

307
00:23:18,528 --> 00:23:23,835
وهو ما يعني أن لديك الآن قائمة من القيم الموجبة، وبعد ذلك يمكنك جمع كل تلك القيم 

308
00:23:23,835 --> 00:23:29,470
الموجبة وتقسيمها كل مصطلح بهذا المبلغ، مما يؤدي إلى تطبيعه في قائمة تضيف ما يصل إلى 1.

309
00:23:30,170 --> 00:23:34,445
ستلاحظ أنه إذا كان أحد الأرقام في المدخلات أكبر بكثير من الباقي، 

310
00:23:34,445 --> 00:23:38,523
ففي المخرجات، يهيمن المصطلح المقابل على التوزيع، لذلك إذا كنت 

311
00:23:38,523 --> 00:23:42,470
تأخذ عينات منه فمن المؤكد تقريبًا أنك تختار المدخلات القصوى.

312
00:23:42,990 --> 00:23:46,818
ولكنه أكثر ليونة من مجرد اختيار الحد الأقصى، بمعنى أنه عندما تكون 

313
00:23:46,818 --> 00:23:50,705
القيم الأخرى كبيرة بشكل مماثل، فإنها تحصل أيضًا على وزن ذي معنى في 

314
00:23:50,705 --> 00:23:54,650
التوزيع، وكل شيء يتغير بشكل مستمر حيث تقوم باستمرار بتغيير المدخلات.

315
00:23:55,130 --> 00:23:59,631
في بعض المواقف، مثل عندما يستخدم ChatGPT هذا التوزيع لإنشاء كلمة 

316
00:23:59,631 --> 00:24:04,132
تالية، هناك مساحة لقليل من المرح الإضافي عن طريق إضافة القليل من 

317
00:24:04,132 --> 00:24:08,910
الإثارة الإضافية إلى هذه الوظيفة، مع إضافة ثابت t إلى مقام تلك الأسس.

318
00:24:09,550 --> 00:24:15,376
نحن نسميها درجة الحرارة، لأنها تشبه بشكل غامض دور درجة الحرارة في بعض معادلات الديناميكا 

319
00:24:15,376 --> 00:24:21,137
الحرارية، والتأثير هو أنه عندما تكون t أكبر، فإنك تعطي وزنًا أكبر للقيم الأقل، مما يعني 

320
00:24:21,137 --> 00:24:27,029
أن التوزيع يكون أكثر تجانسًا قليلاً، وإذا إذا كان t أصغر، فإن القيم الأكبر سوف تهيمن بقوة 

321
00:24:27,029 --> 00:24:32,790
أكبر، حيث في الحالة القصوى، تعيين t يساوي الصفر يعني أن كل الوزن يذهب إلى القيمة القصوى.

322
00:24:33,470 --> 00:24:38,249
على سبيل المثال، سأطلب من GPT-3 إنشاء قصة بالنص الأساسي، ذات 

323
00:24:38,249 --> 00:24:42,950
مرة كان هناك A، لكنني سأستخدم درجات حرارة مختلفة في كل حالة.

324
00:24:43,630 --> 00:24:47,819
درجة الحرارة صفر تعني أنها تتوافق دائمًا مع الكلمة الأكثر 

325
00:24:47,819 --> 00:24:52,370
توقعًا، وما تحصل عليه في نهاية المطاف هو مشتق مبتذل من المعتدل.

326
00:24:53,010 --> 00:24:57,910
تمنحك درجة الحرارة المرتفعة فرصة لاختيار كلمات أقل احتمالية، ولكنها تنطوي على مخاطرة.

327
00:24:58,230 --> 00:25:01,941
في هذه الحالة، تبدأ القصة بشكل أكثر أصالة، حول فنان 

328
00:25:01,941 --> 00:25:06,010
ويب شاب من كوريا الجنوبية، لكنها سرعان ما تتحول إلى هراء.

329
00:25:06,950 --> 00:25:10,830
من الناحية الفنية، لا تسمح لك واجهة برمجة التطبيقات (API) باختيار درجة حرارة أكبر من 2.

330
00:25:11,170 --> 00:25:15,218
لا يوجد سبب رياضي لذلك، إنه مجرد قيد تعسفي مفروض 

331
00:25:15,218 --> 00:25:19,350
لمنع أدواتهم من الظهور وهي تولد أشياء لا معنى لها.

332
00:25:19,870 --> 00:25:24,145
لذا، إذا كنت فضوليًا، فإن الطريقة التي تعمل بها هذه الرسوم المتحركة في الواقع 

333
00:25:24,145 --> 00:25:28,530
هي أنني آخذ الـ 20 رمزًا التاليًا الأكثر احتمالية التي ينشئها GPT-3، والذي يبدو 

334
00:25:28,530 --> 00:25:32,970
أنه الحد الأقصى الذي سيعطونه لي، ثم أقوم بتعديل الاحتمالات بناءً على على الأس 15.

335
00:25:33,130 --> 00:25:37,532
كمصطلح آخر، بنفس الطريقة التي يمكنك من خلالها تسمية مكونات مخرجات هذه 

336
00:25:37,532 --> 00:25:42,061
الدالة بالاحتمالات، غالبًا ما يشير الأشخاص إلى المدخلات على أنها سجلات، 

337
00:25:42,061 --> 00:25:46,150
أو يقول بعض الأشخاص سجلات، ويقول بعض الأشخاص سجلات، سأقول سجلات .

338
00:25:46,530 --> 00:25:49,995
على سبيل المثال، عندما تقوم بتغذية بعض النصوص، فإن كل هذه الكلمات 

339
00:25:49,995 --> 00:25:53,828
المضمنة تتدفق عبر الشبكة، وتقوم بإجراء هذا الضرب النهائي باستخدام مصفوفة 

340
00:25:53,828 --> 00:25:57,609
إلغاء التضمين، وسيشير الأشخاص الذين يتعلمون الآلة إلى المكونات الموجودة 

341
00:25:57,609 --> 00:26:01,390
في هذا الناتج الأولي غير الطبيعي باسم اللوجيستات للتنبؤ بالكلمة التالية.

342
00:26:03,330 --> 00:26:06,850
كان الكثير من الهدف في هذا الفصل هو وضع الأسس لفهم 

343
00:26:06,850 --> 00:26:10,370
آلية الانتباه، أسلوب طفل الكاراتيه الشمع على الشمع.

344
00:26:10,850 --> 00:26:16,091
كما ترى، إذا كان لديك حدس قوي لتضمين الكلمات، ولسوفت ماكس، لكيفية قياس المنتجات 

345
00:26:16,091 --> 00:26:21,398
النقطية للتشابه، وكذلك الفرضية الأساسية التي مفادها أن معظم الحسابات يجب أن تبدو 

346
00:26:21,398 --> 00:26:26,706
مثل ضرب المصفوفات بمصفوفات مليئة بالمعلمات القابلة للضبط، ثم فهم الاهتمام يجب أن 

347
00:26:26,706 --> 00:26:32,210
تكون هذه الآلية، وهي حجر الزاوية في الطفرة الحديثة في الذكاء الاصطناعي، سلسة نسبيًا.

348
00:26:32,650 --> 00:26:34,510
لذلك، تعال وانضم إلي في الفصل التالي.

349
00:26:36,390 --> 00:26:41,210
بينما أنشر هذا، تتوفر مسودة الفصل التالي للمراجعة من قبل مؤيدي Patreon.

350
00:26:41,770 --> 00:26:44,605
من المفترض أن يتم نشر النسخة النهائية للعامة خلال أسبوع أو أسبوعين، ويعتمد ذلك 

351
00:26:44,605 --> 00:26:47,370
عادةً على مقدار التغيير الذي سأقوم به في نهاية المطاف بناءً على تلك المراجعة.

352
00:26:47,810 --> 00:26:50,157
في هذه الأثناء، إذا كنت تريد التعمق في الاهتمام، 

353
00:26:50,157 --> 00:26:52,410
وإذا كنت تريد مساعدة القناة قليلاً، فهي تنتظرك.

