[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "الأحرف الأولى من GPT تعني المحول التوليدي المُدرب مسبقًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "لذا فإن الكلمة الأولى واضحة بما فيه الكفاية، فهي روبوتات تولد نصًا جديدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "يشير التدريب المسبق إلى كيفية خضوع النموذج لعملية التعلم من كمية هائلة من البيانات، وتشير البادئة إلى أن هناك مساحة أكبر لضبطه في مهام محددة مع تدريب إضافي.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "لكن الكلمة الأخيرة، هذه هي القطعة الرئيسية الحقيقية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "المحول هو نوع محدد من الشبكات العصبية، وهو نموذج للتعلم الآلي، وهو الاختراع الأساسي الكامن وراء الطفرة الحالية في الذكاء الاصطناعي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "ما أريد أن أفعله بهذا الفيديو والفصول التالية هو تقديم شرح بصري لما يحدث بالفعل داخل المحول.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "سنقوم بمتابعة البيانات التي تتدفق من خلاله ونتحرك خطوة بخطوة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "هناك العديد من أنواع النماذج المختلفة التي يمكنك بنائها باستخدام المحولات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "تأخذ بعض النماذج الصوت وتنتج نصًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "تأتي هذه الجملة من نموذج يسير في الاتجاه المعاكس، وينتج خطابًا تركيبيًا من النص فقط.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "كل تلك الأدوات التي اجتاحت العالم في عام 2022 مثل Dolly وMidjourney التي تأخذ وصفًا نصيًا وتنتج صورة تعتمد على المحولات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "حتى لو لم أتمكن من فهم ما يفترض أن يكون عليه مخلوق الفطيرة، ما زلت مندهشًا من أن هذا النوع من الأشياء ممكن حتى ولو عن بعد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "وتم اختراع المحول الأصلي الذي قدمته Google في عام 2017 لحالة الاستخدام المحددة لترجمة النص من لغة إلى أخرى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "لكن المتغير الذي سنركز عليه أنا وأنت، وهو النوع الذي يكمن وراء أدوات مثل ChatGPT، سيكون نموذجًا تم تدريبه على استيعاب جزء من النص، ربما حتى مع بعض الصور المحيطة أو الصوت المصاحب له، وإنتاج تنبؤ لما سيأتي بعد ذلك في المقطع.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "يأخذ هذا التنبؤ شكل توزيع احتمالي على العديد من أجزاء النص المختلفة التي قد تتبعها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "للوهلة الأولى، قد تعتقد أن التنبؤ بالكلمة التالية يبدو وكأنه هدف مختلف تمامًا عن إنشاء نص جديد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "ولكن بمجرد أن يكون لديك نموذج تنبؤ مثل هذا، فإن الشيء البسيط الذي يمكنك إنشاء جزء أطول من النص هو إعطائه مقتطفًا أوليًا للعمل معه، وجعله يأخذ عينة عشوائية من التوزيع الذي أنشأه للتو، وإلحاق تلك العينة بالنص ، ثم قم بتشغيل العملية برمتها مرة أخرى لإجراء تنبؤ جديد استنادًا إلى النص الجديد بالكامل، بما في ذلك ما تمت إضافته للتو.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "لا أعرف عنك، لكن يبدو أن هذا لا ينبغي أن ينجح حقًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "في هذه الرسوم المتحركة، على سبيل المثال، أقوم بتشغيل GPT-2 على جهاز الكمبيوتر المحمول الخاص بي وأطلب منه التنبؤ بشكل متكرر وأخذ عينات من الجزء التالي من النص لإنشاء قصة بناءً على النص الأولي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "القصة ليس لها معنى كبير حقًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "ولكن إذا قمت باستبدالها باستدعاءات واجهة برمجة التطبيقات (API) إلى GPT-3 بدلاً من ذلك، وهو نفس النموذج الأساسي، ولكنه أكبر بكثير، فسنحصل فجأة وبطريقة سحرية تقريبًا على قصة معقولة، قصة يبدو أنها تستنتج أن مخلوق باي سيعيش في عالم أرض الرياضيات والحساب.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "هذه العملية هنا من التنبؤ المتكرر وأخذ العينات هي في الأساس ما يحدث عندما تتفاعل مع ChatGPT أو أي من نماذج اللغات الكبيرة الأخرى هذه وتراهم ينتجون كلمة واحدة في كل مرة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "في الواقع، إحدى الميزات التي سأستمتع بها كثيرًا هي القدرة على رؤية التوزيع الأساسي لكل كلمة جديدة تختارها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "دعونا نبدأ الأمور بمعاينة عالية المستوى لكيفية تدفق البيانات عبر المحول.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "سنقضي المزيد من الوقت في التحفيز والتفسير والتوسع في تفاصيل كل خطوة، ولكن بشكل عام، عندما يقوم أحد روبوتات الدردشة هذه بإنشاء كلمة معينة، إليك ما يحدث تحت الغطاء.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "أولاً، يتم تقسيم المدخلات إلى مجموعة من القطع الصغيرة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "تسمى هذه القطع بالرموز، وفي حالة النص، تميل هذه إلى أن تكون كلمات أو أجزاء صغيرة من الكلمات أو مجموعات أحرف مشتركة أخرى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "إذا كانت الصور أو الصوت متضمنة، فيمكن أن تكون الرموز المميزة عبارة عن بقع صغيرة من تلك الصورة أو أجزاء صغيرة من هذا الصوت.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "يتم بعد ذلك ربط كل واحدة من هذه الرموز المميزة بمتجه، مما يعني قائمة من الأرقام، والتي تهدف إلى تشفير معنى تلك القطعة بطريقة ما.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "إذا كنت تعتقد أن هذه المتجهات تعطي إحداثيات في مساحة ذات أبعاد عالية جدًا، فإن الكلمات ذات المعاني المتشابهة تميل إلى الهبوط على ناقلات قريبة من بعضها البعض في ذلك الفضاء.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "يمر تسلسل المتجهات هذا عبر عملية تُعرف باسم كتلة الانتباه، وهذا يسمح للمتجهات بالتحدث مع بعضها البعض وتمرير المعلومات ذهابًا وإيابًا لتحديث قيمها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "على سبيل المثال، يختلف معنى كلمة نموذج في عبارة نموذج التعلم الآلي عن معناها في عبارة نموذج أزياء.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "إن كتلة الانتباه هي المسؤولة عن معرفة الكلمات في السياق ذات الصلة بتحديث معاني الكلمات الأخرى، وكيف يجب تحديث هذه المعاني بالضبط.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "ومرة أخرى، كلما استخدمت معنى الكلمة، يتم تشفيرها بالكامل بطريقة أو بأخرى في مدخلات تلك المتجهات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "بعد ذلك، تمر هذه المتجهات من خلال نوع مختلف من العمليات، واعتمادًا على المصدر الذي تقرأه، سيشار إلى ذلك باسم الإدراك الحسي متعدد الطبقات أو ربما طبقة التغذية الأمامية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "وهنا لا تتحدث المتجهات مع بعضها البعض، بل تمر جميعها بنفس العملية بالتوازي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "وعلى الرغم من صعوبة تفسير هذه الكتلة قليلًا، سنتحدث لاحقًا عن كيف أن الخطوة تشبه إلى حدٍ ما طرح قائمة طويلة من الأسئلة حول كل متجه، ثم تحديثها بناءً على إجابات تلك الأسئلة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "تبدو جميع العمليات في كلتا الكتلتين وكأنها كومة ضخمة من مضاعفات المصفوفات، وستكون مهمتنا الأساسية هي فهم كيفية قراءة المصفوفات الأساسية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "أقوم بتغطية بعض التفاصيل حول بعض خطوات التطبيع التي تحدث بينهما، ولكن هذه في النهاية معاينة عالية المستوى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "بعد ذلك، تتكرر العملية بشكل أساسي، وتتنقل ذهابًا وإيابًا بين كتل الانتباه وكتل الإدراك الحسي متعددة الطبقات، حتى النهاية، يكون الأمل هو أن كل المعنى الأساسي للمقطع قد تم بطريقة ما خبزه في المتجه الأخير في الترتيب.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "نقوم بعد ذلك بإجراء عملية معينة على المتجه الأخير الذي ينتج توزيعًا احتماليًا على جميع الرموز المميزة المحتملة، وجميع الأجزاء الصغيرة المحتملة من النص التي قد تأتي بعد ذلك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "وكما قلت، بمجرد أن يكون لديك أداة تتنبأ بما سيأتي بعد ذلك في ضوء مقتطف من النص، يمكنك تغذيتها بقليل من النص الأولي وجعلها تلعب بشكل متكرر لعبة التنبؤ بما سيأتي بعد ذلك، وأخذ عينات من التوزيع، والإلحاق ذلك، ثم تكرره مراراً وتكراراً.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "ربما يتذكر البعض منكم من ذوي الخبرة المدة التي سبقت ظهور ChatGPT في المشهد، هذا هو الشكل الذي كانت تبدو عليه العروض التوضيحية المبكرة لـ GPT-3، حيث يمكنك إكمال القصص والمقالات تلقائيًا بناءً على مقتطف أولي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "لتحويل أداة كهذه إلى روبوت دردشة، فإن أسهل نقطة بداية هي الحصول على القليل من النص الذي يحدد إعدادات المستخدم الذي يتفاعل مع مساعد الذكاء الاصطناعي المفيد، وهو ما يمكن أن تسميه موجه النظام، وبعد ذلك ستستخدم السؤال الأولي للمستخدم أو المطالبة به هو الجزء الأول من الحوار، وبعد ذلك يمكنك البدء في التنبؤ بما سيقوله مساعد الذكاء الاصطناعي المفيد ردًا على ذلك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "هناك الكثير مما يمكن قوله عن خطوة التدريب المطلوبة لإنجاح هذا الأمر، ولكن على مستوى عالٍ، هذه هي الفكرة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "في هذا الفصل، سنتوسع أنا وأنت في تفاصيل ما يحدث في بداية الشبكة، وفي نهايتها، وأريد أيضًا قضاء الكثير من الوقت في مراجعة بعض الأجزاء المهمة من المعرفة الأساسية أشياء كانت ستصبح طبيعة أي مهندس تعلم آلي بحلول الوقت الذي ظهرت فيه المحولات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "إذا كنت مرتاحًا لهذه المعرفة الأساسية وقليل الصبر، فلا تتردد في الانتقال إلى الفصل التالي، والذي سيركز على كتل الانتباه، والتي تعتبر بشكل عام قلب المحول.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "بعد ذلك أريد أن أتحدث أكثر عن كتل الإدراك الحسي متعددة الطبقات، وكيفية عمل التدريب، وعدد من التفاصيل الأخرى التي سيتم تخطيها حتى تلك النقطة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "للحصول على سياق أوسع، تعد مقاطع الفيديو هذه إضافات إلى سلسلة مصغرة حول التعلم العميق، ولا بأس إذا لم تكن قد شاهدت مقاطع الفيديو السابقة، أعتقد أنه يمكنك القيام بذلك خارج النظام، ولكن قبل الغوص في المحولات على وجه التحديد، أعتقد من الجدير التأكد من أننا على نفس الصفحة حول الفرضية الأساسية وبنية التعلم العميق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "على الرغم من المخاطرة بتوضيح ما هو واضح، فهذا هو أحد أساليب التعلم الآلي، والذي يصف أي نموذج تستخدم فيه البيانات لتحديد كيفية تصرف النموذج بطريقة أو بأخرى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "ما أعنيه بذلك هو، لنفترض أنك تريد وظيفة تلتقط صورة وتنتج علامة تصفها، أو مثالنا للتنبؤ بالكلمة التالية في ضوء مقطع من النص، أو أي مهمة أخرى يبدو أنها تتطلب بعض العناصر الحدس والتعرف على الأنماط.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "نحن نعتبر هذا الأمر أمرا مفروغا منه هذه الأيام، ولكن الفكرة في التعلم الآلي هي أنه بدلا من محاولة تحديد إجراء واضح لكيفية القيام بهذه المهمة في التعليمات البرمجية، وهو ما كان سيفعله الناس في الأيام الأولى للذكاء الاصطناعي، بدلا من ذلك قم بإعداد بنية مرنة للغاية مع معلمات قابلة للضبط، مثل مجموعة من المقابض والأقراص، ثم تستخدم بطريقة ما العديد من الأمثلة حول الشكل الذي يجب أن يبدو عليه الإخراج لمدخل معين لتعديل وضبط قيم تلك المعلمات لتقليد هذا السلوك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "على سبيل المثال، ربما يكون أبسط شكل من أشكال التعلم الآلي هو الانحدار الخطي، حيث تكون المدخلات والمخرجات عبارة عن أرقام فردية، شيء مثل اللقطات المربعة للمنزل وسعره، وما تريده هو العثور على خط أفضل ملاءمة من خلال هذا البيانات، كما تعلمون، للتنبؤ بأسعار المنازل في المستقبل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "يتم وصف هذا الخط بمعلمتين مستمرتين، على سبيل المثال الميل والتقاطع y، والهدف من الانحدار الخطي هو تحديد تلك المعلمات لمطابقة البيانات بشكل وثيق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "وغني عن القول أن نماذج التعلم العميق تصبح أكثر تعقيدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "GPT-3، على سبيل المثال، لا يحتوي على اثنين، بل 175 مليار معلمة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "ولكن هذا هو الأمر، ليس من المسلم به أنه يمكنك إنشاء نموذج عملاق يحتوي على عدد كبير من المعلمات دون الحاجة إلى الإفراط في تجهيز بيانات التدريب بشكل كبير أو استعصاء التدريب تمامًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "يصف التعلم العميق فئة من النماذج التي أثبتت في العقدين الماضيين أنها قابلة للتوسع بشكل ملحوظ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "ما يوحدهم هو نفس خوارزمية التدريب، التي تسمى الانتشار العكسي، والسياق الذي أريدك أن تحصل عليه أثناء تقدمنا هو أنه لكي تعمل خوارزمية التدريب هذه بشكل جيد على نطاق واسع، يجب أن تتبع هذه النماذج تنسيقًا محددًا معينًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "إذا كنت تعرف هذا التنسيق، فمن المفيد أن تشرح العديد من الاختيارات الخاصة بكيفية معالجة المحول للغة، والتي قد تتعرض لخطر الشعور بالتعسف.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "أولاً، أيًا كان النموذج الذي تقوم بإنشائه، يجب تنسيق الإدخال كمصفوفة من الأرقام الحقيقية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "قد يعني هذا قائمة من الأرقام، أو يمكن أن تكون مصفوفة ثنائية الأبعاد، أو في كثير من الأحيان تتعامل مع مصفوفات ذات أبعاد أعلى، حيث المصطلح العام المستخدم هو الموتر.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "غالبًا ما تفكر في أن بيانات الإدخال يتم تحويلها تدريجيًا إلى العديد من الطبقات المتميزة، حيث يتم تنظيم كل طبقة دائمًا كنوع من مجموعة من الأرقام الحقيقية، حتى تصل إلى الطبقة النهائية التي تعتبرها المخرجات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "على سبيل المثال، الطبقة الأخيرة في نموذج معالجة النص لدينا هي قائمة من الأرقام التي تمثل التوزيع الاحتمالي لجميع الرموز المميزة التالية الممكنة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "في التعلم العميق، يُشار دائمًا إلى معلمات النموذج هذه بالأوزان، وذلك لأن الميزة الرئيسية لهذه النماذج هي أن الطريقة الوحيدة لتفاعل هذه المعلمات مع البيانات التي تتم معالجتها هي من خلال المبالغ المرجحة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "يمكنك أيضًا رش بعض الوظائف غير الخطية طوال الوقت، لكنها لن تعتمد على المعلمات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "عادةً، بدلًا من رؤية المجاميع المرجحة كلها عارية ومكتوبة بشكل واضح بهذه الطريقة، ستجدها مجمعة معًا كمكونات مختلفة في منتج متجه المصفوفة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "إنه يعني قول الشيء نفسه، إذا فكرت مرة أخرى في كيفية عمل ضرب متجه المصفوفة، فإن كل مكون في الإخراج يبدو وكأنه مجموع مرجح.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "غالبًا ما يكون من الأنظف من الناحية المفاهيمية بالنسبة لي ولكم التفكير في المصفوفات المملوءة بمعلمات قابلة للضبط والتي تحول المتجهات التي يتم استخلاصها من البيانات التي تتم معالجتها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "على سبيل المثال، تم تنظيم تلك الأوزان البالغ عددها 175 مليارًا في GPT-3 في ما يقل قليلاً عن 28000 مصفوفة متميزة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "تنقسم هذه المصفوفات بدورها إلى ثماني فئات مختلفة، وما سنفعله أنا وأنت هو المرور عبر كل واحدة من هذه الفئات لفهم ما يفعله هذا النوع.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "بينما نمضي قدمًا، أعتقد أنه من الممتع الرجوع إلى الأرقام المحددة من GPT-3 لحساب مصدر تلك الـ 175 مليارًا بالضبط.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "حتى لو كانت هناك نماذج أكبر وأفضل في الوقت الحاضر، فإن هذا النموذج يتمتع بسحر معين باعتباره نموذج اللغة الكبيرة لجذب انتباه العالم خارج مجتمعات تعلم الآلة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "ومن الناحية العملية أيضًا، تميل الشركات إلى الالتزام بأرقام محددة للشبكات الأكثر حداثة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "أريد فقط أن أبدأ المشهد، فبينما تنظر إلى أسفل الغطاء لترى ما يحدث داخل أداة مثل ChatGPT، تبدو كل العمليات الحسابية الفعلية تقريبًا مثل مضاعفة متجهات المصفوفات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "هناك القليل من المخاطرة بالضياع في بحر مليارات الأرقام، ولكن يجب أن ترسم تمييزًا حادًا للغاية في عقلك بين أوزان النموذج، والتي سألونها دائمًا باللون الأزرق أو الأحمر، والبيانات التي يتم الحصول عليها تمت معالجتها، والتي سألونها دائمًا باللون الرمادي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "الأوزان هي العقول الفعلية، وهي الأشياء التي يتم تعلمها أثناء التدريب، وهي التي تحدد كيفية تصرفه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "تقوم البيانات التي تتم معالجتها ببساطة بتشفير أي مدخلات محددة يتم إدخالها في النموذج لتشغيل معين، مثل مقتطف من النص.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "مع كل ذلك كأساس، دعونا نتعمق في الخطوة الأولى من مثال معالجة النص هذا، وهو تقسيم المدخلات إلى أجزاء صغيرة وتحويل تلك الأجزاء إلى متجهات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "لقد ذكرت كيف تسمى هذه القطع بالرموز، والتي قد تكون أجزاء من الكلمات أو علامات الترقيم، ولكن بين الحين والآخر في هذا الفصل وخاصة في الفصل التالي، أود فقط أن أتظاهر بأنها مقسمة بشكل أكثر وضوحًا إلى كلمات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "نظرًا لأننا نحن البشر نفكر بالكلمات، فإن هذا سيجعل من الأسهل بكثير الرجوع إلى أمثلة صغيرة وتوضيح كل خطوة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "يحتوي النموذج على مفردات محددة مسبقًا، وقائمة من كل الكلمات الممكنة، على سبيل المثال 50000 منها، والمصفوفة الأولى التي سنواجهها، والمعروفة باسم مصفوفة التضمين، تحتوي على عمود واحد لكل كلمة من هذه الكلمات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "هذه الأعمدة هي التي تحدد المتجه الذي تتحول إليه كل كلمة في تلك الخطوة الأولى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "نسميها نحن، ومثل كل المصفوفات التي نراها، تبدأ قيمها بشكل عشوائي، ولكن سيتم تعلمها بناءً على البيانات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "كان تحويل الكلمات إلى متجهات ممارسة شائعة في التعلم الآلي قبل فترة طويلة من المحولات، ولكنه أمر غريب بعض الشيء إذا لم يسبق لك رؤيته من قبل، وهو يضع الأساس لكل ما يلي، لذلك دعونا نتوقف لحظة للتعرف عليه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "غالبًا ما نطلق على هذا التضمين كلمة، مما يدعوك إلى التفكير في هذه المتجهات بشكل هندسي للغاية كنقاط في مساحة عالية الأبعاد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "لن يكون تصور قائمة من ثلاثة أرقام كإحداثيات لنقاط في مساحة ثلاثية الأبعاد مشكلة، لكن تضمين الكلمات يميل إلى أن يكون ذو أبعاد أعلى بكثير.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "في GPT-3 لديهم 12288 بُعدًا، وكما سترون، من المهم العمل في مساحة بها الكثير من الاتجاهات المميزة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "بنفس الطريقة التي يمكنك من خلالها أخذ شريحة ثنائية الأبعاد عبر مساحة ثلاثية الأبعاد وإسقاط جميع النقاط على تلك الشريحة، من أجل تحريك تضمينات الكلمات التي يقدمها لي نموذج بسيط، سأفعل شيئًا مشابهًا عن طريق اختيار شريحة ثلاثية الأبعاد عبر هذا الفضاء ذي الأبعاد العالية جدًا، وإسقاط متجهات الكلمات عليها وعرض النتائج.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "الفكرة الكبيرة هنا هي أنه عندما يقوم النموذج بتعديل وضبط أوزانه لتحديد كيفية دمج الكلمات كمتجهات أثناء التدريب، فإنه يميل إلى الاستقرار على مجموعة من التضمينات حيث يكون للاتجاهات في الفضاء نوع من المعنى الدلالي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "بالنسبة لنموذج تحويل الكلمة إلى ناقل البسيط الذي أستخدمه هنا، إذا قمت بإجراء بحث عن جميع الكلمات التي تكون تضميناتها أقرب إلى كلمة برج، ستلاحظ كيف تبدو جميعها وكأنها تعطي مشاعر برجية متشابهة جدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "وإذا كنت تريد تعلم بعض لغة بايثون واللعب بها في المنزل، فهذا هو النموذج المحدد الذي أستخدمه لصنع الرسوم المتحركة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "إنه ليس محولاً، لكنه يكفي لتوضيح فكرة أن الاتجاهات في الفضاء يمكن أن تحمل معنى دلاليًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "أحد الأمثلة الكلاسيكية على ذلك هو أنه إذا أخذت الفرق بين المتجهات الخاصة بالمرأة والرجل، وهو شيء يمكن أن تتخيله كمتجه صغير يربط طرف أحدهما بطرف الآخر، فهو مشابه جدًا للفرق بين الملك والرجل ملكة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "لنفترض أنك لا تعرف كلمة ملكة أنثى، يمكنك العثور عليها عن طريق أخذ الملك، وإضافة اتجاه المرأة-الرجل، والبحث عن التضمينات الأقرب إلى تلك النقطة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "على الأقل نوعا ما.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "على الرغم من كونه مثالًا كلاسيكيًا للنموذج الذي ألعب به، فإن التضمين الحقيقي للملكة هو في الواقع أبعد قليلاً عما قد يوحي به هذا، ربما لأن الطريقة التي يتم بها استخدام الملكة في بيانات التدريب ليست مجرد نسخة أنثوية من الملك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "وعندما تجولت في الأمر، بدا أن العلاقات الأسرية توضح الفكرة بشكل أفضل بكثير.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "النقطة المهمة هي أنه يبدو أثناء التدريب أن النموذج وجد أنه من المفيد اختيار التضمينات بحيث يقوم اتجاه واحد في هذا الفضاء بتشفير المعلومات المتعلقة بالجنس.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "مثال آخر هو أنك إذا أخذت تضمين إيطاليا، وطرحت تضمين ألمانيا، وأضفت ذلك إلى تضمين هتلر، فستحصل على شيء قريب جدًا من تضمين موسوليني.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "يبدو الأمر كما لو أن النموذج تعلم ربط بعض الاتجاهات بالهوية الإيطالية، وأخرى بقادة محور الحرب العالمية الثانية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "ربما المثال المفضل لدي في هذا السياق هو كيف أنه في بعض النماذج، إذا أخذت الفرق بين ألمانيا واليابان، وأضفته إلى السوشي، فسينتهي بك الأمر قريبًا جدًا من النقانق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "أيضًا أثناء لعب لعبة العثور على أقرب الجيران، سررت برؤية مدى قرب كات من الوحش والوحش.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "أحد الأمور الرياضية البديهية التي من المفيد أن نأخذها في الاعتبار، خاصة في الفصل التالي، هو كيف يمكن اعتبار المنتج النقطي لمتجهين وسيلة لقياس مدى توافقهما.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "من الناحية الحسابية، تتضمن المنتجات النقطية ضرب جميع المكونات المقابلة ثم إضافة النتائج، وهو أمر جيد، نظرًا لأن الكثير من حساباتنا يجب أن تبدو وكأنها مبالغ مرجحة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "هندسيًا، يكون حاصل الضرب النقطي موجبًا عندما تشير المتجهات إلى اتجاهات متشابهة، ويكون صفرًا إذا كانت متعامدة، ويكون سالبًا عندما تشير إلى اتجاهات متعاكسة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "على سبيل المثال، لنفترض أنك كنت تلعب بهذا النموذج، وتفترض أن تضمين القطط ناقص القطة قد يمثل نوعًا من اتجاه التعددية في هذا الفضاء.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "لاختبار ذلك، سأأخذ هذا المتجه وأحسب حاصل ضربه النقطي مقابل تضمينات بعض الأسماء المفردة، ومقارنته مع نواتج الضرب النقطية مع أسماء الجمع المقابلة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "إذا تلاعبت بهذا، ستلاحظ أن الجمع يبدو أنه يعطي دائمًا قيمًا أعلى من القيم المفردة، مما يشير إلى أنها تتماشى أكثر مع هذا الاتجاه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "ومن الممتع أيضًا أنه إذا أخذت هذا المنتج النقطي مع تضمينات الكلمات 1، 2، 3، وما إلى ذلك، فإنها تعطي قيمًا متزايدة، لذا يبدو الأمر كما لو أننا نستطيع قياس كمي مدى عثور النموذج على كلمة معينة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "مرة أخرى، يتم تعلم تفاصيل كيفية تضمين الكلمات باستخدام البيانات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "إن مصفوفة التضمين هذه، التي تخبرنا أعمدتها بما يحدث لكل كلمة، هي أول كومة من الأوزان في نموذجنا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "باستخدام أرقام GPT-3، يبلغ حجم المفردات على وجه التحديد 50257، ومرة أخرى، لا يتكون هذا من الناحية الفنية من كلمات في حد ذاتها، بل من الرموز المميزة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "بُعد التضمين هو 12,288، وبضرب ذلك يخبرنا أن هذا يتكون من حوالي 617 مليون وزن.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "دعونا نمضي قدمًا ونضيف هذا إلى حصيلة جارية، متذكرين أنه في النهاية يجب أن نحصي ما يصل إلى 175 مليارًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "في حالة المحولات، أنت تريد حقًا أن تفكر في المتجهات الموجودة في مساحة التضمين هذه على أنها لا تمثل مجرد كلمات فردية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "لسبب واحد، أنها تقوم أيضًا بتشفير معلومات حول موضع تلك الكلمة، وهو ما سنتحدث عنه لاحقًا، ولكن الأهم من ذلك، يجب أن تفكر فيها على أنها تتمتع بالقدرة على استيعاب السياق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "على سبيل المثال، قد يتم سحب وسحب المتجه الذي بدأ حياته كدمج لكلمة &quot;ملك&quot; بواسطة كتل مختلفة في هذه الشبكة، بحيث يشير في النهاية إلى اتجاه أكثر تحديدًا ودقة والذي يشفر بطريقة أو بأخرى. كان ملكًا عاش في اسكتلندا، وقد وصل إلى منصبه بعد قتل الملك السابق، ويتم وصفه باللغة الشكسبيرية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "فكر في فهمك لكلمة معينة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "يتم تحديد معنى هذه الكلمة بوضوح من خلال البيئة المحيطة، وفي بعض الأحيان يتضمن ذلك السياق من مسافة بعيدة، لذلك عند تجميع نموذج لديه القدرة على التنبؤ بالكلمة التي تأتي بعد ذلك، فإن الهدف هو تمكينه بطريقة ما من دمج السياق بكفاءة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "لكي نكون واضحين، في تلك الخطوة الأولى، عندما تقوم بإنشاء مجموعة من المتجهات بناءً على نص الإدخال، يتم انتزاع كل واحد منها ببساطة من مصفوفة التضمين، لذلك في البداية يمكن لكل واحد فقط تشفير معنى كلمة واحدة بدون أي مدخلات من محيطه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "لكن يجب أن تفكر في الهدف الأساسي لهذه الشبكة التي تتدفق من خلالها على أنه تمكين كل واحد من تلك المتجهات من استيعاب معنى أكثر ثراءً وتحديدًا مما يمكن أن تمثله مجرد كلمات فردية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "يمكن للشبكة معالجة عدد ثابت فقط من المتجهات في المرة الواحدة، وهو ما يُعرف بحجم السياق الخاص بها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "بالنسبة لـ GPT-3، تم تدريبه بحجم سياق يبلغ 2048، وبالتالي فإن البيانات المتدفقة عبر الشبكة تبدو دائمًا مثل هذه المجموعة المكونة من 2048 عمودًا، يحتوي كل منها على 12000 بُعدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "يحد حجم السياق هذا من مقدار النص الذي يمكن للمحول دمجه عند التنبؤ بالكلمة التالية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "وهذا هو السبب في أن المحادثات الطويلة مع بعض برامج الدردشة الآلية، مثل الإصدارات الأولى من ChatGPT، غالبًا ما أعطت شعورًا بأن الروبوت يفقد خيط المحادثة مع استمرارك لفترة طويلة جدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "سنتناول تفاصيل الاهتمام في الوقت المناسب، ولكن بالتخطي للأمام، أريد أن أتحدث لمدة دقيقة عما يحدث في النهاية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "تذكر أن الناتج المطلوب هو توزيع احتمالي على جميع الرموز المميزة التي قد تأتي بعد ذلك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "على سبيل المثال، إذا كانت الكلمة الأخيرة هي &quot;بروفيسور&quot;، وكان السياق يتضمن كلمات مثل &quot;هاري بوتر&quot;، وقبل ذلك مباشرة نرى المعلم الأقل تفضيلًا، وأيضًا إذا أعطيتني بعض الحرية من خلال السماح لي بالتظاهر بأن الرموز تبدو ببساطة وكأنها كلمات كاملة، إذن من المفترض أن تقوم الشبكة المدربة جيدًا والتي اكتسبت المعرفة بهاري بوتر بتخصيص رقم كبير لكلمة Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "وهذا ينطوي على خطوتين مختلفتين.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "الأول هو استخدام مصفوفة أخرى تقوم بتعيين المتجه الأخير في هذا السياق إلى قائمة مكونة من 50000 قيمة، واحدة لكل رمز مميز في المفردات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "ثم هناك دالة تعمل على تطبيع هذا إلى توزيع احتمالي، تسمى Softmax وسنتحدث عنها أكثر خلال ثانية واحدة فقط، ولكن قبل ذلك قد يبدو غريبًا بعض الشيء استخدام هذا التضمين الأخير فقط للتنبؤ، عندما بعد كل شيء، في تلك الخطوة الأخيرة، هناك الآلاف من المتجهات الأخرى في الطبقة الموجودة هناك مع معانيها الغنية بالسياق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "يتعلق هذا بحقيقة أنه في عملية التدريب يتبين أن الأمر أكثر كفاءة إذا استخدمت كل واحد من تلك المتجهات في الطبقة النهائية للتنبؤ في نفس الوقت بما سيأتي بعده مباشرة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "هناك الكثير مما يمكن قوله عن التدريب لاحقًا، لكني أريد فقط أن أذكر ذلك الآن.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "تسمى هذه المصفوفة بمصفوفة Unembedding ونعطيها التسمية WU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "مرة أخرى، مثل جميع مصفوفات الوزن التي نراها، تبدأ إدخالاتها بشكل عشوائي، ولكن يتم تعلمها أثناء عملية التدريب.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "للحفاظ على النتيجة في إجمالي عدد المعلمات لدينا، تحتوي مصفوفة إلغاء التضمين هذه على صف واحد لكل كلمة في المفردات، وكل صف يحتوي على نفس عدد العناصر مثل بُعد التضمين.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "إنها تشبه إلى حد كبير مصفوفة التضمين، فقط مع تبديل الترتيب، لذا فهي تضيف 617 مليون معلمة أخرى إلى الشبكة، مما يعني أن عددنا حتى الآن يزيد قليلاً عن مليار، وهو جزء صغير ولكنه ليس ضئيلًا تمامًا من الـ 175 مليارًا التي لدينا. سوف ينتهي في المجموع.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "كدرس صغير أخير في هذا الفصل، أريد أن أتحدث أكثر عن وظيفة softmax هذه، لأنها تظهر لنا مرة أخرى عندما نغوص في كتل الانتباه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "الفكرة هي أنه إذا كنت تريد أن تعمل سلسلة من الأرقام كتوزيع احتمالي، مثل التوزيع على جميع الكلمات التالية المحتملة، فيجب أن تكون كل قيمة بين 0 و1، وتحتاج أيضًا إلى جمعها جميعًا حتى 1 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "ومع ذلك، إذا كنت تلعب لعبة تعليمية حيث يبدو كل ما تفعله مثل الضرب بمصفوفة ومتجه، فإن المخرجات التي تحصل عليها افتراضيًا لا تلتزم بهذا على الإطلاق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "غالبًا ما تكون القيم سالبة، أو أكبر بكثير من 1، ومن المؤكد تقريبًا ألا يكون مجموعها 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax هي الطريقة القياسية لتحويل قائمة عشوائية من الأرقام إلى توزيع صالح بطريقة تجعل القيم الأكبر تنتهي الأقرب إلى 1، والقيم الأصغر تنتهي قريبة جدًا من 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "هذا كل ما تحتاج إلى معرفته حقًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "لكن إذا كنت فضوليًا، فإن الطريقة التي يتم بها الأمر هي أولاً رفع e إلى قوة كل رقم، وهو ما يعني أن لديك الآن قائمة من القيم الموجبة، وبعد ذلك يمكنك جمع كل تلك القيم الموجبة وتقسيمها كل مصطلح بهذا المبلغ، مما يؤدي إلى تطبيعه في قائمة تضيف ما يصل إلى 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "ستلاحظ أنه إذا كان أحد الأرقام في المدخلات أكبر بكثير من الباقي، ففي المخرجات، يهيمن المصطلح المقابل على التوزيع، لذلك إذا كنت تأخذ عينات منه فمن المؤكد تقريبًا أنك تختار المدخلات القصوى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "ولكنه أكثر ليونة من مجرد اختيار الحد الأقصى، بمعنى أنه عندما تكون القيم الأخرى كبيرة بشكل مماثل، فإنها تحصل أيضًا على وزن ذي معنى في التوزيع، وكل شيء يتغير بشكل مستمر حيث تقوم باستمرار بتغيير المدخلات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "في بعض المواقف، مثل عندما يستخدم ChatGPT هذا التوزيع لإنشاء كلمة تالية، هناك مساحة لقليل من المرح الإضافي عن طريق إضافة القليل من الإثارة الإضافية إلى هذه الوظيفة، مع إضافة ثابت t إلى مقام تلك الأسس.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "نحن نسميها درجة الحرارة، لأنها تشبه بشكل غامض دور درجة الحرارة في بعض معادلات الديناميكا الحرارية، والتأثير هو أنه عندما تكون t أكبر، فإنك تعطي وزنًا أكبر للقيم الأقل، مما يعني أن التوزيع يكون أكثر تجانسًا قليلاً، وإذا إذا كان t أصغر، فإن القيم الأكبر سوف تهيمن بقوة أكبر، حيث في الحالة القصوى، تعيين t يساوي الصفر يعني أن كل الوزن يذهب إلى القيمة القصوى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "على سبيل المثال، سأطلب من GPT-3 إنشاء قصة بالنص الأساسي، ذات مرة كان هناك A، لكنني سأستخدم درجات حرارة مختلفة في كل حالة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "درجة الحرارة صفر تعني أنها تتوافق دائمًا مع الكلمة الأكثر توقعًا، وما تحصل عليه في نهاية المطاف هو مشتق مبتذل من المعتدل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "تمنحك درجة الحرارة المرتفعة فرصة لاختيار كلمات أقل احتمالية، ولكنها تنطوي على مخاطرة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "في هذه الحالة، تبدأ القصة بشكل أكثر أصالة، حول فنان ويب شاب من كوريا الجنوبية، لكنها سرعان ما تتحول إلى هراء.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "من الناحية الفنية، لا تسمح لك واجهة برمجة التطبيقات (API) باختيار درجة حرارة أكبر من 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "لا يوجد سبب رياضي لذلك، إنه مجرد قيد تعسفي مفروض لمنع أدواتهم من الظهور وهي تولد أشياء لا معنى لها.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "لذا، إذا كنت فضوليًا، فإن الطريقة التي تعمل بها هذه الرسوم المتحركة في الواقع هي أنني آخذ الـ 20 رمزًا التاليًا الأكثر احتمالية التي ينشئها GPT-3، والذي يبدو أنه الحد الأقصى الذي سيعطونه لي، ثم أقوم بتعديل الاحتمالات بناءً على على الأس 15.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "كمصطلح آخر، بنفس الطريقة التي يمكنك من خلالها تسمية مكونات مخرجات هذه الدالة بالاحتمالات، غالبًا ما يشير الأشخاص إلى المدخلات على أنها سجلات، أو يقول بعض الأشخاص سجلات، ويقول بعض الأشخاص سجلات، سأقول سجلات .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "على سبيل المثال، عندما تقوم بتغذية بعض النصوص، فإن كل هذه الكلمات المضمنة تتدفق عبر الشبكة، وتقوم بإجراء هذا الضرب النهائي باستخدام مصفوفة إلغاء التضمين، وسيشير الأشخاص الذين يتعلمون الآلة إلى المكونات الموجودة في هذا الناتج الأولي غير الطبيعي باسم اللوجيستات للتنبؤ بالكلمة التالية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "كان الكثير من الهدف في هذا الفصل هو وضع الأسس لفهم آلية الانتباه، أسلوب طفل الكاراتيه الشمع على الشمع.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "كما ترى، إذا كان لديك حدس قوي لتضمين الكلمات، ولسوفت ماكس، لكيفية قياس المنتجات النقطية للتشابه، وكذلك الفرضية الأساسية التي مفادها أن معظم الحسابات يجب أن تبدو مثل ضرب المصفوفات بمصفوفات مليئة بالمعلمات القابلة للضبط، ثم فهم الاهتمام يجب أن تكون هذه الآلية، وهي حجر الزاوية في الطفرة الحديثة في الذكاء الاصطناعي، سلسة نسبيًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "لذلك، تعال وانضم إلي في الفصل التالي.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "بينما أنشر هذا، تتوفر مسودة الفصل التالي للمراجعة من قبل مؤيدي Patreon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "من المفترض أن يتم نشر النسخة النهائية للعامة خلال أسبوع أو أسبوعين، ويعتمد ذلك عادةً على مقدار التغيير الذي سأقوم به في نهاية المطاف بناءً على تلك المراجعة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "في هذه الأثناء، إذا كنت تريد التعمق في الاهتمام، وإذا كنت تريد مساعدة القناة قليلاً، فهي تنتظرك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
