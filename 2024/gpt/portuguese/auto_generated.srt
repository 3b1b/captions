1
00:00:00,000 --> 00:00:04,560
As iniciais GPT significam Transformador Generativo Pré-treinado.

2
00:00:05,220 --> 00:00:09,020
A primeira palavra é bastante direta, são bots que geram novo texto.

3
00:00:09,800 --> 00:00:13,480
Pré-treinado refere-se a como o modelo passou por um processo de aprendizagem 

4
00:00:13,480 --> 00:00:16,736
a partir de uma enorme quantidade de dados, e o prefixo sugere que é 

5
00:00:16,736 --> 00:00:20,040
possível ajustá-lo para tarefas específicas com treinamento adicional.

6
00:00:20,720 --> 00:00:22,900
Mas a última palavra é a verdadeira peça chave.

7
00:00:23,380 --> 00:00:26,105
Um transformador é um tipo específico de rede neural, 

8
00:00:26,105 --> 00:00:29,788
um modelo de aprendizado de máquina, e é a principal invenção na base do 

9
00:00:29,788 --> 00:00:31,000
atual crescimento de IA.

10
00:00:31,740 --> 00:00:35,210
O que quero fazer com este vídeo e com os capítulos seguintes é apresentar uma 

11
00:00:35,210 --> 00:00:39,120
explicação baseada em gráficos sobre o que realmente acontece dentro de um transformador.

12
00:00:39,700 --> 00:00:42,820
Vamos seguir os dados que fluem por ele e seguir passo a passo.

13
00:00:43,440 --> 00:00:47,380
Existem muitos tipos diferentes de modelos que você pode construir usando transformadores.

14
00:00:47,800 --> 00:00:50,800
Alguns modelos captam áudio e produzem uma transcrição.

15
00:00:51,340 --> 00:00:53,731
Essa frase vem de um modelo que faz o contrário, 

16
00:00:53,731 --> 00:00:56,220
produzindo fala sintética apenas a partir do texto.

17
00:00:56,660 --> 00:01:01,014
Todas aquelas ferramentas que conquistaram o mundo em 2022, como DALL-E e Midjourney, 

18
00:01:01,014 --> 00:01:05,519
que captam uma descrição de texto e produzem uma imagem, são baseadas em transformadores.

19
00:01:06,000 --> 00:01:09,830
Mesmo que eu não consiga explicar pro modelo o que uma criatura "pi" deveria ser, 

20
00:01:09,830 --> 00:01:13,100
ainda estou surpreso que esse tipo de coisa seja remotamente possível.

21
00:01:13,900 --> 00:01:17,944
E o transformador original introduzido em 2017 pelo Google foi inventado 

22
00:01:17,944 --> 00:01:22,100
para o caso de uso específico de tradução de texto de um idioma para outro.

23
00:01:22,660 --> 00:01:26,588
Mas a variante na qual vamos nos concentrar, que é o tipo que dá base 

24
00:01:26,588 --> 00:01:31,413
a ferramentas como o ChatGPT, será um modelo treinado para captar um pedaço de texto, 

25
00:01:31,413 --> 00:01:34,893
talvez até acompanhado de algumas imagens ou sons associados, 

26
00:01:34,893 --> 00:01:38,260
e produzir uma previsão para o que vem a seguir na passagem.

27
00:01:38,600 --> 00:01:41,302
Essa previsão assume a forma de uma distribuição de probabilidade 

28
00:01:41,302 --> 00:01:43,800
sobre muitos trechos diferentes de texto que podem se seguir.

29
00:01:45,040 --> 00:01:47,610
À primeira vista, você pode pensar que prever a próxima palavra 

30
00:01:47,610 --> 00:01:49,940
parece um objetivo muito diferente de gerar um novo texto.

31
00:01:50,180 --> 00:01:52,627
Mas quando você tiver um modelo de previsão como este, 

32
00:01:52,627 --> 00:01:56,277
uma coisa simples que você pode tentar para gerar um trecho de texto mais longo é 

33
00:01:56,277 --> 00:02:00,238
fornecer um trecho inicial para trabalhar, fazer com que ele pegue uma amostra aleatória 

34
00:02:00,238 --> 00:02:03,843
da distribuição que acabou de gerar e anexe essa amostra ao texto e, em seguida, 

35
00:02:03,843 --> 00:02:07,715
execute todo o processo novamente para fazer uma nova previsão com base no novo texto, 

36
00:02:07,715 --> 00:02:09,539
incluindo o que acabou de ser adicionado.

37
00:02:10,100 --> 00:02:13,000
Não sei o que você acha, mas realmente não parece que isso deveria funcionar.

38
00:02:13,420 --> 00:02:16,464
Nesta animação, por exemplo, estou executando o GPT-2 em meu laptop 

39
00:02:16,464 --> 00:02:19,509
e fazendo com que ele preveja e experimente repetidamente o próximo 

40
00:02:19,509 --> 00:02:22,420
pedaço de texto para gerar uma história baseada no texto inicial.

41
00:02:22,420 --> 00:02:26,120
A história não faz muito sentido.

42
00:02:26,500 --> 00:02:31,029
Mas se eu trocá-lo por chamadas de API para GPT-3, que é o mesmo modelo básico, 

43
00:02:31,029 --> 00:02:35,614
só que muito maior, de repente, quase magicamente, teremos uma história sensata, 

44
00:02:35,614 --> 00:02:40,257
que até parece inferir que uma criatura pi viveria em um terra da matemática e da 

45
00:02:40,257 --> 00:02:40,880
computação.

46
00:02:41,580 --> 00:02:45,061
Esse processo de predição e amostragem repetidas é essencialmente o que 

47
00:02:45,061 --> 00:02:48,591
acontece quando você interage com o ChatGPT ou qualquer um desses outros 

48
00:02:48,591 --> 00:02:51,880
grandes modelos de linguagem e os vê produzindo uma palavra por vez.

49
00:02:52,480 --> 00:02:55,904
Na verdade, um recurso que eu gostaria muito é a capacidade de 

50
00:02:55,904 --> 00:02:59,220
ver a distribuição subjacente de cada nova palavra escolhida.

51
00:03:03,820 --> 00:03:05,953
Vamos começar com uma prévia de alto nível de 

52
00:03:05,953 --> 00:03:08,180
como os dados fluem através de um transformador.

53
00:03:08,640 --> 00:03:11,828
Passaremos muito mais tempo explicando, interpretando e expandindo os 

54
00:03:11,828 --> 00:03:15,107
detalhes de cada etapa, mas em linhas gerais, quando um desses chatbots 

55
00:03:15,107 --> 00:03:18,660
gera uma determinada palavra, aqui está o que está acontecendo nos bastidores.

56
00:03:19,080 --> 00:03:22,040
Primeiro, a entrada é dividida em vários pequenos pedaços.

57
00:03:22,620 --> 00:03:25,255
Essas peças são chamadas de tokens e, no caso de texto, 

58
00:03:25,255 --> 00:03:28,831
tendem a ser palavras ou pequenos pedaços de palavras ou outras combinações 

59
00:03:28,831 --> 00:03:29,820
de caracteres comuns.

60
00:03:30,740 --> 00:03:33,961
Se imagens ou som estiverem envolvidos, os tokens poderão ser 

61
00:03:33,961 --> 00:03:37,080
pequenos pedaços dessa imagem ou pequenos pedaços desse som.

62
00:03:37,580 --> 00:03:41,735
Cada um desses tokens é então associado a um vetor, ou seja, alguma lista de números, 

63
00:03:41,735 --> 00:03:45,360
que tem como objetivo codificar de alguma forma o significado daquela peça.

64
00:03:45,880 --> 00:03:50,280
Se você pensar nesses vetores como coordenadas em algum espaço de dimensão muito elevada, 

65
00:03:50,280 --> 00:03:54,680
palavras com significados semelhantes tendem a se situar em vetores próximos nesse espaço.

66
00:03:55,280 --> 00:03:59,550
Essa sequência de vetores passa então por uma operação conhecida como bloco de atenção, 

67
00:03:59,550 --> 00:04:02,558
e isso permite que os vetores se comuniquem entre si e passem 

68
00:04:02,558 --> 00:04:04,500
informações para atualizar seus valores.

69
00:04:04,880 --> 00:04:08,500
Por exemplo, o significado da palavra "modelo" na frase "modelo de aprendizado 

70
00:04:08,500 --> 00:04:11,800
de máquina" é diferente de seu significado na frase "modelo de desfile".

71
00:04:12,260 --> 00:04:15,350
O bloco de atenção é responsável por descobrir quais palavras no 

72
00:04:15,350 --> 00:04:18,726
contexto são relevantes para atualizar os significados de quais outras 

73
00:04:18,726 --> 00:04:21,959
palavras e como exatamente esses significados devem ser atualizados.

74
00:04:22,500 --> 00:04:24,801
E, novamente, sempre que uso a palavra "significado", 

75
00:04:24,801 --> 00:04:28,040
isso está de alguma forma totalmente codificado nas entradas desses vetores.

76
00:04:29,180 --> 00:04:32,580
Depois disso, esses vetores passam por um tipo diferente de operação e, 

77
00:04:32,580 --> 00:04:35,508
dependendo da fonte que você está lendo, isso será chamado de 

78
00:04:35,508 --> 00:04:38,200
perceptron multicamadas ou talvez de camada feed-forward.

79
00:04:38,580 --> 00:04:42,660
E aqui os vetores não conversam entre si, todos fazem a mesma operação em paralelo.

80
00:04:43,060 --> 00:04:46,005
E embora esse bloco seja um pouco mais difícil de interpretar, 

81
00:04:46,005 --> 00:04:49,652
mais tarde falaremos sobre como a etapa é um pouco como fazer uma longa lista 

82
00:04:49,652 --> 00:04:53,251
de perguntas sobre cada vetor e depois atualizá-las com base nas respostas a 

83
00:04:53,251 --> 00:04:54,000
essas perguntas.

84
00:04:54,900 --> 00:05:00,016
Todas as operações em ambos os blocos parecem uma pilha gigante de multiplicações 

85
00:05:00,016 --> 00:05:05,320
de matrizes, e nossa principal tarefa será entender como ler as matrizes subjacentes.

86
00:05:06,980 --> 00:05:10,022
Estou ignorando alguns detalhes sobre certas etapas de normalização que 

87
00:05:10,022 --> 00:05:12,980
acontecem no meio, mas, afinal, esta é uma visualização de alto nível.

88
00:05:13,680 --> 00:05:16,479
Depois disso, o processo essencialmente se repete, 

89
00:05:16,479 --> 00:05:20,596
você vai e volta entre blocos de atenção e blocos perceptron multicamadas, 

90
00:05:20,596 --> 00:05:25,536
até que no final a esperança é que todo o significado essencial da passagem tenha sido de 

91
00:05:25,536 --> 00:05:28,500
alguma forma incorporado ao último vetor na sequência.

92
00:05:28,920 --> 00:05:32,086
Em seguida, realizamos uma determinada operação nesse último vetor que 

93
00:05:32,086 --> 00:05:35,387
produz uma distribuição de probabilidade sobre todos os tokens possíveis, 

94
00:05:35,387 --> 00:05:38,420
todos os pequenos pedaços de texto possíveis que podem vir a seguir.

95
00:05:38,980 --> 00:05:42,527
E como falei, uma vez que você tem uma ferramenta que prevê o que vem a seguir 

96
00:05:42,527 --> 00:05:45,940
com base em um trecho de texto, você pode alimentá-la com um pouco de texto 

97
00:05:45,940 --> 00:05:49,891
inicial e fazer com que ela jogue repetidamente esse jogo de prever o que vem a seguir, 

98
00:05:49,891 --> 00:05:53,080
amostrando a distribuição, anexando e depois repetindo indefinidamente.

99
00:05:53,640 --> 00:05:57,578
Alguns de vocês que conhecem podem se lembrar de como antes do ChatGPT entrar em cena, 

100
00:05:57,578 --> 00:06:00,067
era assim que se pareciam as primeiras demos do GPT-3: 

101
00:06:00,067 --> 00:06:03,689
você faria com que ele preenchesse automaticamente histórias e ensaios com base 

102
00:06:03,689 --> 00:06:04,640
em um trecho inicial.

103
00:06:05,580 --> 00:06:08,424
Para transformar uma ferramenta como essa em um chatbot, 

104
00:06:08,424 --> 00:06:12,716
o ponto de partida mais fácil é ter um pequeno texto que estabeleça a configuração de 

105
00:06:12,716 --> 00:06:15,661
um usuário interagindo com um prestativo assistente de IA, 

106
00:06:15,661 --> 00:06:19,903
o que você chamaria de prompt do sistema, e então você usaria o a pergunta ou prompt 

107
00:06:19,903 --> 00:06:22,548
inicial do usuário como a primeira parte do diálogo, 

108
00:06:22,548 --> 00:06:26,940
e então você começa a prever o que um assistente de IA tão prestativo diria em resposta.

109
00:06:27,720 --> 00:06:32,412
Há mais a dizer sobre uma etapa do treinamento necessária para que isso funcione bem, 

110
00:06:32,412 --> 00:06:33,940
mas por alto essa é a ideia.

111
00:06:35,720 --> 00:06:40,170
Neste capítulo, vamos expandir os detalhes do que acontece bem no início da rede, 

112
00:06:40,170 --> 00:06:44,567
no final da rede, e também quero passar muito tempo revisando alguns importantes 

113
00:06:44,567 --> 00:06:48,800
conhecimentos básicos, coisas que seriam instintivas para qualquer engenheiro 

114
00:06:48,800 --> 00:06:52,600
de aprendizado de máquina na época em que os transformadores surgiram.

115
00:06:53,060 --> 00:06:56,525
Se você se sentir confortável com esse conhecimento prévio e um pouco impaciente, 

116
00:06:56,525 --> 00:06:58,722
fique à vontade para pular para o próximo capítulo, 

117
00:06:58,722 --> 00:07:02,061
que se concentrará nos bloqueios de atenção, geralmente considerados o coração 

118
00:07:02,061 --> 00:07:02,780
do transformador.

119
00:07:03,360 --> 00:07:07,030
Depois disso, quero falar mais sobre esses blocos perceptron multicamadas, 

120
00:07:07,030 --> 00:07:11,141
como funciona o treinamento, e uma série de outros detalhes que foram ignorados até 

121
00:07:11,141 --> 00:07:11,680
esse ponto.

122
00:07:12,180 --> 00:07:15,474
Para um contexto mais amplo, esses vídeos são acréscimos a uma minissérie 

123
00:07:15,474 --> 00:07:18,813
sobre aprendizado profundo, e tudo bem se você não assistiu os anteriores, 

124
00:07:18,813 --> 00:07:21,752
acho que você pode assistir fora de ordem, mas antes de mergulhar 

125
00:07:21,752 --> 00:07:24,913
especificamente nos transformadores, achoque  vale a pena garantir que 

126
00:07:24,913 --> 00:07:28,520
estamos alinhados quanto à premissa básica e a estrutura do aprendizado profundo.

127
00:07:29,020 --> 00:07:33,065
Correndo o risco de afirmar o óbvio, esta é uma abordagem de aprendizado de máquina, 

128
00:07:33,065 --> 00:07:36,253
que descreve qualquer modelo em que você usa dados para determinar 

129
00:07:36,253 --> 00:07:38,300
de alguma forma como um modelo se comporta.

130
00:07:39,140 --> 00:07:42,642
O que quero dizer com isso é, digamos que você queira uma função que receba 

131
00:07:42,642 --> 00:07:46,098
uma imagem e produza um rótulo descrevendo-a, ou nosso exemplo de previsão 

132
00:07:46,098 --> 00:07:48,264
da próxima palavra dada uma passagem de texto, 

133
00:07:48,264 --> 00:07:51,581
ou qualquer outra tarefa que pareça exigir algum elemento de intuição e 

134
00:07:51,581 --> 00:07:52,780
reconhecimento de padrões.

135
00:07:53,200 --> 00:07:57,583
Hoje em dia, quase não damos valor a isso, mas a ideia do aprendizado de máquina é que, 

136
00:07:57,583 --> 00:08:01,966
em vez de tentar definir explicitamente um procedimento sobre como realizar essa tarefa 

137
00:08:01,966 --> 00:08:05,603
em código, que é o que as pessoas teriam feito nos primeiros dias da IA, 

138
00:08:05,603 --> 00:08:09,139
você configure uma estrutura muito flexível com parâmetros ajustáveis, 

139
00:08:09,139 --> 00:08:12,377
como um monte de botões e indicadores, e então, de alguma forma, 

140
00:08:12,377 --> 00:08:16,711
use muitos exemplos de como a saída deve ser para uma determinada entrada para ajustar 

141
00:08:16,711 --> 00:08:19,700
os valores desses parâmetros para imitar esse comportamento.

142
00:08:19,700 --> 00:08:23,742
Por exemplo, talvez a forma mais simples de aprendizado de máquina seja a 

143
00:08:23,742 --> 00:08:27,348
regressão linear, onde suas entradas e saídas são números únicos, 

144
00:08:27,348 --> 00:08:30,353
algo como a metragem quadrada de uma casa e seu preço, 

145
00:08:30,353 --> 00:08:34,560
e você deseja encontrar uma linha que melhor se ajuste através desses dados, 

146
00:08:34,560 --> 00:08:36,799
para prever os preços futuros de imóveis.

147
00:08:37,440 --> 00:08:40,368
Essa linha é descrita por dois parâmetros contínuos, 

148
00:08:40,368 --> 00:08:43,960
como a inclinação e a interceptação y, e o objetivo da regressão 

149
00:08:43,960 --> 00:08:48,160
linear é determinar esses parâmetros para que correspondam melhor aos dados.

150
00:08:48,880 --> 00:08:52,100
Escusado será dizer que os modelos de aprendizagem profunda ficam muito mais complicados.

151
00:08:52,620 --> 00:08:57,660
O GPT-3, por exemplo, não possui dois, mas 175 bilhões de parâmetros.

152
00:08:58,120 --> 00:09:02,025
Mas o problema é o seguinte: não é certo que você pode criar um modelo 

153
00:09:02,025 --> 00:09:05,875
gigante com um grande número de parâmetros sem que ele superajuste os 

154
00:09:05,875 --> 00:09:09,560
dados de treinamento ou seja completamente intratável para treinar.

155
00:09:10,260 --> 00:09:13,170
O aprendizado profundo descreve uma classe de modelos que, 

156
00:09:13,170 --> 00:09:16,180
nas últimas décadas, provou ser escalonável notavelmente bem.

157
00:09:16,480 --> 00:09:20,708
O que os unifica é o mesmo algoritmo de treinamento, chamado backpropagation, 

158
00:09:20,708 --> 00:09:24,395
e o contexto que quero que você tenha à medida que avançamos é que, 

159
00:09:24,395 --> 00:09:27,810
para que esse algoritmo de treinamento funcione bem em escala, 

160
00:09:27,810 --> 00:09:31,280
esses modelos precisam seguir um determinado formato específico.

161
00:09:31,800 --> 00:09:35,810
Se você conhece esse formato, isso ajuda a explicar muitas das escolhas de como um 

162
00:09:35,810 --> 00:09:39,820
transformador processa a linguagem, que de outra forma correm o risco de parecerem 

163
00:09:39,820 --> 00:09:40,400
arbitrárias.

164
00:09:41,440 --> 00:09:44,090
Primeiro, qualquer que seja o modelo que você esteja criando, 

165
00:09:44,090 --> 00:09:46,740
a entrada deverá ser formatada como um array de números reais.

166
00:09:46,740 --> 00:09:50,977
Isso pode significar uma lista de números, pode ser uma matriz bidimensional ou, 

167
00:09:50,977 --> 00:09:54,221
muitas vezes, você lida com matrizes de dimensões superiores, 

168
00:09:54,221 --> 00:09:56,000
onde o termo geral usado é tensor.

169
00:09:56,560 --> 00:10:00,523
Muitas vezes você pensa que os dados de entrada são progressivamente transformados em 

170
00:10:00,523 --> 00:10:04,532
muitas camadas distintas, onde, novamente, cada camada é sempre estruturada como algum 

171
00:10:04,532 --> 00:10:08,680
tipo de matriz de números reais, até chegar a uma camada final que você considera a saída.

172
00:10:09,280 --> 00:10:12,953
Por exemplo, a camada final em nosso modelo de processamento de texto é uma lista de 

173
00:10:12,953 --> 00:10:16,627
números que representa a distribuição de probabilidade para todos os próximos tokens 

174
00:10:16,627 --> 00:10:17,060
possíveis.

175
00:10:17,820 --> 00:10:21,549
No aprendizado profundo, esses parâmetros do modelo são quase sempre chamados de pesos, 

176
00:10:21,549 --> 00:10:24,601
e isso ocorre porque uma característica importante desses modelos é que 

177
00:10:24,601 --> 00:10:27,568
a única maneira pela qual esses parâmetros interagem com os dados que 

178
00:10:27,568 --> 00:10:29,900
estão sendo processados é por meio de somas ponderadas.

179
00:10:30,340 --> 00:10:34,360
Você também espalha algumas funções não lineares, mas elas não dependem de parâmetros.

180
00:10:35,200 --> 00:10:38,709
Porém, normalmente, em vez de ver as somas ponderadas todas nuas 

181
00:10:38,709 --> 00:10:42,164
e escritas explicitamente assim, você as encontrará empacotadas 

182
00:10:42,164 --> 00:10:45,620
juntas como vários componentes em um produto vetorial de matriz.

183
00:10:46,740 --> 00:10:50,724
Isso equivale a dizer a mesma coisa: se você pensar em como funciona a multiplicação 

184
00:10:50,724 --> 00:10:54,240
de vetores de matrizes, cada componente na saída parece uma soma ponderada.

185
00:10:54,780 --> 00:10:58,271
Muitas vezes é conceitualmente mais limpo para você e para mim 

186
00:10:58,271 --> 00:11:01,651
pensar em matrizes preenchidas com parâmetros ajustáveis que 

187
00:11:01,651 --> 00:11:05,420
transformam vetores extraídos dos dados que estão sendo processados.

188
00:11:06,340 --> 00:11:10,352
Por exemplo, esses 175 mil milhões de pesos no GPT-3 estão 

189
00:11:10,352 --> 00:11:14,160
organizados em pouco menos de 28.000 matrizes distintas.

190
00:11:14,660 --> 00:11:18,072
Essas matrizes, por sua vez, se enquadram em oito categorias diferentes, 

191
00:11:18,072 --> 00:11:22,045
e o que você e eu faremos é percorrer cada uma dessas categorias para entender o que 

192
00:11:22,045 --> 00:11:22,700
esse tipo faz.

193
00:11:23,160 --> 00:11:27,086
À medida que avançamos, acho divertido fazer referência aos números 

194
00:11:27,086 --> 00:11:31,360
específicos do GPT-3 para contar exatamente de onde vêm esses 175 bilhões.

195
00:11:31,880 --> 00:11:34,447
Mesmo que hoje existam modelos maiores e melhores, 

196
00:11:34,447 --> 00:11:38,927
este tem um certo charme como modelo de linguagem grande para realmente captar a atenção 

197
00:11:38,927 --> 00:11:40,740
do mundo fora das comunidades de ML.

198
00:11:41,440 --> 00:11:43,921
Além disso, na prática, as empresas tendem a manter a boca 

199
00:11:43,921 --> 00:11:46,740
fechada em relação aos números específicos das redes mais modernas.

200
00:11:47,360 --> 00:11:50,591
Eu só quero definir o cenário, que quando você espia os bastidores 

201
00:11:50,591 --> 00:11:53,726
para ver o que acontece dentro de uma ferramenta como o ChatGPT, 

202
00:11:53,726 --> 00:11:57,440
quase toda a computação real parece uma multiplicação de vetores de matrizes.

203
00:11:57,900 --> 00:12:01,342
Há um certo risco de se perder no mar de bilhões de números, 

204
00:12:01,342 --> 00:12:06,139
mas você deve traçar uma distinção bem nítida em sua mente entre os pesos do modelo, 

205
00:12:06,139 --> 00:12:10,146
que sempre colorirei em azul ou vermelho, e os dados sendo processado, 

206
00:12:10,146 --> 00:12:11,840
que sempre colorirei de cinza.

207
00:12:12,180 --> 00:12:14,997
Os pesos são o cérebro real, são as coisas aprendidas 

208
00:12:14,997 --> 00:12:17,920
durante o treinamento e determinam como ele se comporta.

209
00:12:18,280 --> 00:12:22,461
Os dados que estão sendo processados simplesmente codificam qualquer entrada específica 

210
00:12:22,461 --> 00:12:26,500
inserida no modelo para uma determinada execução, como um exemplo de trecho de texto.

211
00:12:27,480 --> 00:12:30,523
Com tudo isso como base, vamos nos aprofundar na primeira etapa 

212
00:12:30,523 --> 00:12:33,614
deste exemplo de processamento de texto, que é dividir a entrada 

213
00:12:33,614 --> 00:12:36,420
em pequenos pedaços e transformar esses pedaços em vetores.

214
00:12:37,020 --> 00:12:39,432
Mencionei como esses pedaços são chamados de tokens, 

215
00:12:39,432 --> 00:12:41,616
que podem ser pedaços de palavras ou pontuação, 

216
00:12:41,616 --> 00:12:44,529
mas de vez em quando neste capítulo e especialmente no próximo, 

217
00:12:44,529 --> 00:12:48,080
gostaria apenas de fingir que estão divididos de forma mais clara em palavras.

218
00:12:48,600 --> 00:12:51,298
Como nós, humanos, pensamos em palavras, isso tornará muito mais 

219
00:12:51,298 --> 00:12:54,080
fácil fazer referência a pequenos exemplos e esclarecer cada etapa.

220
00:12:55,260 --> 00:12:59,753
O modelo possui um vocabulário predefinido, uma lista de todas as palavras possíveis, 

221
00:12:59,753 --> 00:13:02,940
digamos 50.000 delas, e a primeira matriz que encontraremos, 

222
00:13:02,940 --> 00:13:06,964
conhecida como matriz de incorporação, possui uma única coluna para cada uma 

223
00:13:06,964 --> 00:13:07,800
dessas palavras.

224
00:13:08,940 --> 00:13:11,324
Essas colunas são o que determina em que vetor 

225
00:13:11,324 --> 00:13:13,760
cada palavra se transforma nessa primeira etapa.

226
00:13:15,100 --> 00:13:18,434
Nós a rotulamos como Nós, e como todas as matrizes que vemos, 

227
00:13:18,434 --> 00:13:22,360
seus valores começam aleatórios, mas serão aprendidos com base nos dados.

228
00:13:23,620 --> 00:13:26,633
Transformar palavras em vetores era uma prática comum no aprendizado 

229
00:13:26,633 --> 00:13:29,602
de máquina muito antes dos transformadores, mas é um pouco estranho 

230
00:13:29,602 --> 00:13:32,877
se você nunca viu isso antes e estabelece a base para tudo o que se segue, 

231
00:13:32,877 --> 00:13:35,760
então vamos reservar um momento para nos familiarizarmos com isso.

232
00:13:36,040 --> 00:13:38,492
Muitas vezes chamamos isso de incorporação de palavra, 

233
00:13:38,492 --> 00:13:41,613
o que convida você a pensar nesses vetores de forma muito geométrica, 

234
00:13:41,613 --> 00:13:43,620
como pontos em algum espaço de alta dimensão.

235
00:13:44,180 --> 00:13:47,912
Visualizar uma lista de três números como coordenadas para pontos no espaço 3D não 

236
00:13:47,912 --> 00:13:51,780
seria problema, mas os embeddings de palavras tendem a ter dimensões muito superiores.

237
00:13:52,280 --> 00:13:55,777
No GPT-3 eles têm 12.288 dimensões e, como você verá, 

238
00:13:55,777 --> 00:14:00,440
é importante trabalhar em um espaço que tenha muitas direções distintas.

239
00:14:01,180 --> 00:14:05,040
Da mesma forma que você poderia pegar uma fatia bidimensional através de um 

240
00:14:05,040 --> 00:14:07,579
espaço 3D e projetar todos os pontos nessa fatia, 

241
00:14:07,579 --> 00:14:11,845
para animar os embeddings de palavras que um modelo simples está me proporcionando, 

242
00:14:11,845 --> 00:14:15,604
farei uma coisa análoga escolhendo uma fatia tridimensional através deste 

243
00:14:15,604 --> 00:14:19,311
espaço de dimensão muito alta e projetando a palavra vetores sobre ela e 

244
00:14:19,311 --> 00:14:20,480
exibindo os resultados.

245
00:14:21,280 --> 00:14:24,646
A grande ideia aqui é que, à medida que um modelo ajusta e ajusta seus pesos 

246
00:14:24,646 --> 00:14:27,881
para determinar exatamente como as palavras são incorporadas como vetores 

247
00:14:27,881 --> 00:14:30,854
durante o treinamento, ele tende a se estabelecer em um conjunto de 

248
00:14:30,854 --> 00:14:34,440
incorporações onde as direções no espaço têm uma espécie de significado semântico.

249
00:14:34,980 --> 00:14:38,350
Para o modelo simples de palavra para vetor que estou executando aqui, 

250
00:14:38,350 --> 00:14:42,244
se eu pesquisar todas as palavras cujas incorporações são mais próximas de torre, 

251
00:14:42,244 --> 00:14:45,900
você notará como todas elas parecem dar vibrações de torre muito semelhantes.

252
00:14:46,340 --> 00:14:48,723
E se você quiser pegar um pouco de Python e brincar em casa, 

253
00:14:48,723 --> 00:14:51,380
este é o modelo específico que estou usando para fazer as animações.

254
00:14:51,620 --> 00:14:54,658
Não é um transformador, mas é suficiente para ilustrar a ideia 

255
00:14:54,658 --> 00:14:57,600
de que as direções no espaço podem ter significado semântico.

256
00:14:58,300 --> 00:15:03,286
Um exemplo muito clássico disso é como se você pegar a diferença entre os vetores 

257
00:15:03,286 --> 00:15:08,213
para mulher e homem, algo que você visualizaria como um pequeno vetor conectando 

258
00:15:08,213 --> 00:15:13,200
a ponta de um à ponta do outro, é muito semelhante à diferença entre rei e rainha.

259
00:15:15,080 --> 00:15:18,928
Então, digamos que você não conhecesse a palavra para uma monarca feminina, 

260
00:15:18,928 --> 00:15:22,624
você poderia encontrá-la tomando rei, adicionando a direção mulher-homem 

261
00:15:22,624 --> 00:15:25,460
e procurando as incorporações mais próximas desse ponto.

262
00:15:27,000 --> 00:15:28,200
Pelo menos, mais ou menos.

263
00:15:28,480 --> 00:15:31,891
Apesar de este ser um exemplo clássico para o modelo com o qual estou brincando, 

264
00:15:31,891 --> 00:15:35,051
a verdadeira incorporação da rainha está na verdade um pouco mais distante 

265
00:15:35,051 --> 00:15:38,041
do que isso sugere, provavelmente porque a forma como a rainha é usada 

266
00:15:38,041 --> 00:15:40,780
nos dados de treinamento não é apenas uma versão feminina do rei.

267
00:15:41,620 --> 00:15:45,260
Quando brinquei, as relações familiares pareciam ilustrar muito melhor a ideia.

268
00:15:46,340 --> 00:15:50,836
A questão é que durante o treinamento o modelo achou vantajoso escolher embeddings 

269
00:15:50,836 --> 00:15:54,900
de forma que uma direção neste espaço codificasse as informações de gênero.

270
00:15:56,800 --> 00:16:00,081
Outro exemplo é que se pegarmos na incorporação da Itália, 

271
00:16:00,081 --> 00:16:04,975
e subtrairmos a incorporação da Alemanha, e adicionarmos isso à incorporação de Hitler, 

272
00:16:04,975 --> 00:16:08,090
obtemos algo muito próximo da incorporação de Mussolini.

273
00:16:08,570 --> 00:16:12,042
É como se o modelo tivesse aprendido a associar algumas direções à 

274
00:16:12,042 --> 00:16:15,670
italianidade e outras aos líderes dos eixos da Segunda Guerra Mundial.

275
00:16:16,470 --> 00:16:19,251
Talvez meu exemplo favorito nesse sentido seja como, 

276
00:16:19,251 --> 00:16:22,504
em alguns modelos, se você pegar a diferença entre a Alemanha 

277
00:16:22,504 --> 00:16:26,230
e o Japão e adicioná-la ao sushi, você acaba muito próximo da salsicha.

278
00:16:27,350 --> 00:16:30,229
Também ao jogar este jogo de encontrar os vizinhos mais próximos, 

279
00:16:30,229 --> 00:16:33,850
fiquei satisfeito ao ver o quão próxima Kat estava tanto da fera quanto do monstro.

280
00:16:34,690 --> 00:16:37,236
Um pouco de intuição matemática que é útil ter em mente, 

281
00:16:37,236 --> 00:16:40,230
especialmente para o próximo capítulo, é como o produto escalar de 

282
00:16:40,230 --> 00:16:43,850
dois vetores pode ser pensado como uma forma de medir o quão bem eles se alinham.

283
00:16:44,870 --> 00:16:47,968
Computacionalmente, os produtos escalares envolvem a multiplicação de todos 

284
00:16:47,968 --> 00:16:51,149
os componentes correspondentes e depois a adição dos resultados, o que é bom, 

285
00:16:51,149 --> 00:16:54,330
uma vez que grande parte da nossa computação tem que parecer somas ponderadas.

286
00:16:55,190 --> 00:16:58,413
Geometricamente, o produto escalar é positivo quando os 

287
00:16:58,413 --> 00:17:01,695
vetores apontam em direções semelhantes, é zero se forem 

288
00:17:01,695 --> 00:17:05,609
perpendiculares e é negativo sempre que apontam em direções opostas.

289
00:17:06,550 --> 00:17:09,946
Por exemplo, digamos que você estava brincando com este modelo 

290
00:17:09,946 --> 00:17:13,343
e levanta a hipótese de que a incorporação de gatos menos gato 

291
00:17:13,343 --> 00:17:17,010
pode representar uma espécie de direção de pluralidade neste espaço.

292
00:17:17,430 --> 00:17:20,562
Para testar isso, vou pegar esse vetor e calcular seu produto escalar 

293
00:17:20,562 --> 00:17:23,917
em relação às incorporações de certos substantivos singulares e compará-lo 

294
00:17:23,917 --> 00:17:27,050
com os produtos escalares com os substantivos plurais correspondentes.

295
00:17:27,270 --> 00:17:31,523
Se você brincar com isso, notará que os plurais realmente parecem dar consistentemente 

296
00:17:31,523 --> 00:17:35,678
valores mais altos do que os singulares, indicando que eles se alinham mais com essa 

297
00:17:35,678 --> 00:17:36,070
direção.

298
00:17:37,070 --> 00:17:40,904
Também é divertido como se você pegar esse produto escalar com as incorporações das 

299
00:17:40,904 --> 00:17:44,145
palavras 1, 2, 3 e assim por diante, eles fornecem valores crescentes, 

300
00:17:44,145 --> 00:17:48,117
então é como se pudéssemos medir quantitativamente o quão plural o modelo encontra uma 

301
00:17:48,117 --> 00:17:49,030
determinada palavra.

302
00:17:50,250 --> 00:17:51,927
Novamente, os detalhes de como as palavras são 

303
00:17:51,927 --> 00:17:53,570
incorporadas são aprendidos por meio de dados.

304
00:17:54,050 --> 00:17:57,688
Esta matriz de incorporação, cujas colunas nos dizem o que acontece com cada palavra, 

305
00:17:57,688 --> 00:17:59,550
é a primeira pilha de pesos do nosso modelo.

306
00:18:00,030 --> 00:18:05,163
Usando os números GPT-3, o tamanho do vocabulário é especificamente 50.257 e, 

307
00:18:05,163 --> 00:18:09,770
novamente, tecnicamente não consiste em palavras em si, mas em tokens.

308
00:18:10,630 --> 00:18:14,116
A dimensão de incorporação é 12.288, e multiplicar isso 

309
00:18:14,116 --> 00:18:17,790
nos diz que isso consiste em cerca de 617 milhões de pesos.

310
00:18:18,250 --> 00:18:21,030
Vamos em frente e adicionar isto a uma contagem contínua, 

311
00:18:21,030 --> 00:18:23,810
lembrando que no final devemos contar até 175 mil milhões.

312
00:18:25,430 --> 00:18:28,780
No caso dos transformadores, você realmente quer pensar nos vetores neste 

313
00:18:28,780 --> 00:18:32,130
espaço de incorporação como não apenas representando palavras individuais.

314
00:18:32,550 --> 00:18:36,388
Por um lado, eles também codificam informações sobre a posição daquela palavra, 

315
00:18:36,388 --> 00:18:39,891
sobre a qual falaremos mais tarde, mas o mais importante é que você deve 

316
00:18:39,891 --> 00:18:42,770
pensar neles como tendo a capacidade de absorver o contexto.

317
00:18:43,350 --> 00:18:47,479
Um vetor que começou sua vida como a incorporação da palavra rei, por exemplo, 

318
00:18:47,479 --> 00:18:51,922
pode ser progressivamente puxado e puxado por vários blocos nesta rede, de modo que, 

319
00:18:51,922 --> 00:18:56,157
no final, aponte para uma direção muito mais específica e matizada que de alguma 

320
00:18:56,157 --> 00:19:00,495
forma codifica isso. foi um rei que viveu na Escócia e que alcançou seu posto após 

321
00:19:00,495 --> 00:19:04,730
assassinar o rei anterior, e que está sendo descrito na linguagem shakespeariana.

322
00:19:05,210 --> 00:19:07,790
Pense em sua própria compreensão de uma determinada palavra.

323
00:19:08,250 --> 00:19:11,614
O significado dessa palavra é claramente informado pelo ambiente, 

324
00:19:11,614 --> 00:19:15,080
e às vezes isso inclui o contexto de uma longa distância, portanto, 

325
00:19:15,080 --> 00:19:19,158
ao montar um modelo que tenha a capacidade de prever qual palavra vem a seguir, 

326
00:19:19,158 --> 00:19:23,390
o objetivo é de alguma forma capacitá-lo para incorporar o contexto eficientemente.

327
00:19:24,050 --> 00:19:27,097
Para ficar claro, nessa primeira etapa, quando você cria a matriz de 

328
00:19:27,097 --> 00:19:30,454
vetores com base no texto de entrada, cada um deles é simplesmente retirado 

329
00:19:30,454 --> 00:19:33,634
da matriz de incorporação, então inicialmente cada um só pode codificar 

330
00:19:33,634 --> 00:19:36,770
o significado de uma única palavra sem qualquer entrada de seu entorno.

331
00:19:37,710 --> 00:19:41,479
Mas você deve pensar no objetivo principal desta rede pela qual ela flui como 

332
00:19:41,479 --> 00:19:45,200
sendo permitir que cada um desses vetores absorva um significado que é muito 

333
00:19:45,200 --> 00:19:48,970
mais rico e específico do que meras palavras individuais poderiam representar.

334
00:19:49,510 --> 00:19:52,453
A rede só pode processar um número fixo de vetores por vez, 

335
00:19:52,453 --> 00:19:54,170
conhecido como tamanho de contexto.

336
00:19:54,510 --> 00:19:58,176
Para GPT-3, ele foi treinado com um tamanho de contexto de 2.048, 

337
00:19:58,176 --> 00:20:01,676
de modo que os dados que fluem pela rede sempre se parecem com 

338
00:20:01,676 --> 00:20:05,010
esta matriz de 2.048 colunas, cada uma com 12.000 dimensões.

339
00:20:05,590 --> 00:20:08,448
Esse tamanho de contexto limita a quantidade de texto que o 

340
00:20:08,448 --> 00:20:11,830
transformador pode incorporar ao fazer uma previsão da próxima palavra.

341
00:20:12,370 --> 00:20:14,778
É por isso que longas conversas com certos chatbots, 

342
00:20:14,778 --> 00:20:18,005
como as primeiras versões do ChatGPT, muitas vezes davam a sensação de 

343
00:20:18,005 --> 00:20:22,050
que o bot estava perdendo o fio da conversa à medida que você continuava por muito tempo.

344
00:20:23,030 --> 00:20:25,686
Entraremos nos detalhes da atenção no devido tempo, mas, 

345
00:20:25,686 --> 00:20:28,810
avançando, quero falar por um minuto sobre o que acontece no final.

346
00:20:29,450 --> 00:20:31,936
Lembre-se, a saída desejada é uma distribuição de 

347
00:20:31,936 --> 00:20:34,870
probabilidade sobre todos os tokens que podem vir a seguir.

348
00:20:35,170 --> 00:20:37,553
Por exemplo, se a última palavra for Professor, 

349
00:20:37,553 --> 00:20:39,987
e o contexto incluir palavras como Harry Potter, 

350
00:20:39,987 --> 00:20:42,818
e imediatamente antes virmos o professor menos favorito, 

351
00:20:42,818 --> 00:20:45,301
e também se você me der alguma margem de manobra, 

352
00:20:45,301 --> 00:20:48,976
deixando-me fingir que os tokens simplesmente parecem palavras completas, 

353
00:20:48,976 --> 00:20:53,048
então uma rede bem treinada que tivesse acumulado conhecimento sobre Harry Potter 

354
00:20:53,048 --> 00:20:55,830
provavelmente atribuiria um número alto à palavra Snape.

355
00:20:56,510 --> 00:20:57,970
Isso envolve duas etapas diferentes.

356
00:20:58,310 --> 00:21:03,225
A primeira é usar outra matriz que mapeie o último vetor naquele contexto 

357
00:21:03,225 --> 00:21:07,610
em uma lista de 50.000 valores, um para cada token do vocabulário.

358
00:21:08,170 --> 00:21:12,309
Depois, há uma função que normaliza isso em uma distribuição de probabilidade, 

359
00:21:12,309 --> 00:21:15,767
chamada Softmax e falaremos mais sobre isso em apenas um segundo, 

360
00:21:15,767 --> 00:21:19,487
mas antes disso pode parecer um pouco estranho usar apenas esta última 

361
00:21:19,487 --> 00:21:23,312
incorporação para fazer uma previsão, quando afinal, nessa última etapa, 

362
00:21:23,312 --> 00:21:27,346
existem milhares de outros vetores na camada, com seus próprios significados 

363
00:21:27,346 --> 00:21:28,290
ricos em contexto.

364
00:21:28,930 --> 00:21:32,710
Isso tem a ver com o fato de que no processo de treinamento acaba sendo 

365
00:21:32,710 --> 00:21:36,385
muito mais eficiente usar cada um desses vetores na camada final para 

366
00:21:36,385 --> 00:21:40,270
fazer simultaneamente uma previsão do que viria imediatamente depois dele.

367
00:21:40,970 --> 00:21:43,495
Há muito mais a ser dito sobre o treinamento mais tarde, 

368
00:21:43,495 --> 00:21:45,090
mas eu só quero destacar isso agora.

369
00:21:45,730 --> 00:21:49,690
Esta matriz é chamada de matriz de desincorporação e damos a ela o rótulo WU.

370
00:21:50,210 --> 00:21:52,307
Novamente, como todas as matrizes de peso que vemos, 

371
00:21:52,307 --> 00:21:55,316
suas entradas começam aleatoriamente, mas são aprendidas durante o processo 

372
00:21:55,316 --> 00:21:55,910
de treinamento.

373
00:21:56,470 --> 00:21:59,043
Mantendo a pontuação em nossa contagem total de parâmetros, 

374
00:21:59,043 --> 00:22:01,875
esta matriz de desincorporação tem uma linha para cada palavra no 

375
00:22:01,875 --> 00:22:05,650
vocabulário e cada linha tem o mesmo número de elementos que a dimensão de incorporação.

376
00:22:06,410 --> 00:22:09,985
É muito semelhante à matriz de incorporação, apenas com a ordem trocada, 

377
00:22:09,985 --> 00:22:12,875
por isso adiciona outros 617 milhões de parâmetros à rede, 

378
00:22:12,875 --> 00:22:16,647
o que significa que a nossa contagem até agora é de pouco mais de um bilhão, 

379
00:22:16,647 --> 00:22:20,663
uma fração pequena, mas não totalmente insignificante, dos 175 bilhões que temos. 

380
00:22:20,663 --> 00:22:21,790
vou acabar com o total.

381
00:22:22,550 --> 00:22:26,629
Como última minilição deste capítulo, quero falar mais sobre essa função softmax, 

382
00:22:26,629 --> 00:22:30,610
pois ela aparece novamente para nós quando mergulhamos nos bloqueios de atenção.

383
00:22:31,430 --> 00:22:35,735
A ideia é que se você deseja que uma sequência de números atue como uma distribuição de 

384
00:22:35,735 --> 00:22:39,893
probabilidade, digamos, uma distribuição sobre todas as próximas palavras possíveis, 

385
00:22:39,893 --> 00:22:44,198
então cada valor deve estar entre 0 e 1, e você também precisa que a soma de todos eles 

386
00:22:44,198 --> 00:22:44,590
seja 1 .

387
00:22:45,250 --> 00:22:48,305
No entanto, se você estiver jogando o jogo de aprendizagem em 

388
00:22:48,305 --> 00:22:52,198
que tudo o que você faz se parece com uma multiplicação de matrizes e vetores, 

389
00:22:52,198 --> 00:22:54,810
os resultados obtidos por padrão não obedecem a isso.

390
00:22:55,330 --> 00:22:59,870
Os valores costumam ser negativos ou muito maiores que 1 e quase certamente não somam 1.

391
00:23:00,510 --> 00:23:03,930
Softmax é a maneira padrão de transformar uma lista arbitrária de 

392
00:23:03,930 --> 00:23:07,454
números em uma distribuição válida, de forma que os valores maiores 

393
00:23:07,454 --> 00:23:11,290
fiquem mais próximos de 1 e os valores menores fiquem muito próximos de 0.

394
00:23:11,830 --> 00:23:13,070
Isso é tudo que você realmente precisa saber.

395
00:23:13,090 --> 00:23:17,116
Mas se você estiver curioso, a forma como funciona é primeiro elevar e à 

396
00:23:17,116 --> 00:23:21,086
potência de cada um dos números, o que significa que agora você tem uma 

397
00:23:21,086 --> 00:23:25,278
lista de valores positivos, e então você soma todos esses valores positivos 

398
00:23:25,278 --> 00:23:29,470
e divide cada termo por essa soma, o que o normaliza em uma lista de soma 1.

399
00:23:30,170 --> 00:23:34,520
Você notará que se um dos números na entrada for significativamente maior que o resto, 

400
00:23:34,520 --> 00:23:38,070
então na saída o termo correspondente domina a distribuição, portanto, 

401
00:23:38,070 --> 00:23:42,470
se você estivesse amostrando, quase certamente escolheria apenas a entrada maximizadora.

402
00:23:42,990 --> 00:23:45,340
Mas é mais suave do que apenas escolher o máximo, 

403
00:23:45,340 --> 00:23:48,349
no sentido de que quando outros valores são igualmente grandes, 

404
00:23:48,349 --> 00:23:52,299
eles também recebem um peso significativo na distribuição e tudo muda continuamente 

405
00:23:52,299 --> 00:23:54,650
à medida que você varia continuamente as entradas.

406
00:23:55,130 --> 00:23:59,776
Em algumas situações, como quando ChatGPT está usando esta distribuição para criar uma 

407
00:23:59,776 --> 00:24:04,316
próxima palavra, há espaço para um pouco de diversão extra adicionando um pouco mais 

408
00:24:04,316 --> 00:24:08,910
de tempero a esta função, com uma constante t lançada no denominador desses expoentes.

409
00:24:09,550 --> 00:24:14,254
Chamamos de temperatura, pois parece vagamente com o papel da temperatura em certas 

410
00:24:14,254 --> 00:24:17,614
equações termodinâmicas, e o efeito é que quando t é maior, 

411
00:24:17,614 --> 00:24:22,150
dá-se mais peso aos valores mais baixos, o que significa que a distribuição é um 

412
00:24:22,150 --> 00:24:26,797
pouco mais uniforme, e se t for menor, então os valores maiores dominarão de forma 

413
00:24:26,797 --> 00:24:31,446
mais agressiva, onde, no extremo, definir t igual a zero significa que todo o peso 

414
00:24:31,446 --> 00:24:32,790
vai para o valor máximo.

415
00:24:33,470 --> 00:24:38,445
Por exemplo, farei com que o GPT-3 gere uma história com o texto inicial: 

416
00:24:38,445 --> 00:24:42,950
"era uma vez uma", mas usarei temperaturas diferentes em cada caso.

417
00:24:43,630 --> 00:24:48,030
Temperatura zero significa que sempre vem com a palavra mais previsível, 

418
00:24:48,030 --> 00:24:52,370
e o que você recebe acaba sendo um derivado banal de Cachinhos Dourados.

419
00:24:53,010 --> 00:24:56,971
Uma temperatura mais alta dá a chance de escolher palavras menos prováveis, 

420
00:24:56,971 --> 00:24:57,910
mas traz um risco.

421
00:24:58,230 --> 00:25:01,412
Neste caso, a história começa de forma mais original, 

422
00:25:01,412 --> 00:25:06,010
sobre um jovem web-artista da Coreia do Sul, mas rapidamente decai ao absurdo.

423
00:25:06,950 --> 00:25:10,830
Tecnicamente falando, a API não permite escolher uma temperatura maior que 2.

424
00:25:11,170 --> 00:25:15,341
Não há razão matemática para isso, é apenas uma restrição arbitrária imposta 

425
00:25:15,341 --> 00:25:19,350
para evitar que essa ferramenta seja vista gerando coisas absurdas demais.

426
00:25:19,870 --> 00:25:23,259
Então, se você está curioso, para essa animação funcionar, 

427
00:25:23,259 --> 00:25:26,822
peguei os 20 próximos tokens mais prováveis que o GPT-3 gera, 

428
00:25:26,822 --> 00:25:31,188
que parece ser o máximo que eles me darão, e então ajusto as probabilidades 

429
00:25:31,188 --> 00:25:32,970
com base em um expoente de 1/5.

430
00:25:33,130 --> 00:25:37,415
Como outro jargão, da mesma forma que você pode chamar os componentes da saída 

431
00:25:37,415 --> 00:25:41,647
desta função de probabilidades, geralmente se refere às entradas como logits, 

432
00:25:41,647 --> 00:25:46,150
ou algumas pessoas dizem logits, algumas pessoas dizem logits, eu vou dizer logits.

433
00:25:46,530 --> 00:25:48,791
Então, por exemplo, quando você insere um texto, 

434
00:25:48,791 --> 00:25:51,329
todas essas incorporações de palavras fluem pela rede, 

435
00:25:51,329 --> 00:25:54,421
e se faz essa multiplicação final com a matriz de desincorporação, 

436
00:25:54,421 --> 00:25:57,974
o pessoal de aprendizado de máquina se referiria aos componentes dessa saída 

437
00:25:57,974 --> 00:26:01,390
bruta e não normalizada como os logits para a próxima previsão de palavra.

438
00:26:03,330 --> 00:26:06,621
Grande parte do objetivo deste capítulo foi estabelecer as bases para a 

439
00:26:06,621 --> 00:26:10,370
compreensão do mecanismo de atenção, estilo bota-casaco-tira-casado de Karate Kid.

440
00:26:10,850 --> 00:26:14,881
Se você tem uma forte intuição para incorporações de palavras, para softmax, 

441
00:26:14,881 --> 00:26:17,708
para como os produtos escalares medem a similaridade, 

442
00:26:17,708 --> 00:26:21,948
e também a premissa subjacente de que a maioria dos cálculos deve se parecer com 

443
00:26:21,948 --> 00:26:25,822
a multiplicação de matrizes com matrizes cheias de parâmetros ajustáveis, 

444
00:26:25,822 --> 00:26:30,220
então entender o mecanismo de atenção, esta peça fundamental de todo o boom moderno 

445
00:26:30,220 --> 00:26:32,210
da IA, deve ser relativamente simples.

446
00:26:32,650 --> 00:26:34,510
Para isso, junte-se a mim no próximo capítulo.

447
00:26:36,390 --> 00:26:38,841
Enquanto lanço este vídeo, um rascunho do próximo capítulo 

448
00:26:38,841 --> 00:26:41,210
está disponível para revisão pelos apoiadores do Patreon.

449
00:26:41,770 --> 00:26:44,667
Uma versão final deve estar disponível ao público em uma ou duas semanas, 

450
00:26:44,667 --> 00:26:47,370
geralmente depende de quanto eu acabo mudando com base nessa revisão.

451
00:26:47,810 --> 00:26:51,427
Enquanto isso, se você quiser chamar a atenção e quiser ajudar um pouco o canal, 

452
00:26:51,427 --> 00:26:52,410
ele está aí esperando.

