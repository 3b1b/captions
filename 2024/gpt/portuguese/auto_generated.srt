1
00:00:00,000 --> 00:00:04,560
As iniciais GPT significam Transformador Pré-treinado Generativo.

2
00:00:05,220 --> 00:00:09,020
Portanto, essa primeira palavra é bastante direta, são bots que geram novo texto.

3
00:00:09,800 --> 00:00:13,349
Pré-treinado refere-se a como o modelo passou por um processo de aprendizagem 

4
00:00:13,349 --> 00:00:16,808
a partir de uma enorme quantidade de dados, e o prefixo insinua que há mais 

5
00:00:16,808 --> 00:00:20,040
espaço para ajustá-lo em tarefas específicas com treinamento adicional.

6
00:00:20,720 --> 00:00:22,900
Mas a última palavra é a verdadeira peça chave.

7
00:00:23,380 --> 00:00:26,179
Um transformador é um tipo específico de rede neural, 

8
00:00:26,179 --> 00:00:29,963
um modelo de aprendizado de máquina, e é a principal invenção subjacente 

9
00:00:29,963 --> 00:00:31,000
ao atual boom da IA.

10
00:00:31,740 --> 00:00:35,647
O que quero fazer com este vídeo e com os capítulos seguintes é apresentar uma explicação 

11
00:00:35,647 --> 00:00:39,120
visualmente orientada sobre o que realmente acontece dentro de um transformador.

12
00:00:39,700 --> 00:00:42,820
Vamos seguir os dados que fluem por ele e seguir passo a passo.

13
00:00:43,440 --> 00:00:47,380
Existem muitos tipos diferentes de modelos que você pode construir usando transformadores.

14
00:00:47,800 --> 00:00:50,800
Alguns modelos captam áudio e produzem uma transcrição.

15
00:00:51,340 --> 00:00:53,731
Essa frase vem de um modelo que faz o contrário, 

16
00:00:53,731 --> 00:00:56,220
produzindo fala sintética apenas a partir do texto.

17
00:00:56,660 --> 00:01:00,988
Todas aquelas ferramentas que conquistaram o mundo em 2022, como Dolly e Midjourney, 

18
00:01:00,988 --> 00:01:05,519
que captam uma descrição de texto e produzem uma imagem, são baseadas em transformadores.

19
00:01:06,000 --> 00:01:09,600
Mesmo que eu não consiga entender o que uma criatura torta deveria ser, 

20
00:01:09,600 --> 00:01:13,100
ainda estou surpreso que esse tipo de coisa seja remotamente possível.

21
00:01:13,900 --> 00:01:17,944
E o transformador original introduzido em 2017 pelo Google foi inventado 

22
00:01:17,944 --> 00:01:22,100
para o caso de uso específico de tradução de texto de um idioma para outro.

23
00:01:22,660 --> 00:01:25,618
Mas a variante na qual você e eu vamos nos concentrar, 

24
00:01:25,618 --> 00:01:28,523
que é o tipo subjacente a ferramentas como o ChatGPT, 

25
00:01:28,523 --> 00:01:31,535
será um modelo treinado para captar um pedaço de texto, 

26
00:01:31,535 --> 00:01:34,978
talvez até acompanhado de algumas imagens ou sons circundantes, 

27
00:01:34,978 --> 00:01:38,260
e produzir uma previsão. para o que vem a seguir na passagem.

28
00:01:38,600 --> 00:01:41,302
Essa previsão assume a forma de uma distribuição de probabilidade 

29
00:01:41,302 --> 00:01:43,800
sobre muitos trechos diferentes de texto que podem se seguir.

30
00:01:45,040 --> 00:01:47,610
À primeira vista, você pode pensar que prever a próxima palavra 

31
00:01:47,610 --> 00:01:49,940
parece um objetivo muito diferente de gerar um novo texto.

32
00:01:50,180 --> 00:01:52,868
Mas uma vez que você tenha um modelo de previsão como este, 

33
00:01:52,868 --> 00:01:56,722
uma coisa simples para gerar um trecho de texto mais longo é fornecer a ele um trecho 

34
00:01:56,722 --> 00:02:00,577
inicial para trabalhar, fazer com que ele pegue uma amostra aleatória da distribuição 

35
00:02:00,577 --> 00:02:03,490
que acabou de gerar e anexe essa amostra ao texto e, em seguida, 

36
00:02:03,490 --> 00:02:07,388
execute todo o processo novamente para fazer uma nova previsão com base em todo o novo 

37
00:02:07,388 --> 00:02:09,539
texto, incluindo o que acabou de ser adicionado.

38
00:02:10,100 --> 00:02:13,000
Não sei sobre você, mas realmente não parece que isso deveria funcionar.

39
00:02:13,420 --> 00:02:16,464
Nesta animação, por exemplo, estou executando o GPT-2 em meu laptop 

40
00:02:16,464 --> 00:02:19,509
e fazendo com que ele preveja e experimente repetidamente o próximo 

41
00:02:19,509 --> 00:02:22,420
pedaço de texto para gerar uma história baseada no texto inicial.

42
00:02:22,420 --> 00:02:26,120
A história simplesmente não faz muito sentido.

43
00:02:26,500 --> 00:02:31,029
Mas se eu trocá-lo por chamadas de API para GPT-3, que é o mesmo modelo básico, 

44
00:02:31,029 --> 00:02:35,614
só que muito maior, de repente, quase magicamente, teremos uma história sensata, 

45
00:02:35,614 --> 00:02:40,257
que até parece inferir que uma criatura pi viveria em um terra da matemática e da 

46
00:02:40,257 --> 00:02:40,880
computação.

47
00:02:41,580 --> 00:02:44,934
Esse processo aqui de predição e amostragem repetidas é essencialmente 

48
00:02:44,934 --> 00:02:48,336
o que acontece quando você interage com o ChatGPT ou qualquer um desses 

49
00:02:48,336 --> 00:02:51,880
outros grandes modelos de linguagem e os vê produzindo uma palavra por vez.

50
00:02:52,480 --> 00:02:55,904
Na verdade, um recurso que eu gostaria muito é a capacidade de 

51
00:02:55,904 --> 00:02:59,220
ver a distribuição subjacente de cada nova palavra escolhida.

52
00:03:03,820 --> 00:03:05,953
Vamos começar com uma prévia de alto nível de 

53
00:03:05,953 --> 00:03:08,180
como os dados fluem através de um transformador.

54
00:03:08,640 --> 00:03:11,796
Passaremos muito mais tempo motivando, interpretando e expandindo os 

55
00:03:11,796 --> 00:03:15,091
detalhes de cada etapa, mas em linhas gerais, quando um desses chatbots 

56
00:03:15,091 --> 00:03:18,660
gera uma determinada palavra, aqui está o que está acontecendo nos bastidores.

57
00:03:19,080 --> 00:03:22,040
Primeiro, a entrada é dividida em vários pequenos pedaços.

58
00:03:22,620 --> 00:03:25,255
Essas peças são chamadas de tokens e, no caso de texto, 

59
00:03:25,255 --> 00:03:28,831
tendem a ser palavras ou pequenos pedaços de palavras ou outras combinações 

60
00:03:28,831 --> 00:03:29,820
de caracteres comuns.

61
00:03:30,740 --> 00:03:33,961
Se imagens ou som estiverem envolvidos, os tokens poderão ser 

62
00:03:33,961 --> 00:03:37,080
pequenos pedaços dessa imagem ou pequenos pedaços desse som.

63
00:03:37,580 --> 00:03:41,735
Cada um desses tokens é então associado a um vetor, ou seja, alguma lista de números, 

64
00:03:41,735 --> 00:03:45,360
que tem como objetivo codificar de alguma forma o significado daquela peça.

65
00:03:45,880 --> 00:03:48,665
Se você pensar que esses vetores fornecem coordenadas em algum 

66
00:03:48,665 --> 00:03:51,319
espaço de dimensão muito elevada, palavras com significados 

67
00:03:51,319 --> 00:03:54,680
semelhantes tendem a pousar em vetores próximos uns dos outros nesse espaço.

68
00:03:55,280 --> 00:03:59,550
Essa sequência de vetores passa então por uma operação conhecida como bloco de atenção, 

69
00:03:59,550 --> 00:04:02,558
e isso permite que os vetores se comuniquem entre si e passem 

70
00:04:02,558 --> 00:04:04,500
informações para atualizar seus valores.

71
00:04:04,880 --> 00:04:08,583
Por exemplo, o significado da palavra modelo na frase modelo de aprendizado 

72
00:04:08,583 --> 00:04:11,800
de máquina é diferente de seu significado na frase modelo de moda.

73
00:04:12,260 --> 00:04:15,350
O bloco de atenção é responsável por descobrir quais palavras no 

74
00:04:15,350 --> 00:04:18,726
contexto são relevantes para atualizar os significados de quais outras 

75
00:04:18,726 --> 00:04:21,959
palavras e como exatamente esses significados devem ser atualizados.

76
00:04:22,500 --> 00:04:24,750
E, novamente, sempre que uso a palavra significado, 

77
00:04:24,750 --> 00:04:28,040
isso está de alguma forma totalmente codificado nas entradas desses vetores.

78
00:04:29,180 --> 00:04:32,580
Depois disso, esses vetores passam por um tipo diferente de operação e, 

79
00:04:32,580 --> 00:04:35,508
dependendo da fonte que você está lendo, isso será chamado de 

80
00:04:35,508 --> 00:04:38,200
perceptron multicamadas ou talvez de camada feed-forward.

81
00:04:38,580 --> 00:04:42,660
E aqui os vetores não conversam entre si, todos fazem a mesma operação em paralelo.

82
00:04:43,060 --> 00:04:46,005
E embora esse bloco seja um pouco mais difícil de interpretar, 

83
00:04:46,005 --> 00:04:49,652
mais tarde falaremos sobre como a etapa é um pouco como fazer uma longa lista 

84
00:04:49,652 --> 00:04:53,251
de perguntas sobre cada vetor e depois atualizá-las com base nas respostas a 

85
00:04:53,251 --> 00:04:54,000
essas perguntas.

86
00:04:54,900 --> 00:05:00,016
Todas as operações em ambos os blocos parecem uma pilha gigante de multiplicações 

87
00:05:00,016 --> 00:05:05,320
de matrizes, e nossa principal tarefa será entender como ler as matrizes subjacentes.

88
00:05:06,980 --> 00:05:09,896
Estou encobrindo alguns detalhes sobre algumas etapas de normalização 

89
00:05:09,896 --> 00:05:12,980
que acontecem no meio, mas, afinal, esta é uma visualização de alto nível.

90
00:05:13,680 --> 00:05:16,458
Depois disso, o processo essencialmente se repete, 

91
00:05:16,458 --> 00:05:20,545
você vai e volta entre blocos de atenção e blocos perceptron multicamadas, 

92
00:05:20,545 --> 00:05:24,195
até que no final a esperança é que todo o significado essencial da 

93
00:05:24,195 --> 00:05:28,500
passagem tenha sido de alguma forma incorporado ao último vetor em a sequência.

94
00:05:28,920 --> 00:05:32,086
Em seguida, realizamos uma determinada operação nesse último vetor que 

95
00:05:32,086 --> 00:05:35,387
produz uma distribuição de probabilidade sobre todos os tokens possíveis, 

96
00:05:35,387 --> 00:05:38,420
todos os pequenos pedaços de texto possíveis que podem vir a seguir.

97
00:05:38,980 --> 00:05:42,627
E como eu disse, uma vez que você tem uma ferramenta que prevê o que vem a seguir 

98
00:05:42,627 --> 00:05:46,007
com base em um trecho de texto, você pode alimentá-la com um pouco de texto 

99
00:05:46,007 --> 00:05:49,921
inicial e fazer com que ela jogue repetidamente esse jogo de prever o que vem a seguir, 

100
00:05:49,921 --> 00:05:53,080
amostrando a distribuição, anexando e depois repetindo indefinidamente.

101
00:05:53,640 --> 00:05:57,408
Alguns de vocês que conhecem podem se lembrar de quanto tempo antes do ChatGPT entrar 

102
00:05:57,408 --> 00:06:00,213
em cena, era assim que se pareciam as primeiras demos do GPT-3: 

103
00:06:00,213 --> 00:06:03,851
você faria com que ele preenchesse automaticamente histórias e ensaios com base em 

104
00:06:03,851 --> 00:06:04,640
um trecho inicial.

105
00:06:05,580 --> 00:06:08,506
Para transformar uma ferramenta como essa em um chatbot, 

106
00:06:08,506 --> 00:06:12,768
o ponto de partida mais fácil é ter um pequeno texto que estabeleça a configuração 

107
00:06:12,768 --> 00:06:15,643
de um usuário interagindo com um assistente de IA útil, 

108
00:06:15,643 --> 00:06:20,008
o que você chamaria de prompt do sistema, e então você usaria o a pergunta ou prompt 

109
00:06:20,008 --> 00:06:22,729
inicial do usuário como a primeira parte do diálogo, 

110
00:06:22,729 --> 00:06:26,940
e então você começa a prever o que um assistente de IA tão útil diria em resposta.

111
00:06:27,720 --> 00:06:32,215
Há mais a dizer sobre uma etapa do treinamento necessária para que isso funcione bem, 

112
00:06:32,215 --> 00:06:33,940
mas em alto nível essa é a ideia.

113
00:06:35,720 --> 00:06:39,940
Neste capítulo, você e eu vamos expandir os detalhes do que acontece bem no início 

114
00:06:39,940 --> 00:06:43,905
da rede, no final da rede, e também quero passar muito tempo revisando alguns 

115
00:06:43,905 --> 00:06:48,024
importantes conhecimentos básicos. , coisas que seriam uma segunda natureza para 

116
00:06:48,024 --> 00:06:52,600
qualquer engenheiro de aprendizado de máquina na época em que os transformadores surgiram.

117
00:06:53,060 --> 00:06:56,480
Se você se sentir confortável com esse conhecimento prévio e um pouco impaciente, 

118
00:06:56,480 --> 00:06:58,775
sinta-se à vontade para pular para o próximo capítulo, 

119
00:06:58,775 --> 00:07:02,070
que se concentrará nos bloqueios de atenção, geralmente considerados o coração 

120
00:07:02,070 --> 00:07:02,780
do transformador.

121
00:07:03,360 --> 00:07:07,052
Depois disso, quero falar mais sobre esses blocos perceptron multicamadas, 

122
00:07:07,052 --> 00:07:11,138
como funciona o treinamento e uma série de outros detalhes que foram ignorados até 

123
00:07:11,138 --> 00:07:11,680
esse ponto.

124
00:07:12,180 --> 00:07:15,328
Para um contexto mais amplo, esses vídeos são acréscimos a uma minissérie 

125
00:07:15,328 --> 00:07:18,733
sobre aprendizado profundo, e está tudo bem se você não assistiu os anteriores, 

126
00:07:18,733 --> 00:07:21,626
acho que você pode fazer isso fora de ordem, mas antes de mergulhar 

127
00:07:21,626 --> 00:07:24,860
especificamente nos transformadores, eu acho vale a pena ter certeza de que 

128
00:07:24,860 --> 00:07:28,520
estamos na mesma página sobre a premissa básica e a estrutura do aprendizado profundo.

129
00:07:29,020 --> 00:07:33,065
Correndo o risco de afirmar o óbvio, esta é uma abordagem de aprendizado de máquina, 

130
00:07:33,065 --> 00:07:36,253
que descreve qualquer modelo em que você usa dados para determinar 

131
00:07:36,253 --> 00:07:38,300
de alguma forma como um modelo se comporta.

132
00:07:39,140 --> 00:07:42,642
O que quero dizer com isso é, digamos que você queira uma função que receba 

133
00:07:42,642 --> 00:07:46,098
uma imagem e produza um rótulo descrevendo-a, ou nosso exemplo de previsão 

134
00:07:46,098 --> 00:07:48,264
da próxima palavra dada uma passagem de texto, 

135
00:07:48,264 --> 00:07:51,581
ou qualquer outra tarefa que pareça exigir algum elemento de intuição e 

136
00:07:51,581 --> 00:07:52,780
reconhecimento de padrões.

137
00:07:53,200 --> 00:07:55,826
Hoje em dia, consideramos isso quase um dado adquirido, 

138
00:07:55,826 --> 00:07:59,813
mas a ideia do aprendizado de máquina é que, em vez de tentar definir explicitamente 

139
00:07:59,813 --> 00:08:02,580
um procedimento sobre como realizar essa tarefa em código, 

140
00:08:02,580 --> 00:08:05,488
que é o que as pessoas teriam feito nos primeiros dias da IA, 

141
00:08:05,488 --> 00:08:09,428
em vez disso você configure uma estrutura muito flexível com parâmetros ajustáveis, 

142
00:08:09,428 --> 00:08:13,274
como um monte de botões e dials, e então de alguma forma você usa muitos exemplos 

143
00:08:13,274 --> 00:08:17,026
de como a saída deve ser para uma determinada entrada para ajustar e ajustar os 

144
00:08:17,026 --> 00:08:19,700
valores desses parâmetros para imitar esse comportamento.

145
00:08:19,700 --> 00:08:24,052
Por exemplo, talvez a forma mais simples de aprendizado de máquina seja a regressão 

146
00:08:24,052 --> 00:08:26,954
linear, onde suas entradas e saídas são números únicos, 

147
00:08:26,954 --> 00:08:29,804
algo como a metragem quadrada de uma casa e seu preço, 

148
00:08:29,804 --> 00:08:34,157
e o que você deseja é encontrar uma linha que melhor se ajuste através disso dados, 

149
00:08:34,157 --> 00:08:36,799
você sabe, para prever os preços futuros das casas.

150
00:08:37,440 --> 00:08:40,796
Essa linha é descrita por dois parâmetros contínuos, digamos, 

151
00:08:40,796 --> 00:08:44,424
a inclinação e a interceptação y, e o objetivo da regressão linear 

152
00:08:44,424 --> 00:08:48,160
é determinar esses parâmetros para que correspondam melhor aos dados.

153
00:08:48,880 --> 00:08:52,100
Escusado será dizer que os modelos de aprendizagem profunda ficam muito mais complicados.

154
00:08:52,620 --> 00:08:57,660
O GPT-3, por exemplo, não possui dois, mas 175 bilhões de parâmetros.

155
00:08:58,120 --> 00:09:02,025
Mas o problema é o seguinte: não é certo que você pode criar um modelo 

156
00:09:02,025 --> 00:09:05,875
gigante com um grande número de parâmetros sem que ele superajuste os 

157
00:09:05,875 --> 00:09:09,560
dados de treinamento ou seja completamente intratável para treinar.

158
00:09:10,260 --> 00:09:13,170
O aprendizado profundo descreve uma classe de modelos que, 

159
00:09:13,170 --> 00:09:16,180
nas últimas décadas, provou ser escalonável notavelmente bem.

160
00:09:16,480 --> 00:09:20,708
O que os unifica é o mesmo algoritmo de treinamento, chamado backpropagation, 

161
00:09:20,708 --> 00:09:24,395
e o contexto que quero que você tenha à medida que avançamos é que, 

162
00:09:24,395 --> 00:09:27,810
para que esse algoritmo de treinamento funcione bem em escala, 

163
00:09:27,810 --> 00:09:31,280
esses modelos precisam seguir um determinado formato específico.

164
00:09:31,800 --> 00:09:35,810
Se você conhece esse formato, isso ajuda a explicar muitas das escolhas de como um 

165
00:09:35,810 --> 00:09:39,820
transformador processa a linguagem, que de outra forma correm o risco de parecerem 

166
00:09:39,820 --> 00:09:40,400
arbitrárias.

167
00:09:41,440 --> 00:09:44,090
Primeiro, qualquer que seja o modelo que você esteja criando, 

168
00:09:44,090 --> 00:09:46,740
a entrada deverá ser formatada como um array de números reais.

169
00:09:46,740 --> 00:09:50,977
Isso pode significar uma lista de números, pode ser uma matriz bidimensional ou, 

170
00:09:50,977 --> 00:09:54,221
muitas vezes, você lida com matrizes de dimensões superiores, 

171
00:09:54,221 --> 00:09:56,000
onde o termo geral usado é tensor.

172
00:09:56,560 --> 00:10:00,523
Muitas vezes você pensa que os dados de entrada são progressivamente transformados em 

173
00:10:00,523 --> 00:10:04,532
muitas camadas distintas, onde, novamente, cada camada é sempre estruturada como algum 

174
00:10:04,532 --> 00:10:08,680
tipo de matriz de números reais, até chegar a uma camada final que você considera a saída.

175
00:10:09,280 --> 00:10:12,953
Por exemplo, a camada final em nosso modelo de processamento de texto é uma lista de 

176
00:10:12,953 --> 00:10:16,627
números que representa a distribuição de probabilidade para todos os próximos tokens 

177
00:10:16,627 --> 00:10:17,060
possíveis.

178
00:10:17,820 --> 00:10:21,549
No aprendizado profundo, esses parâmetros do modelo são quase sempre chamados de pesos, 

179
00:10:21,549 --> 00:10:24,601
e isso ocorre porque uma característica importante desses modelos é que 

180
00:10:24,601 --> 00:10:27,568
a única maneira pela qual esses parâmetros interagem com os dados que 

181
00:10:27,568 --> 00:10:29,900
estão sendo processados é por meio de somas ponderadas.

182
00:10:30,340 --> 00:10:34,360
Você também espalha algumas funções não lineares, mas elas não dependem de parâmetros.

183
00:10:35,200 --> 00:10:38,709
Porém, normalmente, em vez de ver as somas ponderadas todas nuas 

184
00:10:38,709 --> 00:10:42,164
e escritas explicitamente assim, você as encontrará empacotadas 

185
00:10:42,164 --> 00:10:45,620
juntas como vários componentes em um produto vetorial de matriz.

186
00:10:46,740 --> 00:10:50,724
Isso equivale a dizer a mesma coisa: se você pensar em como funciona a multiplicação 

187
00:10:50,724 --> 00:10:54,240
de vetores de matrizes, cada componente na saída parece uma soma ponderada.

188
00:10:54,780 --> 00:10:58,271
Muitas vezes é conceitualmente mais limpo para você e para mim 

189
00:10:58,271 --> 00:11:01,651
pensar em matrizes preenchidas com parâmetros ajustáveis que 

190
00:11:01,651 --> 00:11:05,420
transformam vetores extraídos dos dados que estão sendo processados.

191
00:11:06,340 --> 00:11:10,352
Por exemplo, esses 175 mil milhões de pesos no GPT-3 estão 

192
00:11:10,352 --> 00:11:14,160
organizados em pouco menos de 28.000 matrizes distintas.

193
00:11:14,660 --> 00:11:18,072
Essas matrizes, por sua vez, se enquadram em oito categorias diferentes, 

194
00:11:18,072 --> 00:11:22,045
e o que você e eu faremos é percorrer cada uma dessas categorias para entender o que 

195
00:11:22,045 --> 00:11:22,700
esse tipo faz.

196
00:11:23,160 --> 00:11:27,086
À medida que avançamos, acho divertido fazer referência aos números 

197
00:11:27,086 --> 00:11:31,360
específicos do GPT-3 para contar exatamente de onde vêm esses 175 bilhões.

198
00:11:31,880 --> 00:11:34,447
Mesmo que hoje existam modelos maiores e melhores, 

199
00:11:34,447 --> 00:11:38,927
este tem um certo charme como modelo de linguagem grande para realmente captar a atenção 

200
00:11:38,927 --> 00:11:40,740
do mundo fora das comunidades de ML.

201
00:11:41,440 --> 00:11:43,921
Além disso, na prática, as empresas tendem a manter a boca 

202
00:11:43,921 --> 00:11:46,740
fechada em relação aos números específicos das redes mais modernas.

203
00:11:47,360 --> 00:11:50,591
Eu só quero definir o cenário, que quando você espia os bastidores 

204
00:11:50,591 --> 00:11:53,726
para ver o que acontece dentro de uma ferramenta como o ChatGPT, 

205
00:11:53,726 --> 00:11:57,440
quase toda a computação real parece uma multiplicação de vetores de matrizes.

206
00:11:57,900 --> 00:12:01,342
Há um certo risco de se perder no mar de bilhões de números, 

207
00:12:01,342 --> 00:12:06,139
mas você deve traçar uma distinção bem nítida em sua mente entre os pesos do modelo, 

208
00:12:06,139 --> 00:12:10,146
que sempre colorirei em azul ou vermelho, e os dados sendo processado, 

209
00:12:10,146 --> 00:12:11,840
que sempre colorirei de cinza.

210
00:12:12,180 --> 00:12:14,997
Os pesos são o cérebro real, são as coisas aprendidas 

211
00:12:14,997 --> 00:12:17,920
durante o treinamento e determinam como ele se comporta.

212
00:12:18,280 --> 00:12:22,461
Os dados que estão sendo processados simplesmente codificam qualquer entrada específica 

213
00:12:22,461 --> 00:12:26,500
inserida no modelo para uma determinada execução, como um exemplo de trecho de texto.

214
00:12:27,480 --> 00:12:30,523
Com tudo isso como base, vamos nos aprofundar na primeira etapa 

215
00:12:30,523 --> 00:12:33,614
deste exemplo de processamento de texto, que é dividir a entrada 

216
00:12:33,614 --> 00:12:36,420
em pequenos pedaços e transformar esses pedaços em vetores.

217
00:12:37,020 --> 00:12:39,432
Mencionei como esses pedaços são chamados de tokens, 

218
00:12:39,432 --> 00:12:41,616
que podem ser pedaços de palavras ou pontuação, 

219
00:12:41,616 --> 00:12:44,529
mas de vez em quando neste capítulo e especialmente no próximo, 

220
00:12:44,529 --> 00:12:48,080
gostaria apenas de fingir que estão divididos de forma mais clara em palavras.

221
00:12:48,600 --> 00:12:51,298
Como nós, humanos, pensamos em palavras, isso tornará muito mais 

222
00:12:51,298 --> 00:12:54,080
fácil fazer referência a pequenos exemplos e esclarecer cada etapa.

223
00:12:55,260 --> 00:12:59,753
O modelo possui um vocabulário predefinido, uma lista de todas as palavras possíveis, 

224
00:12:59,753 --> 00:13:02,940
digamos 50.000 delas, e a primeira matriz que encontraremos, 

225
00:13:02,940 --> 00:13:06,964
conhecida como matriz de incorporação, possui uma única coluna para cada uma 

226
00:13:06,964 --> 00:13:07,800
dessas palavras.

227
00:13:08,940 --> 00:13:11,324
Essas colunas são o que determina em que vetor 

228
00:13:11,324 --> 00:13:13,760
cada palavra se transforma nessa primeira etapa.

229
00:13:15,100 --> 00:13:18,434
Nós a rotulamos como Nós, e como todas as matrizes que vemos, 

230
00:13:18,434 --> 00:13:22,360
seus valores começam aleatórios, mas serão aprendidos com base nos dados.

231
00:13:23,620 --> 00:13:26,633
Transformar palavras em vetores era uma prática comum no aprendizado 

232
00:13:26,633 --> 00:13:29,602
de máquina muito antes dos transformadores, mas é um pouco estranho 

233
00:13:29,602 --> 00:13:32,877
se você nunca viu isso antes e estabelece a base para tudo o que se segue, 

234
00:13:32,877 --> 00:13:35,760
então vamos reservar um momento para nos familiarizarmos com isso.

235
00:13:36,040 --> 00:13:38,492
Muitas vezes chamamos isso de incorporação de palavra, 

236
00:13:38,492 --> 00:13:41,613
o que convida você a pensar nesses vetores de forma muito geométrica, 

237
00:13:41,613 --> 00:13:43,620
como pontos em algum espaço de alta dimensão.

238
00:13:44,180 --> 00:13:47,912
Visualizar uma lista de três números como coordenadas para pontos no espaço 3D não 

239
00:13:47,912 --> 00:13:51,780
seria problema, mas os embeddings de palavras tendem a ter dimensões muito superiores.

240
00:13:52,280 --> 00:13:55,777
No GPT-3 eles têm 12.288 dimensões e, como você verá, 

241
00:13:55,777 --> 00:14:00,440
é importante trabalhar em um espaço que tenha muitas direções distintas.

242
00:14:01,180 --> 00:14:05,040
Da mesma forma que você poderia pegar uma fatia bidimensional através de um 

243
00:14:05,040 --> 00:14:07,579
espaço 3D e projetar todos os pontos nessa fatia, 

244
00:14:07,579 --> 00:14:11,845
para animar os embeddings de palavras que um modelo simples está me proporcionando, 

245
00:14:11,845 --> 00:14:15,604
farei uma coisa análoga escolhendo uma fatia tridimensional através deste 

246
00:14:15,604 --> 00:14:19,311
espaço de dimensão muito alta e projetando a palavra vetores sobre ela e 

247
00:14:19,311 --> 00:14:20,480
exibindo os resultados.

248
00:14:21,280 --> 00:14:24,646
A grande ideia aqui é que, à medida que um modelo ajusta e ajusta seus pesos 

249
00:14:24,646 --> 00:14:27,881
para determinar exatamente como as palavras são incorporadas como vetores 

250
00:14:27,881 --> 00:14:30,854
durante o treinamento, ele tende a se estabelecer em um conjunto de 

251
00:14:30,854 --> 00:14:34,440
incorporações onde as direções no espaço têm uma espécie de significado semântico.

252
00:14:34,980 --> 00:14:38,350
Para o modelo simples de palavra para vetor que estou executando aqui, 

253
00:14:38,350 --> 00:14:42,244
se eu pesquisar todas as palavras cujas incorporações são mais próximas de torre, 

254
00:14:42,244 --> 00:14:45,900
você notará como todas elas parecem dar vibrações de torre muito semelhantes.

255
00:14:46,340 --> 00:14:48,723
E se você quiser pegar um pouco de Python e brincar em casa, 

256
00:14:48,723 --> 00:14:51,380
este é o modelo específico que estou usando para fazer as animações.

257
00:14:51,620 --> 00:14:54,658
Não é um transformador, mas é suficiente para ilustrar a ideia 

258
00:14:54,658 --> 00:14:57,600
de que as direções no espaço podem ter significado semântico.

259
00:14:58,300 --> 00:15:03,286
Um exemplo muito clássico disso é como se você pegar a diferença entre os vetores 

260
00:15:03,286 --> 00:15:08,213
para mulher e homem, algo que você visualizaria como um pequeno vetor conectando 

261
00:15:08,213 --> 00:15:13,200
a ponta de um à ponta do outro, é muito semelhante à diferença entre rei e rainha.

262
00:15:15,080 --> 00:15:18,928
Então, digamos que você não conhecesse a palavra para uma monarca feminina, 

263
00:15:18,928 --> 00:15:22,624
você poderia encontrá-la tomando rei, adicionando a direção mulher-homem 

264
00:15:22,624 --> 00:15:25,460
e procurando as incorporações mais próximas desse ponto.

265
00:15:27,000 --> 00:15:28,200
Pelo menos, mais ou menos.

266
00:15:28,480 --> 00:15:31,891
Apesar de este ser um exemplo clássico para o modelo com o qual estou brincando, 

267
00:15:31,891 --> 00:15:35,051
a verdadeira incorporação da rainha está na verdade um pouco mais distante 

268
00:15:35,051 --> 00:15:38,041
do que isso sugere, provavelmente porque a forma como a rainha é usada 

269
00:15:38,041 --> 00:15:40,780
nos dados de treinamento não é apenas uma versão feminina do rei.

270
00:15:41,620 --> 00:15:45,260
Quando brinquei, as relações familiares pareciam ilustrar muito melhor a ideia.

271
00:15:46,340 --> 00:15:50,836
A questão é que durante o treinamento o modelo achou vantajoso escolher embeddings 

272
00:15:50,836 --> 00:15:54,900
de forma que uma direção neste espaço codificasse as informações de gênero.

273
00:15:56,800 --> 00:16:00,081
Outro exemplo é que se pegarmos na incorporação da Itália, 

274
00:16:00,081 --> 00:16:04,975
e subtrairmos a incorporação da Alemanha, e adicionarmos isso à incorporação de Hitler, 

275
00:16:04,975 --> 00:16:08,090
obtemos algo muito próximo da incorporação de Mussolini.

276
00:16:08,570 --> 00:16:12,042
É como se o modelo tivesse aprendido a associar algumas direções à 

277
00:16:12,042 --> 00:16:15,670
italianidade e outras aos líderes dos eixos da Segunda Guerra Mundial.

278
00:16:16,470 --> 00:16:19,251
Talvez meu exemplo favorito nesse sentido seja como, 

279
00:16:19,251 --> 00:16:22,504
em alguns modelos, se você pegar a diferença entre a Alemanha 

280
00:16:22,504 --> 00:16:26,230
e o Japão e adicioná-la ao sushi, você acaba muito próximo da salsicha.

281
00:16:27,350 --> 00:16:30,229
Também ao jogar este jogo de encontrar os vizinhos mais próximos, 

282
00:16:30,229 --> 00:16:33,850
fiquei satisfeito ao ver o quão próxima Kat estava tanto da fera quanto do monstro.

283
00:16:34,690 --> 00:16:37,236
Um pouco de intuição matemática que é útil ter em mente, 

284
00:16:37,236 --> 00:16:40,230
especialmente para o próximo capítulo, é como o produto escalar de 

285
00:16:40,230 --> 00:16:43,850
dois vetores pode ser pensado como uma forma de medir o quão bem eles se alinham.

286
00:16:44,870 --> 00:16:47,968
Computacionalmente, os produtos escalares envolvem a multiplicação de todos 

287
00:16:47,968 --> 00:16:51,149
os componentes correspondentes e depois a adição dos resultados, o que é bom, 

288
00:16:51,149 --> 00:16:54,330
uma vez que grande parte da nossa computação tem que parecer somas ponderadas.

289
00:16:55,190 --> 00:16:58,413
Geometricamente, o produto escalar é positivo quando os 

290
00:16:58,413 --> 00:17:01,695
vetores apontam em direções semelhantes, é zero se forem 

291
00:17:01,695 --> 00:17:05,609
perpendiculares e é negativo sempre que apontam em direções opostas.

292
00:17:06,550 --> 00:17:09,946
Por exemplo, digamos que você estava brincando com este modelo 

293
00:17:09,946 --> 00:17:13,343
e levanta a hipótese de que a incorporação de gatos menos gato 

294
00:17:13,343 --> 00:17:17,010
pode representar uma espécie de direção de pluralidade neste espaço.

295
00:17:17,430 --> 00:17:20,562
Para testar isso, vou pegar esse vetor e calcular seu produto escalar 

296
00:17:20,562 --> 00:17:23,917
em relação às incorporações de certos substantivos singulares e compará-lo 

297
00:17:23,917 --> 00:17:27,050
com os produtos escalares com os substantivos plurais correspondentes.

298
00:17:27,270 --> 00:17:31,523
Se você brincar com isso, notará que os plurais realmente parecem dar consistentemente 

299
00:17:31,523 --> 00:17:35,678
valores mais altos do que os singulares, indicando que eles se alinham mais com essa 

300
00:17:35,678 --> 00:17:36,070
direção.

301
00:17:37,070 --> 00:17:40,904
Também é divertido como se você pegar esse produto escalar com as incorporações das 

302
00:17:40,904 --> 00:17:44,145
palavras 1, 2, 3 e assim por diante, eles fornecem valores crescentes, 

303
00:17:44,145 --> 00:17:48,117
então é como se pudéssemos medir quantitativamente o quão plural o modelo encontra uma 

304
00:17:48,117 --> 00:17:49,030
determinada palavra.

305
00:17:50,250 --> 00:17:51,927
Novamente, os detalhes de como as palavras são 

306
00:17:51,927 --> 00:17:53,570
incorporadas são aprendidos por meio de dados.

307
00:17:54,050 --> 00:17:57,688
Esta matriz de incorporação, cujas colunas nos dizem o que acontece com cada palavra, 

308
00:17:57,688 --> 00:17:59,550
é a primeira pilha de pesos do nosso modelo.

309
00:18:00,030 --> 00:18:05,163
Usando os números GPT-3, o tamanho do vocabulário é especificamente 50.257 e, 

310
00:18:05,163 --> 00:18:09,770
novamente, tecnicamente não consiste em palavras em si, mas em tokens.

311
00:18:10,630 --> 00:18:14,116
A dimensão de incorporação é 12.288, e multiplicar isso 

312
00:18:14,116 --> 00:18:17,790
nos diz que isso consiste em cerca de 617 milhões de pesos.

313
00:18:18,250 --> 00:18:21,030
Vamos em frente e adicionar isto a uma contagem contínua, 

314
00:18:21,030 --> 00:18:23,810
lembrando que no final devemos contar até 175 mil milhões.

315
00:18:25,430 --> 00:18:28,780
No caso dos transformadores, você realmente quer pensar nos vetores neste 

316
00:18:28,780 --> 00:18:32,130
espaço de incorporação como não apenas representando palavras individuais.

317
00:18:32,550 --> 00:18:36,388
Por um lado, eles também codificam informações sobre a posição daquela palavra, 

318
00:18:36,388 --> 00:18:39,891
sobre a qual falaremos mais tarde, mas o mais importante é que você deve 

319
00:18:39,891 --> 00:18:42,770
pensar neles como tendo a capacidade de absorver o contexto.

320
00:18:43,350 --> 00:18:47,479
Um vetor que começou sua vida como a incorporação da palavra rei, por exemplo, 

321
00:18:47,479 --> 00:18:51,922
pode ser progressivamente puxado e puxado por vários blocos nesta rede, de modo que, 

322
00:18:51,922 --> 00:18:56,157
no final, aponte para uma direção muito mais específica e matizada que de alguma 

323
00:18:56,157 --> 00:19:00,495
forma codifica isso. foi um rei que viveu na Escócia e que alcançou seu posto após 

324
00:19:00,495 --> 00:19:04,730
assassinar o rei anterior, e que está sendo descrito na linguagem shakespeariana.

325
00:19:05,210 --> 00:19:07,790
Pense em sua própria compreensão de uma determinada palavra.

326
00:19:08,250 --> 00:19:11,614
O significado dessa palavra é claramente informado pelo ambiente, 

327
00:19:11,614 --> 00:19:15,080
e às vezes isso inclui o contexto de uma longa distância, portanto, 

328
00:19:15,080 --> 00:19:19,158
ao montar um modelo que tenha a capacidade de prever qual palavra vem a seguir, 

329
00:19:19,158 --> 00:19:23,390
o objetivo é de alguma forma capacitá-lo para incorporar o contexto eficientemente.

330
00:19:24,050 --> 00:19:27,097
Para ficar claro, nessa primeira etapa, quando você cria a matriz de 

331
00:19:27,097 --> 00:19:30,454
vetores com base no texto de entrada, cada um deles é simplesmente retirado 

332
00:19:30,454 --> 00:19:33,634
da matriz de incorporação, então inicialmente cada um só pode codificar 

333
00:19:33,634 --> 00:19:36,770
o significado de uma única palavra sem qualquer entrada de seu entorno.

334
00:19:37,710 --> 00:19:41,479
Mas você deve pensar no objetivo principal desta rede pela qual ela flui como 

335
00:19:41,479 --> 00:19:45,200
sendo permitir que cada um desses vetores absorva um significado que é muito 

336
00:19:45,200 --> 00:19:48,970
mais rico e específico do que meras palavras individuais poderiam representar.

337
00:19:49,510 --> 00:19:52,453
A rede só pode processar um número fixo de vetores por vez, 

338
00:19:52,453 --> 00:19:54,170
conhecido como tamanho de contexto.

339
00:19:54,510 --> 00:19:58,176
Para GPT-3, ele foi treinado com um tamanho de contexto de 2.048, 

340
00:19:58,176 --> 00:20:01,676
de modo que os dados que fluem pela rede sempre se parecem com 

341
00:20:01,676 --> 00:20:05,010
esta matriz de 2.048 colunas, cada uma com 12.000 dimensões.

342
00:20:05,590 --> 00:20:08,448
Esse tamanho de contexto limita a quantidade de texto que o 

343
00:20:08,448 --> 00:20:11,830
transformador pode incorporar ao fazer uma previsão da próxima palavra.

344
00:20:12,370 --> 00:20:14,778
É por isso que longas conversas com certos chatbots, 

345
00:20:14,778 --> 00:20:18,005
como as primeiras versões do ChatGPT, muitas vezes davam a sensação de 

346
00:20:18,005 --> 00:20:22,050
que o bot estava perdendo o fio da conversa à medida que você continuava por muito tempo.

347
00:20:23,030 --> 00:20:25,686
Entraremos nos detalhes da atenção no devido tempo, mas, 

348
00:20:25,686 --> 00:20:28,810
avançando, quero falar por um minuto sobre o que acontece no final.

349
00:20:29,450 --> 00:20:31,936
Lembre-se, a saída desejada é uma distribuição de 

350
00:20:31,936 --> 00:20:34,870
probabilidade sobre todos os tokens que podem vir a seguir.

351
00:20:35,170 --> 00:20:37,553
Por exemplo, se a última palavra for Professor, 

352
00:20:37,553 --> 00:20:39,987
e o contexto incluir palavras como Harry Potter, 

353
00:20:39,987 --> 00:20:42,818
e imediatamente antes virmos o professor menos favorito, 

354
00:20:42,818 --> 00:20:45,301
e também se você me der alguma margem de manobra, 

355
00:20:45,301 --> 00:20:48,976
deixando-me fingir que os tokens simplesmente parecem palavras completas, 

356
00:20:48,976 --> 00:20:53,048
então uma rede bem treinada que tivesse acumulado conhecimento sobre Harry Potter 

357
00:20:53,048 --> 00:20:55,830
provavelmente atribuiria um número alto à palavra Snape.

358
00:20:56,510 --> 00:20:57,970
Isso envolve duas etapas diferentes.

359
00:20:58,310 --> 00:21:03,156
A primeira é usar outra matriz que mapeie o último vetor naquele contexto 

360
00:21:03,156 --> 00:21:07,610
para uma lista de 50.000 valores, um para cada token do vocabulário.

361
00:21:08,170 --> 00:21:12,309
Depois, há uma função que normaliza isso em uma distribuição de probabilidade, 

362
00:21:12,309 --> 00:21:15,767
chamada Softmax e falaremos mais sobre isso em apenas um segundo, 

363
00:21:15,767 --> 00:21:19,487
mas antes disso pode parecer um pouco estranho usar apenas esta última 

364
00:21:19,487 --> 00:21:23,312
incorporação para fazer uma previsão, quando afinal, nessa última etapa, 

365
00:21:23,312 --> 00:21:27,346
existem milhares de outros vetores na camada, com seus próprios significados 

366
00:21:27,346 --> 00:21:28,290
ricos em contexto.

367
00:21:28,930 --> 00:21:32,710
Isso tem a ver com o fato de que no processo de treinamento acaba sendo 

368
00:21:32,710 --> 00:21:36,385
muito mais eficiente usar cada um desses vetores na camada final para 

369
00:21:36,385 --> 00:21:40,270
fazer simultaneamente uma previsão do que viria imediatamente depois dele.

370
00:21:40,970 --> 00:21:43,495
Há muito mais a ser dito sobre o treinamento mais tarde, 

371
00:21:43,495 --> 00:21:45,090
mas eu só quero destacar isso agora.

372
00:21:45,730 --> 00:21:49,690
Esta matriz é chamada de matriz de desincorporação e damos a ela o rótulo WU.

373
00:21:50,210 --> 00:21:52,307
Novamente, como todas as matrizes de peso que vemos, 

374
00:21:52,307 --> 00:21:55,316
suas entradas começam aleatoriamente, mas são aprendidas durante o processo 

375
00:21:55,316 --> 00:21:55,910
de treinamento.

376
00:21:56,470 --> 00:21:59,043
Mantendo a pontuação em nossa contagem total de parâmetros, 

377
00:21:59,043 --> 00:22:01,875
esta matriz de desincorporação tem uma linha para cada palavra no 

378
00:22:01,875 --> 00:22:05,650
vocabulário e cada linha tem o mesmo número de elementos que a dimensão de incorporação.

379
00:22:06,410 --> 00:22:09,985
É muito semelhante à matriz de incorporação, apenas com a ordem trocada, 

380
00:22:09,985 --> 00:22:12,875
por isso adiciona outros 617 milhões de parâmetros à rede, 

381
00:22:12,875 --> 00:22:16,647
o que significa que a nossa contagem até agora é de pouco mais de um bilhão, 

382
00:22:16,647 --> 00:22:20,663
uma fração pequena, mas não totalmente insignificante, dos 175 bilhões que temos. 

383
00:22:20,663 --> 00:22:21,790
vou acabar com o total.

384
00:22:22,550 --> 00:22:26,629
Como última minilição deste capítulo, quero falar mais sobre essa função softmax, 

385
00:22:26,629 --> 00:22:30,610
pois ela aparece novamente para nós quando mergulhamos nos bloqueios de atenção.

386
00:22:31,430 --> 00:22:35,732
A ideia é que se você deseja que uma sequência de números atue como uma distribuição 

387
00:22:35,732 --> 00:22:40,186
de probabilidade, digamos, uma distribuição sobre todas as próximas palavras possíveis, 

388
00:22:40,186 --> 00:22:44,590
então cada valor deve estar entre 0 e 1, e você também precisa que todos eles somem 1 .

389
00:22:45,250 --> 00:22:48,305
No entanto, se você estiver jogando o jogo de aprendizagem em 

390
00:22:48,305 --> 00:22:52,198
que tudo o que você faz se parece com uma multiplicação de matrizes e vetores, 

391
00:22:52,198 --> 00:22:54,810
os resultados obtidos por padrão não obedecem a isso.

392
00:22:55,330 --> 00:22:59,870
Os valores costumam ser negativos ou muito maiores que 1 e quase certamente não somam 1.

393
00:23:00,510 --> 00:23:03,930
Softmax é a maneira padrão de transformar uma lista arbitrária de 

394
00:23:03,930 --> 00:23:07,454
números em uma distribuição válida, de forma que os valores maiores 

395
00:23:07,454 --> 00:23:11,290
fiquem mais próximos de 1 e os valores menores fiquem muito próximos de 0.

396
00:23:11,830 --> 00:23:13,070
Isso é tudo que você realmente precisa saber.

397
00:23:13,090 --> 00:23:17,354
Mas se você estiver curioso, a forma como funciona é primeiro elevar e à potência 

398
00:23:17,354 --> 00:23:21,514
de cada um dos números, o que significa que agora você tem uma lista de valores 

399
00:23:21,514 --> 00:23:25,518
positivos, e então você pode pegar a soma de todos esses valores positivos e 

400
00:23:25,518 --> 00:23:29,470
dividir cada termo por essa soma, o que o normaliza em uma lista que soma 1.

401
00:23:30,170 --> 00:23:34,520
Você notará que se um dos números na entrada for significativamente maior que o resto, 

402
00:23:34,520 --> 00:23:38,070
então na saída o termo correspondente domina a distribuição, portanto, 

403
00:23:38,070 --> 00:23:42,470
se você estivesse amostrando, quase certamente escolheria apenas a entrada maximizadora.

404
00:23:42,990 --> 00:23:45,340
Mas é mais suave do que apenas escolher o máximo, 

405
00:23:45,340 --> 00:23:48,349
no sentido de que quando outros valores são igualmente grandes, 

406
00:23:48,349 --> 00:23:52,299
eles também recebem um peso significativo na distribuição e tudo muda continuamente 

407
00:23:52,299 --> 00:23:54,650
à medida que você varia continuamente as entradas.

408
00:23:55,130 --> 00:23:59,776
Em algumas situações, como quando ChatGPT está usando esta distribuição para criar uma 

409
00:23:59,776 --> 00:24:04,316
próxima palavra, há espaço para um pouco de diversão extra adicionando um pouco mais 

410
00:24:04,316 --> 00:24:08,910
de tempero a esta função, com uma constante t lançada no denominador desses expoentes.

411
00:24:09,550 --> 00:24:14,187
Chamamos-lhe temperatura, uma vez que se assemelha vagamente ao papel da temperatura 

412
00:24:14,187 --> 00:24:18,005
em certas equações termodinâmicas, e o efeito é que quando t é maior, 

413
00:24:18,005 --> 00:24:22,752
dá-se mais peso aos valores mais baixos, o que significa que a distribuição é um pouco 

414
00:24:22,752 --> 00:24:27,225
mais uniforme, e se t for menor, então os valores maiores dominarão de forma mais 

415
00:24:27,225 --> 00:24:31,971
agressiva, onde, no extremo, definir t igual a zero significa que todo o peso vai para 

416
00:24:31,971 --> 00:24:32,790
o valor máximo.

417
00:24:33,470 --> 00:24:38,590
Por exemplo, farei com que o GPT-3 gere uma história com o texto inicial, 

418
00:24:38,590 --> 00:24:42,950
era uma vez A, mas usarei temperaturas diferentes em cada caso.

419
00:24:43,630 --> 00:24:48,060
Temperatura zero significa que sempre vem com a palavra mais previsível, 

420
00:24:48,060 --> 00:24:52,370
e o que você obtém acaba sendo um derivado banal de Cachinhos Dourados.

421
00:24:53,010 --> 00:24:56,971
Uma temperatura mais alta dá a chance de escolher palavras menos prováveis, 

422
00:24:56,971 --> 00:24:57,910
mas traz um risco.

423
00:24:58,230 --> 00:25:01,342
Neste caso, a história começa de forma mais original, 

424
00:25:01,342 --> 00:25:06,010
sobre um jovem web artista da Coreia do Sul, mas rapidamente degenera em absurdo.

425
00:25:06,950 --> 00:25:10,830
Tecnicamente falando, a API não permite escolher uma temperatura maior que 2.

426
00:25:11,170 --> 00:25:15,369
Não há razão matemática para isso, é apenas uma restrição arbitrária imposta 

427
00:25:15,369 --> 00:25:19,350
para evitar que sua ferramenta seja vista gerando coisas absurdas demais.

428
00:25:19,870 --> 00:25:24,253
Então, se você está curioso, a forma como esta animação está realmente funcionando é que 

429
00:25:24,253 --> 00:25:27,651
estou pegando os 20 próximos tokens mais prováveis que o GPT-3 gera, 

430
00:25:27,651 --> 00:25:31,985
que parece ser o máximo que eles me darão, e então ajusto as probabilidades com base em 

431
00:25:31,985 --> 00:25:32,970
um expoente de 1 5º.

432
00:25:33,130 --> 00:25:37,504
Como outro jargão, da mesma forma que você pode chamar os componentes da saída desta 

433
00:25:37,504 --> 00:25:41,827
função de probabilidades, as pessoas geralmente se referem às entradas como logits, 

434
00:25:41,827 --> 00:25:46,150
ou algumas pessoas dizem logits, algumas pessoas dizem logits, eu vou dizer logits .

435
00:25:46,530 --> 00:25:48,925
Então, por exemplo, quando você alimenta algum texto, 

436
00:25:48,925 --> 00:25:52,296
você tem todas essas incorporações de palavras fluindo pela rede e faz essa 

437
00:25:52,296 --> 00:25:54,691
multiplicação final com a matriz de não incorporação, 

438
00:25:54,691 --> 00:25:58,373
o pessoal de aprendizado de máquina se referiria aos componentes dessa saída bruta 

439
00:25:58,373 --> 00:26:01,390
e não normalizada como os logits para a próxima previsão de palavra.

440
00:26:03,330 --> 00:26:06,776
Grande parte do objetivo deste capítulo foi estabelecer as bases para 

441
00:26:06,776 --> 00:26:10,370
a compreensão do mecanismo de atenção, estilo Karate Kid cera sobre cera.

442
00:26:10,850 --> 00:26:15,327
Veja bem, se você tem uma forte intuição para incorporações de palavras, para softmax, 

443
00:26:15,327 --> 00:26:18,107
para como os produtos escalares medem a similaridade, 

444
00:26:18,107 --> 00:26:22,379
e também a premissa subjacente de que a maioria dos cálculos deve se parecer com a 

445
00:26:22,379 --> 00:26:26,085
multiplicação de matrizes com matrizes cheias de parâmetros ajustáveis, 

446
00:26:26,085 --> 00:26:30,717
então entender a atenção O mecanismo, esta peça fundamental de todo o boom moderno da IA, 

447
00:26:30,717 --> 00:26:32,210
deve ser relativamente suave.

448
00:26:32,650 --> 00:26:34,510
Para isso, junte-se a mim no próximo capítulo.

449
00:26:36,390 --> 00:26:38,939
Enquanto estou publicando isto, um rascunho do próximo capítulo 

450
00:26:38,939 --> 00:26:41,210
está disponível para revisão pelos apoiadores do Patreon.

451
00:26:41,770 --> 00:26:44,667
Uma versão final deve estar disponível ao público em uma ou duas semanas, 

452
00:26:44,667 --> 00:26:47,370
geralmente depende de quanto eu acabo mudando com base nessa revisão.

453
00:26:47,810 --> 00:26:51,427
Enquanto isso, se você quiser chamar a atenção e quiser ajudar um pouco o canal, 

454
00:26:51,427 --> 00:26:52,410
ele está aí esperando.

