[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "As iniciais GPT significam Transformador Generativo Pré-treinado.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "A primeira palavra é bastante direta, são bots que geram novo texto.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Pré-treinado refere-se a como o modelo passou por um processo de aprendizagem a partir de uma enorme quantidade de dados, e o prefixo sugere que é possível ajustá-lo para tarefas específicas com treinamento adicional.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Mas a última palavra é a verdadeira peça chave.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Um transformador é um tipo específico de rede neural, um modelo de aprendizado de máquina, e é a principal invenção na base do atual crescimento de IA.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "O que quero fazer com este vídeo e com os capítulos seguintes é apresentar uma explicação baseada em gráficos sobre o que realmente acontece dentro de um transformador.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Vamos seguir os dados que fluem por ele e seguir passo a passo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Existem muitos tipos diferentes de modelos que você pode construir usando transformadores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Alguns modelos captam áudio e produzem uma transcrição.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Essa frase vem de um modelo que faz o contrário, produzindo fala sintética apenas a partir do texto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Todas aquelas ferramentas que conquistaram o mundo em 2022, como DALL-E e Midjourney, que captam uma descrição de texto e produzem uma imagem, são baseadas em transformadores.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Mesmo que eu não consiga explicar pro modelo o que uma criatura \"pi\" deveria ser, ainda estou surpreso que esse tipo de coisa seja remotamente possível.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "E o transformador original introduzido em 2017 pelo Google foi inventado para o caso de uso específico de tradução de texto de um idioma para outro.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Mas a variante na qual vamos nos concentrar, que é o tipo que dá base a ferramentas como o ChatGPT, será um modelo treinado para captar um pedaço de texto, talvez até acompanhado de algumas imagens ou sons associados, e produzir uma previsão para o que vem a seguir na passagem.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Essa previsão assume a forma de uma distribuição de probabilidade sobre muitos trechos diferentes de texto que podem se seguir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "À primeira vista, você pode pensar que prever a próxima palavra parece um objetivo muito diferente de gerar um novo texto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Mas quando você tiver um modelo de previsão como este, uma coisa simples que você pode tentar para gerar um trecho de texto mais longo é fornecer um trecho inicial para trabalhar, fazer com que ele pegue uma amostra aleatória da distribuição que acabou de gerar e anexe essa amostra ao texto e, em seguida, execute todo o processo novamente para fazer uma nova previsão com base no novo texto, incluindo o que acabou de ser adicionado.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Não sei o que você acha, mas realmente não parece que isso deveria funcionar.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Nesta animação, por exemplo, estou executando o GPT-2 em meu laptop e fazendo com que ele preveja e experimente repetidamente o próximo pedaço de texto para gerar uma história baseada no texto inicial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "A história não faz muito sentido.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Mas se eu trocá-lo por chamadas de API para GPT-3, que é o mesmo modelo básico, só que muito maior, de repente, quase magicamente, teremos uma história sensata, que até parece inferir que uma criatura pi viveria em um terra da matemática e da computação.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Esse processo de predição e amostragem repetidas é essencialmente o que acontece quando você interage com o ChatGPT ou qualquer um desses outros grandes modelos de linguagem e os vê produzindo uma palavra por vez.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Na verdade, um recurso que eu gostaria muito é a capacidade de ver a distribuição subjacente de cada nova palavra escolhida.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Vamos começar com uma prévia de alto nível de como os dados fluem através de um transformador.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Passaremos muito mais tempo explicando, interpretando e expandindo os detalhes de cada etapa, mas em linhas gerais, quando um desses chatbots gera uma determinada palavra, aqui está o que está acontecendo nos bastidores.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Primeiro, a entrada é dividida em vários pequenos pedaços.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Essas peças são chamadas de tokens e, no caso de texto, tendem a ser palavras ou pequenos pedaços de palavras ou outras combinações de caracteres comuns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Se imagens ou som estiverem envolvidos, os tokens poderão ser pequenos pedaços dessa imagem ou pequenos pedaços desse som.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Cada um desses tokens é então associado a um vetor, ou seja, alguma lista de números, que tem como objetivo codificar de alguma forma o significado daquela peça.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Se você pensar nesses vetores como coordenadas em algum espaço de dimensão muito elevada, palavras com significados semelhantes tendem a se situar em vetores próximos nesse espaço.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Essa sequência de vetores passa então por uma operação conhecida como bloco de atenção, e isso permite que os vetores se comuniquem entre si e passem informações para atualizar seus valores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Por exemplo, o significado da palavra \"modelo\" na frase \"modelo de aprendizado de máquina\" é diferente de seu significado na frase \"modelo de desfile\".",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "O bloco de atenção é responsável por descobrir quais palavras no contexto são relevantes para atualizar os significados de quais outras palavras e como exatamente esses significados devem ser atualizados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "E, novamente, sempre que uso a palavra \"significado\", isso está de alguma forma totalmente codificado nas entradas desses vetores.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Depois disso, esses vetores passam por um tipo diferente de operação e, dependendo da fonte que você está lendo, isso será chamado de perceptron multicamadas ou talvez de camada feed-forward.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "E aqui os vetores não conversam entre si, todos fazem a mesma operação em paralelo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "E embora esse bloco seja um pouco mais difícil de interpretar, mais tarde falaremos sobre como a etapa é um pouco como fazer uma longa lista de perguntas sobre cada vetor e depois atualizá-las com base nas respostas a essas perguntas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Todas as operações em ambos os blocos parecem uma pilha gigante de multiplicações de matrizes, e nossa principal tarefa será entender como ler as matrizes subjacentes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Estou ignorando alguns detalhes sobre certas etapas de normalização que acontecem no meio, mas, afinal, esta é uma visualização de alto nível.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Depois disso, o processo essencialmente se repete, você vai e volta entre blocos de atenção e blocos perceptron multicamadas, até que no final a esperança é que todo o significado essencial da passagem tenha sido de alguma forma incorporado ao último vetor na sequência.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Em seguida, realizamos uma determinada operação nesse último vetor que produz uma distribuição de probabilidade sobre todos os tokens possíveis, todos os pequenos pedaços de texto possíveis que podem vir a seguir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "E como falei, uma vez que você tem uma ferramenta que prevê o que vem a seguir com base em um trecho de texto, você pode alimentá-la com um pouco de texto inicial e fazer com que ela jogue repetidamente esse jogo de prever o que vem a seguir, amostrando a distribuição, anexando e depois repetindo indefinidamente.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Alguns de vocês que conhecem podem se lembrar de como antes do ChatGPT entrar em cena, era assim que se pareciam as primeiras demos do GPT-3: você faria com que ele preenchesse automaticamente histórias e ensaios com base em um trecho inicial.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Para transformar uma ferramenta como essa em um chatbot, o ponto de partida mais fácil é ter um pequeno texto que estabeleça a configuração de um usuário interagindo com um prestativo assistente de IA, o que você chamaria de prompt do sistema, e então você usaria o a pergunta ou prompt inicial do usuário como a primeira parte do diálogo, e então você começa a prever o que um assistente de IA tão prestativo diria em resposta.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Há mais a dizer sobre uma etapa do treinamento necessária para que isso funcione bem, mas por alto essa é a ideia.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "Neste capítulo, vamos expandir os detalhes do que acontece bem no início da rede, no final da rede, e também quero passar muito tempo revisando alguns importantes conhecimentos básicos, coisas que seriam instintivas para qualquer engenheiro de aprendizado de máquina na época em que os transformadores surgiram.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Se você se sentir confortável com esse conhecimento prévio e um pouco impaciente, fique à vontade para pular para o próximo capítulo, que se concentrará nos bloqueios de atenção, geralmente considerados o coração do transformador.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Depois disso, quero falar mais sobre esses blocos perceptron multicamadas, como funciona o treinamento, e uma série de outros detalhes que foram ignorados até esse ponto.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Para um contexto mais amplo, esses vídeos são acréscimos a uma minissérie sobre aprendizado profundo, e tudo bem se você não assistiu os anteriores, acho que você pode assistir fora de ordem, mas antes de mergulhar especificamente nos transformadores, achoque  vale a pena garantir que estamos alinhados quanto à premissa básica e a estrutura do aprendizado profundo.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Correndo o risco de afirmar o óbvio, esta é uma abordagem de aprendizado de máquina, que descreve qualquer modelo em que você usa dados para determinar de alguma forma como um modelo se comporta.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "O que quero dizer com isso é, digamos que você queira uma função que receba uma imagem e produza um rótulo descrevendo-a, ou nosso exemplo de previsão da próxima palavra dada uma passagem de texto, ou qualquer outra tarefa que pareça exigir algum elemento de intuição e reconhecimento de padrões.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Hoje em dia, quase não damos valor a isso, mas a ideia do aprendizado de máquina é que, em vez de tentar definir explicitamente um procedimento sobre como realizar essa tarefa em código, que é o que as pessoas teriam feito nos primeiros dias da IA, você configure uma estrutura muito flexível com parâmetros ajustáveis, como um monte de botões e indicadores, e então, de alguma forma, use muitos exemplos de como a saída deve ser para uma determinada entrada para ajustar os valores desses parâmetros para imitar esse comportamento.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Por exemplo, talvez a forma mais simples de aprendizado de máquina seja a regressão linear, onde suas entradas e saídas são números únicos, algo como a metragem quadrada de uma casa e seu preço, e você deseja encontrar uma linha que melhor se ajuste através desses dados, para prever os preços futuros de imóveis.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Essa linha é descrita por dois parâmetros contínuos, como a inclinação e a interceptação y, e o objetivo da regressão linear é determinar esses parâmetros para que correspondam melhor aos dados.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Escusado será dizer que os modelos de aprendizagem profunda ficam muito mais complicados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "O GPT-3, por exemplo, não possui dois, mas 175 bilhões de parâmetros.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Mas o problema é o seguinte: não é certo que você pode criar um modelo gigante com um grande número de parâmetros sem que ele superajuste os dados de treinamento ou seja completamente intratável para treinar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "O aprendizado profundo descreve uma classe de modelos que, nas últimas décadas, provou ser escalonável notavelmente bem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "O que os unifica é o mesmo algoritmo de treinamento, chamado backpropagation, e o contexto que quero que você tenha à medida que avançamos é que, para que esse algoritmo de treinamento funcione bem em escala, esses modelos precisam seguir um determinado formato específico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Se você conhece esse formato, isso ajuda a explicar muitas das escolhas de como um transformador processa a linguagem, que de outra forma correm o risco de parecerem arbitrárias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Primeiro, qualquer que seja o modelo que você esteja criando, a entrada deverá ser formatada como um array de números reais.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Isso pode significar uma lista de números, pode ser uma matriz bidimensional ou, muitas vezes, você lida com matrizes de dimensões superiores, onde o termo geral usado é tensor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Muitas vezes você pensa que os dados de entrada são progressivamente transformados em muitas camadas distintas, onde, novamente, cada camada é sempre estruturada como algum tipo de matriz de números reais, até chegar a uma camada final que você considera a saída.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Por exemplo, a camada final em nosso modelo de processamento de texto é uma lista de números que representa a distribuição de probabilidade para todos os próximos tokens possíveis.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "No aprendizado profundo, esses parâmetros do modelo são quase sempre chamados de pesos, e isso ocorre porque uma característica importante desses modelos é que a única maneira pela qual esses parâmetros interagem com os dados que estão sendo processados é por meio de somas ponderadas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Você também espalha algumas funções não lineares, mas elas não dependem de parâmetros.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Porém, normalmente, em vez de ver as somas ponderadas todas nuas e escritas explicitamente assim, você as encontrará empacotadas juntas como vários componentes em um produto vetorial de matriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Isso equivale a dizer a mesma coisa: se você pensar em como funciona a multiplicação de vetores de matrizes, cada componente na saída parece uma soma ponderada.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Muitas vezes é conceitualmente mais limpo para você e para mim pensar em matrizes preenchidas com parâmetros ajustáveis que transformam vetores extraídos dos dados que estão sendo processados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Por exemplo, esses 175 mil milhões de pesos no GPT-3 estão organizados em pouco menos de 28.000 matrizes distintas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Essas matrizes, por sua vez, se enquadram em oito categorias diferentes, e o que você e eu faremos é percorrer cada uma dessas categorias para entender o que esse tipo faz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "À medida que avançamos, acho divertido fazer referência aos números específicos do GPT-3 para contar exatamente de onde vêm esses 175 bilhões.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Mesmo que hoje existam modelos maiores e melhores, este tem um certo charme como modelo de linguagem grande para realmente captar a atenção do mundo fora das comunidades de ML.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Além disso, na prática, as empresas tendem a manter a boca fechada em relação aos números específicos das redes mais modernas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Eu só quero definir o cenário, que quando você espia os bastidores para ver o que acontece dentro de uma ferramenta como o ChatGPT, quase toda a computação real parece uma multiplicação de vetores de matrizes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Há um certo risco de se perder no mar de bilhões de números, mas você deve traçar uma distinção bem nítida em sua mente entre os pesos do modelo, que sempre colorirei em azul ou vermelho, e os dados sendo processado, que sempre colorirei de cinza.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Os pesos são o cérebro real, são as coisas aprendidas durante o treinamento e determinam como ele se comporta.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Os dados que estão sendo processados simplesmente codificam qualquer entrada específica inserida no modelo para uma determinada execução, como um exemplo de trecho de texto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Com tudo isso como base, vamos nos aprofundar na primeira etapa deste exemplo de processamento de texto, que é dividir a entrada em pequenos pedaços e transformar esses pedaços em vetores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Mencionei como esses pedaços são chamados de tokens, que podem ser pedaços de palavras ou pontuação, mas de vez em quando neste capítulo e especialmente no próximo, gostaria apenas de fingir que estão divididos de forma mais clara em palavras.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Como nós, humanos, pensamos em palavras, isso tornará muito mais fácil fazer referência a pequenos exemplos e esclarecer cada etapa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "O modelo possui um vocabulário predefinido, uma lista de todas as palavras possíveis, digamos 50.000 delas, e a primeira matriz que encontraremos, conhecida como matriz de incorporação, possui uma única coluna para cada uma dessas palavras.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Essas colunas são o que determina em que vetor cada palavra se transforma nessa primeira etapa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Nós a rotulamos como Nós, e como todas as matrizes que vemos, seus valores começam aleatórios, mas serão aprendidos com base nos dados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Transformar palavras em vetores era uma prática comum no aprendizado de máquina muito antes dos transformadores, mas é um pouco estranho se você nunca viu isso antes e estabelece a base para tudo o que se segue, então vamos reservar um momento para nos familiarizarmos com isso.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Muitas vezes chamamos isso de incorporação de palavra, o que convida você a pensar nesses vetores de forma muito geométrica, como pontos em algum espaço de alta dimensão.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Visualizar uma lista de três números como coordenadas para pontos no espaço 3D não seria problema, mas os embeddings de palavras tendem a ter dimensões muito superiores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "No GPT-3 eles têm 12.288 dimensões e, como você verá, é importante trabalhar em um espaço que tenha muitas direções distintas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Da mesma forma que você poderia pegar uma fatia bidimensional através de um espaço 3D e projetar todos os pontos nessa fatia, para animar os embeddings de palavras que um modelo simples está me proporcionando, farei uma coisa análoga escolhendo uma fatia tridimensional através deste espaço de dimensão muito alta e projetando a palavra vetores sobre ela e exibindo os resultados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "A grande ideia aqui é que, à medida que um modelo ajusta e ajusta seus pesos para determinar exatamente como as palavras são incorporadas como vetores durante o treinamento, ele tende a se estabelecer em um conjunto de incorporações onde as direções no espaço têm uma espécie de significado semântico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Para o modelo simples de palavra para vetor que estou executando aqui, se eu pesquisar todas as palavras cujas incorporações são mais próximas de torre, você notará como todas elas parecem dar vibrações de torre muito semelhantes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "E se você quiser pegar um pouco de Python e brincar em casa, este é o modelo específico que estou usando para fazer as animações.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Não é um transformador, mas é suficiente para ilustrar a ideia de que as direções no espaço podem ter significado semântico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Um exemplo muito clássico disso é como se você pegar a diferença entre os vetores para mulher e homem, algo que você visualizaria como um pequeno vetor conectando a ponta de um à ponta do outro, é muito semelhante à diferença entre rei e rainha.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Então, digamos que você não conhecesse a palavra para uma monarca feminina, você poderia encontrá-la tomando rei, adicionando a direção mulher-homem e procurando as incorporações mais próximas desse ponto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Pelo menos, mais ou menos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Apesar de este ser um exemplo clássico para o modelo com o qual estou brincando, a verdadeira incorporação da rainha está na verdade um pouco mais distante do que isso sugere, provavelmente porque a forma como a rainha é usada nos dados de treinamento não é apenas uma versão feminina do rei.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Quando brinquei, as relações familiares pareciam ilustrar muito melhor a ideia.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "A questão é que durante o treinamento o modelo achou vantajoso escolher embeddings de forma que uma direção neste espaço codificasse as informações de gênero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Outro exemplo é que se pegarmos na incorporação da Itália, e subtrairmos a incorporação da Alemanha, e adicionarmos isso à incorporação de Hitler, obtemos algo muito próximo da incorporação de Mussolini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "É como se o modelo tivesse aprendido a associar algumas direções à italianidade e outras aos líderes dos eixos da Segunda Guerra Mundial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Talvez meu exemplo favorito nesse sentido seja como, em alguns modelos, se você pegar a diferença entre a Alemanha e o Japão e adicioná-la ao sushi, você acaba muito próximo da salsicha.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Também ao jogar este jogo de encontrar os vizinhos mais próximos, fiquei satisfeito ao ver o quão próxima Kat estava tanto da fera quanto do monstro.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Um pouco de intuição matemática que é útil ter em mente, especialmente para o próximo capítulo, é como o produto escalar de dois vetores pode ser pensado como uma forma de medir o quão bem eles se alinham.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Computacionalmente, os produtos escalares envolvem a multiplicação de todos os componentes correspondentes e depois a adição dos resultados, o que é bom, uma vez que grande parte da nossa computação tem que parecer somas ponderadas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Geometricamente, o produto escalar é positivo quando os vetores apontam em direções semelhantes, é zero se forem perpendiculares e é negativo sempre que apontam em direções opostas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Por exemplo, digamos que você estava brincando com este modelo e levanta a hipótese de que a incorporação de gatos menos gato pode representar uma espécie de direção de pluralidade neste espaço.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Para testar isso, vou pegar esse vetor e calcular seu produto escalar em relação às incorporações de certos substantivos singulares e compará-lo com os produtos escalares com os substantivos plurais correspondentes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Se você brincar com isso, notará que os plurais realmente parecem dar consistentemente valores mais altos do que os singulares, indicando que eles se alinham mais com essa direção.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Também é divertido como se você pegar esse produto escalar com as incorporações das palavras 1, 2, 3 e assim por diante, eles fornecem valores crescentes, então é como se pudéssemos medir quantitativamente o quão plural o modelo encontra uma determinada palavra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Novamente, os detalhes de como as palavras são incorporadas são aprendidos por meio de dados.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Esta matriz de incorporação, cujas colunas nos dizem o que acontece com cada palavra, é a primeira pilha de pesos do nosso modelo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Usando os números GPT-3, o tamanho do vocabulário é especificamente 50.257 e, novamente, tecnicamente não consiste em palavras em si, mas em tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "A dimensão de incorporação é 12.288, e multiplicar isso nos diz que isso consiste em cerca de 617 milhões de pesos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Vamos em frente e adicionar isto a uma contagem contínua, lembrando que no final devemos contar até 175 mil milhões.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "No caso dos transformadores, você realmente quer pensar nos vetores neste espaço de incorporação como não apenas representando palavras individuais.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Por um lado, eles também codificam informações sobre a posição daquela palavra, sobre a qual falaremos mais tarde, mas o mais importante é que você deve pensar neles como tendo a capacidade de absorver o contexto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Um vetor que começou sua vida como a incorporação da palavra rei, por exemplo, pode ser progressivamente puxado e puxado por vários blocos nesta rede, de modo que, no final, aponte para uma direção muito mais específica e matizada que de alguma forma codifica isso. foi um rei que viveu na Escócia e que alcançou seu posto após assassinar o rei anterior, e que está sendo descrito na linguagem shakespeariana.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Pense em sua própria compreensão de uma determinada palavra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "O significado dessa palavra é claramente informado pelo ambiente, e às vezes isso inclui o contexto de uma longa distância, portanto, ao montar um modelo que tenha a capacidade de prever qual palavra vem a seguir, o objetivo é de alguma forma capacitá-lo para incorporar o contexto eficientemente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Para ficar claro, nessa primeira etapa, quando você cria a matriz de vetores com base no texto de entrada, cada um deles é simplesmente retirado da matriz de incorporação, então inicialmente cada um só pode codificar o significado de uma única palavra sem qualquer entrada de seu entorno.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Mas você deve pensar no objetivo principal desta rede pela qual ela flui como sendo permitir que cada um desses vetores absorva um significado que é muito mais rico e específico do que meras palavras individuais poderiam representar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "A rede só pode processar um número fixo de vetores por vez, conhecido como tamanho de contexto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Para GPT-3, ele foi treinado com um tamanho de contexto de 2.048, de modo que os dados que fluem pela rede sempre se parecem com esta matriz de 2.048 colunas, cada uma com 12.000 dimensões.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Esse tamanho de contexto limita a quantidade de texto que o transformador pode incorporar ao fazer uma previsão da próxima palavra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "É por isso que longas conversas com certos chatbots, como as primeiras versões do ChatGPT, muitas vezes davam a sensação de que o bot estava perdendo o fio da conversa à medida que você continuava por muito tempo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Entraremos nos detalhes da atenção no devido tempo, mas, avançando, quero falar por um minuto sobre o que acontece no final.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Lembre-se, a saída desejada é uma distribuição de probabilidade sobre todos os tokens que podem vir a seguir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Por exemplo, se a última palavra for Professor, e o contexto incluir palavras como Harry Potter, e imediatamente antes virmos o professor menos favorito, e também se você me der alguma margem de manobra, deixando-me fingir que os tokens simplesmente parecem palavras completas, então uma rede bem treinada que tivesse acumulado conhecimento sobre Harry Potter provavelmente atribuiria um número alto à palavra Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Isso envolve duas etapas diferentes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "A primeira é usar outra matriz que mapeie o último vetor naquele contexto em uma lista de 50.000 valores, um para cada token do vocabulário.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Depois, há uma função que normaliza isso em uma distribuição de probabilidade, chamada Softmax e falaremos mais sobre isso em apenas um segundo, mas antes disso pode parecer um pouco estranho usar apenas esta última incorporação para fazer uma previsão, quando afinal, nessa última etapa, existem milhares de outros vetores na camada, com seus próprios significados ricos em contexto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Isso tem a ver com o fato de que no processo de treinamento acaba sendo muito mais eficiente usar cada um desses vetores na camada final para fazer simultaneamente uma previsão do que viria imediatamente depois dele.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Há muito mais a ser dito sobre o treinamento mais tarde, mas eu só quero destacar isso agora.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Esta matriz é chamada de matriz de desincorporação e damos a ela o rótulo WU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Novamente, como todas as matrizes de peso que vemos, suas entradas começam aleatoriamente, mas são aprendidas durante o processo de treinamento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Mantendo a pontuação em nossa contagem total de parâmetros, esta matriz de desincorporação tem uma linha para cada palavra no vocabulário e cada linha tem o mesmo número de elementos que a dimensão de incorporação.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "É muito semelhante à matriz de incorporação, apenas com a ordem trocada, por isso adiciona outros 617 milhões de parâmetros à rede, o que significa que a nossa contagem até agora é de pouco mais de um bilhão, uma fração pequena, mas não totalmente insignificante, dos 175 bilhões que temos. vou acabar com o total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "Como última minilição deste capítulo, quero falar mais sobre essa função softmax, pois ela aparece novamente para nós quando mergulhamos nos bloqueios de atenção.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "A ideia é que se você deseja que uma sequência de números atue como uma distribuição de probabilidade, digamos, uma distribuição sobre todas as próximas palavras possíveis, então cada valor deve estar entre 0 e 1, e você também precisa que a soma de todos eles seja 1 .",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "No entanto, se você estiver jogando o jogo de aprendizagem em que tudo o que você faz se parece com uma multiplicação de matrizes e vetores, os resultados obtidos por padrão não obedecem a isso.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Os valores costumam ser negativos ou muito maiores que 1 e quase certamente não somam 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax é a maneira padrão de transformar uma lista arbitrária de números em uma distribuição válida, de forma que os valores maiores fiquem mais próximos de 1 e os valores menores fiquem muito próximos de 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Isso é tudo que você realmente precisa saber.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Mas se você estiver curioso, a forma como funciona é primeiro elevar e à potência de cada um dos números, o que significa que agora você tem uma lista de valores positivos, e então você soma todos esses valores positivos e divide cada termo por essa soma, o que o normaliza em uma lista de soma 1.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Você notará que se um dos números na entrada for significativamente maior que o resto, então na saída o termo correspondente domina a distribuição, portanto, se você estivesse amostrando, quase certamente escolheria apenas a entrada maximizadora.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Mas é mais suave do que apenas escolher o máximo, no sentido de que quando outros valores são igualmente grandes, eles também recebem um peso significativo na distribuição e tudo muda continuamente à medida que você varia continuamente as entradas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "Em algumas situações, como quando ChatGPT está usando esta distribuição para criar uma próxima palavra, há espaço para um pouco de diversão extra adicionando um pouco mais de tempero a esta função, com uma constante t lançada no denominador desses expoentes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Chamamos de temperatura, pois parece vagamente com o papel da temperatura em certas equações termodinâmicas, e o efeito é que quando t é maior, dá-se mais peso aos valores mais baixos, o que significa que a distribuição é um pouco mais uniforme, e se t for menor, então os valores maiores dominarão de forma mais agressiva, onde, no extremo, definir t igual a zero significa que todo o peso vai para o valor máximo.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Por exemplo, farei com que o GPT-3 gere uma história com o texto inicial: \"era uma vez uma\", mas usarei temperaturas diferentes em cada caso.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Temperatura zero significa que sempre vem com a palavra mais previsível, e o que você recebe acaba sendo um derivado banal de Cachinhos Dourados.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Uma temperatura mais alta dá a chance de escolher palavras menos prováveis, mas traz um risco.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "Neste caso, a história começa de forma mais original, sobre um jovem web-artista da Coreia do Sul, mas rapidamente decai ao absurdo.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Tecnicamente falando, a API não permite escolher uma temperatura maior que 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Não há razão matemática para isso, é apenas uma restrição arbitrária imposta para evitar que essa ferramenta seja vista gerando coisas absurdas demais.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Então, se você está curioso, para essa animação funcionar, peguei os 20 próximos tokens mais prováveis que o GPT-3 gera, que parece ser o máximo que eles me darão, e então ajusto as probabilidades com base em um expoente de 1/5.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Como outro jargão, da mesma forma que você pode chamar os componentes da saída desta função de probabilidades, geralmente se refere às entradas como logits, ou algumas pessoas dizem logits, algumas pessoas dizem logits, eu vou dizer logits.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Então, por exemplo, quando você insere um texto, todas essas incorporações de palavras fluem pela rede, e se faz essa multiplicação final com a matriz de desincorporação, o pessoal de aprendizado de máquina se referiria aos componentes dessa saída bruta e não normalizada como os logits para a próxima previsão de palavra.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Grande parte do objetivo deste capítulo foi estabelecer as bases para a compreensão do mecanismo de atenção, estilo bota-casaco-tira-casado de Karate Kid.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Se você tem uma forte intuição para incorporações de palavras, para softmax, para como os produtos escalares medem a similaridade, e também a premissa subjacente de que a maioria dos cálculos deve se parecer com a multiplicação de matrizes com matrizes cheias de parâmetros ajustáveis, então entender o mecanismo de atenção, esta peça fundamental de todo o boom moderno da IA, deve ser relativamente simples.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Para isso, junte-se a mim no próximo capítulo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Enquanto lanço este vídeo, um rascunho do próximo capítulo está disponível para revisão pelos apoiadores do Patreon.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Uma versão final deve estar disponível ao público em uma ou duas semanas, geralmente depende de quanto eu acabo mudando com base nessa revisão.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Enquanto isso, se você quiser chamar a atenção e quiser ajudar um pouco o canal, ele está aí esperando.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
