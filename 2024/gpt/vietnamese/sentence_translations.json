[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Các chữ cái đầu GPT là viết tắt của Máy biến đổi Sáng tạo được Đào tạo trước.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Từ đầu tiên đó khá đơn giản, đây là những bot tạo ra văn bản mới.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Được đào tạo trước đề cập đến cách mô hình trải qua quá trình học từ một lượng lớn dữ liệu và tiền tố ám chỉ rằng có nhiều chỗ hơn để tinh chỉnh mô hình trong các nhiệm vụ cụ thể bằng cách đào tạo bổ sung.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Nhưng lời cuối cùng, đó mới là phần quan trọng thực sự.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Transformer (hay máy biến đổi) là một loại mạng thần kinh cụ thể, một mô hình học máy và là phát minh cốt lõi tạo nền tảng cho sự bùng nổ AI hiện nay.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "Điều tôi muốn làm với video này và các chương tiếp theo là giải thích bằng hình ảnh về những gì thực sự xảy ra bên trong Transformer.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Chúng ta sẽ theo dõi dữ liệu chảy qua nó và thực hiện từng bước một.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Có nhiều loại mô hình khác nhau mà bạn có thể xây dựng bằng cách sử dụng Transformer.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Một số kiểu máy tiếp nhận âm thanh và tạo ra bản ghi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Câu này xuất phát từ một mô hình ngược lại, tạo ra lời nói tổng hợp chỉ từ văn bản.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Tất cả những công cụ đã gây bão trên toàn thế giới vào năm 2022 như Dolly và Midjourney có chức năng mô tả văn bản và tạo ra hình ảnh đều dựa trên Transformer.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Ngay cả khi tôi không thể hiểu được sinh vật Pi là gì, tôi vẫn rất ngạc nhiên rằng loại yêu cầu này thậm chí còn có thể khả thi từ xa.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "Và Transformer ban đầu được Google giới thiệu vào năm 2017 đã được phát minh cho trường hợp sử dụng cụ thể là dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Nhưng biến thể mà bạn và tôi sẽ tập trung vào, loại làm nền tảng cho các công cụ như ChatGPT, sẽ là một mô hình được đào tạo để tiếp nhận một đoạn văn bản, thậm chí có thể có một số hình ảnh hoặc âm thanh xung quanh đi kèm và đưa ra dự đoán. cho những gì xảy ra tiếp theo trong đoạn văn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Dự đoán đó có dạng phân bố xác suất trên nhiều đoạn văn bản khác nhau có thể theo sau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "Thoạt nhìn, bạn có thể nghĩ rằng việc dự đoán từ tiếp theo giống như một mục tiêu rất khác với việc tạo văn bản mới.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Nhưng khi bạn có một mô hình dự đoán như thế này, một điều đơn giản là bạn tạo một đoạn văn bản dài hơn là cung cấp cho nó một đoạn mã ban đầu để làm việc, yêu cầu nó lấy một mẫu ngẫu nhiên từ phân phối mà nó vừa tạo, nối mẫu đó vào văn bản, rồi chạy lại toàn bộ quá trình để đưa ra dự đoán mới dựa trên tất cả văn bản mới, bao gồm cả nội dung vừa được thêm vào.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Tôi không biết bạn thế nào, nhưng tôi thực sự không cảm thấy điều này thực sự hiệu quả.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Ví dụ: trong hoạt ảnh này, tôi đang chạy GPT-2 trên máy tính xách tay của mình và để nó liên tục dự đoán cũng như lấy mẫu đoạn văn bản tiếp theo để tạo một câu chuyện dựa trên văn bản gốc.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "Câu chuyện thực sự không có nhiều ý nghĩa như vậy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Nhưng thay vào đó, nếu tôi đổi nó lấy các lệnh gọi API sang GPT-3, mô hình cơ bản tương tự, chỉ lớn hơn nhiều, đột nhiên gần như kỳ diệu, ta có được câu chuyện hợp lý, một câu chuyện thậm chí dường như suy ra rằng một sinh vật pi sẽ sống trong một vùng đất của toán học và tính toán.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Quá trình dự đoán và lấy mẫu lặp lại ở đây về cơ bản là những gì đang xảy ra khi bạn tương tác với ChatGPT hoặc bất kỳ mô hình ngôn ngữ lớn nào khác và bạn thấy chúng tạo ra từng từ một.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "Trên thực tế, một tính năng mà tôi rất thích là khả năng xem sự phân bổ cơ bản cho mỗi từ mới mà nó chọn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Hãy bắt đầu mọi thứ bằng bản xem trước ở mức rất cao về cách dữ liệu truyền qua Transformer.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Ta sẽ dành nhiều thời gian hơn để thúc đẩy, diễn giải và mở rộng chi tiết của từng bước, nhưng nói một cách tổng quát, khi một trong những chatbot này tạo ra một từ nhất định, đây là những gì đang diễn ra.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Đầu tiên, đầu vào được chia thành nhiều phần nhỏ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Những phần này được gọi là mã thông báo và trong trường hợp văn bản, chúng có xu hướng là các từ hoặc các đoạn từ nhỏ hoặc các tổ hợp ký tự phổ biến khác.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Nếu có liên quan đến hình ảnh hoặc âm thanh thì mã thông báo có thể là những mảng nhỏ của hình ảnh đó hoặc những đoạn nhỏ của âm thanh đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Mỗi một trong số các mã thông báo này sau đó được liên kết với một vectơ, nghĩa là một số danh sách các số, nhằm mục đích mã hóa bằng cách nào đó ý nghĩa của phần đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Nếu bạn coi các vectơ này là tọa độ trong một không gian có chiều rất cao, thì các từ có ý nghĩa tương tự có xu hướng nằm trên các vectơ gần nhau trong không gian đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Sau đó, chuỗi vectơ này sẽ đi qua một hoạt động được gọi là khối chú ý và điều này cho phép các vectơ giao tiếp với nhau và truyền thông tin qua lại để cập nhật giá trị của chúng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Ví dụ: ý nghĩa của từ mô hình trong cụm từ mô hình học máy khác với nghĩa của nó trong cụm từ mô hình thời trang.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Khối chú ý là thứ chịu trách nhiệm tìm ra những từ nào trong ngữ cảnh có liên quan đến việc cập nhật ý nghĩa của những từ khác và cách cập nhật chính xác những ý nghĩa đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "Và lần nữa, khi tôi dùng từ có nghĩa, bằng cách nào đó từ này được mã hóa hoàn toàn trong các mục của các vectơ đó.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "Sau đó, các vectơ này trải qua một loại hoạt động khác và tùy vào nguồn mà bạn đang đọc, vectơ này sẽ được gọi là perceptron nhiều lớp hoặc có thể là lớp chuyển tiếp dữ liệu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "Và ở đây các vectơ không liên lạc với nhau, chúng đều đi qua cùng phép toán song song.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "Và dù khối này khó diễn giải hơn một chút, nhưng sau này chúng ta sẽ nói về bước này giống như đặt một danh sách dài các câu hỏi về mỗi vectơ và sau đó cập nhật chúng dựa trên câu trả lời cho những câu hỏi đó.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Tất cả các phép toán trong cả hai khối này trông giống như một đống phép nhân ma trận khổng lồ và công việc chính của chúng ta là hiểu cách đọc các ma trận cơ bản.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Tôi đang xem qua một số chi tiết về một số bước chuẩn hóa diễn ra ở giữa, nhưng xét cho cùng thì đây vẫn là bản xem trước cấp cao.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "Sau đó, về cơ bản, quá trình lặp lại, bạn đi qua lại giữa các khối chú ý và các khối nhận thức nhiều lớp, cho đến khi kết thúc, hy vọng rằng tất cả ý nghĩa thiết yếu của đoạn văn bằng cách nào đó đã được đưa vào vectơ cuối cùng trong trình tự.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Sau đó, ta thực hiện một thao tác nhất định trên vectơ cuối cùng để tạo ra phân bố xác suất trên tất cả các mã thông báo có thể, tất cả các đoạn văn bản nhỏ có thể xuất hiện tiếp theo.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "Và như tôi đã nói, khi bạn có một công cụ dự đoán điều gì sẽ xảy ra tiếp theo với một đoạn văn bản, bạn có thể cung cấp cho nó một ít văn bản gốc và để nó chơi liên tục trò chơi dự đoán điều gì sẽ xảy ra tiếp theo, lấy mẫu từ phân phối, nối thêm nó rồi lặp đi lặp lại nhiều lần.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Một số bạn biết có thể nhớ bao lâu trước khi ChatGPT xuất hiện, đây là bản thử nghiệp ban đầu của GPT-3, bạn sẽ yêu cầu nó tự động hoàn thành các câu chuyện và bài luận dựa trên đoạn trích ban đầu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Để biến một công cụ như thế này thành một chatbot, điểm bắt đầu dễ dàng nhất là có một ít văn bản thiết lập cài đặt của người dùng tương tác với trợ lý AI hữu ích, bạn sẽ gọi lời nhắc hệ thống là gì, sau đó bạn sẽ sử dụng câu hỏi hoặc lời nhắc ban đầu của người dùng làm đoạn hội thoại đầu tiên, sau đó bạn bắt đầu dự đoán trợ lý AI hữu ích như vậy sẽ nói gì để đáp lại.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Còn nhiều điều để nói về một bước đào tạo cần thiết để thực hiện tốt công việc này, nhưng ở cấp độ cao thì đây chính là ý tưởng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "Trong chương này, bạn và tôi sẽ mở rộng chi tiết về những gì xảy ra ở phần đầu của mạng, ở phần cuối của mạng và tôi cũng muốn dành nhiều thời gian để xem lại một số kiến thức nền tảng quan trọng, những thứ lẽ ra là bản chất thứ hai đối với bất kỳ kỹ sư máy học nào vào thời điểm Transformer xuất hiện.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Nếu bạn cảm thấy thoải mái với kiến thức nền tảng đó và hơi thiếu kiên nhẫn, bạn có thể thoải mái chuyển sang chương tiếp theo, chương này sẽ tập trung vào các khối chú ý, thường được coi là trái tim của Transformer.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "Sau đó, tôi muốn nói nhiều hơn về các khối perceptron nhiều lớp này, cách đào tạo hoạt động và một số chi tiết khác sẽ bị bỏ qua cho đến thời điểm đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Đối với bối cảnh rộng hơn, những video này là phần bổ sung cho một loạt video nhỏ về học sâu và cũng không sao nếu bạn chưa xem những video trước đó, tôi nghĩ bạn có thể xem không theo thứ tự, nhưng trước khi đi sâu vào cụ thể về Transformer, tôi nghĩ cần đảm bảo rằng ta có cùng quan điểm về tiền đề và cấu trúc cơ bản của học sâu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Có nguy cơ nêu rõ điều hiển nhiên, đây là một cách tiếp cận với học máy, mô tả bất kỳ mô hình nào mà bạn đang sử dụng dữ liệu để bằng cách nào đó xác định cách hoạt động của mô hình.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Ý tôi là, giả sử bạn muốn một hàm nhận một hình ảnh và nó tạo ra một nhãn mô tả nó hoặc ví dụ của chúng ta về dự đoán từ tiếp theo cho một đoạn văn bản hoặc bất kỳ tác vụ nào khác có vẻ yêu cầu một số yếu tố trực quan và nhận dạng khuôn mẫu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "Ngày nay, ta gần như coi điều này là đương nhiên, nhưng ý tưởng của học máy là thay vì cố gắng xác định rõ ràng quy trình về cách thực hiện tác vụ đó trong mã, đó là những gì mọi người sẽ làm trong những ngày đầu tiên của AI, thay vào đó bạn thiết lập một cấu trúc rất linh hoạt với các tham số có thể điều chỉnh, chẳng hạn như một loạt các nút bấm và nút xoay, sau đó bằng cách nào đó, bạn sử dụng nhiều ví dụ về giao diện đầu ra của một đầu vào nhất định để điều chỉnh và điều chỉnh giá trị của các tham số đó nhằm bắt chước hành vi này.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Ví dụ: có thể hình thức học máy đơn giản nhất là hồi quy tuyến tính, trong đó đầu vào và đầu ra của bạn là mỗi số đơn lẻ, chẳng hạn như diện tích của một ngôi nhà và giá của nó, và điều bạn muốn là tìm một dòng phù hợp nhất thông qua điều này. bạn biết đấy, dữ liệu để dự đoán giá nhà trong tương lai.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Đường đó được mô tả bởi hai tham số liên tục, chẳng hạn như độ dốc và điểm chặn y, và mục tiêu của hồi quy tuyến tính là xác định các tham số đó khớp chặt chẽ với dữ liệu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Không cần phải nói, các mô hình học sâu trở nên phức tạp hơn nhiều.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "Ví dụ: GPT-3 không phải có hai mà có 175 tỷ tham số.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Nhưng vấn đề ở đây là, không có gì chắc chắn rằng bạn có thể tạo ra một mô hình khổng lồ nào đó với một số lượng lớn các tham số mà nó không quá phù hợp với dữ liệu huấn luyện hoặc hoàn toàn khó huấn luyện.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "Học sâu mô tả một lớp mô hình mà trong vài thập kỷ qua đã được chứng minh là có khả năng mở rộng đáng kể.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Điều thống nhất chúng là cùng một thuật toán huấn luyện, được gọi là lan truyền ngược, và bối cảnh mà tôi muốn bạn hiểu khi chúng ta đi vào là để thuật toán huấn luyện này hoạt động tốt trên quy mô lớn, các mô hình này phải tuân theo một định dạng cụ thể nhất định.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Nếu bạn biết định dạng này sẽ được áp dụng, nó sẽ giúp giải thích nhiều lựa chọn về cách Transformer xử lý ngôn ngữ, nếu không sẽ có nguy cơ cảm thấy tùy tiện.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Đầu tiên, bất kể bạn đang tạo mô hình nào, đầu vào phải được định dạng dưới dạng một mảng số thực.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Điều này có thể có nghĩa là một danh sách các số, nó có thể là một mảng hai chiều hoặc rất thường xuyên bạn xử lý các mảng có chiều cao hơn, trong đó thuật ngữ chung được sử dụng là tensor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Bạn thường nghĩ dữ liệu đầu vào đó được chuyển đổi dần dần thành nhiều lớp riêng biệt, trong đó, mỗi lớp luôn được cấu trúc như một loại mảng số thực nào đó, cho đến khi bạn đến lớp cuối cùng mà bạn coi là đầu ra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Ví dụ: lớp cuối cùng trong mô hình xử lý văn bản của ta là danh sách các số biểu thị phân bố xác suất cho tất cả các mã thông báo tiếp theo có thể có.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "Trong học sâu, các tham số mô hình này hầu như luôn được gọi là trọng số và điều này là do đặc điểm chính của các mô hình này là cách duy nhất các tham số này tương tác với dữ liệu đang được xử lý là thông qua tổng có trọng số.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Bạn cũng sẽ dùng suốt các hàm phi tuyến tính nhưng chúng sẽ không phụ thuộc vào tham số.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Tuy nhiên, thông thường, thay vì thấy các tổng có trọng số hoàn toàn trơ trọi và được viết rõ như này, bạn sẽ thấy chúng được đóng gói cùng nhau dưới dạng các thành phần khác nhau trong một tích vectơ ma trận.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Điều tương tự cũng xảy ra, nếu bạn nghĩ lại cách hoạt động của phép nhân vectơ ma trận, mỗi thành phần trong đầu ra trông giống như một tổng có trọng số.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Về mặt khái niệm, bạn và tôi thường dễ dàng hơn khi nghĩ về các ma trận chứa đầy các tham số có thể điều chỉnh được để biến đổi các vectơ được rút ra từ dữ liệu đang được xử lý.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Ví dụ: 175 tỷ trọng số đó trong GPT-3 được sắp xếp thành dưới 28.000 ma trận riêng biệt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Những ma trận đó lần lượt được chia thành tám loại khác nhau, và điều bạn và tôi sắp làm là xem qua từng loại trong số đó để hiểu loại đó làm gì.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "Khi chúng ta xét, tôi nghĩ thật thú vị khi tham khảo những con số cụ thể từ GPT-3 để đếm chính xác 175 tỷ đó đến từ đâu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Ngay cả khi ngày nay có những mô hình lớn hơn và tốt hơn, thì mô hình này vẫn có một sức hấp dẫn nhất định là mô hình ngôn ngữ lớn để thực sự thu hút sự chú ý của thế giới bên ngoài cộng đồng học máy.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Ngoài ra, trên thực tế mà nói, các công ty có xu hướng giữ kín những con số cụ thể đối với các mạng hiện đại hơn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Tôi chỉ muốn dựng bối cảnh khi bạn nhìn kỹ để xem điều gì xảy ra bên trong một công cụ như ChatGPT, gần như tất cả các phép tính thực tế trông giống như phép nhân vectơ ma trận.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Có một chút rủi ro khi bị lạc trong biển hàng tỷ con số, nhưng bạn nên hình dung rõ ràng sự khác biệt giữa trọng số của mô hình mà tôi sẽ luôn tô màu xanh lam hoặc đỏ và dữ liệu được đã được xử lý, tôi sẽ luôn tô màu xám.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Trọng số chính là bộ não thực sự, chúng là những thứ học được trong quá trình huấn luyện và quyết định cách nó hoạt động.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Dữ liệu đang được xử lý chỉ mã hóa bất kỳ đầu vào cụ thể nào được đưa vào mô hình cho một lần chạy nhất định, chẳng hạn như một đoạn văn bản mẫu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "Với tất cả những nền tảng đó, chúng ta sẽ đi sâu vào bước đầu tiên của ví dụ xử lý văn bản này, đó là chia dữ liệu đầu vào thành các phần nhỏ và biến các phần đó thành vectơ.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Tôi đã đề cập đến cách những phần đó được gọi là mã thông báo, có thể là các từ hoặc dấu câu, nhưng thỉnh thoảng trong chương này và đặc biệt là trong chương tiếp theo, tôi chỉ muốn giả vờ rằng nó được chia thành các từ một cách rõ ràng hơn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Bởi vì con người chúng ta suy nghĩ bằng lời nên điều này sẽ giúp việc tham khảo các ví dụ nhỏ và làm rõ từng bước dễ dàng hơn nhiều.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Mô hình này có vốn từ vựng được xác định trước, một số danh sách tất cả các từ có thể, chẳng hạn như 50.000 từ trong số đó và ma trận đầu tiên mà chúng ta sẽ gặp, được gọi là ma trận nhúng, có một cột duy nhất cho mỗi từ trong số những từ này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Các cột này quyết định mỗi từ sẽ chuyển thành vectơ nào trong bước đầu tiên đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Ta gắn nhãn cho nó là We, và như tất cả các ma trận mà ta thấy, các giá trị của nó bắt đầu ngẫu nhiên, nhưng chúng sẽ được học dựa trên dữ liệu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Biến các từ thành vectơ là phương pháp phổ biến trong machine learning từ rất lâu trước khi có Transformer, nhưng sẽ hơi kỳ lạ nếu bạn chưa từng thấy nó trước đây và nó đặt nền tảng cho mọi thứ tiếp theo, vậy hãy dành chút thời gian để làm quen với nó.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Ta thường gọi đây là việc nhúng một từ, nó làm bạn nghĩ về các vectơ này theo cách rất hình học như các điểm trong một không gian có chiều cao nào đó.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Việc hiển thị danh sách ba số làm tọa độ cho các điểm trong không gian 3D sẽ không có vấn đề gì, nhưng việc nhúng từ có xu hướng có chiều cao hơn nhiều.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "Trong GPT-3, chúng có 12.288 chiều và như bạn sẽ thấy, điều quan trọng là phải làm việc trong một không gian có nhiều hướng khác nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Theo cách tương tự, bạn có thể đưa một lát cắt hai chiều xuyên qua không gian 3D và chiếu tất cả các điểm lên lát cắt đó, nhằm mục đích tạo hoạt ảnh cho các phần nhúng từ mà một mô hình đơn giản mang lại cho tôi, tôi sẽ làm một điều tương tự bằng cách chọn một lát cắt ba chiều xuyên qua không gian có nhiều chiều này và chiếu các vectơ từ xuống đó và hiển thị kết quả.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "Ý tưởng lớn ở đây là khi một mô hình điều chỉnh và điều chỉnh trọng số của nó để xác định chính xác cách các từ được nhúng dưới dạng vectơ trong quá trình huấn luyện, nó có xu hướng giải quyết trên một tập hợp các phần nhúng trong đó các hướng trong không gian có một loại ngữ nghĩa có nghĩa.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Đối với mô hình từ-thành-vectơ đơn giản mà tôi đang chạy ở đây, nếu tôi thực hiện tìm kiếm tất cả các từ có phần nhúng gần nhất với từ có nghĩa là tháp, bạn sẽ nhận thấy tất cả chúng dường như đều mang lại cảm giác giống như tháp rất giống nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "Và nếu bạn muốn sử dụng một số Python và tự làm ở nhà, thì đây là mô hình cụ thể mà tôi đang sử dụng để tạo hoạt ảnh.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Nó không phải là một Transformer, nhưng nó đủ để minh họa ý tưởng rằng các hướng trong không gian có thể mang ý nghĩa ngữ nghĩa.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Một ví dụ rất cổ điển về điều này là nếu bạn lấy sự khác biệt giữa các vectơ của phụ nữ và đàn ông, một cái gì đó bạn sẽ hình dung như một vectơ nhỏ nối đầu của cái này với đầu của cái kia, nó rất giống với sự khác biệt giữa vua và nữ hoàng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Vì vậy, giả sử bạn không biết từ dành cho nữ quân vương, bạn có thể tìm từ đó bằng cách lấy vua, thêm hướng phụ nữ-nam giới này và tìm kiếm các phần nhúng gần nhất với điểm đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "Ít nhất đại loại như thế.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Dù đây là một ví dụ cổ điển cho mô hình mà tôi đang sử dụng, nhưng việc nhúng nữ hoàng thực sự còn xa hơn một chút so với điều này gợi ý, có lẽ là do cách sử dụng nữ hoàng trong dữ liệu huấn luyện không chỉ đơn thuần là một phiên bản nữ tính của vua.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Khi tôi bật lên, các mối quan hệ gia đình dường như minh họa ý tưởng này tốt hơn nhiều.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Vấn đề là, có vẻ như trong quá trình đào tạo, mô hình nhận thấy thuận lợi khi chọn các phần nhúng sao cho một hướng trong không gian này mã hóa thông tin giới tính.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Một ví dụ khác là nếu bạn lấy phần nhúng của Ý, và bạn trừ đi phần nhúng của Đức, và thêm phần đó vào phần nhúng của Hitler, bạn sẽ có được thứ gì đó rất gần với phần nhúng của Mussolini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Cứ như thể mô hình đã học cách liên kết một số hướng với người Ý và những hướng khác với các nhà lãnh đạo trục trong Thế chiến thứ hai.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Có lẽ ví dụ yêu thích của tôi trong trường hợp này là trong một số mô hình, nếu bạn lấy sự khác biệt giữa Đức và Nhật Bản và thêm nó vào món sushi, bạn sẽ kết thúc rất gần với xúc xích bratwurst.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Cũng khi chơi trò chơi tìm hàng xóm gần nhất này, tôi rất vui khi thấy Kat thân thiết với cả quái thú và quái vật đến mức nào.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Một chút trực quan toán học hữu ích cần nhớ, đặc biệt là để cho chương sau, là làm thế nào tích vô hướng của hai vectơ có thể được coi là một cách để đo mức độ chúng thẳng hàng.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "Về mặt tính toán, tích vô hướng liên quan đến việc nhân tất cả các thành phần tương ứng rồi cộng các kết quả lại, điều này là tốt vì rất nhiều phép tính phải trông như tổng có trọng số.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Về mặt hình học, tích vô hướng là dương khi các vectơ hướng cùng hướng, nó bằng 0 nếu chúng vuông góc và nó âm bất cứ khi nào chúng hướng ngược nhau.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Ví dụ: giả sử bạn đang chơi với mô hình này và bạn đưa ra giả thuyết rằng việc nhúng các con mèo trừ một con mèo có thể đại diện cho một loại hướng đa dạng trong không gian này.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Để kiểm tra điều này, tôi sẽ lấy vectơ này và tính tích vô hướng của nó dựa trên các phần nhúng của các danh từ số ít nhất định và so sánh nó với tích vô hướng của các danh từ số nhiều tương ứng.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Nếu bạn thử nghiệm điều này, bạn sẽ nhận thấy rằng số nhiều thực sự dường như luôn cho giá trị cao hơn số ít, cho thấy rằng chúng phù hợp hơn với hướng này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Cũng thật thú vị nếu bạn lấy tích vô hướng này với các phần nhúng của các từ 1, 2, 3, v.v., chúng sẽ cho các giá trị tăng dần, do đó, như thể ta có thể đo lường một cách định lượng mức độ số nhiều của mô hình tìm thấy một từ nhất định.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Một lần nữa, thông tin cụ thể về cách nhúng các từ được học bằng cách sử dụng dữ liệu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Ma trận nhúng này, có các cột cho ta biết điều gì xảy ra với mỗi từ, là chồng trọng số đầu tiên trong mô hình của chúng ta.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Sử dụng số GPT-3, kích thước từ vựng cụ thể là 50.257 và một lần nữa, về mặt kỹ thuật, điều này không bao gồm các từ mà là các mã thông báo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "Kích thước nhúng là 12.288 và nhân số đó cho chúng ta biết nó bao gồm khoảng 617 triệu trọng số.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Hãy tiếp tục và thêm số này vào bảng kiểm đếm đang chạy, hãy nhớ rằng đến cuối cùng chúng ta sẽ đếm được tới 175 tỷ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "Trong trường hợp Transformer, bạn thực sự muốn coi các vectơ trong không gian nhúng này không chỉ đơn thuần là biểu thị các từ riêng lẻ.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Thứ nhất, chúng cũng mã hóa thông tin về vị trí của từ đó, điều mà chúng ta sẽ nói đến sau, nhưng quan trọng hơn, bạn nên nghĩ chúng có khả năng hiểu rõ ngữ cảnh.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Ví dụ: một vectơ bắt đầu tồn tại dưới dạng nhúng từ vua, có thể dần dần bị kéo và kéo bởi nhiều khối khác nhau trong mạng này, để cuối cùng, nó chỉ theo một hướng cụ thể và nhiều sắc thái hơn mà bằng cách nào đó mã hóa nó là một vị vua sống ở Scotland và đã đạt được chức vụ của mình sau khi sát hại vị vua trước đó và được mô tả bằng ngôn ngữ của Shakespeare.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Hãy suy nghĩ về sự hiểu biết của riêng bạn về một từ nhất định.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Ý nghĩa của từ đó được môi trường xung quanh thông báo rõ ràng và đôi khi điều này bao gồm ngữ cảnh từ khoảng cách xa, vì vậy, khi kết hợp một mô hình có khả năng dự đoán từ nào tiếp theo, mục tiêu là bằng cách nào đó cho phép nó kết hợp ngữ cảnh một cách hiệu quả.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Nói rõ hơn, ngay trong bước đầu tiên đó, khi bạn tạo mảng vectơ dựa trên văn bản đầu vào, mỗi vectơ đó chỉ được lấy ra khỏi ma trận nhúng, vậy ban đầu mỗi vectơ chỉ có thể mã hóa nghĩa của một từ mà không cần bất kỳ đầu vào nào từ môi trường xung quanh nó.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Nhưng bạn nên nghĩ đến mục tiêu chính của mạng lưới này mà nó chảy qua là cho phép mỗi vectơ đó hấp thụ một ý nghĩa phong phú và cụ thể hơn nhiều so với những gì mà các từ riêng lẻ có thể biểu thị.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Mạng chỉ có thể xử lý một số vectơ cố định tại một thời điểm, được gọi là kích thước ngữ cảnh của nó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Đối với GPT-3, nó được đào tạo với kích thước ngữ cảnh là 2048, do đó, dữ liệu truyền qua mạng luôn trông giống như mảng gồm 2048 cột này, mỗi cột có 12.000 chiều.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Kích thước ngữ cảnh này giới hạn số lượng văn bản mà Transformer có thể kết hợp khi đưa ra dự đoán về từ tiếp theo.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Đây là lý do tại sao các cuộc trò chuyện dài với một số chatbot nhất định, chẳng hạn như các phiên bản đầu tiên của ChatGPT, thường mang lại cảm giác như bot sẽ mất mạch trò chuyện khi bạn tiếp tục quá lâu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "Chúng ta sẽ đi vào chi tiết cần chú ý vào thời gian thích hợp, nhưng bỏ qua phần trước tôi muốn nói một chút về những gì xảy ra ở phần cuối.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Hãy nhớ rằng, đầu ra mong muốn là phân bố xác suất trên tất cả các mã thông báo có thể xuất hiện tiếp theo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Ví dụ: nếu từ cuối cùng là Giáo sư và ngữ cảnh bao gồm những từ như Harry Potter và ngay trước đó chúng ta thấy giáo viên ít được yêu thích nhất, đồng thời nếu bạn cho tôi chút thời gian bằng cách để tôi giả vờ rằng các mã thông báo trông giống như các từ đầy đủ, thì một mạng lưới được đào tạo bài bản đã xây dựng được kiến thức về Harry Potter có lẽ sẽ đánh giá cao từ Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Điều này bao gồm hai bước khác nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Cách đầu tiên là sử dụng một ma trận khác ánh xạ vectơ cuối cùng trong ngữ cảnh đó tới danh sách 50.000 giá trị, một giá trị cho mỗi mã thông báo trong từ vựng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Sau đó, có một hàm bình thường hóa điều này thành phân bố xác suất, nó được gọi là Softmax và chúng ta sẽ nói nhiều hơn về nó chỉ sau một giây, nhưng trước đó có vẻ hơi kỳ lạ khi chỉ sử dụng phép nhúng cuối cùng này để đưa ra dự đoán, khi xét cho cùng, ở bước cuối cùng đó, có hàng nghìn vectơ khác trong lớp chỉ nằm ở đó với ý nghĩa giàu ngữ cảnh của riêng chúng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Điều này liên quan đến thực tế là trong quá trình đào tạo, nó sẽ hiệu quả hơn nhiều nếu bạn sử dụng từng vectơ đó ở lớp cuối cùng để đồng thời đưa ra dự đoán về những gì sẽ xảy ra ngay sau nó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "Còn rất nhiều điều để nói về việc huấn luyện sau này, nhưng tôi chỉ muốn nói ngay bây giờ.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Ma trận này được gọi là ma trận Hủy nhúng và ta đặt cho nó ký hiệu WU.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Một lần nữa, giống như tất cả các ma trận trọng số mà ta thấy, các mục của nó bắt đầu một cách ngẫu nhiên, nhưng chúng được học trong quá trình huấn luyện.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Giữ điểm trên tổng số tham số của chúng ta, ma trận Hủy nhúng này có một hàng cho mỗi từ trong từ vựng và mỗi hàng có cùng số phần tử như chiều nhúng.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Nó rất giống với ma trận nhúng, chỉ với thứ tự được hoán đổi, do đó, nó bổ sung thêm 617 triệu tham số khác vào mạng, nghĩa là số lượng của chúng ta cho đến nay là hơn một tỷ một chút, một phần nhỏ nhưng không hoàn toàn không đáng kể trong số tổng cộng 175 tỷ mà chúng ta kết thúc.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "Bài học nhỏ cuối cùng của chương này, tôi muốn nói nhiều hơn về hàm softmax này, vì nó sẽ xuất hiện một cách khác khi chúng ta đi sâu vào các khối chú ý.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Ý tưởng là nếu bạn muốn một chuỗi số hoạt động như một phân phối xác suất, chẳng hạn như phân phối trên tất cả các từ tiếp theo có thể có, thì mỗi giá trị phải nằm trong khoảng từ 0 đến 1 và bạn cũng cần tất cả chúng để có tổng bằng 1 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Tuy nhiên, nếu bạn đang chơi trò chơi học tập trong đó mọi thứ bạn làm trông giống như phép nhân vectơ ma trận, thì kết quả đầu ra bạn nhận được theo mặc định hoàn toàn không tuân theo điều này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Các giá trị thường âm hoặc lớn hơn 1 rất nhiều và gần như chắc chắn chúng không có tổng bằng 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax là cách tiêu chuẩn để biến một danh sách các số tùy ý thành một phân phối hợp lệ sao cho các giá trị lớn nhất gần bằng 1 và các giá trị nhỏ hơn có giá trị rất gần với 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Đó là tất cả những gì bạn thực sự cần biết.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Nhưng nếu bạn tò mò, cách thức hoạt động là trước tiên nâng e lên lũy thừa của mỗi số, nghĩa là bây giờ bạn có một danh sách các giá trị dương, sau đó bạn có thể lấy tổng của tất cả các giá trị dương đó và chia mỗi số hạng theo tổng đó, nó sẽ chuẩn hóa nó thành một danh sách có tổng bằng 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Bạn sẽ nhận thấy rằng nếu một trong các số ở đầu vào lớn hơn đáng kể so với các số còn lại, thì trong đầu ra, số hạng tương ứng sẽ chiếm ưu thế trong phân phối, vậy nếu bạn lấy mẫu từ số đó thì bạn gần như chắc chắn chỉ chọn đầu vào tối đa hóa.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Nhưng nó nhẹ nhàng hơn việc chỉ chọn mức tối đa theo nghĩa là khi các giá trị khác lớn tương tự, chúng cũng có trọng số có ý nghĩa trong phân phối và mọi thứ thay đổi liên tục khi bạn liên tục thay đổi đầu vào.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "Trong một số trường hợp, chẳng hạn như khi ChatGPT đang sử dụng phân phối này để tạo từ tiếp theo, sẽ có chỗ cho một chút thú vị hơn bằng cách thêm một chút gia vị bổ sung vào hàm này, với hằng số t được đưa vào mẫu số của các số mũ đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Chúng ta gọi nó là nhiệt độ, vì nó gần giống với vai trò của nhiệt độ trong các phương trình nhiệt động lực học nhất định, và kết quả là khi t lớn hơn, bạn chú trọng nhiều hơn đến các giá trị thấp hơn, nghĩa là sự phân bố đồng đều hơn một chút, và nếu t nhỏ hơn thì giá trị lớn hơn sẽ chiếm ưu thế mạnh hơn, trong đó ở mức cực đoan, đặt t bằng 0 có nghĩa là tất cả trọng số sẽ chuyển sang giá trị tối đa.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Ví dụ: tôi sẽ yêu cầu GPT-3 tạo một câu chuyện với văn bản gốc, ngày xưa có A, nhưng tôi sẽ sử dụng nhiệt độ khác nhau trong từng trường hợp.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Nhiệt độ bằng 0 có nghĩa là nó luôn đi theo từ dễ đoán nhất và những gì bạn nhận được cuối cùng chỉ là một đạo hàm sáo rỗng của Goldilocks.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Nhiệt độ cao hơn giúp nó có cơ hội chọn những từ ít có khả năng xảy ra hơn, nhưng nó đi kèm với rủi ro.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "Trong trường hợp này, câu chuyện bắt đầu độc đáo hơn, về một nghệ sĩ web trẻ đến từ Hàn Quốc, nhưng nó nhanh chóng trở nên vô nghĩa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Về mặt kỹ thuật, API không thực sự cho phép bạn chọn nhiệt độ lớn hơn 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Không có lý do toán học nào cho việc này, đó chỉ là một ràng buộc tùy ý được áp đặt để giữ cho công cụ của họ không bị nhìn thấy đang tạo ra những thứ quá vô nghĩa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Vậy nếu bạn tò mò, cách hoạt động thực sự của hoạt ảnh này là tôi đang lấy 20 mã thông báo tiếp theo có khả năng xảy ra cao nhất mà GPT-3 tạo ra, có vẻ như là tổng tối đa mà họ sẽ cung cấp cho tôi và sau đó tôi điều chỉnh xác suất dựa trên theo số mũ của 1 phần 5.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "Là một số hạng khác, giống như cách bạn có thể gọi các thành phần đầu ra của xác suất của hàm này, mọi người thường coi đầu vào là logits, hoặc một số người nói logits, một số người nói logits, tôi sẽ nói logits .",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Vì vậy, chẳng hạn, khi bạn nhập một số văn bản, bạn có tất cả các từ nhúng này chảy qua mạng và bạn thực hiện phép nhân cuối cùng này với ma trận không nhúng, những người học máy sẽ coi các thành phần trong đầu ra thô, không chuẩn hóa đó là nhật ký để dự đoán từ tiếp theo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Phần lớn mục tiêu của chương này là đặt nền móng cho việc hiểu cơ chế chú ý, phong cách \"làm đi làm lại\" của Karate Kid.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Bạn thấy đấy, nếu bạn có trực quan tốt về cách nhúng từ, về softmax, về cách các tích vô hướng đo lường độ tương tự và cũng là tiền đề cơ bản rằng hầu hết các phép tính phải trông giống như phép nhân ma trận với ma trận chứa đầy các tham số có thể điều chỉnh được, thì hãy hiểu sự chú ý Cơ chế, phần nền tảng này trong toàn bộ sự bùng nổ AI hiện đại, phải tương đối trơn tru.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Vì điều đó, tham gia cùng tôi trong chương sau.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Khi tôi xuất bản video này, bản nháp của chương sau sẽ có sẵn để những người ủng hộ Patreon xem xét.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Phiên bản cuối cùng sẽ được công bố rộng rãi sau một hoặc hai tuần, điều này thường phụ thuộc vào việc tôi sẽ thay đổi bao nhiêu dựa trên đánh giá đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "Trong khi chờ đợi, nếu bạn muốn thu hút sự chú ý và nếu bạn muốn giúp đỡ kênh một chút thì kênh vẫn đang chờ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
