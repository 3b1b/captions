1
00:00:00,000 --> 00:00:04,560
Các chữ cái đầu GPT là viết tắt của Máy biến đổi Sáng tạo được Đào tạo trước.

2
00:00:05,220 --> 00:00:09,020
Từ đầu tiên đó khá đơn giản, đây là những bot tạo ra văn bản mới.

3
00:00:09,800 --> 00:00:13,246
Được đào tạo trước đề cập đến cách mô hình trải qua quá trình học từ 

4
00:00:13,246 --> 00:00:16,743
một lượng lớn dữ liệu và tiền tố ám chỉ rằng có nhiều chỗ hơn để tinh 

5
00:00:16,743 --> 00:00:20,040
chỉnh mô hình trong các nhiệm vụ cụ thể bằng cách đào tạo bổ sung.

6
00:00:20,720 --> 00:00:22,900
Nhưng lời cuối cùng, đó mới là phần quan trọng thực sự.

7
00:00:23,380 --> 00:00:26,732
Transformer (hay máy biến đổi) là một loại mạng thần kinh cụ thể, 

8
00:00:26,732 --> 00:00:31,000
một mô hình học máy và là phát minh cốt lõi tạo nền tảng cho sự bùng nổ AI hiện nay.

9
00:00:31,740 --> 00:00:35,291
Điều tôi muốn làm với video này và các chương tiếp theo là giải 

10
00:00:35,291 --> 00:00:39,120
thích bằng hình ảnh về những gì thực sự xảy ra bên trong Transformer.

11
00:00:39,700 --> 00:00:42,820
Chúng ta sẽ theo dõi dữ liệu chảy qua nó và thực hiện từng bước một.

12
00:00:43,440 --> 00:00:47,380
Có nhiều loại mô hình khác nhau mà bạn có thể xây dựng bằng cách sử dụng Transformer.

13
00:00:47,800 --> 00:00:50,800
Một số kiểu máy tiếp nhận âm thanh và tạo ra bản ghi.

14
00:00:51,340 --> 00:00:56,220
Câu này xuất phát từ một mô hình ngược lại, tạo ra lời nói tổng hợp chỉ từ văn bản.

15
00:00:56,660 --> 00:01:00,950
Tất cả những công cụ đã gây bão trên toàn thế giới vào năm 2022 như Dolly và 

16
00:01:00,950 --> 00:01:05,519
Midjourney có chức năng mô tả văn bản và tạo ra hình ảnh đều dựa trên Transformer.

17
00:01:06,000 --> 00:01:08,914
Ngay cả khi tôi không thể hiểu được sinh vật Pi là gì, 

18
00:01:08,914 --> 00:01:13,100
tôi vẫn rất ngạc nhiên rằng loại yêu cầu này thậm chí còn có thể khả thi từ xa.

19
00:01:13,900 --> 00:01:17,896
Và Transformer ban đầu được Google giới thiệu vào năm 2017 đã được phát minh 

20
00:01:17,896 --> 00:01:22,100
cho trường hợp sử dụng cụ thể là dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác.

21
00:01:22,660 --> 00:01:25,214
Nhưng biến thể mà bạn và tôi sẽ tập trung vào, 

22
00:01:25,214 --> 00:01:27,769
loại làm nền tảng cho các công cụ như ChatGPT, 

23
00:01:27,769 --> 00:01:31,139
sẽ là một mô hình được đào tạo để tiếp nhận một đoạn văn bản, 

24
00:01:31,139 --> 00:01:35,814
thậm chí có thể có một số hình ảnh hoặc âm thanh xung quanh đi kèm và đưa ra dự đoán. 

25
00:01:35,814 --> 00:01:38,260
cho những gì xảy ra tiếp theo trong đoạn văn.

26
00:01:38,600 --> 00:01:43,800
Dự đoán đó có dạng phân bố xác suất trên nhiều đoạn văn bản khác nhau có thể theo sau.

27
00:01:45,040 --> 00:01:47,532
Thoạt nhìn, bạn có thể nghĩ rằng việc dự đoán từ tiếp theo 

28
00:01:47,532 --> 00:01:49,940
giống như một mục tiêu rất khác với việc tạo văn bản mới.

29
00:01:50,180 --> 00:01:52,854
Nhưng khi bạn có một mô hình dự đoán như thế này, 

30
00:01:52,854 --> 00:01:56,758
một điều đơn giản là bạn tạo một đoạn văn bản dài hơn là cung cấp cho nó 

31
00:01:56,758 --> 00:02:00,501
một đoạn mã ban đầu để làm việc, yêu cầu nó lấy một mẫu ngẫu nhiên từ 

32
00:02:00,501 --> 00:02:03,122
phân phối mà nó vừa tạo, nối mẫu đó vào văn bản, 

33
00:02:03,122 --> 00:02:07,507
rồi chạy lại toàn bộ quá trình để đưa ra dự đoán mới dựa trên tất cả văn bản mới, 

34
00:02:07,507 --> 00:02:09,539
bao gồm cả nội dung vừa được thêm vào.

35
00:02:10,100 --> 00:02:13,000
Tôi không biết bạn thế nào, nhưng tôi thực sự không cảm thấy điều này thực sự hiệu quả.

36
00:02:13,420 --> 00:02:16,340
Ví dụ: trong hoạt ảnh này, tôi đang chạy GPT-2 trên máy tính 

37
00:02:16,340 --> 00:02:19,260
xách tay của mình và để nó liên tục dự đoán cũng như lấy mẫu 

38
00:02:19,260 --> 00:02:22,420
đoạn văn bản tiếp theo để tạo một câu chuyện dựa trên văn bản gốc.

39
00:02:22,420 --> 00:02:26,120
Câu chuyện thực sự không có nhiều ý nghĩa như vậy.

40
00:02:26,500 --> 00:02:29,904
Nhưng thay vào đó, nếu tôi đổi nó lấy các lệnh gọi API sang GPT-3, 

41
00:02:29,904 --> 00:02:33,512
mô hình cơ bản tương tự, chỉ lớn hơn nhiều, đột nhiên gần như kỳ diệu, 

42
00:02:33,512 --> 00:02:37,119
ta có được câu chuyện hợp lý, một câu chuyện thậm chí dường như suy ra 

43
00:02:37,119 --> 00:02:40,880
rằng một sinh vật pi sẽ sống trong một vùng đất của toán học và tính toán.

44
00:02:41,580 --> 00:02:45,013
Quá trình dự đoán và lấy mẫu lặp lại ở đây về cơ bản là những 

45
00:02:45,013 --> 00:02:48,335
gì đang xảy ra khi bạn tương tác với ChatGPT hoặc bất kỳ mô 

46
00:02:48,335 --> 00:02:51,880
hình ngôn ngữ lớn nào khác và bạn thấy chúng tạo ra từng từ một.

47
00:02:52,480 --> 00:02:55,817
Trên thực tế, một tính năng mà tôi rất thích là khả 

48
00:02:55,817 --> 00:02:59,220
năng xem sự phân bổ cơ bản cho mỗi từ mới mà nó chọn.

49
00:03:03,820 --> 00:03:06,142
Hãy bắt đầu mọi thứ bằng bản xem trước ở mức rất 

50
00:03:06,142 --> 00:03:08,180
cao về cách dữ liệu truyền qua Transformer.

51
00:03:08,640 --> 00:03:12,990
Ta sẽ dành nhiều thời gian hơn để thúc đẩy, diễn giải và mở rộng chi tiết của từng bước, 

52
00:03:12,990 --> 00:03:17,242
nhưng nói một cách tổng quát, khi một trong những chatbot này tạo ra một từ nhất định, 

53
00:03:17,242 --> 00:03:18,660
đây là những gì đang diễn ra.

54
00:03:19,080 --> 00:03:22,040
Đầu tiên, đầu vào được chia thành nhiều phần nhỏ.

55
00:03:22,620 --> 00:03:25,845
Những phần này được gọi là mã thông báo và trong trường hợp văn bản, 

56
00:03:25,845 --> 00:03:29,820
chúng có xu hướng là các từ hoặc các đoạn từ nhỏ hoặc các tổ hợp ký tự phổ biến khác.

57
00:03:30,740 --> 00:03:33,864
Nếu có liên quan đến hình ảnh hoặc âm thanh thì mã thông báo có thể 

58
00:03:33,864 --> 00:03:37,080
là những mảng nhỏ của hình ảnh đó hoặc những đoạn nhỏ của âm thanh đó.

59
00:03:37,580 --> 00:03:41,048
Mỗi một trong số các mã thông báo này sau đó được liên kết với một vectơ, 

60
00:03:41,048 --> 00:03:44,985
nghĩa là một số danh sách các số, nhằm mục đích mã hóa bằng cách nào đó ý nghĩa của 

61
00:03:44,985 --> 00:03:45,360
phần đó.

62
00:03:45,880 --> 00:03:49,855
Nếu bạn coi các vectơ này là tọa độ trong một không gian có chiều rất cao, 

63
00:03:49,855 --> 00:03:54,255
thì các từ có ý nghĩa tương tự có xu hướng nằm trên các vectơ gần nhau trong không 

64
00:03:54,255 --> 00:03:54,680
gian đó.

65
00:03:55,280 --> 00:03:59,761
Sau đó, chuỗi vectơ này sẽ đi qua một hoạt động được gọi là khối chú ý và điều này cho 

66
00:03:59,761 --> 00:04:04,190
phép các vectơ giao tiếp với nhau và truyền thông tin qua lại để cập nhật giá trị của 

67
00:04:04,190 --> 00:04:04,500
chúng.

68
00:04:04,880 --> 00:04:08,248
Ví dụ: ý nghĩa của từ mô hình trong cụm từ mô hình học 

69
00:04:08,248 --> 00:04:11,800
máy khác với nghĩa của nó trong cụm từ mô hình thời trang.

70
00:04:12,260 --> 00:04:16,968
Khối chú ý là thứ chịu trách nhiệm tìm ra những từ nào trong ngữ cảnh có liên quan 

71
00:04:16,968 --> 00:04:21,959
đến việc cập nhật ý nghĩa của những từ khác và cách cập nhật chính xác những ý nghĩa đó.

72
00:04:22,500 --> 00:04:25,294
Và lần nữa, khi tôi dùng từ có nghĩa, bằng cách nào đó từ 

73
00:04:25,294 --> 00:04:28,040
này được mã hóa hoàn toàn trong các mục của các vectơ đó.

74
00:04:29,180 --> 00:04:33,793
Sau đó, các vectơ này trải qua một loại hoạt động khác và tùy vào nguồn mà bạn đang đọc, 

75
00:04:33,793 --> 00:04:38,200
vectơ này sẽ được gọi là perceptron nhiều lớp hoặc có thể là lớp chuyển tiếp dữ liệu.

76
00:04:38,580 --> 00:04:42,660
Và ở đây các vectơ không liên lạc với nhau, chúng đều đi qua cùng phép toán song song.

77
00:04:43,060 --> 00:04:46,671
Và dù khối này khó diễn giải hơn một chút, nhưng sau này chúng ta sẽ 

78
00:04:46,671 --> 00:04:50,178
nói về bước này giống như đặt một danh sách dài các câu hỏi về mỗi 

79
00:04:50,178 --> 00:04:54,000
vectơ và sau đó cập nhật chúng dựa trên câu trả lời cho những câu hỏi đó.

80
00:04:54,900 --> 00:05:00,078
Tất cả các phép toán trong cả hai khối này trông giống như một đống phép nhân ma 

81
00:05:00,078 --> 00:05:05,320
trận khổng lồ và công việc chính của chúng ta là hiểu cách đọc các ma trận cơ bản.

82
00:05:06,980 --> 00:05:10,395
Tôi đang xem qua một số chi tiết về một số bước chuẩn hóa diễn ra ở giữa, 

83
00:05:10,395 --> 00:05:12,980
nhưng xét cho cùng thì đây vẫn là bản xem trước cấp cao.

84
00:05:13,680 --> 00:05:18,559
Sau đó, về cơ bản, quá trình lặp lại, bạn đi qua lại giữa các khối chú ý và các 

85
00:05:18,559 --> 00:05:21,486
khối nhận thức nhiều lớp, cho đến khi kết thúc, 

86
00:05:21,486 --> 00:05:26,365
hy vọng rằng tất cả ý nghĩa thiết yếu của đoạn văn bằng cách nào đó đã được đưa 

87
00:05:26,365 --> 00:05:28,500
vào vectơ cuối cùng trong trình tự.

88
00:05:28,920 --> 00:05:32,017
Sau đó, ta thực hiện một thao tác nhất định trên vectơ cuối 

89
00:05:32,017 --> 00:05:35,580
cùng để tạo ra phân bố xác suất trên tất cả các mã thông báo có thể, 

90
00:05:35,580 --> 00:05:38,420
tất cả các đoạn văn bản nhỏ có thể xuất hiện tiếp theo.

91
00:05:38,980 --> 00:05:42,428
Và như tôi đã nói, khi bạn có một công cụ dự đoán điều gì sẽ xảy ra 

92
00:05:42,428 --> 00:05:45,979
tiếp theo với một đoạn văn bản, bạn có thể cung cấp cho nó một ít văn 

93
00:05:45,979 --> 00:05:49,884
bản gốc và để nó chơi liên tục trò chơi dự đoán điều gì sẽ xảy ra tiếp theo, 

94
00:05:49,884 --> 00:05:53,080
lấy mẫu từ phân phối, nối thêm nó rồi lặp đi lặp lại nhiều lần.

95
00:05:53,640 --> 00:05:57,213
Một số bạn biết có thể nhớ bao lâu trước khi ChatGPT xuất hiện, 

96
00:05:57,213 --> 00:06:00,954
đây là bản thử nghiệp ban đầu của GPT-3, bạn sẽ yêu cầu nó tự động 

97
00:06:00,954 --> 00:06:04,640
hoàn thành các câu chuyện và bài luận dựa trên đoạn trích ban đầu.

98
00:06:05,580 --> 00:06:08,524
Để biến một công cụ như thế này thành một chatbot, 

99
00:06:08,524 --> 00:06:12,853
điểm bắt đầu dễ dàng nhất là có một ít văn bản thiết lập cài đặt của người 

100
00:06:12,853 --> 00:06:17,125
dùng tương tác với trợ lý AI hữu ích, bạn sẽ gọi lời nhắc hệ thống là gì, 

101
00:06:17,125 --> 00:06:21,513
sau đó bạn sẽ sử dụng câu hỏi hoặc lời nhắc ban đầu của người dùng làm đoạn 

102
00:06:21,513 --> 00:06:25,727
hội thoại đầu tiên, sau đó bạn bắt đầu dự đoán trợ lý AI hữu ích như vậy 

103
00:06:25,727 --> 00:06:26,940
sẽ nói gì để đáp lại.

104
00:06:27,720 --> 00:06:31,801
Còn nhiều điều để nói về một bước đào tạo cần thiết để thực hiện tốt công việc này, 

105
00:06:31,801 --> 00:06:33,940
nhưng ở cấp độ cao thì đây chính là ý tưởng.

106
00:06:35,720 --> 00:06:40,694
Trong chương này, bạn và tôi sẽ mở rộng chi tiết về những gì xảy ra ở phần đầu của mạng, 

107
00:06:40,694 --> 00:06:44,830
ở phần cuối của mạng và tôi cũng muốn dành nhiều thời gian để xem lại một 

108
00:06:44,830 --> 00:06:48,966
số kiến thức nền tảng quan trọng, những thứ lẽ ra là bản chất thứ hai đối 

109
00:06:48,966 --> 00:06:52,600
với bất kỳ kỹ sư máy học nào vào thời điểm Transformer xuất hiện.

110
00:06:53,060 --> 00:06:56,525
Nếu bạn cảm thấy thoải mái với kiến thức nền tảng đó và hơi thiếu kiên nhẫn, 

111
00:06:56,525 --> 00:06:58,819
bạn có thể thoải mái chuyển sang chương tiếp theo, 

112
00:06:58,819 --> 00:07:02,780
chương này sẽ tập trung vào các khối chú ý, thường được coi là trái tim của Transformer.

113
00:07:03,360 --> 00:07:07,187
Sau đó, tôi muốn nói nhiều hơn về các khối perceptron nhiều lớp này, 

114
00:07:07,187 --> 00:07:11,680
cách đào tạo hoạt động và một số chi tiết khác sẽ bị bỏ qua cho đến thời điểm đó.

115
00:07:12,180 --> 00:07:16,215
Đối với bối cảnh rộng hơn, những video này là phần bổ sung cho một loạt video nhỏ 

116
00:07:16,215 --> 00:07:19,562
về học sâu và cũng không sao nếu bạn chưa xem những video trước đó, 

117
00:07:19,562 --> 00:07:23,499
tôi nghĩ bạn có thể xem không theo thứ tự, nhưng trước khi đi sâu vào cụ thể về 

118
00:07:23,499 --> 00:07:27,584
Transformer, tôi nghĩ cần đảm bảo rằng ta có cùng quan điểm về tiền đề và cấu trúc 

119
00:07:27,584 --> 00:07:28,520
cơ bản của học sâu.

120
00:07:29,020 --> 00:07:32,742
Có nguy cơ nêu rõ điều hiển nhiên, đây là một cách tiếp cận với học máy, 

121
00:07:32,742 --> 00:07:35,852
mô tả bất kỳ mô hình nào mà bạn đang sử dụng dữ liệu để bằng 

122
00:07:35,852 --> 00:07:38,300
cách nào đó xác định cách hoạt động của mô hình.

123
00:07:39,140 --> 00:07:43,667
Ý tôi là, giả sử bạn muốn một hàm nhận một hình ảnh và nó tạo ra một nhãn mô tả 

124
00:07:43,667 --> 00:07:48,252
nó hoặc ví dụ của chúng ta về dự đoán từ tiếp theo cho một đoạn văn bản hoặc bất 

125
00:07:48,252 --> 00:07:52,780
kỳ tác vụ nào khác có vẻ yêu cầu một số yếu tố trực quan và nhận dạng khuôn mẫu.

126
00:07:53,200 --> 00:07:55,658
Ngày nay, ta gần như coi điều này là đương nhiên, 

127
00:07:55,658 --> 00:08:00,083
nhưng ý tưởng của học máy là thay vì cố gắng xác định rõ ràng quy trình về cách thực hiện 

128
00:08:00,083 --> 00:08:04,311
tác vụ đó trong mã, đó là những gì mọi người sẽ làm trong những ngày đầu tiên của AI, 

129
00:08:04,311 --> 00:08:08,637
thay vào đó bạn thiết lập một cấu trúc rất linh hoạt với các tham số có thể điều chỉnh, 

130
00:08:08,637 --> 00:08:12,226
chẳng hạn như một loạt các nút bấm và nút xoay, sau đó bằng cách nào đó, 

131
00:08:12,226 --> 00:08:16,504
bạn sử dụng nhiều ví dụ về giao diện đầu ra của một đầu vào nhất định để điều chỉnh và 

132
00:08:16,504 --> 00:08:19,700
điều chỉnh giá trị của các tham số đó nhằm bắt chước hành vi này.

133
00:08:19,700 --> 00:08:23,619
Ví dụ: có thể hình thức học máy đơn giản nhất là hồi quy tuyến tính, 

134
00:08:23,619 --> 00:08:26,630
trong đó đầu vào và đầu ra của bạn là mỗi số đơn lẻ, 

135
00:08:26,630 --> 00:08:29,812
chẳng hạn như diện tích của một ngôi nhà và giá của nó, 

136
00:08:29,812 --> 00:08:34,357
và điều bạn muốn là tìm một dòng phù hợp nhất thông qua điều này. bạn biết đấy, 

137
00:08:34,357 --> 00:08:36,799
dữ liệu để dự đoán giá nhà trong tương lai.

138
00:08:37,440 --> 00:08:42,643
Đường đó được mô tả bởi hai tham số liên tục, chẳng hạn như độ dốc và điểm chặn y, 

139
00:08:42,643 --> 00:08:48,160
và mục tiêu của hồi quy tuyến tính là xác định các tham số đó khớp chặt chẽ với dữ liệu.

140
00:08:48,880 --> 00:08:52,100
Không cần phải nói, các mô hình học sâu trở nên phức tạp hơn nhiều.

141
00:08:52,620 --> 00:08:57,660
Ví dụ: GPT-3 không phải có hai mà có 175 tỷ tham số.

142
00:08:58,120 --> 00:09:01,896
Nhưng vấn đề ở đây là, không có gì chắc chắn rằng bạn có thể tạo ra 

143
00:09:01,896 --> 00:09:05,617
một mô hình khổng lồ nào đó với một số lượng lớn các tham số mà nó 

144
00:09:05,617 --> 00:09:09,560
không quá phù hợp với dữ liệu huấn luyện hoặc hoàn toàn khó huấn luyện.

145
00:09:10,260 --> 00:09:13,135
Học sâu mô tả một lớp mô hình mà trong vài thập kỷ 

146
00:09:13,135 --> 00:09:16,180
qua đã được chứng minh là có khả năng mở rộng đáng kể.

147
00:09:16,480 --> 00:09:21,338
Điều thống nhất chúng là cùng một thuật toán huấn luyện, được gọi là lan truyền ngược, 

148
00:09:21,338 --> 00:09:26,365
và bối cảnh mà tôi muốn bạn hiểu khi chúng ta đi vào là để thuật toán huấn luyện này hoạt 

149
00:09:26,365 --> 00:09:31,280
động tốt trên quy mô lớn, các mô hình này phải tuân theo một định dạng cụ thể nhất định.

150
00:09:31,800 --> 00:09:36,181
Nếu bạn biết định dạng này sẽ được áp dụng, nó sẽ giúp giải thích nhiều lựa chọn 

151
00:09:36,181 --> 00:09:40,400
về cách Transformer xử lý ngôn ngữ, nếu không sẽ có nguy cơ cảm thấy tùy tiện.

152
00:09:41,440 --> 00:09:43,981
Đầu tiên, bất kể bạn đang tạo mô hình nào, đầu 

153
00:09:43,981 --> 00:09:46,740
vào phải được định dạng dưới dạng một mảng số thực.

154
00:09:46,740 --> 00:09:49,189
Điều này có thể có nghĩa là một danh sách các số, 

155
00:09:49,189 --> 00:09:52,325
nó có thể là một mảng hai chiều hoặc rất thường xuyên bạn xử lý 

156
00:09:52,325 --> 00:09:56,000
các mảng có chiều cao hơn, trong đó thuật ngữ chung được sử dụng là tensor.

157
00:09:56,560 --> 00:10:01,510
Bạn thường nghĩ dữ liệu đầu vào đó được chuyển đổi dần dần thành nhiều lớp riêng biệt, 

158
00:10:01,510 --> 00:10:05,550
trong đó, mỗi lớp luôn được cấu trúc như một loại mảng số thực nào đó, 

159
00:10:05,550 --> 00:10:08,680
cho đến khi bạn đến lớp cuối cùng mà bạn coi là đầu ra.

160
00:10:09,280 --> 00:10:13,221
Ví dụ: lớp cuối cùng trong mô hình xử lý văn bản của ta là danh sách các số 

161
00:10:13,221 --> 00:10:17,060
biểu thị phân bố xác suất cho tất cả các mã thông báo tiếp theo có thể có.

162
00:10:17,820 --> 00:10:21,864
Trong học sâu, các tham số mô hình này hầu như luôn được gọi là trọng số và 

163
00:10:21,864 --> 00:10:25,908
điều này là do đặc điểm chính của các mô hình này là cách duy nhất các tham 

164
00:10:25,908 --> 00:10:29,900
số này tương tác với dữ liệu đang được xử lý là thông qua tổng có trọng số.

165
00:10:30,340 --> 00:10:34,360
Bạn cũng sẽ dùng suốt các hàm phi tuyến tính nhưng chúng sẽ không phụ thuộc vào tham số.

166
00:10:35,200 --> 00:10:38,640
Tuy nhiên, thông thường, thay vì thấy các tổng có trọng số hoàn toàn 

167
00:10:38,640 --> 00:10:42,179
trơ trọi và được viết rõ như này, bạn sẽ thấy chúng được đóng gói cùng 

168
00:10:42,179 --> 00:10:45,620
nhau dưới dạng các thành phần khác nhau trong một tích vectơ ma trận.

169
00:10:46,740 --> 00:10:51,053
Điều tương tự cũng xảy ra, nếu bạn nghĩ lại cách hoạt động của phép nhân vectơ ma trận, 

170
00:10:51,053 --> 00:10:54,240
mỗi thành phần trong đầu ra trông giống như một tổng có trọng số.

171
00:10:54,780 --> 00:11:00,190
Về mặt khái niệm, bạn và tôi thường dễ dàng hơn khi nghĩ về các ma trận chứa đầy các tham 

172
00:11:00,190 --> 00:11:05,420
số có thể điều chỉnh được để biến đổi các vectơ được rút ra từ dữ liệu đang được xử lý.

173
00:11:06,340 --> 00:11:14,160
Ví dụ: 175 tỷ trọng số đó trong GPT-3 được sắp xếp thành dưới 28.000 ma trận riêng biệt.

174
00:11:14,660 --> 00:11:18,097
Những ma trận đó lần lượt được chia thành tám loại khác nhau, 

175
00:11:18,097 --> 00:11:22,700
và điều bạn và tôi sắp làm là xem qua từng loại trong số đó để hiểu loại đó làm gì.

176
00:11:23,160 --> 00:11:27,191
Khi chúng ta xét, tôi nghĩ thật thú vị khi tham khảo những 

177
00:11:27,191 --> 00:11:31,360
con số cụ thể từ GPT-3 để đếm chính xác 175 tỷ đó đến từ đâu.

178
00:11:31,880 --> 00:11:34,449
Ngay cả khi ngày nay có những mô hình lớn hơn và tốt hơn, 

179
00:11:34,449 --> 00:11:37,328
thì mô hình này vẫn có một sức hấp dẫn nhất định là mô hình ngôn 

180
00:11:37,328 --> 00:11:40,740
ngữ lớn để thực sự thu hút sự chú ý của thế giới bên ngoài cộng đồng học máy.

181
00:11:41,440 --> 00:11:44,019
Ngoài ra, trên thực tế mà nói, các công ty có xu hướng 

182
00:11:44,019 --> 00:11:46,740
giữ kín những con số cụ thể đối với các mạng hiện đại hơn.

183
00:11:47,360 --> 00:11:52,314
Tôi chỉ muốn dựng bối cảnh khi bạn nhìn kỹ để xem điều gì xảy ra bên trong một công cụ 

184
00:11:52,314 --> 00:11:57,440
như ChatGPT, gần như tất cả các phép tính thực tế trông giống như phép nhân vectơ ma trận.

185
00:11:57,900 --> 00:12:01,495
Có một chút rủi ro khi bị lạc trong biển hàng tỷ con số, 

186
00:12:01,495 --> 00:12:06,163
nhưng bạn nên hình dung rõ ràng sự khác biệt giữa trọng số của mô hình mà 

187
00:12:06,163 --> 00:12:11,840
tôi sẽ luôn tô màu xanh lam hoặc đỏ và dữ liệu được đã được xử lý, tôi sẽ luôn tô màu xám.

188
00:12:12,180 --> 00:12:15,121
Trọng số chính là bộ não thực sự, chúng là những thứ học được 

189
00:12:15,121 --> 00:12:17,920
trong quá trình huấn luyện và quyết định cách nó hoạt động.

190
00:12:18,280 --> 00:12:22,248
Dữ liệu đang được xử lý chỉ mã hóa bất kỳ đầu vào cụ thể nào được đưa 

191
00:12:22,248 --> 00:12:26,500
vào mô hình cho một lần chạy nhất định, chẳng hạn như một đoạn văn bản mẫu.

192
00:12:27,480 --> 00:12:31,950
Với tất cả những nền tảng đó, chúng ta sẽ đi sâu vào bước đầu tiên của ví dụ xử lý văn 

193
00:12:31,950 --> 00:12:36,420
bản này, đó là chia dữ liệu đầu vào thành các phần nhỏ và biến các phần đó thành vectơ.

194
00:12:37,020 --> 00:12:39,911
Tôi đã đề cập đến cách những phần đó được gọi là mã thông báo, 

195
00:12:39,911 --> 00:12:43,536
có thể là các từ hoặc dấu câu, nhưng thỉnh thoảng trong chương này và đặc biệt 

196
00:12:43,536 --> 00:12:47,116
là trong chương tiếp theo, tôi chỉ muốn giả vờ rằng nó được chia thành các từ 

197
00:12:47,116 --> 00:12:48,080
một cách rõ ràng hơn.

198
00:12:48,600 --> 00:12:51,298
Bởi vì con người chúng ta suy nghĩ bằng lời nên điều này sẽ giúp 

199
00:12:51,298 --> 00:12:54,080
việc tham khảo các ví dụ nhỏ và làm rõ từng bước dễ dàng hơn nhiều.

200
00:12:55,260 --> 00:12:59,749
Mô hình này có vốn từ vựng được xác định trước, một số danh sách tất cả các từ có thể, 

201
00:12:59,749 --> 00:13:03,671
chẳng hạn như 50.000 từ trong số đó và ma trận đầu tiên mà chúng ta sẽ gặp, 

202
00:13:03,671 --> 00:13:07,800
được gọi là ma trận nhúng, có một cột duy nhất cho mỗi từ trong số những từ này.

203
00:13:08,940 --> 00:13:13,760
Các cột này quyết định mỗi từ sẽ chuyển thành vectơ nào trong bước đầu tiên đó.

204
00:13:15,100 --> 00:13:18,326
Ta gắn nhãn cho nó là We, và như tất cả các ma trận mà ta thấy, 

205
00:13:18,326 --> 00:13:22,360
các giá trị của nó bắt đầu ngẫu nhiên, nhưng chúng sẽ được học dựa trên dữ liệu.

206
00:13:23,620 --> 00:13:27,570
Biến các từ thành vectơ là phương pháp phổ biến trong machine learning từ rất lâu 

207
00:13:27,570 --> 00:13:31,616
trước khi có Transformer, nhưng sẽ hơi kỳ lạ nếu bạn chưa từng thấy nó trước đây và 

208
00:13:31,616 --> 00:13:35,760
nó đặt nền tảng cho mọi thứ tiếp theo, vậy hãy dành chút thời gian để làm quen với nó.

209
00:13:36,040 --> 00:13:39,728
Ta thường gọi đây là việc nhúng một từ, nó làm bạn nghĩ về các vectơ này 

210
00:13:39,728 --> 00:13:43,620
theo cách rất hình học như các điểm trong một không gian có chiều cao nào đó.

211
00:13:44,180 --> 00:13:48,030
Việc hiển thị danh sách ba số làm tọa độ cho các điểm trong không gian 3D sẽ 

212
00:13:48,030 --> 00:13:51,780
không có vấn đề gì, nhưng việc nhúng từ có xu hướng có chiều cao hơn nhiều.

213
00:13:52,280 --> 00:13:55,629
Trong GPT-3, chúng có 12.288 chiều và như bạn sẽ thấy, 

214
00:13:55,629 --> 00:14:00,440
điều quan trọng là phải làm việc trong một không gian có nhiều hướng khác nhau.

215
00:14:01,180 --> 00:14:04,998
Theo cách tương tự, bạn có thể đưa một lát cắt hai chiều xuyên qua không 

216
00:14:04,998 --> 00:14:07,561
gian 3D và chiếu tất cả các điểm lên lát cắt đó, 

217
00:14:07,561 --> 00:14:11,379
nhằm mục đích tạo hoạt ảnh cho các phần nhúng từ mà một mô hình đơn giản 

218
00:14:11,379 --> 00:14:15,249
mang lại cho tôi, tôi sẽ làm một điều tương tự bằng cách chọn một lát cắt 

219
00:14:15,249 --> 00:14:18,963
ba chiều xuyên qua không gian có nhiều chiều này và chiếu các vectơ từ 

220
00:14:18,963 --> 00:14:20,480
xuống đó và hiển thị kết quả.

221
00:14:21,280 --> 00:14:24,479
Ý tưởng lớn ở đây là khi một mô hình điều chỉnh và điều chỉnh trọng số 

222
00:14:24,479 --> 00:14:27,814
của nó để xác định chính xác cách các từ được nhúng dưới dạng vectơ trong 

223
00:14:27,814 --> 00:14:31,150
quá trình huấn luyện, nó có xu hướng giải quyết trên một tập hợp các phần 

224
00:14:31,150 --> 00:14:34,440
nhúng trong đó các hướng trong không gian có một loại ngữ nghĩa có nghĩa.

225
00:14:34,980 --> 00:14:37,832
Đối với mô hình từ-thành-vectơ đơn giản mà tôi đang chạy ở đây, 

226
00:14:37,832 --> 00:14:41,799
nếu tôi thực hiện tìm kiếm tất cả các từ có phần nhúng gần nhất với từ có nghĩa là tháp, 

227
00:14:41,799 --> 00:14:45,409
bạn sẽ nhận thấy tất cả chúng dường như đều mang lại cảm giác giống như tháp rất 

228
00:14:45,409 --> 00:14:45,900
giống nhau.

229
00:14:46,340 --> 00:14:48,709
Và nếu bạn muốn sử dụng một số Python và tự làm ở nhà, 

230
00:14:48,709 --> 00:14:51,380
thì đây là mô hình cụ thể mà tôi đang sử dụng để tạo hoạt ảnh.

231
00:14:51,620 --> 00:14:54,703
Nó không phải là một Transformer, nhưng nó đủ để minh họa ý tưởng 

232
00:14:54,703 --> 00:14:57,600
rằng các hướng trong không gian có thể mang ý nghĩa ngữ nghĩa.

233
00:14:58,300 --> 00:15:03,307
Một ví dụ rất cổ điển về điều này là nếu bạn lấy sự khác biệt giữa các vectơ của 

234
00:15:03,307 --> 00:15:08,253
phụ nữ và đàn ông, một cái gì đó bạn sẽ hình dung như một vectơ nhỏ nối đầu của 

235
00:15:08,253 --> 00:15:13,200
cái này với đầu của cái kia, nó rất giống với sự khác biệt giữa vua và nữ hoàng.

236
00:15:15,080 --> 00:15:18,441
Vì vậy, giả sử bạn không biết từ dành cho nữ quân vương, 

237
00:15:18,441 --> 00:15:23,572
bạn có thể tìm từ đó bằng cách lấy vua, thêm hướng phụ nữ-nam giới này và tìm kiếm các 

238
00:15:23,572 --> 00:15:25,460
phần nhúng gần nhất với điểm đó.

239
00:15:27,000 --> 00:15:28,200
Ít nhất đại loại như thế.

240
00:15:28,480 --> 00:15:31,481
Dù đây là một ví dụ cổ điển cho mô hình mà tôi đang sử dụng, 

241
00:15:31,481 --> 00:15:35,269
nhưng việc nhúng nữ hoàng thực sự còn xa hơn một chút so với điều này gợi ý, 

242
00:15:35,269 --> 00:15:39,304
có lẽ là do cách sử dụng nữ hoàng trong dữ liệu huấn luyện không chỉ đơn thuần là 

243
00:15:39,304 --> 00:15:40,780
một phiên bản nữ tính của vua.

244
00:15:41,620 --> 00:15:45,260
Khi tôi bật lên, các mối quan hệ gia đình dường như minh họa ý tưởng này tốt hơn nhiều.

245
00:15:46,340 --> 00:15:50,672
Vấn đề là, có vẻ như trong quá trình đào tạo, mô hình nhận thấy thuận lợi khi chọn 

246
00:15:50,672 --> 00:15:54,900
các phần nhúng sao cho một hướng trong không gian này mã hóa thông tin giới tính.

247
00:15:56,800 --> 00:15:59,697
Một ví dụ khác là nếu bạn lấy phần nhúng của Ý, 

248
00:15:59,697 --> 00:16:04,346
và bạn trừ đi phần nhúng của Đức, và thêm phần đó vào phần nhúng của Hitler, 

249
00:16:04,346 --> 00:16:08,090
bạn sẽ có được thứ gì đó rất gần với phần nhúng của Mussolini.

250
00:16:08,570 --> 00:16:12,146
Cứ như thể mô hình đã học cách liên kết một số hướng với người Ý và 

251
00:16:12,146 --> 00:16:15,670
những hướng khác với các nhà lãnh đạo trục trong Thế chiến thứ hai.

252
00:16:16,470 --> 00:16:20,273
Có lẽ ví dụ yêu thích của tôi trong trường hợp này là trong một số mô hình, 

253
00:16:20,273 --> 00:16:23,877
nếu bạn lấy sự khác biệt giữa Đức và Nhật Bản và thêm nó vào món sushi, 

254
00:16:23,877 --> 00:16:26,230
bạn sẽ kết thúc rất gần với xúc xích bratwurst.

255
00:16:27,350 --> 00:16:29,929
Cũng khi chơi trò chơi tìm hàng xóm gần nhất này, 

256
00:16:29,929 --> 00:16:33,850
tôi rất vui khi thấy Kat thân thiết với cả quái thú và quái vật đến mức nào.

257
00:16:34,690 --> 00:16:38,623
Một chút trực quan toán học hữu ích cần nhớ, đặc biệt là để cho chương sau, 

258
00:16:38,623 --> 00:16:43,280
là làm thế nào tích vô hướng của hai vectơ có thể được coi là một cách để đo mức độ chúng 

259
00:16:43,280 --> 00:16:43,850
thẳng hàng.

260
00:16:44,870 --> 00:16:47,938
Về mặt tính toán, tích vô hướng liên quan đến việc nhân tất 

261
00:16:47,938 --> 00:16:50,699
cả các thành phần tương ứng rồi cộng các kết quả lại, 

262
00:16:50,699 --> 00:16:54,330
điều này là tốt vì rất nhiều phép tính phải trông như tổng có trọng số.

263
00:16:55,190 --> 00:17:00,225
Về mặt hình học, tích vô hướng là dương khi các vectơ hướng cùng hướng, 

264
00:17:00,225 --> 00:17:05,609
nó bằng 0 nếu chúng vuông góc và nó âm bất cứ khi nào chúng hướng ngược nhau.

265
00:17:06,550 --> 00:17:11,809
Ví dụ: giả sử bạn đang chơi với mô hình này và bạn đưa ra giả thuyết rằng việc nhúng các 

266
00:17:11,809 --> 00:17:17,010
con mèo trừ một con mèo có thể đại diện cho một loại hướng đa dạng trong không gian này.

267
00:17:17,430 --> 00:17:20,636
Để kiểm tra điều này, tôi sẽ lấy vectơ này và tính tích vô hướng 

268
00:17:20,636 --> 00:17:23,892
của nó dựa trên các phần nhúng của các danh từ số ít nhất định và 

269
00:17:23,892 --> 00:17:27,050
so sánh nó với tích vô hướng của các danh từ số nhiều tương ứng.

270
00:17:27,270 --> 00:17:31,670
Nếu bạn thử nghiệm điều này, bạn sẽ nhận thấy rằng số nhiều thực sự dường như 

271
00:17:31,670 --> 00:17:36,070
luôn cho giá trị cao hơn số ít, cho thấy rằng chúng phù hợp hơn với hướng này.

272
00:17:37,070 --> 00:17:41,141
Cũng thật thú vị nếu bạn lấy tích vô hướng này với các phần nhúng của các từ 1, 

273
00:17:41,141 --> 00:17:43,889
2, 3, v.v., chúng sẽ cho các giá trị tăng dần, do đó, 

274
00:17:43,889 --> 00:17:47,910
như thể ta có thể đo lường một cách định lượng mức độ số nhiều của mô hình tìm 

275
00:17:47,910 --> 00:17:49,030
thấy một từ nhất định.

276
00:17:50,250 --> 00:17:53,570
Một lần nữa, thông tin cụ thể về cách nhúng các từ được học bằng cách sử dụng dữ liệu.

277
00:17:54,050 --> 00:17:57,135
Ma trận nhúng này, có các cột cho ta biết điều gì xảy ra với mỗi từ, 

278
00:17:57,135 --> 00:17:59,550
là chồng trọng số đầu tiên trong mô hình của chúng ta.

279
00:18:00,030 --> 00:18:04,900
Sử dụng số GPT-3, kích thước từ vựng cụ thể là 50.257 và một lần nữa, 

280
00:18:04,900 --> 00:18:09,770
về mặt kỹ thuật, điều này không bao gồm các từ mà là các mã thông báo.

281
00:18:10,630 --> 00:18:14,433
Kích thước nhúng là 12.288 và nhân số đó cho chúng 

282
00:18:14,433 --> 00:18:17,790
ta biết nó bao gồm khoảng 617 triệu trọng số.

283
00:18:18,250 --> 00:18:20,982
Hãy tiếp tục và thêm số này vào bảng kiểm đếm đang chạy, 

284
00:18:20,982 --> 00:18:23,810
hãy nhớ rằng đến cuối cùng chúng ta sẽ đếm được tới 175 tỷ.

285
00:18:25,430 --> 00:18:28,730
Trong trường hợp Transformer, bạn thực sự muốn coi các vectơ trong 

286
00:18:28,730 --> 00:18:32,130
không gian nhúng này không chỉ đơn thuần là biểu thị các từ riêng lẻ.

287
00:18:32,550 --> 00:18:36,272
Thứ nhất, chúng cũng mã hóa thông tin về vị trí của từ đó, 

288
00:18:36,272 --> 00:18:39,741
điều mà chúng ta sẽ nói đến sau, nhưng quan trọng hơn, 

289
00:18:39,741 --> 00:18:42,770
bạn nên nghĩ chúng có khả năng hiểu rõ ngữ cảnh.

290
00:18:43,350 --> 00:18:46,735
Ví dụ: một vectơ bắt đầu tồn tại dưới dạng nhúng từ vua, 

291
00:18:46,735 --> 00:18:51,723
có thể dần dần bị kéo và kéo bởi nhiều khối khác nhau trong mạng này, để cuối cùng, 

292
00:18:51,723 --> 00:18:56,950
nó chỉ theo một hướng cụ thể và nhiều sắc thái hơn mà bằng cách nào đó mã hóa nó là một 

293
00:18:56,950 --> 00:19:02,295
vị vua sống ở Scotland và đã đạt được chức vụ của mình sau khi sát hại vị vua trước đó và 

294
00:19:02,295 --> 00:19:04,730
được mô tả bằng ngôn ngữ của Shakespeare.

295
00:19:05,210 --> 00:19:07,790
Hãy suy nghĩ về sự hiểu biết của riêng bạn về một từ nhất định.

296
00:19:08,250 --> 00:19:13,220
Ý nghĩa của từ đó được môi trường xung quanh thông báo rõ ràng và đôi khi điều này bao 

297
00:19:13,220 --> 00:19:18,190
gồm ngữ cảnh từ khoảng cách xa, vì vậy, khi kết hợp một mô hình có khả năng dự đoán từ 

298
00:19:18,190 --> 00:19:23,161
nào tiếp theo, mục tiêu là bằng cách nào đó cho phép nó kết hợp ngữ cảnh một cách hiệu 

299
00:19:23,161 --> 00:19:23,390
quả.

300
00:19:24,050 --> 00:19:28,521
Nói rõ hơn, ngay trong bước đầu tiên đó, khi bạn tạo mảng vectơ dựa trên văn bản đầu vào, 

301
00:19:28,521 --> 00:19:30,956
mỗi vectơ đó chỉ được lấy ra khỏi ma trận nhúng, 

302
00:19:30,956 --> 00:19:35,179
vậy ban đầu mỗi vectơ chỉ có thể mã hóa nghĩa của một từ mà không cần bất kỳ đầu vào 

303
00:19:35,179 --> 00:19:36,770
nào từ môi trường xung quanh nó.

304
00:19:37,710 --> 00:19:41,539
Nhưng bạn nên nghĩ đến mục tiêu chính của mạng lưới này mà nó chảy 

305
00:19:41,539 --> 00:19:45,254
qua là cho phép mỗi vectơ đó hấp thụ một ý nghĩa phong phú và cụ 

306
00:19:45,254 --> 00:19:48,970
thể hơn nhiều so với những gì mà các từ riêng lẻ có thể biểu thị.

307
00:19:49,510 --> 00:19:52,370
Mạng chỉ có thể xử lý một số vectơ cố định tại một thời điểm, 

308
00:19:52,370 --> 00:19:54,170
được gọi là kích thước ngữ cảnh của nó.

309
00:19:54,510 --> 00:19:59,083
Đối với GPT-3, nó được đào tạo với kích thước ngữ cảnh là 2048, do đó, 

310
00:19:59,083 --> 00:20:03,463
dữ liệu truyền qua mạng luôn trông giống như mảng gồm 2048 cột này, 

311
00:20:03,463 --> 00:20:05,010
mỗi cột có 12.000 chiều.

312
00:20:05,590 --> 00:20:08,465
Kích thước ngữ cảnh này giới hạn số lượng văn bản mà 

313
00:20:08,465 --> 00:20:11,830
Transformer có thể kết hợp khi đưa ra dự đoán về từ tiếp theo.

314
00:20:12,370 --> 00:20:15,894
Đây là lý do tại sao các cuộc trò chuyện dài với một số chatbot nhất định, 

315
00:20:15,894 --> 00:20:18,243
chẳng hạn như các phiên bản đầu tiên của ChatGPT, 

316
00:20:18,243 --> 00:20:22,050
thường mang lại cảm giác như bot sẽ mất mạch trò chuyện khi bạn tiếp tục quá lâu.

317
00:20:23,030 --> 00:20:25,631
Chúng ta sẽ đi vào chi tiết cần chú ý vào thời gian thích hợp, 

318
00:20:25,631 --> 00:20:28,810
nhưng bỏ qua phần trước tôi muốn nói một chút về những gì xảy ra ở phần cuối.

319
00:20:29,450 --> 00:20:32,033
Hãy nhớ rằng, đầu ra mong muốn là phân bố xác suất 

320
00:20:32,033 --> 00:20:34,870
trên tất cả các mã thông báo có thể xuất hiện tiếp theo.

321
00:20:35,170 --> 00:20:39,225
Ví dụ: nếu từ cuối cùng là Giáo sư và ngữ cảnh bao gồm những từ như Harry 

322
00:20:39,225 --> 00:20:43,170
Potter và ngay trước đó chúng ta thấy giáo viên ít được yêu thích nhất, 

323
00:20:43,170 --> 00:20:47,226
đồng thời nếu bạn cho tôi chút thời gian bằng cách để tôi giả vờ rằng các 

324
00:20:47,226 --> 00:20:51,336
mã thông báo trông giống như các từ đầy đủ, thì một mạng lưới được đào tạo 

325
00:20:51,336 --> 00:20:55,830
bài bản đã xây dựng được kiến thức về Harry Potter có lẽ sẽ đánh giá cao từ Snape.

326
00:20:56,510 --> 00:20:57,970
Điều này bao gồm hai bước khác nhau.

327
00:20:58,310 --> 00:21:02,960
Cách đầu tiên là sử dụng một ma trận khác ánh xạ vectơ cuối cùng trong ngữ cảnh 

328
00:21:02,960 --> 00:21:07,610
đó tới danh sách 50.000 giá trị, một giá trị cho mỗi mã thông báo trong từ vựng.

329
00:21:08,170 --> 00:21:11,928
Sau đó, có một hàm bình thường hóa điều này thành phân bố xác suất, 

330
00:21:11,928 --> 00:21:16,129
nó được gọi là Softmax và chúng ta sẽ nói nhiều hơn về nó chỉ sau một giây, 

331
00:21:16,129 --> 00:21:20,109
nhưng trước đó có vẻ hơi kỳ lạ khi chỉ sử dụng phép nhúng cuối cùng này 

332
00:21:20,109 --> 00:21:23,315
để đưa ra dự đoán, khi xét cho cùng, ở bước cuối cùng đó, 

333
00:21:23,315 --> 00:21:28,290
có hàng nghìn vectơ khác trong lớp chỉ nằm ở đó với ý nghĩa giàu ngữ cảnh của riêng chúng.

334
00:21:28,930 --> 00:21:32,414
Điều này liên quan đến thực tế là trong quá trình đào tạo, 

335
00:21:32,414 --> 00:21:36,312
nó sẽ hiệu quả hơn nhiều nếu bạn sử dụng từng vectơ đó ở lớp cuối 

336
00:21:36,312 --> 00:21:40,270
cùng để đồng thời đưa ra dự đoán về những gì sẽ xảy ra ngay sau nó.

337
00:21:40,970 --> 00:21:45,090
Còn rất nhiều điều để nói về việc huấn luyện sau này, nhưng tôi chỉ muốn nói ngay bây giờ.

338
00:21:45,730 --> 00:21:49,690
Ma trận này được gọi là ma trận Hủy nhúng và ta đặt cho nó ký hiệu WU.

339
00:21:50,210 --> 00:21:52,526
Một lần nữa, giống như tất cả các ma trận trọng số mà ta thấy, 

340
00:21:52,526 --> 00:21:55,284
các mục của nó bắt đầu một cách ngẫu nhiên, nhưng chúng được học trong quá 

341
00:21:55,284 --> 00:21:55,910
trình huấn luyện.

342
00:21:56,470 --> 00:22:00,937
Giữ điểm trên tổng số tham số của chúng ta, ma trận Hủy nhúng này có một 

343
00:22:00,937 --> 00:22:05,650
hàng cho mỗi từ trong từ vựng và mỗi hàng có cùng số phần tử như chiều nhúng.

344
00:22:06,410 --> 00:22:10,186
Nó rất giống với ma trận nhúng, chỉ với thứ tự được hoán đổi, do đó, 

345
00:22:10,186 --> 00:22:12,868
nó bổ sung thêm 617 triệu tham số khác vào mạng, 

346
00:22:12,868 --> 00:22:16,535
nghĩa là số lượng của chúng ta cho đến nay là hơn một tỷ một chút, 

347
00:22:16,535 --> 00:22:20,257
một phần nhỏ nhưng không hoàn toàn không đáng kể trong số tổng cộng 

348
00:22:20,257 --> 00:22:21,790
175 tỷ mà chúng ta kết thúc.

349
00:22:22,550 --> 00:22:26,817
Bài học nhỏ cuối cùng của chương này, tôi muốn nói nhiều hơn về hàm softmax này, 

350
00:22:26,817 --> 00:22:30,610
vì nó sẽ xuất hiện một cách khác khi chúng ta đi sâu vào các khối chú ý.

351
00:22:31,430 --> 00:22:35,612
Ý tưởng là nếu bạn muốn một chuỗi số hoạt động như một phân phối xác suất, 

352
00:22:35,612 --> 00:22:39,181
chẳng hạn như phân phối trên tất cả các từ tiếp theo có thể có, 

353
00:22:39,181 --> 00:22:43,530
thì mỗi giá trị phải nằm trong khoảng từ 0 đến 1 và bạn cũng cần tất cả chúng 

354
00:22:43,530 --> 00:22:44,590
để có tổng bằng 1 .

355
00:22:45,250 --> 00:22:48,354
Tuy nhiên, nếu bạn đang chơi trò chơi học tập trong đó mọi thứ 

356
00:22:48,354 --> 00:22:50,769
bạn làm trông giống như phép nhân vectơ ma trận, 

357
00:22:50,769 --> 00:22:54,810
thì kết quả đầu ra bạn nhận được theo mặc định hoàn toàn không tuân theo điều này.

358
00:22:55,330 --> 00:22:57,576
Các giá trị thường âm hoặc lớn hơn 1 rất nhiều 

359
00:22:57,576 --> 00:22:59,870
và gần như chắc chắn chúng không có tổng bằng 1.

360
00:23:00,510 --> 00:23:05,930
Softmax là cách tiêu chuẩn để biến một danh sách các số tùy ý thành một phân phối hợp lệ 

361
00:23:05,930 --> 00:23:11,290
sao cho các giá trị lớn nhất gần bằng 1 và các giá trị nhỏ hơn có giá trị rất gần với 0.

362
00:23:11,830 --> 00:23:13,070
Đó là tất cả những gì bạn thực sự cần biết.

363
00:23:13,090 --> 00:23:17,987
Nhưng nếu bạn tò mò, cách thức hoạt động là trước tiên nâng e lên lũy thừa của mỗi số, 

364
00:23:17,987 --> 00:23:21,195
nghĩa là bây giờ bạn có một danh sách các giá trị dương, 

365
00:23:21,195 --> 00:23:25,192
sau đó bạn có thể lấy tổng của tất cả các giá trị dương đó và chia mỗi 

366
00:23:25,192 --> 00:23:29,470
số hạng theo tổng đó, nó sẽ chuẩn hóa nó thành một danh sách có tổng bằng 1.

367
00:23:30,170 --> 00:23:34,202
Bạn sẽ nhận thấy rằng nếu một trong các số ở đầu vào lớn hơn đáng kể so với các 

368
00:23:34,202 --> 00:23:38,285
số còn lại, thì trong đầu ra, số hạng tương ứng sẽ chiếm ưu thế trong phân phối, 

369
00:23:38,285 --> 00:23:42,470
vậy nếu bạn lấy mẫu từ số đó thì bạn gần như chắc chắn chỉ chọn đầu vào tối đa hóa.

370
00:23:42,990 --> 00:23:46,876
Nhưng nó nhẹ nhàng hơn việc chỉ chọn mức tối đa theo nghĩa là khi các 

371
00:23:46,876 --> 00:23:50,874
giá trị khác lớn tương tự, chúng cũng có trọng số có ý nghĩa trong phân 

372
00:23:50,874 --> 00:23:54,650
phối và mọi thứ thay đổi liên tục khi bạn liên tục thay đổi đầu vào.

373
00:23:55,130 --> 00:23:59,684
Trong một số trường hợp, chẳng hạn như khi ChatGPT đang sử dụng phân phối này 

374
00:23:59,684 --> 00:24:04,297
để tạo từ tiếp theo, sẽ có chỗ cho một chút thú vị hơn bằng cách thêm một chút 

375
00:24:04,297 --> 00:24:08,910
gia vị bổ sung vào hàm này, với hằng số t được đưa vào mẫu số của các số mũ đó.

376
00:24:09,550 --> 00:24:14,151
Chúng ta gọi nó là nhiệt độ, vì nó gần giống với vai trò của nhiệt độ trong các 

377
00:24:14,151 --> 00:24:18,293
phương trình nhiệt động lực học nhất định, và kết quả là khi t lớn hơn, 

378
00:24:18,293 --> 00:24:21,170
bạn chú trọng nhiều hơn đến các giá trị thấp hơn, 

379
00:24:21,170 --> 00:24:25,771
nghĩa là sự phân bố đồng đều hơn một chút, và nếu t nhỏ hơn thì giá trị lớn hơn 

380
00:24:25,771 --> 00:24:28,705
sẽ chiếm ưu thế mạnh hơn, trong đó ở mức cực đoan, 

381
00:24:28,705 --> 00:24:32,790
đặt t bằng 0 có nghĩa là tất cả trọng số sẽ chuyển sang giá trị tối đa.

382
00:24:33,470 --> 00:24:37,772
Ví dụ: tôi sẽ yêu cầu GPT-3 tạo một câu chuyện với văn bản gốc, 

383
00:24:37,772 --> 00:24:42,950
ngày xưa có A, nhưng tôi sẽ sử dụng nhiệt độ khác nhau trong từng trường hợp.

384
00:24:43,630 --> 00:24:47,968
Nhiệt độ bằng 0 có nghĩa là nó luôn đi theo từ dễ đoán nhất và những 

385
00:24:47,968 --> 00:24:52,370
gì bạn nhận được cuối cùng chỉ là một đạo hàm sáo rỗng của Goldilocks.

386
00:24:53,010 --> 00:24:56,625
Nhiệt độ cao hơn giúp nó có cơ hội chọn những từ ít có khả năng xảy ra hơn, 

387
00:24:56,625 --> 00:24:57,910
nhưng nó đi kèm với rủi ro.

388
00:24:58,230 --> 00:25:01,412
Trong trường hợp này, câu chuyện bắt đầu độc đáo hơn, 

389
00:25:01,412 --> 00:25:06,010
về một nghệ sĩ web trẻ đến từ Hàn Quốc, nhưng nó nhanh chóng trở nên vô nghĩa.

390
00:25:06,950 --> 00:25:10,830
Về mặt kỹ thuật, API không thực sự cho phép bạn chọn nhiệt độ lớn hơn 2.

391
00:25:11,170 --> 00:25:15,160
Không có lý do toán học nào cho việc này, đó chỉ là một ràng buộc tùy ý được áp 

392
00:25:15,160 --> 00:25:19,350
đặt để giữ cho công cụ của họ không bị nhìn thấy đang tạo ra những thứ quá vô nghĩa.

393
00:25:19,870 --> 00:25:24,187
Vậy nếu bạn tò mò, cách hoạt động thực sự của hoạt ảnh này là tôi đang lấy 20 mã thông 

394
00:25:24,187 --> 00:25:27,114
báo tiếp theo có khả năng xảy ra cao nhất mà GPT-3 tạo ra, 

395
00:25:27,114 --> 00:25:31,530
có vẻ như là tổng tối đa mà họ sẽ cung cấp cho tôi và sau đó tôi điều chỉnh xác suất dựa 

396
00:25:31,530 --> 00:25:32,970
trên theo số mũ của 1 phần 5.

397
00:25:33,130 --> 00:25:37,408
Là một số hạng khác, giống như cách bạn có thể gọi các thành phần đầu 

398
00:25:37,408 --> 00:25:41,626
ra của xác suất của hàm này, mọi người thường coi đầu vào là logits, 

399
00:25:41,626 --> 00:25:46,150
hoặc một số người nói logits, một số người nói logits, tôi sẽ nói logits .

400
00:25:46,530 --> 00:25:49,142
Vì vậy, chẳng hạn, khi bạn nhập một số văn bản, 

401
00:25:49,142 --> 00:25:52,789
bạn có tất cả các từ nhúng này chảy qua mạng và bạn thực hiện phép 

402
00:25:52,789 --> 00:25:56,436
nhân cuối cùng này với ma trận không nhúng, những người học máy sẽ 

403
00:25:56,436 --> 00:26:00,083
coi các thành phần trong đầu ra thô, không chuẩn hóa đó là nhật ký 

404
00:26:00,083 --> 00:26:01,390
để dự đoán từ tiếp theo.

405
00:26:03,330 --> 00:26:07,847
Phần lớn mục tiêu của chương này là đặt nền móng cho việc hiểu cơ chế chú ý, 

406
00:26:07,847 --> 00:26:10,370
phong cách "làm đi làm lại" của Karate Kid.

407
00:26:10,850 --> 00:26:14,780
Bạn thấy đấy, nếu bạn có trực quan tốt về cách nhúng từ, về softmax, 

408
00:26:14,780 --> 00:26:18,938
về cách các tích vô hướng đo lường độ tương tự và cũng là tiền đề cơ bản 

409
00:26:18,938 --> 00:26:23,096
rằng hầu hết các phép tính phải trông giống như phép nhân ma trận với ma 

410
00:26:23,096 --> 00:26:27,653
trận chứa đầy các tham số có thể điều chỉnh được, thì hãy hiểu sự chú ý Cơ chế, 

411
00:26:27,653 --> 00:26:32,210
phần nền tảng này trong toàn bộ sự bùng nổ AI hiện đại, phải tương đối trơn tru.

412
00:26:32,650 --> 00:26:34,510
Vì điều đó, tham gia cùng tôi trong chương sau.

413
00:26:36,390 --> 00:26:38,703
Khi tôi xuất bản video này, bản nháp của chương 

414
00:26:38,703 --> 00:26:41,210
sau sẽ có sẵn để những người ủng hộ Patreon xem xét.

415
00:26:41,770 --> 00:26:44,308
Phiên bản cuối cùng sẽ được công bố rộng rãi sau một hoặc hai tuần, 

416
00:26:44,308 --> 00:26:47,370
điều này thường phụ thuộc vào việc tôi sẽ thay đổi bao nhiêu dựa trên đánh giá đó.

417
00:26:47,810 --> 00:26:50,004
Trong khi chờ đợi, nếu bạn muốn thu hút sự chú ý và 

418
00:26:50,004 --> 00:26:52,410
nếu bạn muốn giúp đỡ kênh một chút thì kênh vẫn đang chờ.

