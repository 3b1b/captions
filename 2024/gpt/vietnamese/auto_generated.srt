1
00:00:00,000 --> 00:00:04,560
Các chữ cái đầu GPT là viết tắt của Máy biến áp được đào tạo trước.

2
00:00:05,220 --> 00:00:09,020
Từ đầu tiên đó khá đơn giản, đây là những bot tạo ra văn bản mới.

3
00:00:09,800 --> 00:00:13,229
Được đào tạo trước đề cập đến cách mô hình trải qua quá trình học hỏi 

4
00:00:13,229 --> 00:00:16,561
từ một lượng lớn dữ liệu và tiền tố ám chỉ rằng có nhiều chỗ hơn để 

5
00:00:16,561 --> 00:00:20,040
tinh chỉnh mô hình trong các nhiệm vụ cụ thể bằng cách đào tạo bổ sung.

6
00:00:20,720 --> 00:00:22,900
Nhưng lời cuối cùng, đó mới là phần quan trọng thực sự.

7
00:00:23,380 --> 00:00:26,293
Máy biến áp là một loại mạng lưới thần kinh cụ thể, 

8
00:00:26,293 --> 00:00:31,000
một mô hình học máy và là phát minh cốt lõi tạo nền tảng cho sự bùng nổ AI hiện nay.

9
00:00:31,740 --> 00:00:35,291
Điều tôi muốn làm với video này và các chương tiếp theo là giải 

10
00:00:35,291 --> 00:00:39,120
thích bằng hình ảnh về những gì thực sự xảy ra bên trong máy biến áp.

11
00:00:39,700 --> 00:00:42,820
Chúng tôi sẽ theo dõi dữ liệu chảy qua nó và thực hiện từng bước một.

12
00:00:43,440 --> 00:00:47,380
Có nhiều loại mô hình khác nhau mà bạn có thể xây dựng bằng cách sử dụng máy biến áp.

13
00:00:47,800 --> 00:00:50,800
Một số kiểu máy tiếp nhận âm thanh và tạo ra bản ghi.

14
00:00:51,340 --> 00:00:56,220
Câu này xuất phát từ một mô hình ngược lại, tạo ra lời nói tổng hợp chỉ từ văn bản.

15
00:00:56,660 --> 00:01:00,950
Tất cả những công cụ đã gây bão trên toàn thế giới vào năm 2022 như Dolly và 

16
00:01:00,950 --> 00:01:05,519
Midjourney có chức năng mô tả văn bản và tạo ra hình ảnh đều dựa trên máy biến áp.

17
00:01:06,000 --> 00:01:09,065
Ngay cả khi tôi không thể hiểu được sinh vật bánh là gì, 

18
00:01:09,065 --> 00:01:13,100
tôi vẫn rất ngạc nhiên rằng loại điều này thậm chí còn có thể xảy ra từ xa.

19
00:01:13,900 --> 00:01:17,896
Và máy biến áp ban đầu được Google giới thiệu vào năm 2017 đã được phát minh 

20
00:01:17,896 --> 00:01:22,100
cho trường hợp sử dụng cụ thể là dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác.

21
00:01:22,660 --> 00:01:25,214
Nhưng biến thể mà bạn và tôi sẽ tập trung vào, 

22
00:01:25,214 --> 00:01:27,769
loại làm nền tảng cho các công cụ như ChatGPT, 

23
00:01:27,769 --> 00:01:31,139
sẽ là một mô hình được đào tạo để tiếp nhận một đoạn văn bản, 

24
00:01:31,139 --> 00:01:35,814
thậm chí có thể có một số hình ảnh hoặc âm thanh xung quanh đi kèm và đưa ra dự đoán. 

25
00:01:35,814 --> 00:01:38,260
cho những gì xảy ra tiếp theo trong đoạn văn.

26
00:01:38,600 --> 00:01:43,800
Dự đoán đó có dạng phân bố xác suất trên nhiều đoạn văn bản khác nhau có thể theo sau.

27
00:01:45,040 --> 00:01:47,532
Thoạt nhìn, bạn có thể nghĩ rằng việc dự đoán từ tiếp theo 

28
00:01:47,532 --> 00:01:49,940
giống như một mục tiêu rất khác với việc tạo văn bản mới.

29
00:01:50,180 --> 00:01:52,846
Nhưng khi bạn có một mô hình dự đoán như thế này, 

30
00:01:52,846 --> 00:01:56,740
một điều đơn giản là bạn tạo một đoạn văn bản dài hơn là cung cấp cho nó 

31
00:01:56,740 --> 00:02:00,473
một đoạn mã ban đầu để làm việc, yêu cầu nó lấy một mẫu ngẫu nhiên từ 

32
00:02:00,473 --> 00:02:03,140
phân phối mà nó vừa tạo, nối mẫu đó vào văn bản , 

33
00:02:03,140 --> 00:02:07,513
rồi chạy lại toàn bộ quá trình để đưa ra dự đoán mới dựa trên tất cả văn bản mới, 

34
00:02:07,513 --> 00:02:09,539
bao gồm cả nội dung vừa được thêm vào.

35
00:02:10,100 --> 00:02:13,000
Tôi không biết bạn thế nào, nhưng tôi thực sự không cảm thấy điều này thực sự hiệu quả.

36
00:02:13,420 --> 00:02:16,340
Ví dụ: trong hoạt ảnh này, tôi đang chạy GPT-2 trên máy tính 

37
00:02:16,340 --> 00:02:19,260
xách tay của mình và để nó liên tục dự đoán cũng như lấy mẫu 

38
00:02:19,260 --> 00:02:22,420
đoạn văn bản tiếp theo để tạo một câu chuyện dựa trên văn bản gốc.

39
00:02:22,420 --> 00:02:26,120
Câu chuyện thực sự không có nhiều ý nghĩa như vậy.

40
00:02:26,500 --> 00:02:29,788
Nhưng thay vào đó, nếu tôi đổi nó lấy các lệnh gọi API sang GPT-3, 

41
00:02:29,788 --> 00:02:33,272
mô hình cơ bản tương tự, chỉ lớn hơn nhiều, đột nhiên gần như kỳ diệu, 

42
00:02:33,272 --> 00:02:36,904
chúng ta có được một câu chuyện hợp lý, một câu chuyện thậm chí dường như 

43
00:02:36,904 --> 00:02:40,880
suy ra rằng một sinh vật pi sẽ sống trong một vùng đất của toán học và tính toán.

44
00:02:41,580 --> 00:02:45,013
Quá trình dự đoán và lấy mẫu lặp lại ở đây về cơ bản là những 

45
00:02:45,013 --> 00:02:48,335
gì đang xảy ra khi bạn tương tác với ChatGPT hoặc bất kỳ mô 

46
00:02:48,335 --> 00:02:51,880
hình ngôn ngữ lớn nào khác và bạn thấy chúng tạo ra từng từ một.

47
00:02:52,480 --> 00:02:55,817
Trên thực tế, một tính năng mà tôi rất thích là khả 

48
00:02:55,817 --> 00:02:59,220
năng xem sự phân bổ cơ bản cho mỗi từ mới mà nó chọn.

49
00:03:03,820 --> 00:03:06,142
Hãy bắt đầu mọi thứ bằng bản xem trước ở mức rất 

50
00:03:06,142 --> 00:03:08,180
cao về cách dữ liệu truyền qua máy biến áp.

51
00:03:08,640 --> 00:03:11,050
Chúng tôi sẽ dành nhiều thời gian hơn để thúc đẩy, 

52
00:03:11,050 --> 00:03:14,595
diễn giải và mở rộng chi tiết của từng bước, nhưng nói một cách tổng quát, 

53
00:03:14,595 --> 00:03:18,660
khi một trong những chatbot này tạo ra một từ nhất định, đây là những gì đang diễn ra.

54
00:03:19,080 --> 00:03:22,040
Đầu tiên, đầu vào được chia thành nhiều phần nhỏ.

55
00:03:22,620 --> 00:03:25,845
Những phần này được gọi là mã thông báo và trong trường hợp văn bản, 

56
00:03:25,845 --> 00:03:29,820
chúng có xu hướng là các từ hoặc các đoạn từ nhỏ hoặc các tổ hợp ký tự phổ biến khác.

57
00:03:30,740 --> 00:03:33,864
Nếu có liên quan đến hình ảnh hoặc âm thanh thì mã thông báo có thể 

58
00:03:33,864 --> 00:03:37,080
là những mảng nhỏ của hình ảnh đó hoặc những đoạn nhỏ của âm thanh đó.

59
00:03:37,580 --> 00:03:41,048
Mỗi một trong số các mã thông báo này sau đó được liên kết với một vectơ, 

60
00:03:41,048 --> 00:03:44,985
nghĩa là một số danh sách các số, nhằm mục đích mã hóa bằng cách nào đó ý nghĩa của 

61
00:03:44,985 --> 00:03:45,360
phần đó.

62
00:03:45,880 --> 00:03:49,855
Nếu bạn coi các vectơ này là tọa độ trong một không gian có chiều rất cao, 

63
00:03:49,855 --> 00:03:54,255
thì các từ có ý nghĩa tương tự có xu hướng nằm trên các vectơ gần nhau trong không 

64
00:03:54,255 --> 00:03:54,680
gian đó.

65
00:03:55,280 --> 00:03:59,761
Sau đó, chuỗi vectơ này sẽ đi qua một hoạt động được gọi là khối chú ý và điều này cho 

66
00:03:59,761 --> 00:04:04,190
phép các vectơ giao tiếp với nhau và truyền thông tin qua lại để cập nhật giá trị của 

67
00:04:04,190 --> 00:04:04,500
chúng.

68
00:04:04,880 --> 00:04:08,248
Ví dụ: ý nghĩa của từ mô hình trong cụm từ mô hình học 

69
00:04:08,248 --> 00:04:11,800
máy khác với nghĩa của nó trong cụm từ mô hình thời trang.

70
00:04:12,260 --> 00:04:16,968
Khối chú ý là thứ chịu trách nhiệm tìm ra những từ nào trong ngữ cảnh có liên quan 

71
00:04:16,968 --> 00:04:21,959
đến việc cập nhật ý nghĩa của những từ khác và cách cập nhật chính xác những ý nghĩa đó.

72
00:04:22,500 --> 00:04:24,798
Và một lần nữa, bất cứ khi nào tôi sử dụng từ có nghĩa, 

73
00:04:24,798 --> 00:04:28,040
điều này bằng cách nào đó được mã hóa hoàn toàn trong các mục của các vectơ đó.

74
00:04:29,180 --> 00:04:32,054
Sau đó, các vectơ này trải qua một loại hoạt động khác và 

75
00:04:32,054 --> 00:04:34,978
tùy thuộc vào nguồn mà bạn đang đọc, vectơ này sẽ được gọi 

76
00:04:34,978 --> 00:04:38,200
là perceptron nhiều lớp hoặc có thể là lớp chuyển tiếp tiếp liệu.

77
00:04:38,580 --> 00:04:40,821
Và ở đây các vectơ không liên lạc với nhau, chúng 

78
00:04:40,821 --> 00:04:42,660
đều trải qua cùng một thao tác song song.

79
00:04:43,060 --> 00:04:45,473
Và mặc dù khối này khó diễn giải hơn một chút, 

80
00:04:45,473 --> 00:04:49,069
nhưng sau này chúng ta sẽ nói về bước này giống như đặt một danh sách 

81
00:04:49,069 --> 00:04:52,715
dài các câu hỏi về mỗi vectơ và sau đó cập nhật chúng dựa trên câu trả 

82
00:04:52,715 --> 00:04:54,000
lời cho những câu hỏi đó.

83
00:04:54,900 --> 00:05:00,078
Tất cả các phép toán trong cả hai khối này trông giống như một đống phép nhân ma 

84
00:05:00,078 --> 00:05:05,320
trận khổng lồ và công việc chính của chúng ta là hiểu cách đọc các ma trận cơ bản.

85
00:05:06,980 --> 00:05:10,395
Tôi đang xem qua một số chi tiết về một số bước chuẩn hóa diễn ra ở giữa, 

86
00:05:10,395 --> 00:05:12,980
nhưng xét cho cùng thì đây vẫn là bản xem trước cấp cao.

87
00:05:13,680 --> 00:05:18,559
Sau đó, về cơ bản, quá trình lặp lại, bạn đi qua lại giữa các khối chú ý và các 

88
00:05:18,559 --> 00:05:21,486
khối nhận thức nhiều lớp, cho đến khi kết thúc, 

89
00:05:21,486 --> 00:05:26,365
hy vọng rằng tất cả ý nghĩa thiết yếu của đoạn văn bằng cách nào đó đã được đưa 

90
00:05:26,365 --> 00:05:28,500
vào vectơ cuối cùng trong trình tự.

91
00:05:28,920 --> 00:05:31,956
Sau đó, chúng tôi thực hiện một thao tác nhất định trên vectơ 

92
00:05:31,956 --> 00:05:35,726
cuối cùng để tạo ra phân bố xác suất trên tất cả các mã thông báo có thể có, 

93
00:05:35,726 --> 00:05:38,420
tất cả các đoạn văn bản nhỏ có thể xuất hiện tiếp theo.

94
00:05:38,980 --> 00:05:42,428
Và như tôi đã nói, khi bạn có một công cụ dự đoán điều gì sẽ xảy ra 

95
00:05:42,428 --> 00:05:45,979
tiếp theo với một đoạn văn bản, bạn có thể cung cấp cho nó một ít văn 

96
00:05:45,979 --> 00:05:49,884
bản gốc và để nó chơi liên tục trò chơi dự đoán điều gì sẽ xảy ra tiếp theo, 

97
00:05:49,884 --> 00:05:53,080
lấy mẫu từ phân phối, nối thêm nó rồi lặp đi lặp lại nhiều lần.

98
00:05:53,640 --> 00:05:57,325
Một số bạn biết có thể nhớ bao lâu trước khi ChatGPT xuất hiện, 

99
00:05:57,325 --> 00:06:00,838
đây là bản demo ban đầu của GPT-3, bạn sẽ yêu cầu nó tự động 

100
00:06:00,838 --> 00:06:04,640
hoàn thành các câu chuyện và bài luận dựa trên đoạn trích ban đầu.

101
00:06:05,580 --> 00:06:08,524
Để biến một công cụ như thế này thành một chatbot, 

102
00:06:08,524 --> 00:06:12,853
điểm bắt đầu dễ dàng nhất là có một ít văn bản thiết lập cài đặt của người 

103
00:06:12,853 --> 00:06:17,125
dùng tương tác với trợ lý AI hữu ích, bạn sẽ gọi lời nhắc hệ thống là gì, 

104
00:06:17,125 --> 00:06:21,513
sau đó bạn sẽ sử dụng câu hỏi hoặc lời nhắc ban đầu của người dùng làm đoạn 

105
00:06:21,513 --> 00:06:25,727
hội thoại đầu tiên, sau đó bạn bắt đầu dự đoán trợ lý AI hữu ích như vậy 

106
00:06:25,727 --> 00:06:26,940
sẽ nói gì để đáp lại.

107
00:06:27,720 --> 00:06:31,801
Còn nhiều điều để nói về một bước đào tạo cần thiết để thực hiện tốt công việc này, 

108
00:06:31,801 --> 00:06:33,940
nhưng ở cấp độ cao thì đây chính là ý tưởng.

109
00:06:35,720 --> 00:06:40,661
Trong chương này, bạn và tôi sẽ mở rộng chi tiết về những gì xảy ra ở phần đầu của mạng, 

110
00:06:40,661 --> 00:06:44,937
ở phần cuối của mạng và tôi cũng muốn dành nhiều thời gian để xem lại một số 

111
00:06:44,937 --> 00:06:49,212
kiến thức nền tảng quan trọng. , những thứ lẽ ra là bản chất thứ hai đối với 

112
00:06:49,212 --> 00:06:52,600
bất kỳ kỹ sư máy học nào vào thời điểm máy biến áp xuất hiện.

113
00:06:53,060 --> 00:06:56,525
Nếu bạn cảm thấy thoải mái với kiến thức nền tảng đó và hơi thiếu kiên nhẫn, 

114
00:06:56,525 --> 00:06:58,819
bạn có thể thoải mái chuyển sang chương tiếp theo, 

115
00:06:58,819 --> 00:07:02,780
chương này sẽ tập trung vào các khối chú ý, thường được coi là trái tim của máy biến áp.

116
00:07:03,360 --> 00:07:07,187
Sau đó, tôi muốn nói nhiều hơn về các khối perceptron nhiều lớp này, 

117
00:07:07,187 --> 00:07:11,680
cách đào tạo hoạt động và một số chi tiết khác sẽ bị bỏ qua cho đến thời điểm đó.

118
00:07:12,180 --> 00:07:16,289
Đối với bối cảnh rộng hơn, những video này là phần bổ sung cho một loạt video nhỏ về 

119
00:07:16,289 --> 00:07:19,431
học sâu và cũng không sao nếu bạn chưa xem những video trước đó, 

120
00:07:19,431 --> 00:07:23,492
tôi nghĩ bạn có thể xem không theo thứ tự, nhưng trước khi đi sâu vào cụ thể về máy 

121
00:07:23,492 --> 00:07:27,601
biến áp, tôi nghĩ cần đảm bảo rằng chúng ta có cùng quan điểm về tiền đề và cấu trúc 

122
00:07:27,601 --> 00:07:28,520
cơ bản của học sâu.

123
00:07:29,020 --> 00:07:32,742
Có nguy cơ nêu rõ điều hiển nhiên, đây là một cách tiếp cận với học máy, 

124
00:07:32,742 --> 00:07:35,852
mô tả bất kỳ mô hình nào mà bạn đang sử dụng dữ liệu để bằng 

125
00:07:35,852 --> 00:07:38,300
cách nào đó xác định cách hoạt động của mô hình.

126
00:07:39,140 --> 00:07:43,649
Ý tôi là, giả sử bạn muốn một hàm nhận một hình ảnh và nó tạo ra một nhãn mô tả 

127
00:07:43,649 --> 00:07:48,045
nó hoặc ví dụ của chúng tôi về dự đoán từ tiếp theo cho một đoạn văn bản hoặc 

128
00:07:48,045 --> 00:07:52,780
bất kỳ tác vụ nào khác có vẻ yêu cầu một số yếu tố trực giác và nhận dạng khuôn mẫu.

129
00:07:53,200 --> 00:07:55,879
Ngày nay chúng ta gần như coi điều này là đương nhiên, 

130
00:07:55,879 --> 00:07:59,532
nhưng ý tưởng của học máy là thay vì cố gắng xác định rõ ràng quy trình về 

131
00:07:59,532 --> 00:08:03,381
cách thực hiện tác vụ đó trong mã, đó là những gì mọi người sẽ làm trong những 

132
00:08:03,381 --> 00:08:07,034
ngày đầu tiên của AI, thay vào đó bạn thiết lập một cấu trúc rất linh hoạt 

133
00:08:07,034 --> 00:08:11,077
với các tham số có thể điều chỉnh, chẳng hạn như một loạt các nút bấm và nút xoay, 

134
00:08:11,077 --> 00:08:14,828
sau đó bằng cách nào đó, bạn sử dụng nhiều ví dụ về giao diện đầu ra của một 

135
00:08:14,828 --> 00:08:18,628
đầu vào nhất định để điều chỉnh và điều chỉnh giá trị của các tham số đó nhằm 

136
00:08:18,628 --> 00:08:19,700
bắt chước hành vi này.

137
00:08:19,700 --> 00:08:23,619
Ví dụ: có thể hình thức học máy đơn giản nhất là hồi quy tuyến tính, 

138
00:08:23,619 --> 00:08:26,630
trong đó đầu vào và đầu ra của bạn là mỗi số đơn lẻ, 

139
00:08:26,630 --> 00:08:29,812
chẳng hạn như diện tích của một ngôi nhà và giá của nó, 

140
00:08:29,812 --> 00:08:34,357
và điều bạn muốn là tìm một dòng phù hợp nhất thông qua điều này. bạn biết đấy, 

141
00:08:34,357 --> 00:08:36,799
dữ liệu để dự đoán giá nhà trong tương lai.

142
00:08:37,440 --> 00:08:42,643
Đường đó được mô tả bởi hai tham số liên tục, chẳng hạn như độ dốc và điểm chặn y, 

143
00:08:42,643 --> 00:08:48,160
và mục tiêu của hồi quy tuyến tính là xác định các tham số đó khớp chặt chẽ với dữ liệu.

144
00:08:48,880 --> 00:08:52,100
Không cần phải nói, các mô hình học sâu trở nên phức tạp hơn nhiều.

145
00:08:52,620 --> 00:08:57,660
Ví dụ: GPT-3 không phải có hai mà có 175 tỷ tham số.

146
00:08:58,120 --> 00:09:01,896
Nhưng vấn đề ở đây là, không có gì chắc chắn rằng bạn có thể tạo ra 

147
00:09:01,896 --> 00:09:05,617
một mô hình khổng lồ nào đó với một số lượng lớn các tham số mà nó 

148
00:09:05,617 --> 00:09:09,560
không quá phù hợp với dữ liệu huấn luyện hoặc hoàn toàn khó huấn luyện.

149
00:09:10,260 --> 00:09:13,135
Học sâu mô tả một lớp mô hình mà trong vài thập kỷ 

150
00:09:13,135 --> 00:09:16,180
qua đã được chứng minh là có khả năng mở rộng đáng kể.

151
00:09:16,480 --> 00:09:21,338
Điều thống nhất chúng là cùng một thuật toán huấn luyện, được gọi là lan truyền ngược, 

152
00:09:21,338 --> 00:09:26,365
và bối cảnh mà tôi muốn bạn hiểu khi chúng ta đi vào là để thuật toán huấn luyện này hoạt 

153
00:09:26,365 --> 00:09:31,280
động tốt trên quy mô lớn, các mô hình này phải tuân theo một định dạng cụ thể nhất định.

154
00:09:31,800 --> 00:09:36,181
Nếu bạn biết định dạng này sẽ được áp dụng, nó sẽ giúp giải thích nhiều lựa chọn 

155
00:09:36,181 --> 00:09:40,400
về cách máy biến áp xử lý ngôn ngữ, nếu không sẽ có nguy cơ cảm thấy tùy tiện.

156
00:09:41,440 --> 00:09:43,981
Đầu tiên, bất kể bạn đang tạo mô hình nào, đầu 

157
00:09:43,981 --> 00:09:46,740
vào phải được định dạng dưới dạng một mảng số thực.

158
00:09:46,740 --> 00:09:49,189
Điều này có thể có nghĩa là một danh sách các số, 

159
00:09:49,189 --> 00:09:52,325
nó có thể là một mảng hai chiều hoặc rất thường xuyên bạn xử lý 

160
00:09:52,325 --> 00:09:56,000
các mảng có chiều cao hơn, trong đó thuật ngữ chung được sử dụng là tensor.

161
00:09:56,560 --> 00:10:01,510
Bạn thường nghĩ dữ liệu đầu vào đó được chuyển đổi dần dần thành nhiều lớp riêng biệt, 

162
00:10:01,510 --> 00:10:05,550
trong đó, mỗi lớp luôn được cấu trúc như một loại mảng số thực nào đó, 

163
00:10:05,550 --> 00:10:08,680
cho đến khi bạn đến lớp cuối cùng mà bạn coi là đầu ra.

164
00:10:09,280 --> 00:10:13,046
Ví dụ: lớp cuối cùng trong mô hình xử lý văn bản của chúng tôi là danh sách 

165
00:10:13,046 --> 00:10:17,060
các số biểu thị phân bố xác suất cho tất cả các mã thông báo tiếp theo có thể có.

166
00:10:17,820 --> 00:10:21,864
Trong học sâu, các tham số mô hình này hầu như luôn được gọi là trọng số và 

167
00:10:21,864 --> 00:10:25,908
điều này là do đặc điểm chính của các mô hình này là cách duy nhất các tham 

168
00:10:25,908 --> 00:10:29,900
số này tương tác với dữ liệu đang được xử lý là thông qua tổng có trọng số.

169
00:10:30,340 --> 00:10:32,501
Bạn cũng rắc một số hàm phi tuyến tính xuyên suốt 

170
00:10:32,501 --> 00:10:34,360
nhưng chúng sẽ không phụ thuộc vào tham số.

171
00:10:35,200 --> 00:10:38,642
Tuy nhiên, thông thường, thay vì nhìn thấy các tổng có trọng số hoàn toàn 

172
00:10:38,642 --> 00:10:41,991
trần trụi và được viết rõ ràng như thế này, bạn sẽ thấy chúng được đóng 

173
00:10:41,991 --> 00:10:45,620
gói cùng nhau dưới dạng các thành phần khác nhau trong một tích vectơ ma trận.

174
00:10:46,740 --> 00:10:51,053
Điều tương tự cũng xảy ra, nếu bạn nghĩ lại cách hoạt động của phép nhân vectơ ma trận, 

175
00:10:51,053 --> 00:10:54,240
mỗi thành phần trong đầu ra trông giống như một tổng có trọng số.

176
00:10:54,780 --> 00:11:00,190
Về mặt khái niệm, bạn và tôi thường dễ dàng hơn khi nghĩ về các ma trận chứa đầy các tham 

177
00:11:00,190 --> 00:11:05,420
số có thể điều chỉnh được để biến đổi các vectơ được rút ra từ dữ liệu đang được xử lý.

178
00:11:06,340 --> 00:11:14,160
Ví dụ: 175 tỷ trọng số đó trong GPT-3 được sắp xếp thành dưới 28.000 ma trận riêng biệt.

179
00:11:14,660 --> 00:11:18,097
Những ma trận đó lần lượt được chia thành tám loại khác nhau, 

180
00:11:18,097 --> 00:11:22,700
và điều bạn và tôi sắp làm là xem qua từng loại trong số đó để hiểu loại đó làm gì.

181
00:11:23,160 --> 00:11:27,358
Khi chúng tôi xem xét, tôi nghĩ thật thú vị khi tham khảo những 

182
00:11:27,358 --> 00:11:31,360
con số cụ thể từ GPT-3 để đếm chính xác 175 tỷ đó đến từ đâu.

183
00:11:31,880 --> 00:11:34,515
Ngay cả khi ngày nay có những mô hình lớn hơn và tốt hơn, 

184
00:11:34,515 --> 00:11:37,468
thì mô hình này vẫn có một sức hấp dẫn nhất định là mô hình ngôn 

185
00:11:37,468 --> 00:11:40,740
ngữ lớn để thực sự thu hút sự chú ý của thế giới bên ngoài cộng đồng ML.

186
00:11:41,440 --> 00:11:44,019
Ngoài ra, trên thực tế mà nói, các công ty có xu hướng 

187
00:11:44,019 --> 00:11:46,740
giữ kín những con số cụ thể đối với các mạng hiện đại hơn.

188
00:11:47,360 --> 00:11:52,314
Tôi chỉ muốn dựng bối cảnh khi bạn nhìn kỹ để xem điều gì xảy ra bên trong một công cụ 

189
00:11:52,314 --> 00:11:57,440
như ChatGPT, gần như tất cả các phép tính thực tế trông giống như phép nhân vectơ ma trận.

190
00:11:57,900 --> 00:12:01,495
Có một chút rủi ro khi bị lạc trong biển hàng tỷ con số, 

191
00:12:01,495 --> 00:12:06,163
nhưng bạn nên hình dung rõ ràng sự khác biệt giữa trọng số của mô hình mà 

192
00:12:06,163 --> 00:12:11,840
tôi sẽ luôn tô màu xanh lam hoặc đỏ và dữ liệu được đã được xử lý, tôi sẽ luôn tô màu xám.

193
00:12:12,180 --> 00:12:14,980
Trọng lượng chính là bộ não thực sự, chúng là những thứ học 

194
00:12:14,980 --> 00:12:17,920
được trong quá trình luyện tập và quyết định cách nó hoạt động.

195
00:12:18,280 --> 00:12:22,248
Dữ liệu đang được xử lý chỉ mã hóa bất kỳ đầu vào cụ thể nào được đưa 

196
00:12:22,248 --> 00:12:26,500
vào mô hình cho một lần chạy nhất định, chẳng hạn như một đoạn văn bản mẫu.

197
00:12:27,480 --> 00:12:30,346
Với tất cả những điều đó làm nền tảng, chúng ta hãy đi sâu 

198
00:12:30,346 --> 00:12:32,630
vào bước đầu tiên của ví dụ xử lý văn bản này, 

199
00:12:32,630 --> 00:12:36,420
đó là chia dữ liệu đầu vào thành các phần nhỏ và biến các phần đó thành vectơ.

200
00:12:37,020 --> 00:12:39,911
Tôi đã đề cập đến cách những phần đó được gọi là mã thông báo, 

201
00:12:39,911 --> 00:12:43,536
có thể là các từ hoặc dấu câu, nhưng thỉnh thoảng trong chương này và đặc biệt 

202
00:12:43,536 --> 00:12:47,116
là trong chương tiếp theo, tôi chỉ muốn giả vờ rằng nó được chia thành các từ 

203
00:12:47,116 --> 00:12:48,080
một cách rõ ràng hơn.

204
00:12:48,600 --> 00:12:51,298
Bởi vì con người chúng ta suy nghĩ bằng lời nên điều này sẽ giúp 

205
00:12:51,298 --> 00:12:54,080
việc tham khảo các ví dụ nhỏ và làm rõ từng bước dễ dàng hơn nhiều.

206
00:12:55,260 --> 00:12:59,749
Mô hình này có vốn từ vựng được xác định trước, một số danh sách tất cả các từ có thể, 

207
00:12:59,749 --> 00:13:03,671
chẳng hạn như 50.000 từ trong số đó và ma trận đầu tiên mà chúng ta sẽ gặp, 

208
00:13:03,671 --> 00:13:07,800
được gọi là ma trận nhúng, có một cột duy nhất cho mỗi từ trong số những từ này.

209
00:13:08,940 --> 00:13:13,760
Các cột này quyết định mỗi từ sẽ chuyển thành vectơ nào trong bước đầu tiên đó.

210
00:13:15,100 --> 00:13:18,708
Chúng tôi gắn nhãn cho nó là Chúng tôi, và giống như tất cả các ma trận mà chúng tôi 

211
00:13:18,708 --> 00:13:22,360
thấy, các giá trị của nó bắt đầu ngẫu nhiên, nhưng chúng sẽ được học dựa trên dữ liệu.

212
00:13:23,620 --> 00:13:27,508
Biến các từ thành vectơ là phương pháp phổ biến trong machine learning từ rất lâu 

213
00:13:27,508 --> 00:13:31,539
trước khi có Transformers, nhưng sẽ hơi kỳ lạ nếu bạn chưa từng thấy nó trước đây và 

214
00:13:31,539 --> 00:13:35,760
nó đặt nền tảng cho mọi thứ tiếp theo, vì vậy hãy dành chút thời gian để làm quen với nó.

215
00:13:36,040 --> 00:13:38,239
Chúng tôi thường gọi đây là việc nhúng một từ, 

216
00:13:38,239 --> 00:13:41,982
điều này mời bạn nghĩ về các vectơ này một cách rất hình học như các điểm trong 

217
00:13:41,982 --> 00:13:43,620
một không gian có chiều cao nào đó.

218
00:13:44,180 --> 00:13:48,030
Việc hiển thị danh sách ba số làm tọa độ cho các điểm trong không gian 3D sẽ 

219
00:13:48,030 --> 00:13:51,780
không có vấn đề gì, nhưng việc nhúng từ có xu hướng có chiều cao hơn nhiều.

220
00:13:52,280 --> 00:13:55,629
Trong GPT-3, chúng có 12.288 chiều và như bạn sẽ thấy, 

221
00:13:55,629 --> 00:14:00,440
điều quan trọng là phải làm việc trong một không gian có nhiều hướng khác nhau.

222
00:14:01,180 --> 00:14:04,998
Theo cách tương tự, bạn có thể đưa một lát cắt hai chiều xuyên qua không 

223
00:14:04,998 --> 00:14:07,561
gian 3D và chiếu tất cả các điểm lên lát cắt đó, 

224
00:14:07,561 --> 00:14:11,379
nhằm mục đích tạo hoạt ảnh cho các phần nhúng từ mà một mô hình đơn giản 

225
00:14:11,379 --> 00:14:15,249
mang lại cho tôi, tôi sẽ làm một điều tương tự bằng cách chọn một lát cắt 

226
00:14:15,249 --> 00:14:18,963
ba chiều xuyên qua không gian có nhiều chiều này và chiếu các vectơ từ 

227
00:14:18,963 --> 00:14:20,480
xuống đó và hiển thị kết quả.

228
00:14:21,280 --> 00:14:24,490
Ý tưởng lớn ở đây là khi một mô hình điều chỉnh và điều chỉnh trọng số 

229
00:14:24,490 --> 00:14:27,837
của nó để xác định chính xác cách các từ được nhúng dưới dạng vectơ trong 

230
00:14:27,837 --> 00:14:31,183
quá trình huấn luyện, nó có xu hướng giải quyết trên một tập hợp các phần 

231
00:14:31,183 --> 00:14:34,440
nhúng trong đó các hướng trong không gian có một loại ý nghĩa ngữ nghĩa.

232
00:14:34,980 --> 00:14:37,832
Đối với mô hình từ-thành-vectơ đơn giản mà tôi đang chạy ở đây, 

233
00:14:37,832 --> 00:14:41,799
nếu tôi thực hiện tìm kiếm tất cả các từ có phần nhúng gần nhất với từ có nghĩa là tháp, 

234
00:14:41,799 --> 00:14:45,409
bạn sẽ nhận thấy tất cả chúng dường như đều mang lại cảm giác giống như tháp rất 

235
00:14:45,409 --> 00:14:45,900
giống nhau.

236
00:14:46,340 --> 00:14:48,709
Và nếu bạn muốn sử dụng một số Python và tự làm ở nhà, 

237
00:14:48,709 --> 00:14:51,380
thì đây là mô hình cụ thể mà tôi đang sử dụng để tạo hoạt ảnh.

238
00:14:51,620 --> 00:14:54,703
Nó không phải là một máy biến áp, nhưng nó đủ để minh họa ý tưởng 

239
00:14:54,703 --> 00:14:57,600
rằng các hướng trong không gian có thể mang ý nghĩa ngữ nghĩa.

240
00:14:58,300 --> 00:15:03,307
Một ví dụ rất cổ điển về điều này là nếu bạn lấy sự khác biệt giữa các vectơ của 

241
00:15:03,307 --> 00:15:08,253
phụ nữ và đàn ông, một cái gì đó bạn sẽ hình dung như một vectơ nhỏ nối đầu của 

242
00:15:08,253 --> 00:15:13,200
cái này với đầu của cái kia, nó rất giống với sự khác biệt giữa vua và nữ hoàng.

243
00:15:15,080 --> 00:15:18,441
Vì vậy, giả sử bạn không biết từ dành cho nữ quân vương, 

244
00:15:18,441 --> 00:15:23,572
bạn có thể tìm từ đó bằng cách lấy vua, thêm hướng phụ nữ-nam giới này và tìm kiếm các 

245
00:15:23,572 --> 00:15:25,460
phần nhúng gần nhất với điểm đó.

246
00:15:27,000 --> 00:15:28,200
Ít nhất, loại.

247
00:15:28,480 --> 00:15:31,627
Mặc dù đây là một ví dụ cổ điển cho mô hình mà tôi đang sử dụng, 

248
00:15:31,627 --> 00:15:35,356
nhưng việc nhúng nữ hoàng thực sự còn xa hơn một chút so với điều này gợi ý, 

249
00:15:35,356 --> 00:15:39,327
có lẽ là do cách sử dụng nữ hoàng trong dữ liệu huấn luyện không chỉ đơn thuần là 

250
00:15:39,327 --> 00:15:40,780
một phiên bản nữ tính của vua.

251
00:15:41,620 --> 00:15:45,260
Khi tôi chơi đùa, các mối quan hệ gia đình dường như minh họa ý tưởng này tốt hơn nhiều.

252
00:15:46,340 --> 00:15:50,672
Vấn đề là, có vẻ như trong quá trình đào tạo, mô hình nhận thấy thuận lợi khi chọn 

253
00:15:50,672 --> 00:15:54,900
các phần nhúng sao cho một hướng trong không gian này mã hóa thông tin giới tính.

254
00:15:56,800 --> 00:15:59,697
Một ví dụ khác là nếu bạn lấy phần nhúng của Ý, 

255
00:15:59,697 --> 00:16:04,346
và bạn trừ đi phần nhúng của Đức, và thêm phần đó vào phần nhúng của Hitler, 

256
00:16:04,346 --> 00:16:08,090
bạn sẽ có được thứ gì đó rất gần với phần nhúng của Mussolini.

257
00:16:08,570 --> 00:16:12,146
Cứ như thể mô hình đã học cách liên kết một số hướng với người Ý và 

258
00:16:12,146 --> 00:16:15,670
những hướng khác với các nhà lãnh đạo trục trong Thế chiến thứ hai.

259
00:16:16,470 --> 00:16:20,273
Có lẽ ví dụ yêu thích của tôi trong trường hợp này là trong một số mô hình, 

260
00:16:20,273 --> 00:16:23,877
nếu bạn lấy sự khác biệt giữa Đức và Nhật Bản và thêm nó vào món sushi, 

261
00:16:23,877 --> 00:16:26,230
bạn sẽ kết thúc rất gần với xúc xích bratwurst.

262
00:16:27,350 --> 00:16:29,929
Cũng khi chơi trò chơi tìm hàng xóm gần nhất này, 

263
00:16:29,929 --> 00:16:33,850
tôi rất vui khi thấy Kat thân thiết với cả quái thú và quái vật đến mức nào.

264
00:16:34,690 --> 00:16:37,129
Một chút trực giác toán học hữu ích cần ghi nhớ, 

265
00:16:37,129 --> 00:16:40,265
đặc biệt là cho chương tiếp theo, là làm thế nào tích vô hướng 

266
00:16:40,265 --> 00:16:43,850
của hai vectơ có thể được coi là một cách để đo mức độ chúng thẳng hàng.

267
00:16:44,870 --> 00:16:47,945
Về mặt tính toán, tích số chấm liên quan đến việc nhân tất cả các 

268
00:16:47,945 --> 00:16:50,135
thành phần tương ứng rồi cộng các kết quả lại, 

269
00:16:50,135 --> 00:16:54,330
điều này là tốt vì rất nhiều phép tính của chúng ta phải trông giống như tổng có trọng số.

270
00:16:55,190 --> 00:17:00,188
Về mặt hình học, tích số chấm là dương khi các vectơ hướng cùng hướng, 

271
00:17:00,188 --> 00:17:05,609
nó bằng 0 nếu chúng vuông góc và nó âm bất cứ khi nào chúng hướng ngược nhau.

272
00:17:06,550 --> 00:17:11,682
Ví dụ: giả sử bạn đang chơi với mô hình này và bạn đưa ra giả thuyết rằng việc 

273
00:17:11,682 --> 00:17:17,010
nhúng mèo trừ mèo có thể đại diện cho một loại hướng đa dạng trong không gian này.

274
00:17:17,430 --> 00:17:20,620
Để kiểm tra điều này, tôi sẽ lấy vectơ này và tính tích số chấm 

275
00:17:20,620 --> 00:17:23,760
của nó dựa trên các phần nhúng của các danh từ số ít nhất định 

276
00:17:23,760 --> 00:17:27,050
và so sánh nó với tích số chấm của các danh từ số nhiều tương ứng.

277
00:17:27,270 --> 00:17:31,670
Nếu bạn thử nghiệm điều này, bạn sẽ nhận thấy rằng số nhiều thực sự dường như 

278
00:17:31,670 --> 00:17:36,070
luôn cho giá trị cao hơn số ít, cho thấy rằng chúng phù hợp hơn với hướng này.

279
00:17:37,070 --> 00:17:41,006
Cũng thật thú vị nếu bạn lấy tích số chấm này với các phần nhúng của các từ 1, 

280
00:17:41,006 --> 00:17:43,697
2, 3, v.v., chúng sẽ cho các giá trị tăng dần, do đó, 

281
00:17:43,697 --> 00:17:47,734
như thể chúng ta có thể đo lường một cách định lượng mức độ số nhiều của mô hình 

282
00:17:47,734 --> 00:17:49,030
tìm thấy một từ nhất định.

283
00:17:50,250 --> 00:17:53,570
Một lần nữa, thông tin cụ thể về cách nhúng các từ được học bằng cách sử dụng dữ liệu.

284
00:17:54,050 --> 00:17:57,247
Ma trận nhúng này, có các cột cho chúng ta biết điều gì xảy ra với mỗi từ, 

285
00:17:57,247 --> 00:17:59,550
là chồng trọng số đầu tiên trong mô hình của chúng ta.

286
00:18:00,030 --> 00:18:04,900
Sử dụng số GPT-3, kích thước từ vựng cụ thể là 50.257 và một lần nữa, 

287
00:18:04,900 --> 00:18:09,770
về mặt kỹ thuật, điều này không bao gồm các từ mà là các mã thông báo.

288
00:18:10,630 --> 00:18:14,433
Kích thước nhúng là 12.288 và nhân số đó cho chúng 

289
00:18:14,433 --> 00:18:17,790
ta biết nó bao gồm khoảng 617 triệu trọng số.

290
00:18:18,250 --> 00:18:20,982
Hãy tiếp tục và thêm số này vào bảng kiểm đếm đang chạy, 

291
00:18:20,982 --> 00:18:23,810
hãy nhớ rằng đến cuối cùng chúng ta sẽ đếm được tới 175 tỷ.

292
00:18:25,430 --> 00:18:28,730
Trong trường hợp máy biến áp, bạn thực sự muốn coi các vectơ trong 

293
00:18:28,730 --> 00:18:32,130
không gian nhúng này không chỉ đơn thuần là biểu thị các từ riêng lẻ.

294
00:18:32,550 --> 00:18:36,272
Thứ nhất, chúng cũng mã hóa thông tin về vị trí của từ đó, 

295
00:18:36,272 --> 00:18:39,741
điều mà chúng ta sẽ nói đến sau, nhưng quan trọng hơn, 

296
00:18:39,741 --> 00:18:42,770
bạn nên nghĩ chúng có khả năng hiểu rõ ngữ cảnh.

297
00:18:43,350 --> 00:18:46,735
Ví dụ: một vectơ bắt đầu tồn tại dưới dạng nhúng từ vua, 

298
00:18:46,735 --> 00:18:51,723
có thể dần dần bị kéo và kéo bởi nhiều khối khác nhau trong mạng này, để cuối cùng, 

299
00:18:51,723 --> 00:18:56,950
nó chỉ theo một hướng cụ thể và nhiều sắc thái hơn mà bằng cách nào đó mã hóa nó là một 

300
00:18:56,950 --> 00:19:02,295
vị vua sống ở Scotland và đã đạt được chức vụ của mình sau khi sát hại vị vua trước đó và 

301
00:19:02,295 --> 00:19:04,730
được mô tả bằng ngôn ngữ của Shakespeare.

302
00:19:05,210 --> 00:19:07,790
Hãy suy nghĩ về sự hiểu biết của riêng bạn về một từ nhất định.

303
00:19:08,250 --> 00:19:13,220
Ý nghĩa của từ đó được môi trường xung quanh thông báo rõ ràng và đôi khi điều này bao 

304
00:19:13,220 --> 00:19:18,190
gồm ngữ cảnh từ khoảng cách xa, vì vậy, khi kết hợp một mô hình có khả năng dự đoán từ 

305
00:19:18,190 --> 00:19:23,161
nào tiếp theo, mục tiêu là bằng cách nào đó cho phép nó kết hợp ngữ cảnh một cách hiệu 

306
00:19:23,161 --> 00:19:23,390
quả.

307
00:19:24,050 --> 00:19:28,470
Nói rõ hơn, ngay trong bước đầu tiên đó, khi bạn tạo mảng vectơ dựa trên văn bản đầu vào, 

308
00:19:28,470 --> 00:19:30,876
mỗi vectơ đó chỉ được lấy ra khỏi ma trận nhúng, 

309
00:19:30,876 --> 00:19:35,001
vì vậy ban đầu mỗi vectơ chỉ có thể mã hóa nghĩa của một từ mà không cần bất kỳ đầu 

310
00:19:35,001 --> 00:19:36,770
vào nào từ môi trường xung quanh nó.

311
00:19:37,710 --> 00:19:41,539
Nhưng bạn nên nghĩ đến mục tiêu chính của mạng lưới này mà nó chảy 

312
00:19:41,539 --> 00:19:45,254
qua là cho phép mỗi vectơ đó hấp thụ một ý nghĩa phong phú và cụ 

313
00:19:45,254 --> 00:19:48,970
thể hơn nhiều so với những gì mà các từ riêng lẻ có thể biểu thị.

314
00:19:49,510 --> 00:19:52,370
Mạng chỉ có thể xử lý một số vectơ cố định tại một thời điểm, 

315
00:19:52,370 --> 00:19:54,170
được gọi là kích thước ngữ cảnh của nó.

316
00:19:54,510 --> 00:19:58,947
Đối với GPT-3, nó được đào tạo với kích thước ngữ cảnh là 2048, do đó, 

317
00:19:58,947 --> 00:20:03,197
dữ liệu truyền qua mạng luôn trông giống như mảng gồm 2048 cột này, 

318
00:20:03,197 --> 00:20:05,010
mỗi cột có 12.000 thứ nguyên.

319
00:20:05,590 --> 00:20:08,569
Kích thước ngữ cảnh này giới hạn số lượng văn bản mà 

320
00:20:08,569 --> 00:20:11,830
biến áp có thể kết hợp khi đưa ra dự đoán về từ tiếp theo.

321
00:20:12,370 --> 00:20:15,894
Đây là lý do tại sao các cuộc trò chuyện dài với một số chatbot nhất định, 

322
00:20:15,894 --> 00:20:18,243
chẳng hạn như các phiên bản đầu tiên của ChatGPT, 

323
00:20:18,243 --> 00:20:22,050
thường mang lại cảm giác như bot sẽ mất mạch trò chuyện khi bạn tiếp tục quá lâu.

324
00:20:23,030 --> 00:20:25,631
Chúng ta sẽ đi vào chi tiết cần chú ý vào thời gian thích hợp, 

325
00:20:25,631 --> 00:20:28,810
nhưng bỏ qua phần trước tôi muốn nói một chút về những gì xảy ra ở phần cuối.

326
00:20:29,450 --> 00:20:32,033
Hãy nhớ rằng, đầu ra mong muốn là phân bố xác suất 

327
00:20:32,033 --> 00:20:34,870
trên tất cả các mã thông báo có thể xuất hiện tiếp theo.

328
00:20:35,170 --> 00:20:39,225
Ví dụ: nếu từ cuối cùng là Giáo sư và ngữ cảnh bao gồm những từ như Harry 

329
00:20:39,225 --> 00:20:43,170
Potter và ngay trước đó chúng ta thấy giáo viên ít được yêu thích nhất, 

330
00:20:43,170 --> 00:20:47,226
đồng thời nếu bạn cho tôi chút thời gian bằng cách để tôi giả vờ rằng các 

331
00:20:47,226 --> 00:20:51,336
mã thông báo trông giống như các từ đầy đủ, thì một mạng lưới được đào tạo 

332
00:20:51,336 --> 00:20:55,830
bài bản đã xây dựng được kiến thức về Harry Potter có lẽ sẽ đánh giá cao từ Snape.

333
00:20:56,510 --> 00:20:57,970
Điều này bao gồm hai bước khác nhau.

334
00:20:58,310 --> 00:21:02,960
Cách đầu tiên là sử dụng một ma trận khác ánh xạ vectơ cuối cùng trong ngữ cảnh 

335
00:21:02,960 --> 00:21:07,610
đó tới danh sách 50.000 giá trị, một giá trị cho mỗi mã thông báo trong từ vựng.

336
00:21:08,170 --> 00:21:11,928
Sau đó, có một hàm bình thường hóa điều này thành phân bố xác suất, 

337
00:21:11,928 --> 00:21:16,129
nó được gọi là Softmax và chúng ta sẽ nói nhiều hơn về nó chỉ sau một giây, 

338
00:21:16,129 --> 00:21:20,109
nhưng trước đó có vẻ hơi kỳ lạ khi chỉ sử dụng phép nhúng cuối cùng này 

339
00:21:20,109 --> 00:21:23,315
để đưa ra dự đoán, khi xét cho cùng, ở bước cuối cùng đó, 

340
00:21:23,315 --> 00:21:28,290
có hàng nghìn vectơ khác trong lớp chỉ nằm ở đó với ý nghĩa giàu ngữ cảnh của riêng chúng.

341
00:21:28,930 --> 00:21:32,414
Điều này liên quan đến thực tế là trong quá trình đào tạo, 

342
00:21:32,414 --> 00:21:36,312
nó sẽ hiệu quả hơn nhiều nếu bạn sử dụng từng vectơ đó ở lớp cuối 

343
00:21:36,312 --> 00:21:40,270
cùng để đồng thời đưa ra dự đoán về những gì sẽ xảy ra ngay sau nó.

344
00:21:40,970 --> 00:21:43,369
Còn rất nhiều điều để nói về việc tập luyện sau này, 

345
00:21:43,369 --> 00:21:45,090
nhưng tôi chỉ muốn kể ra ngay bây giờ.

346
00:21:45,730 --> 00:21:49,690
Ma trận này được gọi là ma trận Unembedding và chúng tôi đặt cho nó nhãn WU.

347
00:21:50,210 --> 00:21:52,652
Một lần nữa, giống như tất cả các ma trận trọng số mà chúng ta thấy, 

348
00:21:52,652 --> 00:21:55,520
các mục của nó bắt đầu một cách ngẫu nhiên, nhưng chúng được học trong quá trình 

349
00:21:55,520 --> 00:21:55,910
huấn luyện.

350
00:21:56,470 --> 00:22:01,176
Giữ điểm trên tổng số tham số của chúng tôi, ma trận Unembedding này có một hàng 

351
00:22:01,176 --> 00:22:05,650
cho mỗi từ trong từ vựng và mỗi hàng có cùng số phần tử như thứ nguyên nhúng.

352
00:22:06,410 --> 00:22:10,069
Nó rất giống với ma trận nhúng, chỉ với thứ tự được hoán đổi, do đó, 

353
00:22:10,069 --> 00:22:12,668
nó bổ sung thêm 617 triệu tham số khác vào mạng, 

354
00:22:12,668 --> 00:22:16,274
nghĩa là số lượng của chúng tôi cho đến nay là hơn một tỷ một chút, 

355
00:22:16,274 --> 00:22:20,623
một phần nhỏ nhưng không hoàn toàn không đáng kể trong số 175 tỷ mà chúng tôi có. 

356
00:22:20,623 --> 00:22:21,790
tổng cộng sẽ kết thúc.

357
00:22:22,550 --> 00:22:26,817
Bài học nhỏ cuối cùng của chương này, tôi muốn nói nhiều hơn về hàm softmax này, 

358
00:22:26,817 --> 00:22:30,610
vì nó sẽ xuất hiện một cách khác khi chúng ta đi sâu vào các khối chú ý.

359
00:22:31,430 --> 00:22:35,612
Ý tưởng là nếu bạn muốn một chuỗi số hoạt động như một phân phối xác suất, 

360
00:22:35,612 --> 00:22:39,181
chẳng hạn như phân phối trên tất cả các từ tiếp theo có thể có, 

361
00:22:39,181 --> 00:22:43,530
thì mỗi giá trị phải nằm trong khoảng từ 0 đến 1 và bạn cũng cần tất cả chúng 

362
00:22:43,530 --> 00:22:44,590
để có tổng bằng 1 .

363
00:22:45,250 --> 00:22:48,354
Tuy nhiên, nếu bạn đang chơi trò chơi học tập trong đó mọi thứ 

364
00:22:48,354 --> 00:22:50,769
bạn làm trông giống như phép nhân vectơ ma trận, 

365
00:22:50,769 --> 00:22:54,810
thì kết quả đầu ra bạn nhận được theo mặc định hoàn toàn không tuân theo điều này.

366
00:22:55,330 --> 00:22:57,576
Các giá trị thường âm hoặc lớn hơn 1 rất nhiều 

367
00:22:57,576 --> 00:22:59,870
và gần như chắc chắn chúng không có tổng bằng 1.

368
00:23:00,510 --> 00:23:05,930
Softmax là cách tiêu chuẩn để biến một danh sách các số tùy ý thành một phân phối hợp lệ 

369
00:23:05,930 --> 00:23:11,290
sao cho các giá trị lớn nhất gần bằng 1 và các giá trị nhỏ hơn có giá trị rất gần với 0.

370
00:23:11,830 --> 00:23:13,070
Đó là tất cả những gì bạn thực sự cần biết.

371
00:23:13,090 --> 00:23:17,987
Nhưng nếu bạn tò mò, cách thức hoạt động là trước tiên nâng e lên lũy thừa của mỗi số, 

372
00:23:17,987 --> 00:23:21,195
nghĩa là bây giờ bạn có một danh sách các giá trị dương, 

373
00:23:21,195 --> 00:23:25,192
sau đó bạn có thể lấy tổng của tất cả các giá trị dương đó và chia mỗi 

374
00:23:25,192 --> 00:23:29,470
số hạng theo tổng đó, nó sẽ chuẩn hóa nó thành một danh sách có tổng bằng 1.

375
00:23:30,170 --> 00:23:34,303
Bạn sẽ nhận thấy rằng nếu một trong các số ở đầu vào lớn hơn đáng kể so với các số 

376
00:23:34,303 --> 00:23:38,187
còn lại, thì trong đầu ra, số hạng tương ứng sẽ chiếm ưu thế trong phân phối, 

377
00:23:38,187 --> 00:23:42,470
vì vậy nếu bạn lấy mẫu từ số đó thì bạn gần như chắc chắn chỉ chọn đầu vào tối đa hóa.

378
00:23:42,990 --> 00:23:46,876
Nhưng nó nhẹ nhàng hơn việc chỉ chọn mức tối đa theo nghĩa là khi các 

379
00:23:46,876 --> 00:23:50,874
giá trị khác lớn tương tự, chúng cũng có trọng số có ý nghĩa trong phân 

380
00:23:50,874 --> 00:23:54,650
phối và mọi thứ thay đổi liên tục khi bạn liên tục thay đổi đầu vào.

381
00:23:55,130 --> 00:23:59,684
Trong một số trường hợp, chẳng hạn như khi ChatGPT đang sử dụng phân phối này 

382
00:23:59,684 --> 00:24:04,297
để tạo từ tiếp theo, sẽ có chỗ cho một chút thú vị hơn bằng cách thêm một chút 

383
00:24:04,297 --> 00:24:08,910
gia vị bổ sung vào hàm này, với hằng số t được đưa vào mẫu số của các số mũ đó.

384
00:24:09,550 --> 00:24:14,198
Chúng tôi gọi nó là nhiệt độ, vì nó gần giống với vai trò của nhiệt độ trong các 

385
00:24:14,198 --> 00:24:18,329
phương trình nhiệt động lực học nhất định, và kết quả là khi t lớn hơn, 

386
00:24:18,329 --> 00:24:21,198
bạn chú trọng nhiều hơn đến các giá trị thấp hơn, 

387
00:24:21,198 --> 00:24:25,789
nghĩa là sự phân bố đồng đều hơn một chút, và nếu t nhỏ hơn thì giá trị lớn hơn 

388
00:24:25,789 --> 00:24:28,715
sẽ chiếm ưu thế mạnh hơn, trong đó ở mức cực đoan, 

389
00:24:28,715 --> 00:24:32,790
đặt t bằng 0 có nghĩa là tất cả trọng số sẽ chuyển sang giá trị tối đa.

390
00:24:33,470 --> 00:24:37,772
Ví dụ: tôi sẽ yêu cầu GPT-3 tạo một câu chuyện với văn bản gốc, 

391
00:24:37,772 --> 00:24:42,950
ngày xưa có A, nhưng tôi sẽ sử dụng nhiệt độ khác nhau trong từng trường hợp.

392
00:24:43,630 --> 00:24:47,937
Nhiệt độ bằng 0 có nghĩa là nó luôn đi theo từ dễ đoán nhất và những 

393
00:24:47,937 --> 00:24:52,370
gì bạn nhận được cuối cùng chỉ là một dẫn xuất sáo rỗng của Goldilocks.

394
00:24:53,010 --> 00:24:56,625
Nhiệt độ cao hơn giúp nó có cơ hội chọn những từ ít có khả năng xảy ra hơn, 

395
00:24:56,625 --> 00:24:57,910
nhưng nó đi kèm với rủi ro.

396
00:24:58,230 --> 00:25:01,412
Trong trường hợp này, câu chuyện bắt đầu độc đáo hơn, 

397
00:25:01,412 --> 00:25:06,010
về một nghệ sĩ web trẻ đến từ Hàn Quốc, nhưng nó nhanh chóng trở nên vô nghĩa.

398
00:25:06,950 --> 00:25:10,830
Về mặt kỹ thuật, API không thực sự cho phép bạn chọn nhiệt độ lớn hơn 2.

399
00:25:11,170 --> 00:25:15,160
Không có lý do toán học nào cho việc này, đó chỉ là một ràng buộc tùy ý được áp 

400
00:25:15,160 --> 00:25:19,350
đặt để giữ cho công cụ của họ không bị nhìn thấy đang tạo ra những thứ quá vô nghĩa.

401
00:25:19,870 --> 00:25:23,060
Vì vậy, nếu bạn tò mò, cách hoạt động thực sự của hoạt ảnh này là 

402
00:25:23,060 --> 00:25:27,120
tôi đang lấy 20 mã thông báo tiếp theo có khả năng xảy ra cao nhất mà GPT-3 tạo ra, 

403
00:25:27,120 --> 00:25:30,408
có vẻ như là số tiền tối đa mà họ sẽ cung cấp cho tôi và sau đó tôi 

404
00:25:30,408 --> 00:25:32,970
điều chỉnh xác suất dựa trên theo số mũ của 1 phần 5.

405
00:25:33,130 --> 00:25:37,490
Là một thuật ngữ khác, giống như cách bạn có thể gọi các thành phần đầu 

406
00:25:37,490 --> 00:25:41,668
ra của xác suất của hàm này, mọi người thường coi đầu vào là logits, 

407
00:25:41,668 --> 00:25:46,150
hoặc một số người nói logits, một số người nói logits, tôi sẽ nói logits .

408
00:25:46,530 --> 00:25:49,142
Vì vậy, chẳng hạn, khi bạn nhập một số văn bản, 

409
00:25:49,142 --> 00:25:52,789
bạn có tất cả các từ nhúng này chảy qua mạng và bạn thực hiện phép 

410
00:25:52,789 --> 00:25:56,436
nhân cuối cùng này với ma trận không nhúng, những người học máy sẽ 

411
00:25:56,436 --> 00:26:00,083
coi các thành phần trong đầu ra thô, không chuẩn hóa đó là nhật ký 

412
00:26:00,083 --> 00:26:01,390
để dự đoán từ tiếp theo.

413
00:26:03,330 --> 00:26:07,847
Phần lớn mục tiêu của chương này là đặt nền móng cho việc hiểu cơ chế chú ý, 

414
00:26:07,847 --> 00:26:10,370
phong cách sáp-trên-tẩy-tắt của Karate Kid.

415
00:26:10,850 --> 00:26:14,975
Bạn thấy đấy, nếu bạn có trực giác mạnh mẽ về cách nhúng từ, về softmax, 

416
00:26:14,975 --> 00:26:19,326
về cách các tích số chấm đo lường độ tương tự và cũng là tiền đề cơ bản rằng 

417
00:26:19,326 --> 00:26:23,451
hầu hết các phép tính phải trông giống như phép nhân ma trận với ma trận 

418
00:26:23,451 --> 00:26:27,689
chứa đầy các tham số có thể điều chỉnh được, thì hãy hiểu sự chú ý Cơ chế, 

419
00:26:27,689 --> 00:26:32,210
phần nền tảng này trong toàn bộ sự bùng nổ AI hiện đại, phải tương đối trơn tru.

420
00:26:32,650 --> 00:26:34,510
Vì điều đó, hãy tham gia cùng tôi trong chương tiếp theo.

421
00:26:36,390 --> 00:26:38,887
Khi tôi xuất bản cuốn sách này, bản nháp của chương tiếp 

422
00:26:38,887 --> 00:26:41,210
theo sẽ có sẵn để những người ủng hộ Patreon xem xét.

423
00:26:41,770 --> 00:26:44,308
Phiên bản cuối cùng sẽ được công bố rộng rãi sau một hoặc hai tuần, 

424
00:26:44,308 --> 00:26:47,370
điều này thường phụ thuộc vào việc tôi sẽ thay đổi bao nhiêu dựa trên đánh giá đó.

425
00:26:47,810 --> 00:26:50,004
Trong khi chờ đợi, nếu bạn muốn thu hút sự chú ý và 

426
00:26:50,004 --> 00:26:52,410
nếu bạn muốn giúp đỡ kênh một chút thì kênh vẫn đang chờ.

