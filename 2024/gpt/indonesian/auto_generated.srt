1
00:00:00,000 --> 00:00:04,560
Inisial GPT adalah singkatan dari Generative Pretrained Transformer.

2
00:00:05,220 --> 00:00:09,020
Jadi kata pertama cukup mudah, ini adalah bot yang menghasilkan teks baru.

3
00:00:09,800 --> 00:00:13,058
Pretrained mengacu pada bagaimana model ini melalui proses pembelajaran dari 

4
00:00:13,058 --> 00:00:16,527
sejumlah besar data, dan awalan tersebut menyiratkan bahwa ada lebih banyak ruang 

5
00:00:16,527 --> 00:00:20,040
untuk menyempurnakan model ini pada tugas-tugas tertentu dengan pelatihan tambahan.

6
00:00:20,720 --> 00:00:22,900
Tetapi kata terakhir, itulah bagian kunci yang sebenarnya.

7
00:00:23,380 --> 00:00:27,756
Transformator adalah jenis jaringan saraf tertentu, sebuah model pembelajaran mesin, 

8
00:00:27,756 --> 00:00:31,000
dan merupakan penemuan inti yang mendasari ledakan AI saat ini.

9
00:00:31,740 --> 00:00:35,473
Apa yang ingin saya lakukan dengan video ini dan bab-bab berikutnya adalah memberikan 

10
00:00:35,473 --> 00:00:39,120
penjelasan secara visual tentang apa yang sebenarnya terjadi di dalam transformator.

11
00:00:39,700 --> 00:00:42,820
Kita akan mengikuti data yang mengalir melaluinya dan melangkah selangkah demi selangkah.

12
00:00:43,440 --> 00:00:47,380
Ada banyak jenis model yang bisa Anda buat dengan menggunakan transformer.

13
00:00:47,800 --> 00:00:50,800
Beberapa model mengambil audio dan menghasilkan transkrip.

14
00:00:51,340 --> 00:00:54,045
Kalimat ini berasal dari model yang bekerja sebaliknya, 

15
00:00:54,045 --> 00:00:56,220
menghasilkan ucapan sintetis hanya dari teks.

16
00:00:56,660 --> 00:01:01,035
Semua alat yang menghebohkan dunia pada tahun 2022 seperti Dolly dan Midjourney 

17
00:01:01,035 --> 00:01:05,519
yang mengambil deskripsi teks dan menghasilkan gambar didasarkan pada transformer.

18
00:01:06,000 --> 00:01:09,385
Meskipun saya tidak bisa memahami apa yang dimaksud dengan makhluk pai, 

19
00:01:09,385 --> 00:01:13,100
namun saya tetap terpesona bahwa hal semacam ini bahkan sangat mungkin terjadi.

20
00:01:13,900 --> 00:01:17,853
Dan transformator asli yang diperkenalkan pada tahun 2017 oleh Google diciptakan 

21
00:01:17,853 --> 00:01:22,100
untuk kasus penggunaan khusus untuk menerjemahkan teks dari satu bahasa ke bahasa lain.

22
00:01:22,660 --> 00:01:25,007
Tetapi varian yang akan Anda dan saya fokuskan, 

23
00:01:25,007 --> 00:01:27,843
yang merupakan jenis yang mendasari alat seperti ChatGPT, 

24
00:01:27,843 --> 00:01:30,631
adalah model yang dilatih untuk mengambil sepotong teks, 

25
00:01:30,631 --> 00:01:33,956
bahkan mungkin dengan beberapa gambar atau suara yang menyertainya, 

26
00:01:33,956 --> 00:01:38,260
dan menghasilkan prediksi untuk apa yang akan terjadi selanjutnya dalam bagian tersebut.

27
00:01:38,600 --> 00:01:41,277
Prediksi tersebut berbentuk distribusi probabilitas 

28
00:01:41,277 --> 00:01:43,800
dari berbagai potongan teks yang mungkin terjadi.

29
00:01:45,040 --> 00:01:47,364
Sekilas, Anda mungkin berpikir bahwa memprediksi kata berikutnya 

30
00:01:47,364 --> 00:01:49,940
terasa seperti tujuan yang sangat berbeda dengan menghasilkan teks baru.

31
00:01:50,180 --> 00:01:52,648
Namun, setelah Anda memiliki model prediksi seperti ini, 

32
00:01:52,648 --> 00:01:56,460
hal sederhana yang dapat Anda lakukan untuk menghasilkan teks yang lebih panjang adalah 

33
00:01:56,460 --> 00:01:58,582
dengan memberinya cuplikan awal untuk digunakan, 

34
00:01:58,582 --> 00:02:01,700
memintanya mengambil sampel acak dari distribusi yang baru saja dibuat, 

35
00:02:01,700 --> 00:02:05,512
menambahkan sampel tersebut ke dalam teks, dan kemudian menjalankan seluruh proses lagi 

36
00:02:05,512 --> 00:02:07,980
untuk membuat prediksi baru berdasarkan semua teks baru, 

37
00:02:07,980 --> 00:02:09,539
termasuk yang baru saja ditambahkan.

38
00:02:10,100 --> 00:02:11,661
Saya tidak tahu tentang Anda, tetapi rasanya ini 

39
00:02:11,661 --> 00:02:13,000
benar-benar tidak seperti yang seharusnya.

40
00:02:13,420 --> 00:02:16,405
Dalam animasi ini, misalnya, saya menjalankan GPT-2 di laptop saya 

41
00:02:16,405 --> 00:02:19,568
dan memintanya berulang kali memprediksi dan mengambil sampel potongan 

42
00:02:19,568 --> 00:02:22,420
teks berikutnya untuk menghasilkan cerita berdasarkan teks awal.

43
00:02:22,420 --> 00:02:26,120
Ceritanya tidak terlalu masuk akal.

44
00:02:26,500 --> 00:02:29,497
Tetapi jika saya menukarnya dengan panggilan API ke GPT-3, 

45
00:02:29,497 --> 00:02:32,648
yang merupakan model dasar yang sama, hanya saja lebih besar, 

46
00:02:32,648 --> 00:02:35,900
tiba-tiba secara ajaib kita mendapatkan cerita yang masuk akal, 

47
00:02:35,900 --> 00:02:39,609
yang bahkan tampaknya menyimpulkan bahwa makhluk pi akan hidup di negeri 

48
00:02:39,609 --> 00:02:40,880
matematika dan komputasi.

49
00:02:41,580 --> 00:02:44,940
Proses prediksi dan pengambilan sampel yang berulang-ulang ini pada dasarnya 

50
00:02:44,940 --> 00:02:48,213
adalah apa yang terjadi ketika Anda berinteraksi dengan ChatGPT atau model 

51
00:02:48,213 --> 00:02:51,880
bahasa besar lainnya dan Anda melihat mereka menghasilkan satu kata pada satu waktu.

52
00:02:52,480 --> 00:02:55,693
Bahkan, salah satu fitur yang akan sangat saya nikmati adalah kemampuan 

53
00:02:55,693 --> 00:02:59,220
untuk melihat distribusi yang mendasari untuk setiap kata baru yang dipilihnya.

54
00:03:03,820 --> 00:03:06,114
Mari kita mulai dengan pratinjau tingkat yang sangat tinggi 

55
00:03:06,114 --> 00:03:08,180
tentang bagaimana data mengalir melalui transformator.

56
00:03:08,640 --> 00:03:12,050
Kami akan menghabiskan lebih banyak waktu untuk memotivasi dan menafsirkan serta 

57
00:03:12,050 --> 00:03:14,744
mengembangkan detail setiap langkah, tetapi secara garis besar, 

58
00:03:14,744 --> 00:03:17,186
ketika salah satu chatbot ini menghasilkan kata tertentu, 

59
00:03:17,186 --> 00:03:18,660
inilah yang terjadi di balik layar.

60
00:03:19,080 --> 00:03:22,040
Pertama, input dipecah menjadi sekumpulan potongan-potongan kecil.

61
00:03:22,620 --> 00:03:25,509
Potongan-potongan ini disebut token, dan dalam kasus teks, 

62
00:03:25,509 --> 00:03:29,820
ini cenderung berupa kata atau potongan kecil kata atau kombinasi karakter umum lainnya.

63
00:03:30,740 --> 00:03:33,889
Jika gambar atau suara terlibat, maka token dapat berupa potongan-potongan 

64
00:03:33,889 --> 00:03:37,080
kecil dari gambar tersebut atau potongan-potongan kecil dari suara tersebut.

65
00:03:37,580 --> 00:03:40,701
Masing-masing token ini kemudian dikaitkan dengan sebuah vektor, 

66
00:03:40,701 --> 00:03:44,591
yang berarti beberapa daftar angka, yang dimaksudkan untuk mengkodekan arti dari 

67
00:03:44,591 --> 00:03:45,360
bagian tersebut.

68
00:03:45,880 --> 00:03:48,787
Jika Anda membayangkan vektor-vektor ini memberikan koordinat dalam ruang 

69
00:03:48,787 --> 00:03:51,812
dimensi yang sangat tinggi, kata-kata yang memiliki arti yang sama cenderung 

70
00:03:51,812 --> 00:03:54,680
mendarat pada vektor yang berdekatan satu sama lain dalam ruang tersebut.

71
00:03:55,280 --> 00:03:58,940
Urutan vektor ini kemudian melewati operasi yang dikenal sebagai blok perhatian, 

72
00:03:58,940 --> 00:04:01,788
dan ini memungkinkan vektor untuk berbicara satu sama lain dan 

73
00:04:01,788 --> 00:04:04,500
memberikan informasi bolak-balik untuk memperbarui nilainya.

74
00:04:04,880 --> 00:04:08,433
Misalnya, arti kata model dalam frasa model pembelajaran 

75
00:04:08,433 --> 00:04:11,800
mesin berbeda dengan artinya dalam frasa model fesyen.

76
00:04:12,260 --> 00:04:17,082
Blok perhatian bertanggung jawab untuk mencari tahu kata mana dalam konteks yang relevan 

77
00:04:17,082 --> 00:04:21,959
untuk memperbarui makna kata lain, dan bagaimana tepatnya makna tersebut harus diperbarui.

78
00:04:22,500 --> 00:04:24,630
Dan lagi, setiap kali saya menggunakan arti kata, 

79
00:04:24,630 --> 00:04:28,040
entah bagaimana hal ini sepenuhnya dikodekan dalam entri vektor-vektor tersebut.

80
00:04:29,180 --> 00:04:32,358
Setelah itu, vektor-vektor ini melewati jenis operasi yang berbeda, 

81
00:04:32,358 --> 00:04:35,162
dan tergantung pada sumber yang Anda baca, ini akan disebut 

82
00:04:35,162 --> 00:04:38,200
sebagai perceptron multi-layer atau mungkin lapisan feed-forward.

83
00:04:38,580 --> 00:04:40,838
Dan di sini, vektor-vektornya tidak berbicara satu sama lain, 

84
00:04:40,838 --> 00:04:42,660
semuanya melalui operasi yang sama secara paralel.

85
00:04:43,060 --> 00:04:45,422
Meskipun blok ini sedikit lebih sulit untuk ditafsirkan, 

86
00:04:45,422 --> 00:04:48,778
nanti kita akan berbicara tentang bagaimana langkahnya seperti mengajukan daftar 

87
00:04:48,778 --> 00:04:52,508
pertanyaan panjang tentang setiap vektor, dan kemudian memperbaruinya berdasarkan jawaban 

88
00:04:52,508 --> 00:04:54,000
dari pertanyaan-pertanyaan tersebut.

89
00:04:54,900 --> 00:05:00,339
Semua operasi di kedua blok ini terlihat seperti tumpukan besar perkalian matriks, 

90
00:05:00,339 --> 00:05:05,320
dan tugas utama kita adalah memahami cara membaca matriks yang mendasarinya.

91
00:05:06,980 --> 00:05:10,093
Saya melewatkan beberapa detail tentang beberapa langkah normalisasi yang terjadi 

92
00:05:10,093 --> 00:05:12,980
di antaranya, tetapi bagaimanapun juga, ini adalah pratinjau tingkat tinggi.

93
00:05:13,680 --> 00:05:16,185
Setelah itu, prosesnya pada dasarnya berulang, 

94
00:05:16,185 --> 00:05:20,023
Anda bolak-balik antara blok perhatian dan blok perceptron multi-layer, 

95
00:05:20,023 --> 00:05:23,595
sampai pada akhirnya, harapannya adalah bahwa semua makna esensial 

96
00:05:23,595 --> 00:05:27,327
dari bagian tersebut entah bagaimana telah dimasukkan ke dalam vektor 

97
00:05:27,327 --> 00:05:28,500
terakhir dalam urutan.

98
00:05:28,920 --> 00:05:31,987
Kami kemudian melakukan operasi tertentu pada vektor terakhir 

99
00:05:31,987 --> 00:05:35,599
yang menghasilkan distribusi probabilitas atas semua token yang mungkin, 

100
00:05:35,599 --> 00:05:38,420
semua potongan kecil teks yang mungkin muncul berikutnya.

101
00:05:38,980 --> 00:05:41,838
Dan seperti yang saya katakan, setelah Anda memiliki alat yang memprediksi 

102
00:05:41,838 --> 00:05:44,391
apa yang akan terjadi selanjutnya dengan memberikan potongan teks, 

103
00:05:44,391 --> 00:05:47,058
Anda dapat memberinya sedikit teks benih dan memintanya berulang kali 

104
00:05:47,058 --> 00:05:49,612
memainkan permainan memprediksi apa yang akan terjadi selanjutnya, 

105
00:05:49,612 --> 00:05:51,517
mengambil sampel dari distribusi, menambahkannya, 

106
00:05:51,517 --> 00:05:53,080
dan kemudian mengulanginya lagi dan lagi.

107
00:05:53,640 --> 00:05:57,663
Beberapa dari Anda yang tahu mungkin ingat berapa lama sebelum ChatGPT muncul, 

108
00:05:57,663 --> 00:06:01,227
seperti inilah tampilan demo awal GPT-3, Anda akan mendapatkan cerita 

109
00:06:01,227 --> 00:06:04,640
dan esai yang dilengkapi secara otomatis berdasarkan cuplikan awal.

110
00:06:05,580 --> 00:06:07,780
Untuk membuat alat seperti ini menjadi chatbot, 

111
00:06:07,780 --> 00:06:11,172
titik awal yang paling mudah adalah memiliki sedikit teks yang menetapkan 

112
00:06:11,172 --> 00:06:14,426
pengaturan pengguna yang berinteraksi dengan asisten AI yang membantu, 

113
00:06:14,426 --> 00:06:17,910
yang Anda sebut sebagai perintah sistem, dan kemudian Anda akan menggunakan 

114
00:06:17,910 --> 00:06:21,347
pertanyaan atau perintah awal pengguna sebagai bagian pertama dari dialog, 

115
00:06:21,347 --> 00:06:24,923
dan kemudian Anda akan mulai memprediksi apa yang akan dikatakan oleh asisten 

116
00:06:24,923 --> 00:06:26,940
AI yang membantu tersebut sebagai tanggapan.

117
00:06:27,720 --> 00:06:30,811
Ada lebih banyak yang bisa dikatakan tentang langkah pelatihan yang diperlukan untuk 

118
00:06:30,811 --> 00:06:33,940
membuat ini bekerja dengan baik, tetapi pada tingkat yang lebih tinggi, inilah idenya.

119
00:06:35,720 --> 00:06:39,761
Dalam bab ini, Anda dan saya akan memperluas detail tentang apa yang terjadi di awal 

120
00:06:39,761 --> 00:06:44,041
jaringan, di akhir jaringan, dan saya juga ingin menghabiskan banyak waktu untuk meninjau 

121
00:06:44,041 --> 00:06:46,751
beberapa bagian penting dari pengetahuan latar belakang, 

122
00:06:46,751 --> 00:06:51,030
hal-hal yang seharusnya sudah menjadi sifat alamiah bagi insinyur pembelajaran mesin mana 

123
00:06:51,030 --> 00:06:52,600
pun pada saat transformer muncul.

124
00:06:53,060 --> 00:06:56,100
Jika Anda merasa nyaman dengan pengetahuan latar belakang tersebut dan 

125
00:06:56,100 --> 00:06:58,669
sedikit tidak sabar, Anda dapat melompat ke bab berikutnya, 

126
00:06:58,669 --> 00:07:01,837
yang akan berfokus pada blok perhatian, yang secara umum dianggap sebagai 

127
00:07:01,837 --> 00:07:02,780
jantung transformator.

128
00:07:03,360 --> 00:07:07,360
Setelah itu, saya ingin berbicara lebih banyak tentang blok perceptron multi-layer ini, 

129
00:07:07,360 --> 00:07:10,088
bagaimana cara kerja pelatihan, dan sejumlah detail lainnya 

130
00:07:10,088 --> 00:07:11,680
yang akan dilewati sampai saat itu.

131
00:07:12,180 --> 00:07:15,464
Untuk konteks yang lebih luas, video-video ini merupakan tambahan dari mini-seri 

132
00:07:15,464 --> 00:07:19,113
tentang deep learning, dan tidak apa-apa jika Anda belum menonton video-video sebelumnya, 

133
00:07:19,113 --> 00:07:21,262
saya pikir Anda dapat melakukannya secara berurutan, 

134
00:07:21,262 --> 00:07:23,370
tetapi sebelum menyelami transformer secara khusus, 

135
00:07:23,370 --> 00:07:26,573
saya pikir ada baiknya kita memastikan bahwa kita memiliki pemahaman yang sama 

136
00:07:26,573 --> 00:07:28,520
tentang premis dasar dan struktur deep learning.

137
00:07:29,020 --> 00:07:31,058
Dengan risiko menyatakan hal yang sudah jelas, 

138
00:07:31,058 --> 00:07:33,616
ini adalah salah satu pendekatan untuk pembelajaran mesin, 

139
00:07:33,616 --> 00:07:36,608
yang menggambarkan model apa pun di mana Anda menggunakan data untuk 

140
00:07:36,608 --> 00:07:38,300
menentukan bagaimana model berperilaku.

141
00:07:39,140 --> 00:07:42,690
Yang saya maksud adalah, katakanlah Anda menginginkan fungsi yang mengambil 

142
00:07:42,690 --> 00:07:45,072
gambar dan menghasilkan label yang menjelaskannya, 

143
00:07:45,072 --> 00:07:48,762
atau contoh kita memprediksi kata berikutnya yang diberikan suatu bagian teks, 

144
00:07:48,762 --> 00:07:52,780
atau tugas lain yang tampaknya memerlukan beberapa elemen intuisi dan pengenalan pola.

145
00:07:53,200 --> 00:07:55,275
Kita hampir menganggap remeh hal ini sekarang, 

146
00:07:55,275 --> 00:07:58,853
tetapi ide dari pembelajaran mesin adalah daripada mencoba mendefinisikan secara 

147
00:07:58,853 --> 00:08:01,900
eksplisit prosedur tentang cara melakukan tugas tersebut dalam kode, 

148
00:08:01,900 --> 00:08:05,434
yang merupakan hal yang akan dilakukan orang pada masa-masa awal AI, alih-alih, 

149
00:08:05,434 --> 00:08:09,011
Anda membuat struktur yang sangat fleksibel dengan parameter yang dapat disetel, 

150
00:08:09,011 --> 00:08:12,633
seperti sekumpulan kenop dan tombol, lalu entah bagaimana Anda menggunakan banyak 

151
00:08:12,633 --> 00:08:16,299
contoh tentang seperti apa output yang seharusnya untuk input yang diberikan untuk 

152
00:08:16,299 --> 00:08:19,700
menyesuaikan dan menyetel nilai parameter tersebut untuk meniru perilaku ini.

153
00:08:19,700 --> 00:08:23,827
Sebagai contoh, mungkin bentuk paling sederhana dari machine learning adalah 

154
00:08:23,827 --> 00:08:28,223
regresi linier, di mana input dan output Anda masing-masing adalah angka tunggal, 

155
00:08:28,223 --> 00:08:32,457
seperti luas rumah dan harganya, dan yang Anda inginkan adalah menemukan garis 

156
00:08:32,457 --> 00:08:36,799
yang paling sesuai melalui data ini, untuk memprediksi harga rumah di masa depan.

157
00:08:37,440 --> 00:08:40,609
Garis tersebut digambarkan oleh dua parameter kontinu, 

158
00:08:40,609 --> 00:08:44,298
katakanlah kemiringan dan intersep y, dan tujuan regresi linier 

159
00:08:44,298 --> 00:08:48,160
adalah untuk menentukan parameter tersebut agar sesuai dengan data.

160
00:08:48,880 --> 00:08:52,100
Tidak perlu dikatakan lagi, model pembelajaran mendalam menjadi jauh lebih rumit.

161
00:08:52,620 --> 00:08:57,660
GPT-3, misalnya, tidak hanya memiliki dua, tetapi 175 miliar parameter.

162
00:08:58,120 --> 00:09:01,762
Namun, inilah masalahnya, Anda tidak bisa membuat model raksasa 

163
00:09:01,762 --> 00:09:05,632
dengan jumlah parameter yang sangat banyak tanpa harus membuat data 

164
00:09:05,632 --> 00:09:09,560
pelatihan menjadi terlalu banyak atau tidak bisa dilatih sama sekali.

165
00:09:10,260 --> 00:09:13,220
Deep learning menggambarkan sebuah kelas model yang dalam beberapa 

166
00:09:13,220 --> 00:09:16,180
dekade terakhir telah terbukti dapat berkembang dengan sangat baik.

167
00:09:16,480 --> 00:09:19,681
Yang menyatukan mereka adalah algoritma pelatihan yang sama, 

168
00:09:19,681 --> 00:09:23,355
yang disebut backpropagation, dan konteks yang saya ingin Anda miliki 

169
00:09:23,355 --> 00:09:27,133
saat kita membahasnya adalah agar algoritma pelatihan ini dapat bekerja 

170
00:09:27,133 --> 00:09:31,280
dengan baik dalam skala besar, model-model ini harus mengikuti format tertentu.

171
00:09:31,800 --> 00:09:34,807
Jika Anda mengetahui format ini, maka akan membantu menjelaskan 

172
00:09:34,807 --> 00:09:37,862
banyak pilihan tentang bagaimana transformator memproses bahasa, 

173
00:09:37,862 --> 00:09:40,400
yang jika tidak, akan berisiko terasa sewenang-wenang.

174
00:09:41,440 --> 00:09:46,740
Pertama, apa pun model yang Anda buat, input harus diformat sebagai array bilangan real.

175
00:09:46,740 --> 00:09:50,022
Ini bisa berarti daftar angka, bisa juga berupa larik dua dimensi, 

176
00:09:50,022 --> 00:09:53,550
atau sering kali Anda berurusan dengan larik dimensi yang lebih tinggi, 

177
00:09:53,550 --> 00:09:56,000
di mana istilah umum yang digunakan adalah tensor.

178
00:09:56,560 --> 00:09:59,281
Anda sering berpikir bahwa data input tersebut secara progresif 

179
00:09:59,281 --> 00:10:02,556
ditransformasikan ke dalam banyak lapisan yang berbeda, di mana sekali lagi, 

180
00:10:02,556 --> 00:10:05,575
setiap lapisan selalu terstruktur sebagai semacam array bilangan real, 

181
00:10:05,575 --> 00:10:08,680
sampai Anda sampai pada lapisan terakhir yang Anda anggap sebagai output.

182
00:10:09,280 --> 00:10:13,052
Sebagai contoh, lapisan terakhir dalam model pemrosesan teks kami adalah daftar 

183
00:10:13,052 --> 00:10:17,060
angka yang mewakili distribusi probabilitas untuk semua kemungkinan token berikutnya.

184
00:10:17,820 --> 00:10:21,697
Dalam deep learning, parameter model ini hampir selalu disebut sebagai bobot, 

185
00:10:21,697 --> 00:10:25,724
dan ini karena fitur utama dari model ini adalah satu-satunya cara parameter ini 

186
00:10:25,724 --> 00:10:29,900
berinteraksi dengan data yang sedang diproses adalah melalui penjumlahan tertimbang.

187
00:10:30,340 --> 00:10:32,597
Anda juga dapat menaburkan beberapa fungsi non-linear di seluruh bagian, 

188
00:10:32,597 --> 00:10:34,360
tetapi fungsi-fungsi ini tidak bergantung pada parameter.

189
00:10:35,200 --> 00:10:38,725
Biasanya, alih-alih melihat jumlah tertimbang secara telanjang dan 

190
00:10:38,725 --> 00:10:42,357
ditulis secara eksplisit seperti ini, Anda akan menemukannya dikemas 

191
00:10:42,357 --> 00:10:45,620
bersama sebagai berbagai komponen dalam produk vektor matriks.

192
00:10:46,740 --> 00:10:48,742
Ini sama saja dengan mengatakan hal yang sama, 

193
00:10:48,742 --> 00:10:51,512
jika Anda mengingat kembali cara kerja perkalian vektor matriks, 

194
00:10:51,512 --> 00:10:54,240
setiap komponen dalam output terlihat seperti jumlah tertimbang.

195
00:10:54,780 --> 00:10:58,310
Hanya saja, secara konseptual sering kali lebih mudah bagi Anda dan saya 

196
00:10:58,310 --> 00:11:01,792
untuk memikirkan matriks yang diisi dengan parameter yang dapat disetel 

197
00:11:01,792 --> 00:11:05,420
yang mentransformasikan vektor yang diambil dari data yang sedang diproses.

198
00:11:06,340 --> 00:11:10,443
Sebagai contoh, 175 miliar bobot dalam GPT-3 disusun 

199
00:11:10,443 --> 00:11:14,160
menjadi kurang dari 28.000 matriks yang berbeda.

200
00:11:14,660 --> 00:11:17,840
Matriks-matriks tersebut pada gilirannya terbagi ke dalam delapan kategori yang berbeda, 

201
00:11:17,840 --> 00:11:20,484
dan apa yang akan Anda dan saya lakukan adalah menelusuri setiap kategori 

202
00:11:20,484 --> 00:11:22,700
tersebut untuk memahami apa yang dilakukan oleh tipe tersebut.

203
00:11:23,160 --> 00:11:27,116
Sambil kita bahas, saya pikir akan lebih menyenangkan jika kita mengacu pada angka 

204
00:11:27,116 --> 00:11:31,360
spesifik dari GPT-3 untuk menghitung dengan tepat dari mana angka 175 miliar itu berasal.

205
00:11:31,880 --> 00:11:34,529
Meskipun saat ini ada model yang lebih besar dan lebih baik, 

206
00:11:34,529 --> 00:11:37,352
namun model yang satu ini memiliki pesona tertentu sebagai model 

207
00:11:37,352 --> 00:11:40,740
berbahasa besar yang benar-benar menarik perhatian dunia di luar komunitas ML.

208
00:11:41,440 --> 00:11:43,931
Selain itu, secara praktis, perusahaan cenderung untuk 

209
00:11:43,931 --> 00:11:46,740
menjaga angka-angka tertentu untuk jaringan yang lebih modern.

210
00:11:47,360 --> 00:11:50,672
Saya hanya ingin menjelaskan bahwa saat Anda mengintip di balik layar 

211
00:11:50,672 --> 00:11:53,606
untuk melihat apa yang terjadi di dalam alat seperti ChatGPT, 

212
00:11:53,606 --> 00:11:57,440
hampir semua komputasi yang sebenarnya terlihat seperti perkalian vektor matriks.

213
00:11:57,900 --> 00:12:00,658
Ada sedikit risiko tersesat dalam lautan miliaran angka, 

214
00:12:00,658 --> 00:12:04,095
tetapi Anda harus membuat perbedaan yang sangat tajam dalam benak Anda 

215
00:12:04,095 --> 00:12:07,919
antara bobot model, yang akan selalu saya warnai dengan warna biru atau merah, 

216
00:12:07,919 --> 00:12:11,840
dan data yang sedang diproses, yang akan selalu saya warnai dengan warna abu-abu.

217
00:12:12,180 --> 00:12:14,816
Beban adalah otak yang sebenarnya, mereka adalah hal-hal yang 

218
00:12:14,816 --> 00:12:17,920
dipelajari selama pelatihan, dan mereka menentukan bagaimana perilakunya.

219
00:12:18,280 --> 00:12:22,252
Data yang sedang diproses hanya mengkodekan input spesifik apa pun yang 

220
00:12:22,252 --> 00:12:26,500
dimasukkan ke dalam model untuk menjalankannya, seperti contoh potongan teks.

221
00:12:27,480 --> 00:12:31,775
Dengan semua itu sebagai dasar, mari kita gali langkah pertama dari contoh pemrosesan 

222
00:12:31,775 --> 00:12:36,070
teks ini, yaitu memecah input menjadi potongan-potongan kecil dan mengubahnya menjadi 

223
00:12:36,070 --> 00:12:36,420
vektor.

224
00:12:37,020 --> 00:12:39,775
Saya telah menyebutkan bagaimana potongan-potongan itu disebut token, 

225
00:12:39,775 --> 00:12:41,782
yang mungkin berupa potongan kata atau tanda baca, 

226
00:12:41,782 --> 00:12:44,340
tetapi sesekali dalam bab ini dan terutama dalam bab berikutnya, 

227
00:12:44,340 --> 00:12:47,017
saya ingin berpura-pura bahwa potongan-potongan itu dipecah menjadi 

228
00:12:47,017 --> 00:12:48,080
kata-kata yang lebih jelas.

229
00:12:48,600 --> 00:12:50,493
Karena kita manusia berpikir dengan kata-kata, 

230
00:12:50,493 --> 00:12:54,080
hal ini akan memudahkan untuk merujuk contoh-contoh kecil dan memperjelas setiap langkah.

231
00:12:55,260 --> 00:12:58,345
Model ini memiliki kosakata yang sudah ditentukan sebelumnya, 

232
00:12:58,345 --> 00:13:01,579
beberapa daftar semua kata yang mungkin, katakanlah 50.000 kata, 

233
00:13:01,579 --> 00:13:05,710
dan matriks pertama yang akan kita temui, yang dikenal sebagai matriks penyisipan, 

234
00:13:05,710 --> 00:13:07,800
memiliki satu kolom untuk setiap kata ini.

235
00:13:08,940 --> 00:13:11,398
Kolom-kolom inilah yang menentukan vektor apa yang 

236
00:13:11,398 --> 00:13:13,760
berubah menjadi setiap kata pada langkah pertama.

237
00:13:15,100 --> 00:13:18,538
Kita beri label We, dan seperti semua matriks yang kita lihat, 

238
00:13:18,538 --> 00:13:22,360
nilainya dimulai secara acak, tetapi akan dipelajari berdasarkan data.

239
00:13:23,620 --> 00:13:26,526
Mengubah kata menjadi vektor adalah praktik umum dalam pembelajaran 

240
00:13:26,526 --> 00:13:29,604
mesin jauh sebelum transformer, tetapi agak aneh jika Anda belum pernah 

241
00:13:29,604 --> 00:13:32,724
melihatnya sebelumnya, dan ini menjadi dasar untuk semua hal berikutnya, 

242
00:13:32,724 --> 00:13:35,760
jadi mari kita luangkan waktu sejenak untuk membiasakan diri dengannya.

243
00:13:36,040 --> 00:13:38,122
Kami sering menyebutnya dengan istilah embedding, 

244
00:13:38,122 --> 00:13:40,746
yang mengundang Anda untuk memikirkan vektor-vektor ini secara 

245
00:13:40,746 --> 00:13:43,620
geometris sebagai titik-titik di dalam suatu ruang berdimensi tinggi.

246
00:13:44,180 --> 00:13:46,605
Memvisualisasikan daftar tiga angka sebagai koordinat untuk 

247
00:13:46,605 --> 00:13:48,828
titik-titik dalam ruang 3D tidak akan menjadi masalah, 

248
00:13:48,828 --> 00:13:51,780
tetapi penyematan kata cenderung memiliki dimensi yang jauh lebih tinggi.

249
00:13:52,280 --> 00:13:56,412
Dalam GPT-3, mereka memiliki 12.288 dimensi, dan seperti yang akan Anda lihat, 

250
00:13:56,412 --> 00:14:00,440
sangat penting untuk bekerja di ruang yang memiliki banyak arah yang berbeda.

251
00:14:01,180 --> 00:14:05,049
Dengan cara yang sama seperti Anda dapat mengambil irisan dua dimensi melalui 

252
00:14:05,049 --> 00:14:07,877
ruang 3D dan memproyeksikan semua titik pada irisan itu, 

253
00:14:07,877 --> 00:14:12,095
demi menganimasikan penyematan kata yang diberikan oleh model sederhana kepada saya, 

254
00:14:12,095 --> 00:14:15,766
saya akan melakukan hal serupa dengan memilih irisan tiga dimensi melalui 

255
00:14:15,766 --> 00:14:19,438
ruang yang sangat tinggi ini, dan memproyeksikan vektor kata ke bawah dan 

256
00:14:19,438 --> 00:14:20,480
menampilkan hasilnya.

257
00:14:21,280 --> 00:14:24,365
Ide besarnya di sini adalah bahwa ketika model menyesuaikan dan 

258
00:14:24,365 --> 00:14:27,450
menyetel bobotnya untuk menentukan bagaimana tepatnya kata-kata 

259
00:14:27,450 --> 00:14:30,728
disematkan sebagai vektor selama pelatihan, model cenderung memilih 

260
00:14:30,728 --> 00:14:34,440
satu set penyematan di mana arah dalam ruang memiliki semacam makna semantik.

261
00:14:34,980 --> 00:14:37,807
Untuk model kata-ke-vektor sederhana yang saya jalankan di sini, 

262
00:14:37,807 --> 00:14:41,505
jika saya menjalankan pencarian untuk semua kata yang sematannya paling dekat dengan 

263
00:14:41,505 --> 00:14:45,116
menara, Anda akan melihat bahwa semua kata tersebut tampak memberikan kesan menara 

264
00:14:45,116 --> 00:14:45,900
yang sangat mirip.

265
00:14:46,340 --> 00:14:48,819
Dan jika Anda ingin belajar Python dan bermain-main di rumah, 

266
00:14:48,819 --> 00:14:51,380
ini adalah model khusus yang saya gunakan untuk membuat animasi.

267
00:14:51,620 --> 00:14:54,634
Ini bukan transformator, tetapi cukup untuk mengilustrasikan 

268
00:14:54,634 --> 00:14:57,600
gagasan bahwa arah dalam ruang dapat membawa makna semantik.

269
00:14:58,300 --> 00:15:02,191
Contoh yang sangat klasik dari hal ini adalah bagaimana jika Anda mengambil 

270
00:15:02,191 --> 00:15:04,597
perbedaan antara vektor untuk wanita dan pria, 

271
00:15:04,597 --> 00:15:08,335
sesuatu yang akan Anda bayangkan sebagai vektor kecil yang menghubungkan 

272
00:15:08,335 --> 00:15:12,124
ujung yang satu dengan ujung yang lain, ini sangat mirip dengan perbedaan 

273
00:15:12,124 --> 00:15:13,200
antara raja dan ratu.

274
00:15:15,080 --> 00:15:17,952
Jadi, katakanlah Anda tidak tahu kata untuk raja wanita, 

275
00:15:17,952 --> 00:15:20,572
Anda dapat menemukannya dengan mengambil kata raja, 

276
00:15:20,572 --> 00:15:24,049
menambahkan arah wanita-pria ini, dan mencari penyematan yang paling 

277
00:15:24,049 --> 00:15:25,460
dekat dengan titik tersebut.

278
00:15:27,000 --> 00:15:28,200
Setidaknya, seperti itu.

279
00:15:28,480 --> 00:15:31,797
Meskipun ini adalah contoh klasik untuk model yang saya mainkan, 

280
00:15:31,797 --> 00:15:36,084
penyematan ratu yang sebenarnya sebenarnya sedikit lebih jauh dari yang disarankan, 

281
00:15:36,084 --> 00:15:40,269
mungkin karena cara ratu digunakan dalam data pelatihan bukan hanya versi feminin 

282
00:15:40,269 --> 00:15:40,780
dari raja.

283
00:15:41,620 --> 00:15:43,625
Ketika saya bermain-main, hubungan keluarga tampaknya 

284
00:15:43,625 --> 00:15:45,260
menggambarkan gagasan itu dengan lebih baik.

285
00:15:46,340 --> 00:15:50,423
Intinya, sepertinya selama pelatihan, model ini menemukan keuntungan untuk memilih 

286
00:15:50,423 --> 00:15:54,555
penyematan sedemikian rupa sehingga satu arah dalam ruang ini mengkodekan informasi 

287
00:15:54,555 --> 00:15:54,900
gender.

288
00:15:56,800 --> 00:16:00,048
Contoh lainnya adalah jika Anda mengambil penyematan Italia, 

289
00:16:00,048 --> 00:16:04,042
dan mengurangi penyematan Jerman, dan menambahkannya ke penyematan Hitler, 

290
00:16:04,042 --> 00:16:08,090
Anda akan mendapatkan sesuatu yang sangat mirip dengan penyematan Mussolini.

291
00:16:08,570 --> 00:16:12,670
Seolah-olah sang model belajar mengasosiasikan beberapa arah dengan ke-Italia-an, 

292
00:16:12,670 --> 00:16:15,670
dan yang lainnya dengan para pemimpin poros Perang Dunia II.

293
00:16:16,470 --> 00:16:19,922
Mungkin contoh favorit saya dalam hal ini adalah bagaimana dalam beberapa model, 

294
00:16:19,922 --> 00:16:23,544
jika Anda mengambil perbedaan antara Jerman dan Jepang, dan menambahkannya ke sushi, 

295
00:16:23,544 --> 00:16:26,230
Anda akan mendapatkan sushi yang sangat mirip dengan bratwurst.

296
00:16:27,350 --> 00:16:30,386
Juga dalam memainkan permainan menemukan tetangga terdekat ini, 

297
00:16:30,386 --> 00:16:33,850
saya senang melihat betapa dekatnya Kat dengan binatang buas dan monster.

298
00:16:34,690 --> 00:16:37,307
Satu intuisi matematika yang sangat membantu untuk diingat, 

299
00:16:37,307 --> 00:16:40,316
terutama untuk bab berikutnya, adalah bagaimana dot product dari dua 

300
00:16:40,316 --> 00:16:43,850
vektor dapat dianggap sebagai cara untuk mengukur seberapa baik keduanya sejajar.

301
00:16:44,870 --> 00:16:48,083
Secara komputasi, dot product melibatkan perkalian semua komponen yang 

302
00:16:48,083 --> 00:16:50,663
sesuai dan kemudian menambahkan hasilnya, dan ini bagus, 

303
00:16:50,663 --> 00:16:54,330
karena sebagian besar komputasi kita harus terlihat seperti penjumlahan berbobot.

304
00:16:55,190 --> 00:16:59,585
Secara geometris, dot product bernilai positif ketika vektor mengarah ke arah yang sama, 

305
00:16:59,585 --> 00:17:01,906
bernilai nol jika vektor tersebut tegak lurus, 

306
00:17:01,906 --> 00:17:05,609
dan bernilai negatif jika vektor tersebut mengarah ke arah yang berlawanan.

307
00:17:06,550 --> 00:17:10,001
Contohnya, katakanlah Anda sedang bermain-main dengan model ini, 

308
00:17:10,001 --> 00:17:13,452
dan Anda membuat hipotesis bahwa penyematan kucing minus kucing, 

309
00:17:13,452 --> 00:17:17,010
mungkin merepresentasikan semacam arah kemajemukan dalam ruang ini.

310
00:17:17,430 --> 00:17:20,576
Untuk mengujinya, saya akan mengambil vektor ini dan menghitung hasil 

311
00:17:20,576 --> 00:17:23,408
kali titiknya terhadap penyematan kata benda tunggal tertentu, 

312
00:17:23,408 --> 00:17:27,050
dan membandingkannya dengan hasil kali titik dengan kata benda jamak yang sesuai.

313
00:17:27,270 --> 00:17:30,190
Jika Anda bermain-main dengan hal ini, Anda akan melihat bahwa bentuk jamak 

314
00:17:30,190 --> 00:17:33,418
memang secara konsisten memberikan nilai yang lebih tinggi daripada bentuk tunggal, 

315
00:17:33,418 --> 00:17:36,070
yang mengindikasikan bahwa bentuk jamak lebih sesuai dengan arah ini.

316
00:17:37,070 --> 00:17:40,934
Sangat menyenangkan juga bahwa jika Anda mengambil dot product ini dengan embedding 

317
00:17:40,934 --> 00:17:44,154
kata 1, 2, 3, dan seterusnya, mereka memberikan nilai yang meningkat, 

318
00:17:44,154 --> 00:17:48,110
jadi seolah-olah kita bisa mengukur secara kuantitatif seberapa jamak model menemukan 

319
00:17:48,110 --> 00:17:49,030
kata yang diberikan.

320
00:17:50,250 --> 00:17:51,962
Sekali lagi, secara spesifik bagaimana kata-kata 

321
00:17:51,962 --> 00:17:53,570
disematkan dipelajari dengan menggunakan data.

322
00:17:54,050 --> 00:17:56,761
Matriks penyematan ini, yang kolom-kolomnya memberi tahu kita apa yang 

323
00:17:56,761 --> 00:17:59,550
terjadi pada setiap kata, adalah tumpukan bobot pertama dalam model kami.

324
00:18:00,030 --> 00:18:04,837
Dengan menggunakan angka GPT-3, ukuran kosakata secara khusus adalah 50.257, 

325
00:18:04,837 --> 00:18:09,770
dan sekali lagi, secara teknis, ini tidak terdiri dari kata-kata, tetapi token.

326
00:18:10,630 --> 00:18:14,039
Dimensi penyematan adalah 12.288, dan dengan mengalikannya, 

327
00:18:14,039 --> 00:18:17,790
maka akan diketahui bahwa ini terdiri dari sekitar 617 juta bobot.

328
00:18:18,250 --> 00:18:20,905
Mari kita lanjutkan dan tambahkan ini ke penghitungan berjalan, 

329
00:18:20,905 --> 00:18:23,810
mengingat bahwa pada akhirnya kita harus menghitung hingga 175 miliar.

330
00:18:25,430 --> 00:18:28,705
Dalam kasus transformer, Anda benar-benar ingin menganggap vektor 

331
00:18:28,705 --> 00:18:32,130
dalam ruang penyematan ini tidak hanya mewakili kata-kata individual.

332
00:18:32,550 --> 00:18:36,352
Untuk satu hal, mereka juga menyandikan informasi tentang posisi kata tersebut, 

333
00:18:36,352 --> 00:18:39,157
yang akan kita bicarakan nanti, tetapi yang lebih penting, 

334
00:18:39,157 --> 00:18:42,770
Anda harus menganggap mereka memiliki kapasitas untuk meresap dalam konteks.

335
00:18:43,350 --> 00:18:46,639
Sebuah vektor yang memulai kehidupannya sebagai penyematan kata raja, 

336
00:18:46,639 --> 00:18:50,116
misalnya, mungkin secara progresif ditarik dan ditarik oleh berbagai blok 

337
00:18:50,116 --> 00:18:53,687
dalam jaringan ini, sehingga pada akhirnya vektor tersebut mengarah ke arah 

338
00:18:53,687 --> 00:18:57,070
yang jauh lebih spesifik dan bernuansa yang entah bagaimana mengkodekan 

339
00:18:57,070 --> 00:18:59,420
bahwa itu adalah raja yang tinggal di Skotlandia, 

340
00:18:59,420 --> 00:19:02,568
dan yang telah meraih jabatannya setelah membunuh raja sebelumnya, 

341
00:19:02,568 --> 00:19:04,730
dan yang digambarkan dalam bahasa Shakespeare.

342
00:19:05,210 --> 00:19:07,790
Pikirkan tentang pemahaman Anda sendiri tentang kata yang diberikan.

343
00:19:08,250 --> 00:19:11,633
Makna dari kata tersebut secara jelas diinformasikan oleh lingkungan sekitar, 

344
00:19:11,633 --> 00:19:14,279
dan terkadang hal ini mencakup konteks dari jarak yang jauh, 

345
00:19:14,279 --> 00:19:18,010
sehingga dalam menyusun model yang memiliki kemampuan untuk memprediksi kata apa yang 

346
00:19:18,010 --> 00:19:21,784
akan muncul berikutnya, tujuannya adalah untuk memberdayakan model tersebut agar dapat 

347
00:19:21,784 --> 00:19:23,390
menggabungkan konteks secara efisien.

348
00:19:24,050 --> 00:19:27,141
Untuk lebih jelasnya, pada langkah pertama, ketika Anda membuat larik 

349
00:19:27,141 --> 00:19:30,454
vektor berdasarkan teks input, masing-masing vektor tersebut hanya diambil 

350
00:19:30,454 --> 00:19:33,722
dari matriks penyisipan, sehingga pada awalnya masing-masing vektor hanya 

351
00:19:33,722 --> 00:19:36,770
dapat menyandikan arti dari satu kata tanpa input dari sekelilingnya.

352
00:19:37,710 --> 00:19:41,379
Namun, Anda harus memikirkan tujuan utama dari jaringan yang dilaluinya, 

353
00:19:41,379 --> 00:19:45,049
yaitu untuk memungkinkan setiap vektor tersebut menyerap makna yang jauh 

354
00:19:45,049 --> 00:19:48,970
lebih kaya dan spesifik daripada yang bisa diwakili oleh kata-kata individual.

355
00:19:49,510 --> 00:19:52,509
Jaringan hanya dapat memproses sejumlah vektor dalam satu waktu, 

356
00:19:52,509 --> 00:19:54,170
yang dikenal sebagai ukuran konteks.

357
00:19:54,510 --> 00:19:57,341
Untuk GPT-3 dilatih dengan ukuran konteks 2048, 

358
00:19:57,341 --> 00:20:02,473
sehingga data yang mengalir melalui jaringan selalu terlihat seperti larik 2048 kolom, 

359
00:20:02,473 --> 00:20:05,010
yang masing-masing memiliki 12.000 dimensi.

360
00:20:05,590 --> 00:20:08,495
Ukuran konteks ini membatasi seberapa banyak teks yang dapat 

361
00:20:08,495 --> 00:20:11,830
dimasukkan oleh transformator ketika membuat prediksi kata berikutnya.

362
00:20:12,370 --> 00:20:15,780
Inilah sebabnya mengapa percakapan panjang dengan chatbot tertentu, 

363
00:20:15,780 --> 00:20:18,789
seperti versi awal ChatGPT, sering kali membuat bot seperti 

364
00:20:18,789 --> 00:20:22,050
kehilangan alur percakapan saat Anda melanjutkannya terlalu lama.

365
00:20:23,030 --> 00:20:25,824
Kita akan membahas detail perhatian pada waktunya, tetapi melompati itu, 

366
00:20:25,824 --> 00:20:28,810
saya ingin berbicara sebentar tentang apa yang terjadi di bagian paling akhir.

367
00:20:29,450 --> 00:20:31,836
Ingat, output yang diinginkan adalah distribusi 

368
00:20:31,836 --> 00:20:34,870
probabilitas atas semua token yang mungkin muncul berikutnya.

369
00:20:35,170 --> 00:20:37,520
Sebagai contoh, jika kata terakhir adalah Profesor, 

370
00:20:37,520 --> 00:20:40,052
dan konteksnya mencakup kata-kata seperti Harry Potter, 

371
00:20:40,052 --> 00:20:43,081
dan segera sebelumnya kita melihat guru yang paling tidak disukai, 

372
00:20:43,081 --> 00:20:46,698
dan juga jika Anda memberi saya kelonggaran dengan membiarkan saya berpura-pura 

373
00:20:46,698 --> 00:20:48,913
bahwa token hanya terlihat seperti kata lengkap, 

374
00:20:48,913 --> 00:20:52,213
maka jaringan yang terlatih dengan baik yang telah membangun pengetahuan 

375
00:20:52,213 --> 00:20:55,830
tentang Harry Potter mungkin akan memberikan angka yang tinggi untuk kata Snape.

376
00:20:56,510 --> 00:20:57,970
Ini melibatkan dua langkah yang berbeda.

377
00:20:58,310 --> 00:21:03,017
Yang pertama adalah menggunakan matriks lain yang memetakan vektor terakhir dalam 

378
00:21:03,017 --> 00:21:07,610
konteks tersebut ke daftar 50.000 nilai, satu untuk setiap token dalam kosakata.

379
00:21:08,170 --> 00:21:11,837
Kemudian ada fungsi yang menormalkan ini menjadi distribusi probabilitas, 

380
00:21:11,837 --> 00:21:15,702
ini disebut Softmax dan kita akan membicarakannya lebih lanjut sebentar lagi, 

381
00:21:15,702 --> 00:21:19,766
tetapi sebelumnya mungkin terlihat sedikit aneh jika hanya menggunakan penyematan 

382
00:21:19,766 --> 00:21:23,730
terakhir ini untuk membuat prediksi, padahal pada langkah terakhir tersebut ada 

383
00:21:23,730 --> 00:21:27,893
ribuan vektor lain di lapisan yang hanya duduk di sana dengan makna kaya konteksnya 

384
00:21:27,893 --> 00:21:28,290
sendiri.

385
00:21:28,930 --> 00:21:32,103
Hal ini berkaitan dengan fakta bahwa dalam proses pelatihan, 

386
00:21:32,103 --> 00:21:35,744
akan jauh lebih efisien jika Anda menggunakan masing-masing vektor di 

387
00:21:35,744 --> 00:21:40,270
lapisan akhir untuk secara bersamaan membuat prediksi apa yang akan terjadi setelahnya.

388
00:21:40,970 --> 00:21:43,287
Masih banyak lagi yang bisa dikatakan tentang pelatihan nanti, 

389
00:21:43,287 --> 00:21:45,090
tetapi saya hanya ingin menyampaikannya sekarang.

390
00:21:45,730 --> 00:21:49,690
Matriks ini disebut matriks Unembedding dan kami memberinya label WU.

391
00:21:50,210 --> 00:21:52,640
Sekali lagi, seperti semua matriks bobot yang kita lihat, 

392
00:21:52,640 --> 00:21:55,910
entri-entrinya dimulai secara acak, tetapi dipelajari selama proses pelatihan.

393
00:21:56,470 --> 00:21:58,764
Dengan tetap memperhatikan jumlah parameter total, 

394
00:21:58,764 --> 00:22:02,275
matriks Unembedding ini memiliki satu baris untuk setiap kata dalam kosakata, 

395
00:22:02,275 --> 00:22:05,650
dan setiap baris memiliki jumlah elemen yang sama dengan dimensi embedding.

396
00:22:06,410 --> 00:22:10,105
Ini sangat mirip dengan matriks penyisipan, hanya saja urutannya ditukar, 

397
00:22:10,105 --> 00:22:13,001
jadi ini menambahkan 617 juta parameter lagi ke jaringan, 

398
00:22:13,001 --> 00:22:16,347
yang berarti hitungan kami sejauh ini sedikit di atas satu miliar, 

399
00:22:16,347 --> 00:22:20,192
sebagian kecil tetapi tidak sepenuhnya tidak signifikan dari 175 miliar yang 

400
00:22:20,192 --> 00:22:21,790
akan kami dapatkan secara total.

401
00:22:22,550 --> 00:22:24,678
Sebagai pelajaran mini terakhir untuk bab ini, 

402
00:22:24,678 --> 00:22:27,304
saya ingin membahas lebih lanjut mengenai fungsi softmax, 

403
00:22:27,304 --> 00:22:30,610
karena fungsi ini akan muncul lagi setelah kita menyelami blok perhatian.

404
00:22:31,430 --> 00:22:35,471
Idenya adalah bahwa jika Anda ingin urutan angka bertindak sebagai distribusi 

405
00:22:35,471 --> 00:22:39,408
probabilitas, katakanlah distribusi atas semua kemungkinan kata berikutnya, 

406
00:22:39,408 --> 00:22:41,999
maka setiap nilai harus berada di antara 0 dan 1, 

407
00:22:41,999 --> 00:22:44,590
dan Anda juga harus menambahkan semuanya hingga 1.

408
00:22:45,250 --> 00:22:48,309
Namun, jika Anda memainkan game pembelajaran di mana semua yang 

409
00:22:48,309 --> 00:22:50,985
Anda lakukan terlihat seperti perkalian matriks-vektor, 

410
00:22:50,985 --> 00:22:54,810
output yang Anda dapatkan secara default sama sekali tidak mengikuti aturan ini.

411
00:22:55,330 --> 00:22:58,197
Nilainya sering kali negatif, atau jauh lebih besar dari 1, 

412
00:22:58,197 --> 00:22:59,870
dan hampir pasti tidak berjumlah 1.

413
00:23:00,510 --> 00:23:04,071
Softmax adalah cara standar untuk mengubah daftar angka sembarang menjadi 

414
00:23:04,071 --> 00:23:07,776
distribusi yang valid sedemikian rupa sehingga nilai terbesar menjadi paling 

415
00:23:07,776 --> 00:23:11,290
dekat dengan 1, dan nilai yang lebih kecil menjadi sangat dekat dengan 0.

416
00:23:11,830 --> 00:23:13,070
Hanya itu yang perlu Anda ketahui.

417
00:23:13,090 --> 00:23:17,266
Tetapi jika Anda penasaran, cara kerjanya adalah pertama-tama menaikkan e menjadi pangkat 

418
00:23:17,266 --> 00:23:21,163
dari masing-masing angka, yang berarti Anda sekarang memiliki daftar nilai positif, 

419
00:23:21,163 --> 00:23:25,293
dan kemudian Anda dapat mengambil jumlah semua nilai positif tersebut dan membagi setiap 

420
00:23:25,293 --> 00:23:29,470
suku dengan jumlah tersebut, yang menormalkannya menjadi daftar yang menambahkan hingga 1.

421
00:23:30,170 --> 00:23:34,285
Anda akan melihat bahwa jika salah satu angka dalam input jauh lebih besar daripada yang 

422
00:23:34,285 --> 00:23:37,707
lain, maka dalam output, istilah yang sesuai akan mendominasi distribusi, 

423
00:23:37,707 --> 00:23:41,822
jadi jika Anda mengambil sampel darinya, Anda hampir pasti hanya akan memilih input yang 

424
00:23:41,822 --> 00:23:42,470
memaksimalkan.

425
00:23:42,990 --> 00:23:46,185
Tetapi, ini lebih lembut daripada sekadar memilih nilai maksimum dalam arti, 

426
00:23:46,185 --> 00:23:49,255
apabila nilai lainnya sama besarnya, maka nilai tersebut juga mendapatkan 

427
00:23:49,255 --> 00:23:52,201
bobot yang berarti dalam distribusi, dan semuanya berubah secara terus 

428
00:23:52,201 --> 00:23:54,650
menerus saat Anda memvariasikan input secara terus menerus.

429
00:23:55,130 --> 00:23:58,612
Dalam beberapa situasi, seperti ketika ChatGPT menggunakan distribusi 

430
00:23:58,612 --> 00:24:02,393
ini untuk membuat kata berikutnya, ada ruang untuk sedikit bersenang-senang 

431
00:24:02,393 --> 00:24:05,427
dengan menambahkan sedikit bumbu ekstra ke dalam fungsi ini, 

432
00:24:05,427 --> 00:24:08,910
dengan t konstan yang dilemparkan ke dalam penyebut eksponen tersebut.

433
00:24:09,550 --> 00:24:13,500
Kami menyebutnya suhu, karena secara samar-samar menyerupai peran suhu dalam 

434
00:24:13,500 --> 00:24:17,347
persamaan termodinamika tertentu, dan efeknya adalah ketika t lebih besar, 

435
00:24:17,347 --> 00:24:20,682
Anda memberikan lebih banyak bobot pada nilai yang lebih rendah, 

436
00:24:20,682 --> 00:24:24,479
yang berarti distribusinya sedikit lebih seragam, dan jika t lebih kecil, 

437
00:24:24,479 --> 00:24:28,993
maka nilai yang lebih besar akan mendominasi lebih agresif, di mana pada titik ekstrem, 

438
00:24:28,993 --> 00:24:32,790
pengaturan t sama dengan nol berarti semua bobot menuju ke nilai maksimum.

439
00:24:33,470 --> 00:24:37,978
Sebagai contoh, saya akan meminta GPT-3 menghasilkan cerita dengan teks awal, 

440
00:24:37,978 --> 00:24:42,950
suatu ketika ada A, tetapi saya akan menggunakan suhu yang berbeda dalam setiap kasus.

441
00:24:43,630 --> 00:24:47,875
Suhu nol berarti selalu menggunakan kata yang paling mudah ditebak, 

442
00:24:47,875 --> 00:24:52,370
dan apa yang Anda dapatkan akhirnya adalah turunan basi dari Goldilocks.

443
00:24:53,010 --> 00:24:55,360
Suhu yang lebih tinggi memberikan kesempatan untuk memilih 

444
00:24:55,360 --> 00:24:57,910
kata-kata yang lebih kecil kemungkinannya, tetapi ada risikonya.

445
00:24:58,230 --> 00:25:00,887
Dalam kasus ini, cerita dimulai dengan lebih orisinil, 

446
00:25:00,887 --> 00:25:03,448
tentang seorang seniman web muda dari Korea Selatan, 

447
00:25:03,448 --> 00:25:06,010
tetapi dengan cepat berubah menjadi tidak masuk akal.

448
00:25:06,950 --> 00:25:10,830
Secara teknis, API sebenarnya tidak mengizinkan Anda memilih suhu yang lebih besar dari 2.

449
00:25:11,170 --> 00:25:13,826
Tidak ada alasan matematis untuk hal ini, ini hanyalah batasan 

450
00:25:13,826 --> 00:25:16,567
sewenang-wenang yang diberlakukan untuk menjaga agar alat mereka 

451
00:25:16,567 --> 00:25:19,350
tidak terlihat menghasilkan hal-hal yang terlalu tidak masuk akal.

452
00:25:19,870 --> 00:25:23,051
Jadi, jika Anda penasaran, cara kerja animasi ini sebenarnya adalah 

453
00:25:23,051 --> 00:25:26,466
saya mengambil 20 token berikutnya yang paling mungkin dihasilkan GPT-3, 

454
00:25:26,466 --> 00:25:30,162
yang tampaknya merupakan jumlah maksimum yang akan mereka berikan kepada saya, 

455
00:25:30,162 --> 00:25:32,970
lalu saya mengubah probabilitasnya berdasarkan eksponen 1/5.

456
00:25:33,130 --> 00:25:36,348
Sebagai sedikit jargon lainnya, dengan cara yang sama seperti Anda 

457
00:25:36,348 --> 00:25:39,375
menyebut komponen output dari fungsi ini sebagai probabilitas, 

458
00:25:39,375 --> 00:25:43,267
orang sering menyebut input sebagai logit, atau beberapa orang mengatakan logit, 

459
00:25:43,267 --> 00:25:46,150
beberapa orang mengatakan logit, saya akan mengatakan logit.

460
00:25:46,530 --> 00:25:48,911
Jadi, misalnya, ketika Anda memasukkan beberapa teks, 

461
00:25:48,911 --> 00:25:51,909
Anda memiliki semua kata yang disematkan mengalir melalui jaringan, 

462
00:25:51,909 --> 00:25:54,687
dan Anda melakukan perkalian akhir dengan matriks unembedding, 

463
00:25:54,687 --> 00:25:58,303
orang-orang yang mempelajari mesin akan merujuk pada komponen dalam output mentah 

464
00:25:58,303 --> 00:26:01,390
yang tidak dinormalisasi sebagai logit untuk prediksi kata berikutnya.

465
00:26:03,330 --> 00:26:06,823
Sebagian besar tujuan bab ini adalah untuk meletakkan dasar-dasar 

466
00:26:06,823 --> 00:26:10,370
untuk memahami mekanisme perhatian, gaya wax-on-wax-off Karate Kid.

467
00:26:10,850 --> 00:26:15,341
Anda tahu, jika Anda memiliki intuisi yang kuat untuk penyematan kata, untuk softmax, 

468
00:26:15,341 --> 00:26:17,848
untuk bagaimana dot product mengukur kemiripan, 

469
00:26:17,848 --> 00:26:21,973
dan juga premis yang mendasari bahwa sebagian besar perhitungan harus terlihat 

470
00:26:21,973 --> 00:26:26,621
seperti perkalian matriks dengan matriks yang penuh dengan parameter yang dapat disetel, 

471
00:26:26,621 --> 00:26:30,956
maka memahami mekanisme perhatian, bagian terpenting dari ledakan modern dalam AI, 

472
00:26:30,956 --> 00:26:32,210
harusnya relatif lancar.

473
00:26:32,650 --> 00:26:34,510
Untuk itu, mari bergabung dengan saya di bab berikutnya.

474
00:26:36,390 --> 00:26:38,889
Saat saya mempublikasikan ini, draf dari bab berikutnya 

475
00:26:38,889 --> 00:26:41,210
tersedia untuk ditinjau oleh para pendukung Patreon.

476
00:26:41,770 --> 00:26:44,103
Versi final akan dipublikasikan dalam satu atau dua minggu, 

477
00:26:44,103 --> 00:26:47,370
biasanya tergantung pada seberapa banyak yang saya ubah berdasarkan ulasan tersebut.

478
00:26:47,810 --> 00:26:49,581
Sementara itu, jika Anda ingin menyelami perhatian, 

479
00:26:49,581 --> 00:26:52,410
dan jika Anda ingin sedikit membantu saluran ini, saluran ini ada di sana menunggu.

