1
00:00:00,000 --> 00:00:04,560
Инициалы GPT означают Generative Pretrained Transformer.

2
00:00:05,220 --> 00:00:09,020
Итак, первое слово достаточно простое: это боты, которые генерируют новый текст.

3
00:00:09,800 --> 00:00:13,108
Pretrained означает, что модель прошла через процесс обучения на огромном 

4
00:00:13,108 --> 00:00:15,344
количестве данных, а приставка pre подразумевает, 

5
00:00:15,344 --> 00:00:18,653
что при дополнительном обучении у нее есть больше возможностей для тонкой 

6
00:00:18,653 --> 00:00:20,040
настройки на конкретные задачи.

7
00:00:20,720 --> 00:00:22,900
Но ключевое слово здесь - последнее.

8
00:00:23,380 --> 00:00:27,163
Трансформер - это особый вид нейронной сети, модель машинного обучения, 

9
00:00:27,163 --> 00:00:31,000
и это основное изобретение, лежащее в основе нынешнего бума в области ИИ.

10
00:00:31,740 --> 00:00:35,832
В этом видео и последующих главах я хочу наглядно объяснить, 

11
00:00:35,832 --> 00:00:39,120
что на самом деле происходит внутри трансформера.

12
00:00:39,700 --> 00:00:42,820
Мы пошогово проследим за данными, которые через него проходят.

13
00:00:43,440 --> 00:00:46,415
Существует множество различных видов моделей, которые можно построить, 

14
00:00:46,415 --> 00:00:47,380
используя трансформеры.

15
00:00:47,800 --> 00:00:50,800
Некоторые модели принимают аудиозапись и выдают транскрипт.

16
00:00:51,340 --> 00:00:53,964
Это предложение относится к модели, работающей наоборот, 

17
00:00:53,964 --> 00:00:56,220
производящей синтетическую речь только из текста.

18
00:00:56,660 --> 00:01:01,089
Все те инструменты, которые взяли мир штурмом в 2022 году, вроде Dolly и Midjourney, 

19
00:01:01,089 --> 00:01:05,519
которые принимают текстовое описание и выдают изображение, основаны на трансформерах.

20
00:01:06,000 --> 00:01:10,303
Даже если я не могу заставить его понять, каким должно быть существо из пирога, 

21
00:01:10,303 --> 00:01:13,100
я все равно потрясен тем, что такое вообще возможно.

22
00:01:13,900 --> 00:01:17,506
А оригинальный трансформер, представленный в 2017 году компанией Google, 

23
00:01:17,506 --> 00:01:21,606
был придуман для конкретного случая использования - перевода текста с одного языка 

24
00:01:21,606 --> 00:01:22,100
на другой.

25
00:01:22,660 --> 00:01:26,446
Но вариант, на котором мы с тобой сосредоточимся и который лежит в 

26
00:01:26,446 --> 00:01:29,499
основе таких инструментов, как ChatGPT, - это модель, 

27
00:01:29,499 --> 00:01:32,325
обученная воспринимать фрагмент текста, возможно, 

28
00:01:32,325 --> 00:01:36,733
даже с сопутствующими изображениями или звуком, и выдавать предсказание того, 

29
00:01:36,733 --> 00:01:38,260
что будет дальше в отрывке.

30
00:01:38,600 --> 00:01:41,338
Это предсказание имеет форму распределения вероятностей по множеству 

31
00:01:41,338 --> 00:01:43,800
различных фрагментов текста, которые могут последовать за ним.

32
00:01:45,040 --> 00:01:47,601
На первый взгляд, тебе может показаться, что предсказание следующего 

33
00:01:47,601 --> 00:01:49,940
слова - это совсем другая цель, нежели генерация нового текста.

34
00:01:50,180 --> 00:01:52,768
Но если у тебя есть подобная модель предсказания, 

35
00:01:52,768 --> 00:01:56,650
то для создания более длинного куска текста можно просто дать ей начальный 

36
00:01:56,650 --> 00:02:01,050
фрагмент для работы, взять случайную выборку из только что созданного распределения, 

37
00:02:01,050 --> 00:02:04,156
добавить ее к тексту, а затем запустить весь процесс снова, 

38
00:02:04,156 --> 00:02:08,090
чтобы сделать новое предсказание на основе всего нового текста, включая то, 

39
00:02:08,090 --> 00:02:09,539
что она только что добавила.

40
00:02:10,100 --> 00:02:13,000
Не знаю, как тебе, но мне кажется, что это действительно не должно работать.

41
00:02:13,420 --> 00:02:16,240
Например, в этой анимации я запускаю GPT-2 на своем ноутбуке и 

42
00:02:16,240 --> 00:02:19,912
заставляю его многократно предсказывать и сэмплировать следующий фрагмент текста, 

43
00:02:19,912 --> 00:02:22,420
чтобы сгенерировать историю на основе начального текста.

44
00:02:22,420 --> 00:02:26,120
Просто в этой истории не так много смысла.

45
00:02:26,500 --> 00:02:30,744
Но если я заменю его на API-вызовы к GPT-3, который является той же базовой моделью, 

46
00:02:30,744 --> 00:02:35,038
только намного больше, то внезапно, почти по волшебству, мы получим разумную историю, 

47
00:02:35,038 --> 00:02:37,734
которая, кажется, даже позволяет сделать вывод о том, 

48
00:02:37,734 --> 00:02:40,880
что существо пи могло бы жить в стране математики и вычислений.

49
00:02:41,580 --> 00:02:44,710
Этот процесс повторного предсказания и выборки - по сути, то, 

50
00:02:44,710 --> 00:02:48,194
что происходит, когда ты взаимодействуешь с ChatGPT или любой другой 

51
00:02:48,194 --> 00:02:51,880
большой языковой моделью и видишь, как они выдают по одному слову за раз.

52
00:02:52,480 --> 00:02:55,277
На самом деле, одна из функций, которая мне бы очень понравилась, 

53
00:02:55,277 --> 00:02:58,372
- это возможность видеть базовое распределение для каждого нового слова, 

54
00:02:58,372 --> 00:02:59,220
которое он выбирает.

55
00:03:03,820 --> 00:03:06,284
Давай начнем с того, что очень подробно рассмотрим, 

56
00:03:06,284 --> 00:03:08,180
как данные проходят через трансформатор.

57
00:03:08,640 --> 00:03:11,018
Мы потратим гораздо больше времени, мотивируя, 

58
00:03:11,018 --> 00:03:14,307
интерпретируя и расширяя детали каждого шага, но в общих чертах, 

59
00:03:14,307 --> 00:03:18,660
когда один из этих чатботов генерирует заданное слово, вот что происходит под капотом.

60
00:03:19,080 --> 00:03:22,040
Во-первых, входные данные разбиваются на кучу маленьких кусочков.

61
00:03:22,620 --> 00:03:26,050
Эти части называются лексемами, и в случае с текстом это, как правило, 

62
00:03:26,050 --> 00:03:29,820
слова, маленькие кусочки слов или другие распространенные комбинации символов.

63
00:03:30,740 --> 00:03:33,836
Если речь идет об изображениях или звуке, то токены могут быть 

64
00:03:33,836 --> 00:03:37,080
маленькими фрагментами изображения или маленькими кусочками звука.

65
00:03:37,580 --> 00:03:41,956
Каждый из этих токенов затем ассоциируется с вектором, то есть с некоторым списком чисел, 

66
00:03:41,956 --> 00:03:45,360
который должен каким-то образом закодировать значение этого фрагмента.

67
00:03:45,880 --> 00:03:48,477
Если представить, что эти векторы дают координаты в каком-то очень 

68
00:03:48,477 --> 00:03:51,462
высокоразмерном пространстве, то слова с похожими значениями имеют тенденцию 

69
00:03:51,462 --> 00:03:54,680
приземляться на векторы, которые находятся близко друг к другу в этом пространстве.

70
00:03:55,280 --> 00:03:58,032
Затем эта последовательность векторов проходит через операцию, 

71
00:03:58,032 --> 00:04:01,091
которая известна как блок внимания, и это позволяет векторам общаться 

72
00:04:01,091 --> 00:04:04,500
друг с другом и передавать информацию туда-сюда, чтобы обновить свои значения.

73
00:04:04,880 --> 00:04:08,369
Например, значение слова model во фразе a machine learning 

74
00:04:08,369 --> 00:04:11,800
model отличается от его значения во фразе a fashion model.

75
00:04:12,260 --> 00:04:17,081
Блок внимания отвечает за то, чтобы понять, какие слова в контексте имеют отношение к 

76
00:04:17,081 --> 00:04:21,959
обновлению значений каких других слов, и как именно эти значения должны быть обновлены.

77
00:04:22,500 --> 00:04:25,067
И опять же, всякий раз, когда я использую слово "смысл", 

78
00:04:25,067 --> 00:04:28,040
он каким-то образом полностью закодирован в записях этих векторов.

79
00:04:29,180 --> 00:04:31,984
После этого эти векторы проходят через другой вид операций, 

80
00:04:31,984 --> 00:04:34,320
и в зависимости от источника, который ты читаешь, 

81
00:04:34,320 --> 00:04:38,200
это будет называться многослойным перцептроном или, может быть, слоем feed-forward.

82
00:04:38,580 --> 00:04:40,558
И здесь векторы не разговаривают друг с другом, 

83
00:04:40,558 --> 00:04:42,660
они все проходят одну и ту же операцию параллельно.

84
00:04:43,060 --> 00:04:46,906
И хотя этот блок немного сложнее интерпретировать, позже мы поговорим о том, 

85
00:04:46,906 --> 00:04:51,352
что этот шаг немного похож на то, чтобы задать длинный список вопросов о каждом векторе, 

86
00:04:51,352 --> 00:04:54,000
а затем обновить их на основе ответов на эти вопросы.

87
00:04:54,900 --> 00:04:59,928
Все операции в обоих этих блоках выглядят как гигантская куча матричных умножений, 

88
00:04:59,928 --> 00:05:05,320
и наша основная задача будет заключаться в том, чтобы понять, как читать базовые матрицы.

89
00:05:06,980 --> 00:05:09,707
Я упускаю некоторые подробности о некоторых этапах нормализации, 

90
00:05:09,707 --> 00:05:12,980
которые происходят между ними, но это, в конце концов, высокоуровневое превью.

91
00:05:13,680 --> 00:05:17,385
После этого процесс, по сути, повторяется, ты ходишь туда-сюда между 

92
00:05:17,385 --> 00:05:20,284
блоками внимания и блоками многослойного перцептрона, 

93
00:05:20,284 --> 00:05:24,043
пока в самом конце не появится надежда на то, что весь основной смысл 

94
00:05:24,043 --> 00:05:28,500
отрывка каким-то образом был заложен в самый последний вектор в последовательности.

95
00:05:28,920 --> 00:05:31,836
Затем мы выполняем определенную операцию над этим последним вектором, 

96
00:05:31,836 --> 00:05:35,336
в результате которой получаем распределение вероятности по всем возможным лексемам, 

97
00:05:35,336 --> 00:05:38,420
всем возможным небольшим фрагментам текста, которые могут быть следующими.

98
00:05:38,980 --> 00:05:42,463
И, как я уже сказал, как только у тебя появится инструмент, который предсказывает, 

99
00:05:42,463 --> 00:05:46,113
что будет дальше, учитывая фрагмент текста, ты сможешь скормить ему немного начального 

100
00:05:46,113 --> 00:05:49,764
текста и заставить его многократно играть в эту игру: предсказывать, что будет дальше, 

101
00:05:49,764 --> 00:05:53,080
делать выборку из распределения, добавлять ее, а затем повторять снова и снова.

102
00:05:53,640 --> 00:05:57,451
Некоторые из твоих знакомых могут вспомнить, как задолго до появления 

103
00:05:57,451 --> 00:06:00,283
ChatGPT вот так выглядели ранние демо-версии GPT-3: 

104
00:06:00,283 --> 00:06:04,640
ты должен был автозаполнять рассказы и сочинения на основе начального фрагмента.

105
00:06:05,580 --> 00:06:10,295
Чтобы превратить такой инструмент в чатбота, проще всего начать с небольшого текста, 

106
00:06:10,295 --> 00:06:14,512
задающего обстановку взаимодействия пользователя с полезным ИИ-ассистентом, 

107
00:06:14,512 --> 00:06:18,673
который можно назвать системной подсказкой, а затем использовать начальный 

108
00:06:18,673 --> 00:06:22,668
вопрос или подсказку пользователя в качестве первого фрагмента диалога, 

109
00:06:22,668 --> 00:06:26,940
а затем начать предсказывать, что скажет в ответ такой полезный ИИ-ассистент.

110
00:06:27,720 --> 00:06:31,195
Можно еще много чего сказать о шаге подготовки, который необходим для того, 

111
00:06:31,195 --> 00:06:33,940
чтобы это хорошо работало, но на высоком уровне идея такова.

112
00:06:35,720 --> 00:06:40,142
В этой главе мы с тобой расширим детали того, что происходит в самом начале сети, 

113
00:06:40,142 --> 00:06:44,510
в самом конце сети, а также я хочу уделить много времени обзору некоторых важных 

114
00:06:44,510 --> 00:06:48,878
битов фоновых знаний, вещей, которые стали бы второй натурой для любого инженера 

115
00:06:48,878 --> 00:06:52,600
по машинному обучению к тому времени, когда появились трансформаторы.

116
00:06:53,060 --> 00:06:56,234
Если тебя устраивают эти фоновые знания и ты немного нетерпелив, 

117
00:06:56,234 --> 00:07:00,484
то можешь смело переходить к следующей главе, в которой речь пойдет о блоках внимания, 

118
00:07:00,484 --> 00:07:02,780
которые принято считать сердцем трансформатора.

119
00:07:03,360 --> 00:07:07,496
После этого я хочу подробнее рассказать об этих блоках многослойного перцептрона, о том, 

120
00:07:07,496 --> 00:07:11,680
как происходит обучение, и о ряде других деталей, которые до этого момента были пропущены.

121
00:07:12,180 --> 00:07:16,150
Для более широкого контекста эти видео являются дополнениями к мини-сериалу о глубоком 

122
00:07:16,150 --> 00:07:19,254
обучении, и ничего страшного, если ты не смотрел предыдущие, думаю, 

123
00:07:19,254 --> 00:07:22,814
ты сможешь сделать это и не по порядку, но прежде чем погрузиться конкретно в 

124
00:07:22,814 --> 00:07:26,557
трансформеров, думаю, стоит убедиться, что мы на одной волне в отношении основных 

125
00:07:26,557 --> 00:07:28,520
предпосылок и структуры глубокого обучения.

126
00:07:29,020 --> 00:07:33,011
Рискуя заявить очевидное, скажу, что это один из подходов к машинному обучению, 

127
00:07:33,011 --> 00:07:35,955
который описывает любую модель, где ты используешь данные, 

128
00:07:35,955 --> 00:07:38,300
чтобы как-то определить, как ведет себя модель.

129
00:07:39,140 --> 00:07:41,541
Я имею в виду, что, допустим, тебе нужна функция, 

130
00:07:41,541 --> 00:07:44,567
которая принимает изображение и выдает метку, описывающую его, 

131
00:07:44,567 --> 00:07:47,785
или наш пример с предсказанием следующего слова по отрывку текста, 

132
00:07:47,785 --> 00:07:51,195
или любая другая задача, которая, кажется, требует некоторого элемента 

133
00:07:51,195 --> 00:07:52,780
интуиции и распознавания образов.

134
00:07:53,200 --> 00:07:55,723
В наши дни мы воспринимаем это почти как должное, 

135
00:07:55,723 --> 00:07:58,903
но идея машинного обучения заключается в том, что вместо того, 

136
00:07:58,903 --> 00:08:02,538
чтобы пытаться явно определить процедуру выполнения этой задачи в коде, 

137
00:08:02,538 --> 00:08:07,030
что люди и делали в самые ранние дни ИИ, вместо этого ты создаешь очень гибкую структуру 

138
00:08:07,030 --> 00:08:10,059
с настраиваемыми параметрами, как куча ручек и циферблатов, 

139
00:08:10,059 --> 00:08:13,188
а затем каким-то образом используешь множество примеров того, 

140
00:08:13,188 --> 00:08:17,680
как должен выглядеть выход для данного входа, чтобы подстроить и настроить значения этих 

141
00:08:17,680 --> 00:08:19,700
параметров для имитации этого поведения.

142
00:08:19,700 --> 00:08:24,271
Например, самой простой формой машинного обучения может быть линейная регрессия, 

143
00:08:24,271 --> 00:08:27,996
где твоими входными и выходными данными являются отдельные числа, 

144
00:08:27,996 --> 00:08:31,100
что-то вроде площади дома и его цены, и что ты хочешь, 

145
00:08:31,100 --> 00:08:34,599
так это найти линию наилучшего соответствия через эти данные, 

146
00:08:34,599 --> 00:08:36,799
чтобы предсказать будущие цены на дома.

147
00:08:37,440 --> 00:08:41,112
Эта линия описывается двумя непрерывными параметрами, скажем, 

148
00:08:41,112 --> 00:08:45,850
наклоном и y-интерцептом, и цель линейной регрессии - определить эти параметры, 

149
00:08:45,850 --> 00:08:48,160
чтобы они точно соответствовали данным.

150
00:08:48,880 --> 00:08:52,100
Нет нужды говорить, что модели глубокого обучения становятся намного сложнее.

151
00:08:52,620 --> 00:08:57,660
GPT-3, например, имеет не два, а 175 миллиардов параметров.

152
00:08:58,120 --> 00:09:02,052
Но дело вот в чем: нельзя сказать, что ты можешь создать какую-то гигантскую 

153
00:09:02,052 --> 00:09:04,657
модель с огромным количеством параметров без того, 

154
00:09:04,657 --> 00:09:07,312
чтобы она либо сильно перегружала обучающие данные, 

155
00:09:07,312 --> 00:09:09,560
либо была совершенно неподдающейся обучению.

156
00:09:10,260 --> 00:09:13,108
Глубокое обучение описывает класс моделей, которые за последние 

157
00:09:13,108 --> 00:09:16,180
пару десятилетий доказали, что они удивительно хорошо масштабируются.

158
00:09:16,480 --> 00:09:21,925
Их объединяет один и тот же алгоритм обучения, называемый обратным распространением, 

159
00:09:21,925 --> 00:09:26,795
и я хочу, чтобы ты понял, что для того, чтобы этот алгоритм обучения хорошо 

160
00:09:26,795 --> 00:09:31,280
работал в масштабе, эти модели должны следовать определенному формату.

161
00:09:31,800 --> 00:09:35,966
Если ты знаешь этот формат, то он поможет объяснить многие варианты обработки 

162
00:09:35,966 --> 00:09:40,400
языка трансформатором, которые в противном случае рискуют показаться произвольными.

163
00:09:41,440 --> 00:09:44,136
Во-первых, какую бы модель ты ни создавал, входные данные 

164
00:09:44,136 --> 00:09:46,740
должны быть оформлены в виде массива вещественных чисел.

165
00:09:46,740 --> 00:09:49,995
Это может быть список чисел, может быть двумерный массив, 

166
00:09:49,995 --> 00:09:53,811
а очень часто ты имеешь дело с массивами более высокой размерности, 

167
00:09:53,811 --> 00:09:56,000
где используется общий термин - тензор.

168
00:09:56,560 --> 00:10:00,471
Ты часто думаешь о том, что входные данные постепенно преобразуются во множество 

169
00:10:00,471 --> 00:10:04,479
отдельных слоев, где, опять же, каждый слой всегда структурирован как некий массив 

170
00:10:04,479 --> 00:10:08,680
вещественных чисел, пока ты не дойдешь до последнего слоя, который ты считаешь выходом.

171
00:10:09,280 --> 00:10:13,000
Например, последний слой в нашей модели обработки текста - это список чисел, 

172
00:10:13,000 --> 00:10:17,060
представляющий собой распределение вероятностей для всех возможных следующих лексем.

173
00:10:17,820 --> 00:10:22,057
В глубоком обучении эти параметры модели почти всегда называются весами, и это потому, 

174
00:10:22,057 --> 00:10:24,834
что ключевая особенность этих моделей заключается в том, 

175
00:10:24,834 --> 00:10:28,779
что единственный способ взаимодействия этих параметров с обрабатываемыми данными 

176
00:10:28,779 --> 00:10:29,900
- это взвешенные суммы.

177
00:10:30,340 --> 00:10:34,360
Ты также рассыпаешь повсюду нелинейные функции, но они не будут зависеть от параметров.

178
00:10:35,200 --> 00:10:39,394
Обычно вместо того, чтобы видеть взвешенные суммы в явном виде, 

179
00:10:39,394 --> 00:10:44,243
ты видишь их упакованными вместе в виде различных компонентов в векторном 

180
00:10:44,243 --> 00:10:45,620
произведении матрицы.

181
00:10:46,740 --> 00:10:50,998
Это равносильно тому, что если ты вспомнишь, как работает матрично-векторное умножение, 

182
00:10:50,998 --> 00:10:54,240
то каждый компонент на выходе будет выглядеть как взвешенная сумма.

183
00:10:54,780 --> 00:10:59,110
Просто зачастую для нас с тобой концептуально чище думать о матрицах, 

184
00:10:59,110 --> 00:11:03,440
которые заполнены настраиваемыми параметрами, преобразующими векторы, 

185
00:11:03,440 --> 00:11:05,420
взятые из обрабатываемых данных.

186
00:11:06,340 --> 00:11:11,015
Например, те 175 миллиардов весов в GPT-3 организованы 

187
00:11:11,015 --> 00:11:14,160
в чуть менее 28 000 отдельных матриц.

188
00:11:14,660 --> 00:11:18,055
Эти матрицы, в свою очередь, делятся на восемь различных категорий, 

189
00:11:18,055 --> 00:11:21,301
и мы с тобой собираемся пройтись по каждой из них, чтобы понять, 

190
00:11:21,301 --> 00:11:22,700
что делает тот или иной тип.

191
00:11:23,160 --> 00:11:27,879
По ходу дела я думаю, что будет забавно ссылаться на конкретные цифры из GPT-3, 

192
00:11:27,879 --> 00:11:31,360
чтобы подсчитать, откуда именно взялись эти 175 миллиардов.

193
00:11:31,880 --> 00:11:35,869
Даже если сейчас есть модели больше и лучше, эта имеет определенный шарм как 

194
00:11:35,869 --> 00:11:40,066
крупноязычная модель, которая действительно привлекла внимание мира за пределами 

195
00:11:40,066 --> 00:11:40,740
ML-сообществ.

196
00:11:41,440 --> 00:11:43,845
Кроме того, с практической точки зрения, компании, как правило, 

197
00:11:43,845 --> 00:11:46,740
гораздо жестче обходят стороной конкретные цифры для более современных сетей.

198
00:11:47,360 --> 00:11:50,803
Я просто хочу рассказать, что если заглянуть под капот и посмотреть, 

199
00:11:50,803 --> 00:11:53,547
что происходит внутри такого инструмента, как ChatGPT, 

200
00:11:53,547 --> 00:11:57,440
то почти все фактические вычисления выглядят как матрично-векторное умножение.

201
00:11:57,900 --> 00:12:00,973
Есть небольшой риск заблудиться в море миллиардов цифр, 

202
00:12:00,973 --> 00:12:04,705
но ты должен очень четко разграничить в своем сознании веса модели, 

203
00:12:04,705 --> 00:12:09,315
которые я всегда буду окрашивать в синий или красный цвет, и обрабатываемые данные, 

204
00:12:09,315 --> 00:12:11,840
которые я всегда буду окрашивать в серый цвет.

205
00:12:12,180 --> 00:12:16,209
Веса - это собственно мозг, это то, что усваивается во время тренировок, 

206
00:12:16,209 --> 00:12:17,920
и они определяют его поведение.

207
00:12:18,280 --> 00:12:22,274
Обрабатываемые данные просто кодируют все конкретные входные данные, 

208
00:12:22,274 --> 00:12:26,500
которые подаются в модель для данного запуска, например, фрагмент текста.

209
00:12:27,480 --> 00:12:31,438
С учетом всего этого давайте приступим к первому шагу этого примера по обработке текста, 

210
00:12:31,438 --> 00:12:34,507
который заключается в том, чтобы разбить входной сигнал на маленькие 

211
00:12:34,507 --> 00:12:36,420
кусочки и превратить эти кусочки в векторы.

212
00:12:37,020 --> 00:12:39,576
Я уже упоминал, что эти куски называются лексемами, 

213
00:12:39,576 --> 00:12:41,935
которые могут быть кусками слов или пунктуации, 

214
00:12:41,935 --> 00:12:46,261
но время от времени в этой главе и особенно в следующей я хотел бы просто притворяться, 

215
00:12:46,261 --> 00:12:48,080
что все разбито на слова более чисто.

216
00:12:48,600 --> 00:12:51,516
Поскольку мы, люди, мыслим словами, это лишь значительно упростит 

217
00:12:51,516 --> 00:12:54,080
обращение к небольшим примерам и разъяснение каждого шага.

218
00:12:55,260 --> 00:12:59,894
Модель имеет предопределенный словарный запас, некоторый список всех возможных слов, 

219
00:12:59,894 --> 00:13:03,492
скажем, 50 000 из них, и первая матрица, с которой мы столкнемся, 

220
00:13:03,492 --> 00:13:07,800
известная как матрица встраивания, имеет один столбец для каждого из этих слов.

221
00:13:08,940 --> 00:13:13,760
Именно эти столбцы определяют, в какой вектор превратится каждое слово на первом этапе.

222
00:13:15,100 --> 00:13:18,125
Мы обозначим ее We, и, как и все матрицы, которые мы видим, 

223
00:13:18,125 --> 00:13:22,360
ее значения начинаются случайно, но в дальнейшем они будут выучены на основе данных.

224
00:13:23,620 --> 00:13:26,710
Превращение слов в векторы было обычной практикой в машинном обучении 

225
00:13:26,710 --> 00:13:29,447
задолго до появления трансформаторов, но это немного странно, 

226
00:13:29,447 --> 00:13:33,332
если ты никогда не видел этого раньше, и это закладывает основу для всего последующего, 

227
00:13:33,332 --> 00:13:35,760
так что давай уделим немного времени знакомству с этим.

228
00:13:36,040 --> 00:13:39,737
Мы часто называем это вкрапление словом, которое предлагает тебе думать об этих 

229
00:13:39,737 --> 00:13:43,620
векторах очень геометрически, как о точках в некотором высокоразмерном пространстве.

230
00:13:44,180 --> 00:13:47,915
Визуализировать список из трех чисел как координаты точек в трехмерном пространстве не 

231
00:13:47,915 --> 00:13:51,780
составит труда, но вкрапления слов имеют тенденцию быть гораздо более высокой размерности.

232
00:13:52,280 --> 00:13:55,719
В GPT-3 у них 12 288 измерений, и, как ты увидишь, 

233
00:13:55,719 --> 00:14:00,440
важно работать в пространстве, которое имеет много разных направлений.

234
00:14:01,180 --> 00:14:05,608
Точно так же, как ты можешь взять двумерный срез через трехмерное пространство и 

235
00:14:05,608 --> 00:14:09,326
спроецировать все точки на этот срез, для анимации вкраплений слов, 

236
00:14:09,326 --> 00:14:13,208
которые дает мне простая модель, я собираюсь сделать аналогичную вещь, 

237
00:14:13,208 --> 00:14:16,980
выбрав трехмерный срез через это очень высокоразмерное пространство, 

238
00:14:16,980 --> 00:14:20,480
спроецировать векторы слов вниз на него и отобразить результаты.

239
00:14:21,280 --> 00:14:24,522
Главная идея здесь в том, что по мере того, как модель настраивает и 

240
00:14:24,522 --> 00:14:27,813
подстраивает свои веса, определяющие, как именно слова встраиваются в 

241
00:14:27,813 --> 00:14:31,479
векторы в процессе обучения, она стремится остановиться на наборе вкраплений, 

242
00:14:31,479 --> 00:14:34,440
где направления в пространстве имеют некий семантический смысл.

243
00:14:34,980 --> 00:14:37,885
Для простой модели "слово-вектор", которую я здесь использую, 

244
00:14:37,885 --> 00:14:41,728
если я проведу поиск по всем словам, чьи вкрапления наиболее близки к вкраплениям 

245
00:14:41,728 --> 00:14:45,900
слова "башня", то ты заметишь, что все они дают очень похожие вибрации, похожие на башню.

246
00:14:46,340 --> 00:14:48,638
А если ты хочешь подтянуть Python и поиграть с ним дома, 

247
00:14:48,638 --> 00:14:51,380
то вот конкретная модель, которую я использую для создания анимации.

248
00:14:51,620 --> 00:14:54,840
Это не трансформер, но этого достаточно, чтобы проиллюстрировать идею о том, 

249
00:14:54,840 --> 00:14:57,600
что направления в пространстве могут нести семантическое значение.

250
00:14:58,300 --> 00:15:03,287
Классический пример - если взять разницу между векторами для женщины и мужчины, 

251
00:15:03,287 --> 00:15:06,778
то есть то, что можно представить как маленький вектор, 

252
00:15:06,778 --> 00:15:11,579
соединяющий кончик одного с кончиком другого, то это очень похоже на разницу 

253
00:15:11,579 --> 00:15:13,200
между королем и королевой.

254
00:15:15,080 --> 00:15:18,884
Так что, допустим, ты не знаешь слова, обозначающего монарха-женщину, 

255
00:15:18,884 --> 00:15:22,090
ты можешь найти его, взяв king, добавив к нему направление 

256
00:15:22,090 --> 00:15:25,460
woman-man и поискав вкрапления, наиболее близкие к этой точке.

257
00:15:27,000 --> 00:15:28,200
По крайней мере, в некотором роде.

258
00:15:28,480 --> 00:15:31,935
Несмотря на то, что это классический пример для модели, с которой я играю, 

259
00:15:31,935 --> 00:15:35,896
истинное вложение queen на самом деле немного дальше, чем можно было бы предположить, 

260
00:15:35,896 --> 00:15:39,397
предположительно потому, что то, как queen используется в обучающих данных, 

261
00:15:39,397 --> 00:15:40,780
не просто женская версия king.

262
00:15:41,620 --> 00:15:45,260
Когда я поиграл, семейные отношения показались мне гораздо лучше иллюстрирующими эту идею.

263
00:15:46,340 --> 00:15:50,472
Дело в том, что, похоже, во время обучения модель сочла выгодным выбрать вкрапления 

264
00:15:50,472 --> 00:15:54,900
таким образом, чтобы одно направление в этом пространстве кодировало гендерную информацию.

265
00:15:56,800 --> 00:16:02,444
Другой пример: если взять вкрапления Италии, вычесть вкрапления Германии и прибавить 

266
00:16:02,444 --> 00:16:08,090
это к вкраплениям Гитлера, то получится что-то очень близкое к вкраплениям Муссолини.

267
00:16:08,570 --> 00:16:12,956
Как будто модель научилась ассоциировать одни направления с итальянскостью, 

268
00:16:12,956 --> 00:16:15,670
а другие - с лидерами оси Второй мировой войны.

269
00:16:16,470 --> 00:16:19,127
Возможно, мой любимый пример в этом ключе - это то, 

270
00:16:19,127 --> 00:16:22,295
как в некоторых моделях, если взять разницу между Германией и 

271
00:16:22,295 --> 00:16:26,230
Японией и добавить ее к суши, то в итоге получится очень близко к сарделькам.

272
00:16:27,350 --> 00:16:30,000
Также, играя в эту игру по поиску ближайших соседей, 

273
00:16:30,000 --> 00:16:33,850
я с удовольствием наблюдал, насколько близко Кэт была и к зверю, и к монстру.

274
00:16:34,690 --> 00:16:37,199
Одна из математических интуиций, которую полезно иметь в виду, 

275
00:16:37,199 --> 00:16:39,150
особенно для следующей главы, заключается в том, 

276
00:16:39,150 --> 00:16:42,615
что точечное произведение двух векторов можно рассматривать как способ измерения того, 

277
00:16:42,615 --> 00:16:43,850
насколько хорошо они выровнены.

278
00:16:44,870 --> 00:16:48,023
С вычислительной точки зрения точечные продукты подразумевают умножение всех 

279
00:16:48,023 --> 00:16:51,176
соответствующих компонентов, а затем сложение результатов, что очень хорошо, 

280
00:16:51,176 --> 00:16:54,330
так как большая часть наших вычислений должна выглядеть как взвешенные суммы.

281
00:16:55,190 --> 00:16:57,795
Геометрически точечное произведение положительно, 

282
00:16:57,795 --> 00:17:01,025
если векторы направлены в одинаковые стороны, оно равно нулю, 

283
00:17:01,025 --> 00:17:05,609
если они перпендикулярны, и отрицательно, если они направлены в противоположные стороны.

284
00:17:06,550 --> 00:17:09,694
Допустим, ты играл с этой моделью и предположил, 

285
00:17:09,694 --> 00:17:14,699
что вкрапление "кошки минус кошка" может представлять собой некое направление 

286
00:17:14,699 --> 00:17:17,010
множественности в этом пространстве.

287
00:17:17,430 --> 00:17:20,827
Чтобы проверить это, я собираюсь взять этот вектор и вычислить его точечное произведение 

288
00:17:20,827 --> 00:17:23,156
на вкрапления некоторых существительных единственного числа, 

289
00:17:23,156 --> 00:17:26,248
а затем сравнить его с точечным произведением на соответствующие существительные 

290
00:17:26,248 --> 00:17:27,050
множественного числа.

291
00:17:27,270 --> 00:17:30,507
Если ты поиграешь с этим, то заметишь, что множественное число действительно, 

292
00:17:30,507 --> 00:17:33,247
кажется, постоянно дает более высокие значения, чем единственное, 

293
00:17:33,247 --> 00:17:36,070
что указывает на то, что они больше соответствуют этому направлению.

294
00:17:37,070 --> 00:17:41,107
Также забавно, что если взять это точечное произведение с вкраплениями слов 1, 

295
00:17:41,107 --> 00:17:43,816
2, 3 и так далее, то они дают возрастающие значения, 

296
00:17:43,816 --> 00:17:46,423
так что мы как будто можем количественно измерить, 

297
00:17:46,423 --> 00:17:49,030
насколько многозначным модель находит данное слово.

298
00:17:50,250 --> 00:17:53,570
Опять же, специфику того, как слова встраиваются, можно узнать, используя данные.

299
00:17:54,050 --> 00:17:56,436
Эта матрица вкраплений, столбцы которой говорят нам о том, 

300
00:17:56,436 --> 00:17:59,550
что происходит с каждым словом, является первой стопкой весов в нашей модели.

301
00:18:00,030 --> 00:18:04,583
Если использовать цифры GPT-3, то размер словарного запаса в конкретном случае 

302
00:18:04,583 --> 00:18:09,770
составляет 50 257, и, опять же, технически он состоит не из слов как таковых, а из лексем.

303
00:18:10,630 --> 00:18:15,201
Размерность встраивания равна 12 288, и умножение этих значений говорит нам о том, 

304
00:18:15,201 --> 00:18:17,790
что он состоит примерно из 617 миллионов весов.

305
00:18:18,250 --> 00:18:21,121
Давай добавим это в бегущий счет, помня, что к 

306
00:18:21,121 --> 00:18:23,810
концу мы должны досчитать до 175 миллиардов.

307
00:18:25,430 --> 00:18:28,711
В случае с трансформаторами ты действительно хочешь думать о векторах в 

308
00:18:28,711 --> 00:18:32,130
этом пространстве встраивания не просто как о представлении отдельных слов.

309
00:18:32,550 --> 00:18:36,124
Во-первых, они также кодируют информацию о позиции этого слова, 

310
00:18:36,124 --> 00:18:40,033
о чем мы поговорим позже, но что более важно, ты должен думать о том, 

311
00:18:40,033 --> 00:18:42,770
что они обладают способностью впитывать контекст.

312
00:18:43,350 --> 00:18:46,869
Вектор, который начал свою жизнь как вставка слова king, например, 

313
00:18:46,869 --> 00:18:51,019
может постепенно подтягиваться и подтягиваться различными блоками в этой сети, 

314
00:18:51,019 --> 00:18:55,169
так что к концу он указывает на гораздо более конкретное и тонкое направление, 

315
00:18:55,169 --> 00:18:59,109
которое каким-то образом кодирует, что это был король, живший в Шотландии, 

316
00:18:59,109 --> 00:19:03,521
который добился своего поста после убийства предыдущего короля и которого описывают 

317
00:19:03,521 --> 00:19:04,730
на шекспировском языке.

318
00:19:05,210 --> 00:19:07,790
Подумай о своем собственном понимании того или иного слова.

319
00:19:08,250 --> 00:19:12,699
Значение этого слова явно зависит от окружения, и иногда оно включает в себя контекст, 

320
00:19:12,699 --> 00:19:15,973
находящийся на большом расстоянии, поэтому при создании модели, 

321
00:19:15,973 --> 00:19:19,656
способной предсказать, какое слово будет следующим, цель состоит в том, 

322
00:19:19,656 --> 00:19:23,390
чтобы каким-то образом дать ей возможность эффективно учитывать контекст.

323
00:19:24,050 --> 00:19:27,370
Чтобы было понятно, на первом этапе, когда ты создаешь массив векторов 

324
00:19:27,370 --> 00:19:31,392
на основе входного текста, каждый из них просто выдергивается из матрицы встраивания, 

325
00:19:31,392 --> 00:19:34,478
поэтому изначально каждый из них может кодировать только значение 

326
00:19:34,478 --> 00:19:36,770
одного слова без какого-либо вклада из окружения.

327
00:19:37,710 --> 00:19:41,427
Но ты должен думать, что главная цель этой сети, по которой он течет, 

328
00:19:41,427 --> 00:19:44,189
- позволить каждому из этих векторов впитать смысл, 

329
00:19:44,189 --> 00:19:48,970
который намного богаче и конкретнее, чем то, что могут представлять собой отдельные слова.

330
00:19:49,510 --> 00:19:52,780
Сеть может обрабатывать только фиксированное количество векторов за раз, 

331
00:19:52,780 --> 00:19:54,170
известное как размер контекста.

332
00:19:54,510 --> 00:19:59,879
Для GPT-3 он был обучен с размером контекста 2048, поэтому данные, проходящие через сеть, 

333
00:19:59,879 --> 00:20:05,010
всегда выглядят как массив из 2048 столбцов, каждый из которых имеет 12 000 измерений.

334
00:20:05,590 --> 00:20:09,533
Размер контекста ограничивает то, сколько текста может включить трансформатор, 

335
00:20:09,533 --> 00:20:11,830
когда он делает предсказание следующего слова.

336
00:20:12,370 --> 00:20:15,401
Именно поэтому при длительных беседах с некоторыми чатботами, 

337
00:20:15,401 --> 00:20:18,481
например с ранними версиями ChatGPT, часто возникало ощущение, 

338
00:20:18,481 --> 00:20:22,050
что бот как бы теряет нить разговора, когда ты продолжаешь слишком долго.

339
00:20:23,030 --> 00:20:25,986
В свое время мы углубимся в детали внимания, но, пропуская вперед, 

340
00:20:25,986 --> 00:20:28,810
я хочу на минуту поговорить о том, что происходит в самом конце.

341
00:20:29,450 --> 00:20:33,321
Помни, что желаемый результат - это распределение вероятностей по всем токенам, 

342
00:20:33,321 --> 00:20:34,870
которые могут прийти следующими.

343
00:20:35,170 --> 00:20:39,456
Например, если самое последнее слово - профессор, а контекст включает такие слова, 

344
00:20:39,456 --> 00:20:43,692
как Гарри Поттер, а непосредственно перед ним мы видим наименее любимого учителя, 

345
00:20:43,692 --> 00:20:47,617
а также если ты дашь мне некоторую свободу действий, позволив притвориться, 

346
00:20:47,617 --> 00:20:51,336
что лексемы просто выглядят как полные слова, то хорошо обученная сеть, 

347
00:20:51,336 --> 00:20:55,830
накопившая знания о Гарри Поттере, предположительно присвоит высокий номер слову Снейп.

348
00:20:56,510 --> 00:20:57,970
Это включает в себя два разных этапа.

349
00:20:58,310 --> 00:21:00,901
Первый заключается в использовании другой матрицы, 

350
00:21:00,901 --> 00:21:04,103
которая сопоставляет самый последний вектор в данном контексте 

351
00:21:04,103 --> 00:21:07,610
со списком из 50 000 значений, по одному на каждую лексему в словаре.

352
00:21:08,170 --> 00:21:11,783
Затем есть функция, которая нормализует это в распределение вероятностей, 

353
00:21:11,783 --> 00:21:15,690
она называется Softmax, и мы поговорим о ней подробнее буквально через секунду, 

354
00:21:15,690 --> 00:21:19,597
но до этого может показаться немного странным использовать только это последнее 

355
00:21:19,597 --> 00:21:22,039
вложение для предсказания, когда, в конце концов, 

356
00:21:22,039 --> 00:21:24,676
на последнем шаге в слое есть тысячи других векторов, 

357
00:21:24,676 --> 00:21:28,290
просто сидящих там со своими собственными значениями, богатыми контекстом.

358
00:21:28,930 --> 00:21:33,014
Это связано с тем, что в процессе обучения оказывается гораздо эффективнее, 

359
00:21:33,014 --> 00:21:36,346
если ты используешь каждый из этих векторов в финальном слое, 

360
00:21:36,346 --> 00:21:40,270
чтобы одновременно сделать предсказание того, что будет сразу после него.

361
00:21:40,970 --> 00:21:43,167
О тренировках еще много чего можно будет сказать позже, 

362
00:21:43,167 --> 00:21:45,090
но сейчас я просто хочу обратить на это внимание.

363
00:21:45,730 --> 00:21:49,690
Эта матрица называется матрицей Unembedding, и мы даем ей обозначение WU.

364
00:21:50,210 --> 00:21:52,549
Опять же, как и все весовые матрицы, которые мы видим, 

365
00:21:52,549 --> 00:21:55,910
ее записи начинаются случайным образом, но они усваиваются в процессе обучения.

366
00:21:56,470 --> 00:21:58,844
Продолжая подсчитывать общее количество параметров, 

367
00:21:58,844 --> 00:22:02,087
эта матрица Unembedding имеет одну строку для каждого слова в словаре, 

368
00:22:02,087 --> 00:22:05,650
и каждая строка имеет столько же элементов, сколько и размерность встраивания.

369
00:22:06,410 --> 00:22:10,429
Она очень похожа на матрицу встраивания, только порядок поменялся местами, 

370
00:22:10,429 --> 00:22:13,590
поэтому она добавляет в сеть еще 617 миллионов параметров, 

371
00:22:13,590 --> 00:22:17,502
то есть на данный момент мы насчитали чуть больше миллиарда - небольшая, 

372
00:22:17,502 --> 00:22:21,790
но не совсем незначительная часть от 175 миллиардов, которые мы получим в итоге.

373
00:22:22,550 --> 00:22:25,397
В качестве последнего мини-урока для этой главы я хочу подробнее 

374
00:22:25,397 --> 00:22:28,770
поговорить об этой функции softmax, так как она снова предстанет перед нами, 

375
00:22:28,770 --> 00:22:30,610
как только мы погрузимся в блоки внимания.

376
00:22:31,430 --> 00:22:34,768
Идея заключается в том, что если ты хочешь, чтобы последовательность 

377
00:22:34,768 --> 00:22:37,574
чисел действовала как распределение вероятностей, скажем, 

378
00:22:37,574 --> 00:22:39,993
распределение по всем возможным следующим словам, 

379
00:22:39,993 --> 00:22:43,090
то каждое значение должно быть между 0 и 1, и тебе также нужно, 

380
00:22:43,090 --> 00:22:44,590
чтобы все они складывались в 1.

381
00:22:45,250 --> 00:22:48,660
Однако если ты играешь в обучающую игру, где все, что ты делаешь, 

382
00:22:48,660 --> 00:22:51,451
выглядит как матрично-векторное умножение, то выходы, 

383
00:22:51,451 --> 00:22:54,810
которые ты получаешь по умолчанию, совсем не соответствуют этому.

384
00:22:55,330 --> 00:22:57,982
Значения часто бывают отрицательными или намного больше 1, 

385
00:22:57,982 --> 00:22:59,870
и они почти наверняка не складываются в 1.

386
00:23:00,510 --> 00:23:04,066
Softmax - это стандартный способ превратить произвольный список 

387
00:23:04,066 --> 00:23:06,733
чисел в правильное распределение таким образом, 

388
00:23:06,733 --> 00:23:11,290
чтобы наибольшие значения оказались ближе всего к 1, а меньшие - очень близко к 0.

389
00:23:11,830 --> 00:23:13,070
Это все, что тебе действительно нужно знать.

390
00:23:13,090 --> 00:23:16,193
Но если тебе интересно, то принцип работы заключается в том, 

391
00:23:16,193 --> 00:23:18,889
чтобы сначала возвести e в степень каждого из чисел, 

392
00:23:18,889 --> 00:23:21,839
то есть теперь у тебя есть список положительных значений, 

393
00:23:21,839 --> 00:23:26,010
а затем ты можешь взять сумму всех этих положительных значений и разделить каждый 

394
00:23:26,010 --> 00:23:29,470
член на эту сумму, что нормализует список, который складывается в 1.

395
00:23:30,170 --> 00:23:33,934
Ты заметишь, что если одно из чисел на входе значимо больше остальных, 

396
00:23:33,934 --> 00:23:37,221
то на выходе соответствующий член доминирует в распределении, 

397
00:23:37,221 --> 00:23:41,356
так что если бы ты делал выборку из него, то почти наверняка просто выбрал бы 

398
00:23:41,356 --> 00:23:42,470
максимизирующий вход.

399
00:23:42,990 --> 00:23:46,038
Но это мягче, чем просто выбрать максимум, в том смысле, 

400
00:23:46,038 --> 00:23:50,798
что когда другие значения так же велики, они тоже получают значимый вес в распределении, 

401
00:23:50,798 --> 00:23:54,650
и все непрерывно меняется, когда ты постоянно варьируешь входные данные.

402
00:23:55,130 --> 00:23:59,902
В некоторых ситуациях, например, когда ChatGPT использует это распределение для создания 

403
00:23:59,902 --> 00:24:02,958
следующего слова, есть возможность немного повеселиться, 

404
00:24:02,958 --> 00:24:05,853
добавив в эту функцию немного дополнительной остроты: 

405
00:24:05,853 --> 00:24:08,910
в знаменатель этих экспоненты подбрасывается константа t.

406
00:24:09,550 --> 00:24:14,240
Мы называем его температурой, так как он смутно напоминает роль температуры в некоторых 

407
00:24:14,240 --> 00:24:18,185
уравнениях термодинамики, и эффект заключается в том, что когда t больше, 

408
00:24:18,185 --> 00:24:22,662
ты придаешь больший вес меньшим значениям, то есть распределение становится немного 

409
00:24:22,662 --> 00:24:26,980
более равномерным, а если t меньше, то большие значения будут доминировать более 

410
00:24:26,980 --> 00:24:30,338
агрессивно, а в крайнем случае, если установить t равным нулю, 

411
00:24:30,338 --> 00:24:32,790
то весь вес перейдет к максимальному значению.

412
00:24:33,470 --> 00:24:38,994
Например, я попрошу GPT-3 сгенерировать историю с начальным текстом "Жил-был А", 

413
00:24:38,994 --> 00:24:42,950
но в каждом случае я буду использовать разные температуры.

414
00:24:43,630 --> 00:24:48,027
Нулевая температура означает, что он всегда выбирает самое предсказуемое слово, 

415
00:24:48,027 --> 00:24:52,370
и то, что ты получишь, в итоге окажется банальной производной от "Златовласки".

416
00:24:53,010 --> 00:24:56,685
Более высокая температура дает ему шанс выбирать менее вероятные слова, 

417
00:24:56,685 --> 00:24:57,910
но это связано с риском.

418
00:24:58,230 --> 00:25:01,461
В данном случае история начинается более оригинально, 

419
00:25:01,461 --> 00:25:06,010
о молодом веб-художнике из Южной Кореи, но быстро вырождается в бессмыслицу.

420
00:25:06,950 --> 00:25:10,830
Технически говоря, API на самом деле не позволяет тебе выбрать температуру больше 2.

421
00:25:11,170 --> 00:25:15,069
Для этого нет никаких математических причин, это просто произвольное ограничение, 

422
00:25:15,069 --> 00:25:19,350
наложенное для того, чтобы их инструмент не был замечен в генерации слишком нелепых вещей.

423
00:25:19,870 --> 00:25:23,145
Если тебе интересно, то на самом деле эта анимация работает так: 

424
00:25:23,145 --> 00:25:27,528
я беру 20 наиболее вероятных следующих токенов, которые генерирует GPT-3, что, похоже, 

425
00:25:27,528 --> 00:25:31,458
является максимумом, который они мне выдают, а затем подстраиваю вероятности, 

426
00:25:31,458 --> 00:25:32,970
основываясь на экспоненте 1 5.

427
00:25:33,130 --> 00:25:37,621
В качестве еще одного жаргона: точно так же, как ты можешь называть компоненты 

428
00:25:37,621 --> 00:25:41,828
выхода этой функции вероятностями, люди часто называют входы логарифмами, 

429
00:25:41,828 --> 00:25:46,150
или кто-то говорит логарифмы, кто-то - логарифмы, я буду говорить логарифмы.

430
00:25:46,530 --> 00:25:50,894
Так, например, когда ты вводишь текст, все эти вкрапления слов проходят через сеть, 

431
00:25:50,894 --> 00:25:54,479
и ты делаешь финальное перемножение с матрицей без вкраплений, люди, 

432
00:25:54,479 --> 00:25:57,960
занимающиеся машинным обучением, называют компоненты в этом сыром, 

433
00:25:57,960 --> 00:26:01,390
ненормированном выходе логитами для предсказания следующего слова.

434
00:26:03,330 --> 00:26:08,306
Во многом целью этой главы было заложить основы для понимания механизма внимания, 

435
00:26:08,306 --> 00:26:10,370
в стиле Karate Kid wax-on-wax-off.

436
00:26:10,850 --> 00:26:15,389
Видишь ли, если у тебя есть сильная интуиция в отношении вкраплений слов, софтмакса, 

437
00:26:15,389 --> 00:26:19,661
того, как точечные произведения измеряют сходство, а также базовая предпосылка, 

438
00:26:19,661 --> 00:26:23,986
что большинство вычислений должны выглядеть как матричное умножение с матрицами, 

439
00:26:23,986 --> 00:26:27,564
полными настраиваемых параметров, то понимание механизма внимания, 

440
00:26:27,564 --> 00:26:32,210
этого краеугольного камня во всем современном буме ИИ, должно быть относительно легким.

441
00:26:32,650 --> 00:26:34,510
Для этого присоединяйся ко мне в следующей главе.

442
00:26:36,390 --> 00:26:38,800
Пока я это публикую, черновик следующей главы 

443
00:26:38,800 --> 00:26:41,210
доступен для ознакомления сторонникам Patreon.

444
00:26:41,770 --> 00:26:44,250
Финальная версия должна появиться в паблике через неделю или две, 

445
00:26:44,250 --> 00:26:47,370
обычно это зависит от того, сколько я в итоге изменю, основываясь на этой рецензии.

446
00:26:47,810 --> 00:26:52,410
А пока, если ты хочешь погрузиться во внимание и немного помочь каналу, он ждет тебя.

