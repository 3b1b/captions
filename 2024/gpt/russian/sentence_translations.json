[
 {
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "translatedText": "Инициалы GPT означают Generative Pretrained Transformer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0,
  "end": 4.56
 },
 {
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "translatedText": "Итак, первое слово достаточно простое: это боты, которые генерируют новый текст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "translatedText": "Pretrained означает, что модель прошла через процесс обучения на огромном количестве данных, а приставка pre подразумевает, что при дополнительном обучении у нее есть больше возможностей для тонкой настройки на конкретные задачи.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "input": "But the last word, that's the real key piece.",
  "translatedText": "Но ключевое слово здесь - последнее.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 20.72,
  "end": 22.9
 },
 {
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "translatedText": "Трансформер - это особый вид нейронной сети, модель машинного обучения, и это основное изобретение, лежащее в основе нынешнего бума в области ИИ.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 23.38,
  "end": 31
 },
 {
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "translatedText": "В этом видео и последующих главах я хочу наглядно объяснить, что на самом деле происходит внутри трансформера.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 31.74,
  "end": 39.12
 },
 {
  "input": "We're going to follow the data that flows through it and go step by step.",
  "translatedText": "Мы пошогово проследим за данными, которые через него проходят.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 39.7,
  "end": 42.82
 },
 {
  "input": "There are many different kinds of models that you can build using transformers.",
  "translatedText": "Существует множество различных видов моделей, которые можно построить, используя трансформеры.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 43.44,
  "end": 47.38
 },
 {
  "input": "Some models take in audio and produce a transcript.",
  "translatedText": "Некоторые модели принимают аудиозапись и выдают транскрипт.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 47.8,
  "end": 50.8
 },
 {
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "translatedText": "Это предложение относится к модели, работающей наоборот, производящей синтетическую речь только из текста.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "translatedText": "Все те инструменты, которые взяли мир штурмом в 2022 году, вроде Dolly и Midjourney, которые принимают текстовое описание и выдают изображение, основаны на трансформерах.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "translatedText": "Даже если я не могу заставить его понять, каким должно быть существо из пирога, я все равно потрясен тем, что такое вообще возможно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66,
  "end": 73.1
 },
 {
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "translatedText": "А оригинальный трансформер, представленный в 2017 году компанией Google, был придуман для конкретного случая использования - перевода текста с одного языка на другой.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "translatedText": "Но вариант, на котором мы с тобой сосредоточимся и который лежит в основе таких инструментов, как ChatGPT, - это модель, обученная воспринимать фрагмент текста, возможно, даже с сопутствующими изображениями или звуком, и выдавать предсказание того, что будет дальше в отрывке.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "translatedText": "Это предсказание имеет форму распределения вероятностей по множеству различных фрагментов текста, которые могут последовать за ним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "translatedText": "На первый взгляд, тебе может показаться, что предсказание следующего слова - это совсем другая цель, нежели генерация нового текста.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "translatedText": "Но если у тебя есть подобная модель предсказания, то для создания более длинного куска текста можно просто дать ей начальный фрагмент для работы, взять случайную выборку из только что созданного распределения, добавить ее к тексту, а затем запустить весь процесс снова, чтобы сделать новое предсказание на основе всего нового текста, включая то, что она только что добавила.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "translatedText": "Не знаю, как тебе, но мне кажется, что это действительно не должно работать.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133
 },
 {
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "translatedText": "Например, в этой анимации я запускаю GPT-2 на своем ноутбуке и заставляю его многократно предсказывать и сэмплировать следующий фрагмент текста, чтобы сгенерировать историю на основе начального текста.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "input": "The story just doesn't really make that much sense.",
  "translatedText": "Просто в этой истории не так много смысла.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "translatedText": "Но если я заменю его на API-вызовы к GPT-3, который является той же базовой моделью, только намного больше, то внезапно, почти по волшебству, мы получим разумную историю, которая, кажется, даже позволяет сделать вывод о том, что существо пи могло бы жить в стране математики и вычислений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "translatedText": "Этот процесс повторного предсказания и выборки - по сути, то, что происходит, когда ты взаимодействуешь с ChatGPT или любой другой большой языковой моделью и видишь, как они выдают по одному слову за раз.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "translatedText": "На самом деле, одна из функций, которая мне бы очень понравилась, - это возможность видеть базовое распределение для каждого нового слова, которое он выбирает.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "translatedText": "Давай начнем с того, что очень подробно рассмотрим, как данные проходят через трансформатор.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "translatedText": "Мы потратим гораздо больше времени, мотивируя, интерпретируя и расширяя детали каждого шага, но в общих чертах, когда один из этих чатботов генерирует заданное слово, вот что происходит под капотом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "input": "First, the input is broken up into a bunch of little pieces.",
  "translatedText": "Во-первых, входные данные разбиваются на кучу маленьких кусочков.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "translatedText": "Эти части называются лексемами, и в случае с текстом это, как правило, слова, маленькие кусочки слов или другие распространенные комбинации символов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "translatedText": "Если речь идет об изображениях или звуке, то токены могут быть маленькими фрагментами изображения или маленькими кусочками звука.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "translatedText": "Каждый из этих токенов затем ассоциируется с вектором, то есть с некоторым списком чисел, который должен каким-то образом закодировать значение этого фрагмента.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "translatedText": "Если представить, что эти векторы дают координаты в каком-то очень высокоразмерном пространстве, то слова с похожими значениями имеют тенденцию приземляться на векторы, которые находятся близко друг к другу в этом пространстве.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "translatedText": "Затем эта последовательность векторов проходит через операцию, которая известна как блок внимания, и это позволяет векторам общаться друг с другом и передавать информацию туда-сюда, чтобы обновить свои значения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "translatedText": "Например, значение слова model во фразе a machine learning model отличается от его значения во фразе a fashion model.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "translatedText": "Блок внимания отвечает за то, чтобы понять, какие слова в контексте имеют отношение к обновлению значений каких других слов, и как именно эти значения должны быть обновлены.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "translatedText": "И опять же, всякий раз, когда я использую слово \"смысл\", он каким-то образом полностью закодирован в записях этих векторов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "translatedText": "После этого эти векторы проходят через другой вид операций, и в зависимости от источника, который ты читаешь, это будет называться многослойным перцептроном или, может быть, слоем feed-forward.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "translatedText": "И здесь векторы не разговаривают друг с другом, они все проходят одну и ту же операцию параллельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "translatedText": "И хотя этот блок немного сложнее интерпретировать, позже мы поговорим о том, что этот шаг немного похож на то, чтобы задать длинный список вопросов о каждом векторе, а затем обновить их на основе ответов на эти вопросы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294
 },
 {
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "translatedText": "Все операции в обоих этих блоках выглядят как гигантская куча матричных умножений, и наша основная задача будет заключаться в том, чтобы понять, как читать базовые матрицы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "translatedText": "Я упускаю некоторые подробности о некоторых этапах нормализации, которые происходят между ними, но это, в конце концов, высокоуровневое превью.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "translatedText": "После этого процесс, по сути, повторяется, ты ходишь туда-сюда между блоками внимания и блоками многослойного перцептрона, пока в самом конце не появится надежда на то, что весь основной смысл отрывка каким-то образом был заложен в самый последний вектор в последовательности.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "translatedText": "Затем мы выполняем определенную операцию над этим последним вектором, в результате которой получаем распределение вероятности по всем возможным лексемам, всем возможным небольшим фрагментам текста, которые могут быть следующими.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "translatedText": "И, как я уже сказал, как только у тебя появится инструмент, который предсказывает, что будет дальше, учитывая фрагмент текста, ты сможешь скормить ему немного начального текста и заставить его многократно играть в эту игру: предсказывать, что будет дальше, делать выборку из распределения, добавлять ее, а затем повторять снова и снова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "translatedText": "Некоторые из твоих знакомых могут вспомнить, как задолго до появления ChatGPT вот так выглядели ранние демо-версии GPT-3: ты должен был автозаполнять рассказы и сочинения на основе начального фрагмента.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "translatedText": "Чтобы превратить такой инструмент в чатбота, проще всего начать с небольшого текста, задающего обстановку взаимодействия пользователя с полезным ИИ-ассистентом, который можно назвать системной подсказкой, а затем использовать начальный вопрос или подсказку пользователя в качестве первого фрагмента диалога, а затем начать предсказывать, что скажет в ответ такой полезный ИИ-ассистент.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "translatedText": "Можно еще много чего сказать о шаге подготовки, который необходим для того, чтобы это хорошо работало, но на высоком уровне идея такова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "translatedText": "В этой главе мы с тобой расширим детали того, что происходит в самом начале сети, в самом конце сети, а также я хочу уделить много времени обзору некоторых важных битов фоновых знаний, вещей, которые стали бы второй натурой для любого инженера по машинному обучению к тому времени, когда появились трансформаторы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "translatedText": "Если тебя устраивают эти фоновые знания и ты немного нетерпелив, то можешь смело переходить к следующей главе, в которой речь пойдет о блоках внимания, которые принято считать сердцем трансформатора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "translatedText": "После этого я хочу подробнее рассказать об этих блоках многослойного перцептрона, о том, как происходит обучение, и о ряде других деталей, которые до этого момента были пропущены.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "translatedText": "Для более широкого контекста эти видео являются дополнениями к мини-сериалу о глубоком обучении, и ничего страшного, если ты не смотрел предыдущие, думаю, ты сможешь сделать это и не по порядку, но прежде чем погрузиться конкретно в трансформеров, думаю, стоит убедиться, что мы на одной волне в отношении основных предпосылок и структуры глубокого обучения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "translatedText": "Рискуя заявить очевидное, скажу, что это один из подходов к машинному обучению, который описывает любую модель, где ты используешь данные, чтобы как-то определить, как ведет себя модель.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "translatedText": "Я имею в виду, что, допустим, тебе нужна функция, которая принимает изображение и выдает метку, описывающую его, или наш пример с предсказанием следующего слова по отрывку текста, или любая другая задача, которая, кажется, требует некоторого элемента интуиции и распознавания образов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "translatedText": "В наши дни мы воспринимаем это почти как должное, но идея машинного обучения заключается в том, что вместо того, чтобы пытаться явно определить процедуру выполнения этой задачи в коде, что люди и делали в самые ранние дни ИИ, вместо этого ты создаешь очень гибкую структуру с настраиваемыми параметрами, как куча ручек и циферблатов, а затем каким-то образом используешь множество примеров того, как должен выглядеть выход для данного входа, чтобы подстроить и настроить значения этих параметров для имитации этого поведения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "translatedText": "Например, самой простой формой машинного обучения может быть линейная регрессия, где твоими входными и выходными данными являются отдельные числа, что-то вроде площади дома и его цены, и что ты хочешь, так это найти линию наилучшего соответствия через эти данные, чтобы предсказать будущие цены на дома.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "translatedText": "Эта линия описывается двумя непрерывными параметрами, скажем, наклоном и y-интерцептом, и цель линейной регрессии - определить эти параметры, чтобы они точно соответствовали данным.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "input": "Needless to say, deep learning models get much more complicated.",
  "translatedText": "Нет нужды говорить, что модели глубокого обучения становятся намного сложнее.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "translatedText": "GPT-3, например, имеет не два, а 175 миллиардов параметров.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "translatedText": "Но дело вот в чем: нельзя сказать, что ты можешь создать какую-то гигантскую модель с огромным количеством параметров без того, чтобы она либо сильно перегружала обучающие данные, либо была совершенно неподдающейся обучению.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "translatedText": "Глубокое обучение описывает класс моделей, которые за последние пару десятилетий доказали, что они удивительно хорошо масштабируются.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "translatedText": "Их объединяет один и тот же алгоритм обучения, называемый обратным распространением, и я хочу, чтобы ты понял, что для того, чтобы этот алгоритм обучения хорошо работал в масштабе, эти модели должны следовать определенному формату.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "translatedText": "Если ты знаешь этот формат, то он поможет объяснить многие варианты обработки языка трансформатором, которые в противном случае рискуют показаться произвольными.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "translatedText": "Во-первых, какую бы модель ты ни создавал, входные данные должны быть оформлены в виде массива вещественных чисел.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "translatedText": "Это может быть список чисел, может быть двумерный массив, а очень часто ты имеешь дело с массивами более высокой размерности, где используется общий термин - тензор.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596
 },
 {
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "translatedText": "Ты часто думаешь о том, что входные данные постепенно преобразуются во множество отдельных слоев, где, опять же, каждый слой всегда структурирован как некий массив вещественных чисел, пока ты не дойдешь до последнего слоя, который ты считаешь выходом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "translatedText": "Например, последний слой в нашей модели обработки текста - это список чисел, представляющий собой распределение вероятностей для всех возможных следующих лексем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "translatedText": "В глубоком обучении эти параметры модели почти всегда называются весами, и это потому, что ключевая особенность этих моделей заключается в том, что единственный способ взаимодействия этих параметров с обрабатываемыми данными - это взвешенные суммы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "translatedText": "Ты также рассыпаешь повсюду нелинейные функции, но они не будут зависеть от параметров.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "translatedText": "Обычно вместо того, чтобы видеть взвешенные суммы в явном виде, ты видишь их упакованными вместе в виде различных компонентов в векторном произведении матрицы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "translatedText": "Это равносильно тому, что если ты вспомнишь, как работает матрично-векторное умножение, то каждый компонент на выходе будет выглядеть как взвешенная сумма.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "translatedText": "Просто зачастую для нас с тобой концептуально чище думать о матрицах, которые заполнены настраиваемыми параметрами, преобразующими векторы, взятые из обрабатываемых данных.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "translatedText": "Например, те 175 миллиардов весов в GPT-3 организованы в чуть менее 28 000 отдельных матриц.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "translatedText": "Эти матрицы, в свою очередь, делятся на восемь различных категорий, и мы с тобой собираемся пройтись по каждой из них, чтобы понять, что делает тот или иной тип.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "translatedText": "По ходу дела я думаю, что будет забавно ссылаться на конкретные цифры из GPT-3, чтобы подсчитать, откуда именно взялись эти 175 миллиардов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "translatedText": "Даже если сейчас есть модели больше и лучше, эта имеет определенный шарм как крупноязычная модель, которая действительно привлекла внимание мира за пределами ML-сообществ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "translatedText": "Кроме того, с практической точки зрения, компании, как правило, гораздо жестче обходят стороной конкретные цифры для более современных сетей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "translatedText": "Я просто хочу рассказать, что если заглянуть под капот и посмотреть, что происходит внутри такого инструмента, как ChatGPT, то почти все фактические вычисления выглядят как матрично-векторное умножение.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "translatedText": "Есть небольшой риск заблудиться в море миллиардов цифр, но ты должен очень четко разграничить в своем сознании веса модели, которые я всегда буду окрашивать в синий или красный цвет, и обрабатываемые данные, которые я всегда буду окрашивать в серый цвет.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "translatedText": "Веса - это собственно мозг, это то, что усваивается во время тренировок, и они определяют его поведение.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "translatedText": "Обрабатываемые данные просто кодируют все конкретные входные данные, которые подаются в модель для данного запуска, например, фрагмент текста.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "translatedText": "С учетом всего этого давайте приступим к первому шагу этого примера по обработке текста, который заключается в том, чтобы разбить входной сигнал на маленькие кусочки и превратить эти кусочки в векторы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "translatedText": "Я уже упоминал, что эти куски называются лексемами, которые могут быть кусками слов или пунктуации, но время от времени в этой главе и особенно в следующей я хотел бы просто притворяться, что все разбито на слова более чисто.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "translatedText": "Поскольку мы, люди, мыслим словами, это лишь значительно упростит обращение к небольшим примерам и разъяснение каждого шага.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "translatedText": "Модель имеет предопределенный словарный запас, некоторый список всех возможных слов, скажем, 50 000 из них, и первая матрица, с которой мы столкнемся, известная как матрица встраивания, имеет один столбец для каждого из этих слов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "translatedText": "Именно эти столбцы определяют, в какой вектор превратится каждое слово на первом этапе.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "translatedText": "Мы обозначим ее We, и, как и все матрицы, которые мы видим, ее значения начинаются случайно, но в дальнейшем они будут выучены на основе данных.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "translatedText": "Превращение слов в векторы было обычной практикой в машинном обучении задолго до появления трансформаторов, но это немного странно, если ты никогда не видел этого раньше, и это закладывает основу для всего последующего, так что давай уделим немного времени знакомству с этим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "translatedText": "Мы часто называем это вкрапление словом, которое предлагает тебе думать об этих векторах очень геометрически, как о точках в некотором высокоразмерном пространстве.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "translatedText": "Визуализировать список из трех чисел как координаты точек в трехмерном пространстве не составит труда, но вкрапления слов имеют тенденцию быть гораздо более высокой размерности.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "translatedText": "В GPT-3 у них 12 288 измерений, и, как ты увидишь, важно работать в пространстве, которое имеет много разных направлений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "translatedText": "Точно так же, как ты можешь взять двумерный срез через трехмерное пространство и спроецировать все точки на этот срез, для анимации вкраплений слов, которые дает мне простая модель, я собираюсь сделать аналогичную вещь, выбрав трехмерный срез через это очень высокоразмерное пространство, спроецировать векторы слов вниз на него и отобразить результаты.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "translatedText": "Главная идея здесь в том, что по мере того, как модель настраивает и подстраивает свои веса, определяющие, как именно слова встраиваются в векторы в процессе обучения, она стремится остановиться на наборе вкраплений, где направления в пространстве имеют некий семантический смысл.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "translatedText": "Для простой модели \"слово-вектор\", которую я здесь использую, если я проведу поиск по всем словам, чьи вкрапления наиболее близки к вкраплениям слова \"башня\", то ты заметишь, что все они дают очень похожие вибрации, похожие на башню.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "translatedText": "А если ты хочешь подтянуть Python и поиграть с ним дома, то вот конкретная модель, которую я использую для создания анимации.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "translatedText": "Это не трансформер, но этого достаточно, чтобы проиллюстрировать идею о том, что направления в пространстве могут нести семантическое значение.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "translatedText": "Классический пример - если взять разницу между векторами для женщины и мужчины, то есть то, что можно представить как маленький вектор, соединяющий кончик одного с кончиком другого, то это очень похоже на разницу между королем и королевой.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "translatedText": "Так что, допустим, ты не знаешь слова, обозначающего монарха-женщину, ты можешь найти его, взяв king, добавив к нему направление woman-man и поискав вкрапления, наиболее близкие к этой точке.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "input": "At least, kind of.",
  "translatedText": "По крайней мере, в некотором роде.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927,
  "end": 928.2
 },
 {
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "translatedText": "Несмотря на то, что это классический пример для модели, с которой я играю, истинное вложение queen на самом деле немного дальше, чем можно было бы предположить, предположительно потому, что то, как queen используется в обучающих данных, не просто женская версия king.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "translatedText": "Когда я поиграл, семейные отношения показались мне гораздо лучше иллюстрирующими эту идею.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "translatedText": "Дело в том, что, похоже, во время обучения модель сочла выгодным выбрать вкрапления таким образом, чтобы одно направление в этом пространстве кодировало гендерную информацию.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "translatedText": "Другой пример: если взять вкрапления Италии, вычесть вкрапления Германии и прибавить это к вкраплениям Гитлера, то получится что-то очень близкое к вкраплениям Муссолини.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "translatedText": "Как будто модель научилась ассоциировать одни направления с итальянскостью, а другие - с лидерами оси Второй мировой войны.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "translatedText": "Возможно, мой любимый пример в этом ключе - это то, как в некоторых моделях, если взять разницу между Германией и Японией и добавить ее к суши, то в итоге получится очень близко к сарделькам.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "translatedText": "Также, играя в эту игру по поиску ближайших соседей, я с удовольствием наблюдал, насколько близко Кэт была и к зверю, и к монстру.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "translatedText": "Одна из математических интуиций, которую полезно иметь в виду, особенно для следующей главы, заключается в том, что точечное произведение двух векторов можно рассматривать как способ измерения того, насколько хорошо они выровнены.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "translatedText": "С вычислительной точки зрения точечные продукты подразумевают умножение всех соответствующих компонентов, а затем сложение результатов, что очень хорошо, так как большая часть наших вычислений должна выглядеть как взвешенные суммы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "translatedText": "Геометрически точечное произведение положительно, если векторы направлены в одинаковые стороны, оно равно нулю, если они перпендикулярны, и отрицательно, если они направлены в противоположные стороны.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "translatedText": "Допустим, ты играл с этой моделью и предположил, что вкрапление \"кошки минус кошка\" может представлять собой некое направление множественности в этом пространстве.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "translatedText": "Чтобы проверить это, я собираюсь взять этот вектор и вычислить его точечное произведение на вкрапления некоторых существительных единственного числа, а затем сравнить его с точечным произведением на соответствующие существительные множественного числа.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "translatedText": "Если ты поиграешь с этим, то заметишь, что множественное число действительно, кажется, постоянно дает более высокие значения, чем единственное, что указывает на то, что они больше соответствуют этому направлению.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "translatedText": "Также забавно, что если взять это точечное произведение с вкраплениями слов 1, 2, 3 и так далее, то они дают возрастающие значения, так что мы как будто можем количественно измерить, насколько многозначным модель находит данное слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "translatedText": "Опять же, специфику того, как слова встраиваются, можно узнать, используя данные.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "translatedText": "Эта матрица вкраплений, столбцы которой говорят нам о том, что происходит с каждым словом, является первой стопкой весов в нашей модели.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "translatedText": "Если использовать цифры GPT-3, то размер словарного запаса в конкретном случае составляет 50 257, и, опять же, технически он состоит не из слов как таковых, а из лексем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "translatedText": "Размерность встраивания равна 12 288, и умножение этих значений говорит нам о том, что он состоит примерно из 617 миллионов весов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "translatedText": "Давай добавим это в бегущий счет, помня, что к концу мы должны досчитать до 175 миллиардов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "translatedText": "В случае с трансформаторами ты действительно хочешь думать о векторах в этом пространстве встраивания не просто как о представлении отдельных слов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "translatedText": "Во-первых, они также кодируют информацию о позиции этого слова, о чем мы поговорим позже, но что более важно, ты должен думать о том, что они обладают способностью впитывать контекст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "translatedText": "Вектор, который начал свою жизнь как вставка слова king, например, может постепенно подтягиваться и подтягиваться различными блоками в этой сети, так что к концу он указывает на гораздо более конкретное и тонкое направление, которое каким-то образом кодирует, что это был король, живший в Шотландии, который добился своего поста после убийства предыдущего короля и которого описывают на шекспировском языке.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "input": "Think about your own understanding of a given word.",
  "translatedText": "Подумай о своем собственном понимании того или иного слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "translatedText": "Значение этого слова явно зависит от окружения, и иногда оно включает в себя контекст, находящийся на большом расстоянии, поэтому при создании модели, способной предсказать, какое слово будет следующим, цель состоит в том, чтобы каким-то образом дать ей возможность эффективно учитывать контекст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "translatedText": "Чтобы было понятно, на первом этапе, когда ты создаешь массив векторов на основе входного текста, каждый из них просто выдергивается из матрицы встраивания, поэтому изначально каждый из них может кодировать только значение одного слова без какого-либо вклада из окружения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "translatedText": "Но ты должен думать, что главная цель этой сети, по которой он течет, - позволить каждому из этих векторов впитать смысл, который намного богаче и конкретнее, чем то, что могут представлять собой отдельные слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "translatedText": "Сеть может обрабатывать только фиксированное количество векторов за раз, известное как размер контекста.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "translatedText": "Для GPT-3 он был обучен с размером контекста 2048, поэтому данные, проходящие через сеть, всегда выглядят как массив из 2048 столбцов, каждый из которых имеет 12 000 измерений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "translatedText": "Размер контекста ограничивает то, сколько текста может включить трансформатор, когда он делает предсказание следующего слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "translatedText": "Именно поэтому при длительных беседах с некоторыми чатботами, например с ранними версиями ChatGPT, часто возникало ощущение, что бот как бы теряет нить разговора, когда ты продолжаешь слишком долго.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "translatedText": "В свое время мы углубимся в детали внимания, но, пропуская вперед, я хочу на минуту поговорить о том, что происходит в самом конце.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "translatedText": "Помни, что желаемый результат - это распределение вероятностей по всем токенам, которые могут прийти следующими.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "translatedText": "Например, если самое последнее слово - профессор, а контекст включает такие слова, как Гарри Поттер, а непосредственно перед ним мы видим наименее любимого учителя, а также если ты дашь мне некоторую свободу действий, позволив притвориться, что лексемы просто выглядят как полные слова, то хорошо обученная сеть, накопившая знания о Гарри Поттере, предположительно присвоит высокий номер слову Снейп.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "input": "This involves two different steps.",
  "translatedText": "Это включает в себя два разных этапа.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "translatedText": "Первый заключается в использовании другой матрицы, которая сопоставляет самый последний вектор в данном контексте со списком из 50 000 значений, по одному на каждую лексему в словаре.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "translatedText": "Затем есть функция, которая нормализует это в распределение вероятностей, она называется Softmax, и мы поговорим о ней подробнее буквально через секунду, но до этого может показаться немного странным использовать только это последнее вложение для предсказания, когда, в конце концов, на последнем шаге в слое есть тысячи других векторов, просто сидящих там со своими собственными значениями, богатыми контекстом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "translatedText": "Это связано с тем, что в процессе обучения оказывается гораздо эффективнее, если ты используешь каждый из этих векторов в финальном слое, чтобы одновременно сделать предсказание того, что будет сразу после него.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "translatedText": "О тренировках еще много чего можно будет сказать позже, но сейчас я просто хочу обратить на это внимание.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "translatedText": "Эта матрица называется матрицей Unembedding, и мы даем ей обозначение WU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "translatedText": "Опять же, как и все весовые матрицы, которые мы видим, ее записи начинаются случайным образом, но они усваиваются в процессе обучения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "translatedText": "Продолжая подсчитывать общее количество параметров, эта матрица Unembedding имеет одну строку для каждого слова в словаре, и каждая строка имеет столько же элементов, сколько и размерность встраивания.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "translatedText": "Она очень похожа на матрицу встраивания, только порядок поменялся местами, поэтому она добавляет в сеть еще 617 миллионов параметров, то есть на данный момент мы насчитали чуть больше миллиарда - небольшая, но не совсем незначительная часть от 175 миллиардов, которые мы получим в итоге.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "translatedText": "В качестве последнего мини-урока для этой главы я хочу подробнее поговорить об этой функции softmax, так как она снова предстанет перед нами, как только мы погрузимся в блоки внимания.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "translatedText": "Идея заключается в том, что если ты хочешь, чтобы последовательность чисел действовала как распределение вероятностей, скажем, распределение по всем возможным следующим словам, то каждое значение должно быть между 0 и 1, и тебе также нужно, чтобы все они складывались в 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "translatedText": "Однако если ты играешь в обучающую игру, где все, что ты делаешь, выглядит как матрично-векторное умножение, то выходы, которые ты получаешь по умолчанию, совсем не соответствуют этому.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "translatedText": "Значения часто бывают отрицательными или намного больше 1, и они почти наверняка не складываются в 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "translatedText": "Softmax - это стандартный способ превратить произвольный список чисел в правильное распределение таким образом, чтобы наибольшие значения оказались ближе всего к 1, а меньшие - очень близко к 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "input": "That's all you really need to know.",
  "translatedText": "Это все, что тебе действительно нужно знать.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "translatedText": "Но если тебе интересно, то принцип работы заключается в том, чтобы сначала возвести e в степень каждого из чисел, то есть теперь у тебя есть список положительных значений, а затем ты можешь взять сумму всех этих положительных значений и разделить каждый член на эту сумму, что нормализует список, который складывается в 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "translatedText": "Ты заметишь, что если одно из чисел на входе значимо больше остальных, то на выходе соответствующий член доминирует в распределении, так что если бы ты делал выборку из него, то почти наверняка просто выбрал бы максимизирующий вход.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "translatedText": "Но это мягче, чем просто выбрать максимум, в том смысле, что когда другие значения так же велики, они тоже получают значимый вес в распределении, и все непрерывно меняется, когда ты постоянно варьируешь входные данные.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "translatedText": "В некоторых ситуациях, например, когда ChatGPT использует это распределение для создания следующего слова, есть возможность немного повеселиться, добавив в эту функцию немного дополнительной остроты: в знаменатель этих экспоненты подбрасывается константа t.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "translatedText": "Мы называем его температурой, так как он смутно напоминает роль температуры в некоторых уравнениях термодинамики, и эффект заключается в том, что когда t больше, ты придаешь больший вес меньшим значениям, то есть распределение становится немного более равномерным, а если t меньше, то большие значения будут доминировать более агрессивно, а в крайнем случае, если установить t равным нулю, то весь вес перейдет к максимальному значению.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "translatedText": "Например, я попрошу GPT-3 сгенерировать историю с начальным текстом \"Жил-был А\", но в каждом случае я буду использовать разные температуры.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "translatedText": "Нулевая температура означает, что он всегда выбирает самое предсказуемое слово, и то, что ты получишь, в итоге окажется банальной производной от \"Златовласки\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "translatedText": "Более высокая температура дает ему шанс выбирать менее вероятные слова, но это связано с риском.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "translatedText": "В данном случае история начинается более оригинально, о молодом веб-художнике из Южной Кореи, но быстро вырождается в бессмыслицу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "translatedText": "Технически говоря, API на самом деле не позволяет тебе выбрать температуру больше 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "translatedText": "Для этого нет никаких математических причин, это просто произвольное ограничение, наложенное для того, чтобы их инструмент не был замечен в генерации слишком нелепых вещей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "translatedText": "Если тебе интересно, то на самом деле эта анимация работает так: я беру 20 наиболее вероятных следующих токенов, которые генерирует GPT-3, что, похоже, является максимумом, который они мне выдают, а затем подстраиваю вероятности, основываясь на экспоненте 1 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "translatedText": "В качестве еще одного жаргона: точно так же, как ты можешь называть компоненты выхода этой функции вероятностями, люди часто называют входы логарифмами, или кто-то говорит логарифмы, кто-то - логарифмы, я буду говорить логарифмы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "translatedText": "Так, например, когда ты вводишь текст, все эти вкрапления слов проходят через сеть, и ты делаешь финальное перемножение с матрицей без вкраплений, люди, занимающиеся машинным обучением, называют компоненты в этом сыром, ненормированном выходе логитами для предсказания следующего слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "translatedText": "Во многом целью этой главы было заложить основы для понимания механизма внимания, в стиле Karate Kid wax-on-wax-off.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "translatedText": "Видишь ли, если у тебя есть сильная интуиция в отношении вкраплений слов, софтмакса, того, как точечные произведения измеряют сходство, а также базовая предпосылка, что большинство вычислений должны выглядеть как матричное умножение с матрицами, полными настраиваемых параметров, то понимание механизма внимания, этого краеугольного камня во всем современном буме ИИ, должно быть относительно легким.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "input": "For that, come join me in the next chapter.",
  "translatedText": "Для этого присоединяйся ко мне в следующей главе.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "translatedText": "Пока я это публикую, черновик следующей главы доступен для ознакомления сторонникам Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "translatedText": "Финальная версия должна появиться в паблике через неделю или две, обычно это зависит от того, сколько я в итоге изменю, основываясь на этой рецензии.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "translatedText": "А пока, если ты хочешь погрузиться во внимание и немного помочь каналу, он ждет тебя.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]
