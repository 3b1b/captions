1
00:00:00,000 --> 00:00:04,560
حروف اول GPT مخفف Generative Pretrained Transformer است.

2
00:00:05,220 --> 00:00:09,020
بنابراین اولین کلمه به اندازه کافی ساده است، این ربات ها هستند که متن جدیدی تولید می کنند.

3
00:00:09,800 --> 00:00:13,103
Pretrained به نحوه گذراندن مدل از طریق یک فرآیند یادگیری از 

4
00:00:13,103 --> 00:00:16,516
حجم عظیمی از داده ها اشاره دارد، و پیشوند القا می کند که فضای 

5
00:00:16,516 --> 00:00:20,040
بیشتری برای تنظیم دقیق آن در وظایف خاص با آموزش اضافی وجود دارد.

6
00:00:20,720 --> 00:00:22,900
اما آخرین کلمه، این قطعه کلیدی واقعی است.

7
00:00:23,380 --> 00:00:27,358
ترانسفورماتور نوع خاصی از شبکه عصبی، یک مدل یادگیری ماشینی 

8
00:00:27,358 --> 00:00:31,000
است و اختراع اصلی زیربنای رونق فعلی در هوش مصنوعی است.

9
00:00:31,740 --> 00:00:35,430
کاری که من می‌خواهم با این ویدیو و فصل‌های بعدی انجام دهم، توضیحی مبتنی 

10
00:00:35,430 --> 00:00:39,120
بر بصری برای آنچه که واقعاً در داخل یک ترانسفورماتور اتفاق می‌افتد، است.

11
00:00:39,700 --> 00:00:42,820
ما داده‌هایی را که از طریق آن جریان می‌یابد دنبال می‌کنیم و قدم به قدم پیش می‌رویم.

12
00:00:43,440 --> 00:00:47,380
انواع مختلفی از مدل ها وجود دارد که می توانید با استفاده از ترانسفورماتور بسازید.

13
00:00:47,800 --> 00:00:50,800
برخی از مدل ها صدا را دریافت می کنند و رونوشت تولید می کنند.

14
00:00:51,340 --> 00:00:56,220
این جمله از مدلی می آید که برعکس می شود و گفتار ترکیبی را فقط از متن تولید می کند.

15
00:00:56,660 --> 00:01:01,146
همه آن ابزارهایی که در سال 2022 جهان را طوفان کردند مانند Dolly و Midjourney که 

16
00:01:01,146 --> 00:01:05,519
توضیحات متنی را می گیرند و یک تصویر تولید می کنند بر اساس ترانسفورماتور هستند.

17
00:01:06,000 --> 00:01:09,422
حتی اگر نتوانم کاملاً بفهمم که یک موجود پای قرار است چه چیزی باشد، 

18
00:01:09,422 --> 00:01:13,100
باز هم از این که چنین چیزی حتی از راه دور امکان پذیر است، شگفت زده هستم.

19
00:01:13,900 --> 00:01:18,034
و ترانسفورماتور اصلی که در سال 2017 توسط گوگل معرفی شد برای 

20
00:01:18,034 --> 00:01:22,100
استفاده خاص از ترجمه متن از یک زبان به زبان دیگر اختراع شد.

21
00:01:22,660 --> 00:01:27,559
اما گونه‌ای که من و شما روی آن تمرکز خواهیم کرد، که زیربنای ابزارهایی مانند 

22
00:01:27,559 --> 00:01:32,716
ChatGPT است، مدلی است که برای گرفتن یک قطعه متن، حتی با برخی از تصاویر اطراف یا 

23
00:01:32,716 --> 00:01:38,260
صدای همراه آن، آموزش دیده و یک پیش‌بینی ایجاد می‌کند. برای آنچه در قسمت بعدی آمده است.

24
00:01:38,600 --> 00:01:41,199
این پیش‌بینی به شکل توزیع احتمال بر روی بسیاری از 

25
00:01:41,199 --> 00:01:43,800
تکه‌های متنی که ممکن است به دنبال آن باشد، می‌شود.

26
00:01:45,040 --> 00:01:47,569
در نگاه اول، ممکن است فکر کنید که پیش‌بینی کلمه 

27
00:01:47,569 --> 00:01:49,940
بعدی هدفی بسیار متفاوت از تولید متن جدید است.

28
00:01:50,180 --> 00:01:53,934
اما هنگامی که یک مدل پیش‌بینی مانند این دارید، یک چیز ساده که یک قطعه 

29
00:01:53,934 --> 00:01:57,956
متن طولانی‌تر ایجاد می‌کنید این است که به آن یک قطعه اولیه برای کار بدهید، 

30
00:01:57,956 --> 00:02:02,621
از توزیعی که به تازگی تولید کرده یک نمونه تصادفی بگیرد، آن نمونه را به متن اضافه کنید. 

31
00:02:02,621 --> 00:02:06,375
و سپس کل فرآیند را دوباره اجرا کنید تا بر اساس تمام متن جدید، از جمله 

32
00:02:06,375 --> 00:02:09,539
آنچه که به تازگی اضافه کرده است، پیش بینی جدیدی انجام دهید.

33
00:02:10,100 --> 00:02:13,000
من در مورد شما نمی دانم، اما واقعاً به نظر نمی رسد که این واقعاً کار کند.

34
00:02:13,420 --> 00:02:16,484
به عنوان مثال، در این انیمیشن، من GPT-2 را روی لپ‌تاپ خود اجرا 

35
00:02:16,484 --> 00:02:19,598
می‌کنم و از آن می‌خواهم بارها و بارها قسمت بعدی متن را پیش‌بینی 

36
00:02:19,598 --> 00:02:22,420
و نمونه‌برداری کند تا داستانی بر اساس متن اولیه تولید کند.

37
00:02:22,420 --> 00:02:26,120
داستان واقعاً چندان منطقی نیست.

38
00:02:26,500 --> 00:02:31,313
اما اگر آن را با فراخوان‌های API به GPT-3 عوض کنم، که همان مدل اصلی است، بسیار 

39
00:02:31,313 --> 00:02:36,127
بزرگ‌تر، ناگهان تقریباً به طور جادویی داستان معقولی دریافت می‌کنیم، داستانی که 

40
00:02:36,127 --> 00:02:40,880
حتی به نظر می‌رسد استنباط می‌کند که یک موجود pi در یک سرزمین ریاضیات و محاسبات

41
00:02:41,580 --> 00:02:45,117
این فرآیند پیش‌بینی و نمونه‌برداری مکرر در اینجا اساساً زمانی اتفاق 

42
00:02:45,117 --> 00:02:48,602
می‌افتد که با ChatGPT یا هر یک از این مدل‌های زبان بزرگ دیگر تعامل 

43
00:02:48,602 --> 00:02:51,880
می‌کنید و می‌بینید که آنها در یک زمان یک کلمه را تولید می‌کنند.

44
00:02:52,480 --> 00:02:55,825
در واقع، یکی از ویژگی هایی که من بسیار از آن لذت می برم این است که 

45
00:02:55,825 --> 00:02:59,220
بتوانم توزیع زیربنایی را برای هر کلمه جدیدی که انتخاب می کند، ببینم.

46
00:03:03,820 --> 00:03:05,916
بیایید کارها را با یک پیش نمایش سطح بسیار بالا از 

47
00:03:05,916 --> 00:03:08,180
نحوه جریان داده ها از طریق یک ترانسفورماتور شروع کنیم.

48
00:03:08,640 --> 00:03:11,962
ما زمان بسیار بیشتری را صرف انگیزه و تفسیر و بسط جزئیات هر مرحله 

49
00:03:11,962 --> 00:03:15,183
خواهیم کرد، اما به طور کلی، زمانی که یکی از این ربات‌های چت یک 

50
00:03:15,183 --> 00:03:18,660
کلمه خاص را تولید می‌کند، در اینجا چیزی است که در زیر سرپوش می‌گذرد.

51
00:03:19,080 --> 00:03:22,040
ابتدا، ورودی به دسته‌ای از قطعات کوچک تقسیم می‌شود.

52
00:03:22,620 --> 00:03:26,191
این قطعات نشانه نامیده می شوند، و در مورد متن، اینها به کلمات 

53
00:03:26,191 --> 00:03:29,820
یا قطعات کوچک کلمات یا سایر ترکیبات رایج کاراکترها تمایل دارند.

54
00:03:30,740 --> 00:03:33,648
اگر تصاویر یا صدا درگیر باشند، نشانه‌ها می‌توانند 

55
00:03:33,648 --> 00:03:37,080
تکه‌های کوچکی از آن تصویر یا تکه‌های کوچکی از آن صدا باشند.

56
00:03:37,580 --> 00:03:41,278
سپس هر یک از این نشانه‌ها با یک بردار، به معنای فهرستی از 

57
00:03:41,278 --> 00:03:45,360
اعداد، مرتبط می‌شود که به نوعی معنای آن قطعه را رمزگذاری می‌کند.

58
00:03:45,880 --> 00:03:50,088
اگر فکر می کنید این بردارها مختصاتی را در فضایی با ابعاد بسیار بالا می دهند، 

59
00:03:50,088 --> 00:03:54,680
کلمات با معانی مشابه روی بردارهایی قرار می گیرند که در آن فضا به یکدیگر نزدیک هستند.

60
00:03:55,280 --> 00:03:58,233
این دنباله از بردارها سپس از عملیاتی عبور می کند که به عنوان بلوک 

61
00:03:58,233 --> 00:04:01,277
توجه شناخته می شود، و این به بردارها اجازه می دهد تا با یکدیگر صحبت 

62
00:04:01,277 --> 00:04:04,500
کنند و اطلاعات را به عقب و جلو انتقال دهند تا مقادیر خود را به روز کنند.

63
00:04:04,880 --> 00:04:08,437
برای مثال معنای کلمه model در عبارت a machine learning 

64
00:04:08,437 --> 00:04:11,800
model با معنای آن در عبارت fashion model متفاوت است.

65
00:04:12,260 --> 00:04:17,044
بلوک توجه چیزی است که مسئول تشخیص اینکه کدام کلمات در متن با به روز کردن 

66
00:04:17,044 --> 00:04:21,959
معانی کدام کلمات دیگر مرتبط هستند و دقیقاً چگونه آن معانی باید به روز شوند.

67
00:04:22,500 --> 00:04:25,198
و دوباره، هر زمان که من از کلمه معنی استفاده می کنم، این 

68
00:04:25,198 --> 00:04:28,040
به نوعی به طور کامل در ورودی های آن بردارها رمزگذاری می شود.

69
00:04:29,180 --> 00:04:33,716
پس از آن، این بردارها از نوع دیگری از عملیات عبور می کنند، و بسته به منبعی که در حال 

70
00:04:33,716 --> 00:04:38,200
خواندن آن هستید، به عنوان یک پرسپترون چند لایه یا شاید یک لایه پیشخور نامیده می شود.

71
00:04:38,580 --> 00:04:40,538
و در اینجا بردارها با یکدیگر صحبت نمی کنند، همه 

72
00:04:40,538 --> 00:04:42,660
آنها یک عملیات مشابه را به صورت موازی انجام می دهند.

73
00:04:43,060 --> 00:04:46,741
و در حالی که تفسیر این بلوک کمی سخت‌تر است، بعداً در مورد اینکه چگونه 

74
00:04:46,741 --> 00:04:50,318
این مرحله کمی شبیه پرسیدن یک لیست طولانی از سؤالات در مورد هر بردار 

75
00:04:50,318 --> 00:04:54,000
و سپس به‌روزرسانی آنها بر اساس پاسخ به آن سؤالات است، صحبت خواهیم کرد.

76
00:04:54,900 --> 00:04:59,988
همه عملیات در هر دو بلوک شبیه انبوهی از ضرب‌های ماتریس هستند و 

77
00:04:59,988 --> 00:05:05,320
وظیفه اصلی ما این است که بفهمیم چگونه ماتریس‌های زیرین را بخوانیم.

78
00:05:06,980 --> 00:05:09,887
من برخی از جزئیات را در مورد برخی از مراحل عادی سازی که در این 

79
00:05:09,887 --> 00:05:12,980
بین اتفاق می افتد، پنهان می کنم، اما این یک پیش نمایش سطح بالا است.

80
00:05:13,680 --> 00:05:18,799
پس از آن، فرآیند اساساً تکرار می‌شود، بین بلوک‌های توجه و بلوک‌های پرسپترون 

81
00:05:18,799 --> 00:05:23,851
چندلایه به عقب و جلو می‌روید، تا اینکه در نهایت امید این است که تمام معنای 

82
00:05:23,851 --> 00:05:28,500
اصلی متن به نحوی در آخرین بردار در آن قرار گرفته باشد. تسلسل و توالی.

83
00:05:28,920 --> 00:05:33,611
سپس عملیات مشخصی را بر روی آخرین بردار انجام می‌دهیم که توزیع احتمال را روی همه 

84
00:05:33,611 --> 00:05:38,420
نشانه‌های ممکن، همه تکه‌های کوچک ممکن متنی که ممکن است بعداً بیایند، ایجاد می‌کند.

85
00:05:38,980 --> 00:05:42,479
و همانطور که گفتم، هنگامی که ابزاری دارید که با توجه به یک قطعه متن، 

86
00:05:42,479 --> 00:05:46,080
چیزهای بعدی را پیش‌بینی می‌کند، می‌توانید کمی از متن اولیه به آن بدهید 

87
00:05:46,080 --> 00:05:49,478
و آن را به طور مکرر این بازی پیش‌بینی چیزهای بعدی، نمونه‌برداری از 

88
00:05:49,478 --> 00:05:53,080
توزیع، ضمیمه کردن را انجام دهید. آن را، و سپس بارها و بارها تکرار کنید.

89
00:05:53,640 --> 00:05:57,325
ممکن است برخی از شما که می‌دانید، مدت‌ها قبل از اینکه ChatGPT وارد 

90
00:05:57,325 --> 00:06:00,789
صحنه شود، به یاد بیاورید، دموی اولیه GPT-3 به این شکل بود، شما 

91
00:06:00,789 --> 00:06:04,640
می‌خواهید داستان‌ها و مقالات را براساس یک قطعه اولیه تکمیل خودکار کند.

92
00:06:05,580 --> 00:06:10,860
برای تبدیل ابزاری مانند این به یک ربات چت، ساده ترین نقطه شروع این است که کمی متن داشته 

93
00:06:10,860 --> 00:06:16,260
باشید که تنظیمات تعامل کاربر با یک دستیار مفید هوش مصنوعی را تعیین می کند، چیزی که شما آن 

94
00:06:16,260 --> 00:06:21,539
را اعلان سیستم می نامید، و سپس از سوال یا درخواست اولیه کاربر به عنوان اولین بیت گفتگو، 

95
00:06:21,539 --> 00:06:26,940
و سپس شما باید شروع به پیش بینی کنید که چنین دستیار هوش مصنوعی مفیدی در پاسخ چه خواهد گفت.

96
00:06:27,720 --> 00:06:30,757
چیزهای بیشتری برای گفتن در مورد مرحله ای از آموزش وجود دارد که 

97
00:06:30,757 --> 00:06:33,940
برای انجام این کار به خوبی لازم است، اما در سطح بالا این ایده است.

98
00:06:35,720 --> 00:06:39,837
در این فصل، من و شما قصد داریم جزئیات آنچه را که در همان ابتدای شبکه، 

99
00:06:39,837 --> 00:06:44,012
در انتهای شبکه اتفاق می‌افتد، گسترش دهیم، و همچنین می‌خواهم زمان زیادی 

100
00:06:44,012 --> 00:06:47,071
را صرف بررسی برخی از بخش‌های مهم دانش پس‌زمینه کنم. 

101
00:06:47,071 --> 00:06:51,129
، چیزهایی که تا زمانی که ترانسفورماتورها به وجود آمدند، طبیعت دوم هر 

102
00:06:51,129 --> 00:06:52,600
مهندس یادگیری ماشینی بود.

103
00:06:53,060 --> 00:06:58,058
اگر با این دانش پیشینه راحت هستید و کمی بی حوصله هستید، می توانید راحت به فصل بعدی بروید، 

104
00:06:58,058 --> 00:07:02,780
که بر روی بلوک های توجه تمرکز می کند، که عموما قلب ترانسفورماتور در نظر گرفته می شود.

105
00:07:03,360 --> 00:07:07,547
پس از آن، می‌خواهم در مورد این بلوک‌های پرسپترون چند لایه، نحوه عملکرد آموزش 

106
00:07:07,547 --> 00:07:11,680
و تعدادی از جزئیات دیگر که تا آن مرحله نادیده گرفته شده‌اند، بیشتر صحبت کنم.

107
00:07:12,180 --> 00:07:16,240
برای زمینه وسیع‌تر، این ویدیوها اضافه‌شده‌ای به یک مینی‌سریال درباره یادگیری عمیق 

108
00:07:16,240 --> 00:07:20,300
هستند، و اگر ویدیوهای قبلی را تماشا نکرده‌اید، اشکالی ندارد، فکر می‌کنم می‌توانید 

109
00:07:20,300 --> 00:07:24,410
آن را بدون نظم انجام دهید، اما قبل از فرو رفتن در ترانسفورماتورها، فکر می‌کنم ارزش 

110
00:07:24,410 --> 00:07:28,520
آن را دارد که مطمئن شوید در مورد فرضیه اصلی و ساختار یادگیری عمیق در یک صفحه هستیم.

111
00:07:29,020 --> 00:07:33,718
با خطر بیان چیزهای بدیهی، این یکی از رویکردهای یادگیری ماشینی است که هر مدلی را 

112
00:07:33,718 --> 00:07:38,300
که در آن از داده ها استفاده می کنید برای تعیین نحوه رفتار یک مدل توصیف می کند.

113
00:07:39,140 --> 00:07:43,631
منظور من از آن این است که فرض کنید شما تابعی را می خواهید که یک تصویر را می گیرد 

114
00:07:43,631 --> 00:07:48,122
و برچسبی برای توصیف آن تولید می کند، یا مثال ما از پیش بینی کلمه بعدی با توجه به 

115
00:07:48,122 --> 00:07:52,780
قسمتی از متن، یا هر کار دیگری که به نظر می رسد به عنصری نیاز دارد. شهود و تشخیص الگو

116
00:07:53,200 --> 00:07:57,652
ما این روزها تقریباً این را بدیهی می دانیم، اما ایده یادگیری ماشینی این است که به 

117
00:07:57,652 --> 00:08:01,942
جای تلاش برای تعریف صریح رویه ای برای نحوه انجام آن کار در کد، کاری که مردم در 

118
00:08:01,942 --> 00:08:06,395
اولین روزهای هوش مصنوعی انجام می دادند، به جای شما یک ساختار بسیار انعطاف‌پذیر با 

119
00:08:06,395 --> 00:08:10,902
پارامترهای قابل تنظیم، مانند دسته‌ای از دستگیره‌ها و شماره‌گیرها، راه‌اندازی کنید، 

120
00:08:10,902 --> 00:08:15,301
و سپس از نمونه‌های بسیاری استفاده می‌کنید که خروجی برای یک ورودی مشخص چگونه باید 

121
00:08:15,301 --> 00:08:19,700
باشد تا مقادیر آن پارامترها را تغییر داده و تنظیم کنید تا این رفتار را تقلید کند.

122
00:08:19,700 --> 00:08:23,989
به عنوان مثال، شاید ساده‌ترین شکل یادگیری ماشین، رگرسیون خطی باشد، که در 

123
00:08:23,989 --> 00:08:28,338
آن ورودی‌ها و خروجی‌های شما هر یک اعداد منفرد هستند، چیزی شبیه متراژ مربع 

124
00:08:28,338 --> 00:08:32,627
یک خانه و قیمت آن، و چیزی که شما می‌خواهید این است که از این طریق بهترین 

125
00:08:32,627 --> 00:08:36,799
تناسب را پیدا کنید. می دانید، داده ها برای پیش بینی قیمت خانه در آینده.

126
00:08:37,440 --> 00:08:42,882
آن خط با دو پارامتر پیوسته توصیف می شود، مثلاً شیب و قطع y، و هدف 

127
00:08:42,882 --> 00:08:48,160
رگرسیون خطی تعیین آن پارامترها برای مطابقت نزدیک با داده ها است.

128
00:08:48,880 --> 00:08:52,100
نیازی به گفتن نیست که مدل های یادگیری عمیق بسیار پیچیده تر می شوند.

129
00:08:52,620 --> 00:08:57,660
برای مثال GPT-3 نه دو، بلکه 175 میلیارد پارامتر دارد.

130
00:08:58,120 --> 00:09:03,875
اما نکته اینجاست، اینکه بتوانید مدلی غول پیکر با تعداد زیادی پارامتر بسازید، بدون 

131
00:09:03,875 --> 00:09:09,560
اینکه به شدت بر داده های آموزشی تطبیق داده شود یا آموزش کاملاً غیرقابل تحمل باشد.

132
00:09:10,260 --> 00:09:13,248
یادگیری عمیق دسته‌ای از مدل‌ها را توصیف می‌کند که در 

133
00:09:13,248 --> 00:09:16,180
چند دهه اخیر ثابت شده‌اند که مقیاس قابل‌توجهی دارند.

134
00:09:16,480 --> 00:09:21,223
چیزی که آنها را متحد می کند همان الگوریتم آموزشی است که به آن پس انتشار می 

135
00:09:21,223 --> 00:09:25,967
گویند، و زمینه ای که من می خواهم شما داشته باشید این است که برای اینکه این 

136
00:09:25,967 --> 00:09:31,280
الگوریتم آموزشی به خوبی در مقیاس کار کند، این مدل ها باید از یک قالب خاص پیروی کنند.

137
00:09:31,800 --> 00:09:36,180
اگر این قالب را می‌دانید، به توضیح بسیاری از انتخاب‌ها برای نحوه پردازش زبان توسط 

138
00:09:36,180 --> 00:09:40,400
ترانسفورماتور کمک می‌کند، که در غیر این صورت خطر احساس دلخواه را به دنبال دارد.

139
00:09:41,440 --> 00:09:46,740
اول، هر مدلی که می‌سازید، ورودی باید به صورت آرایه‌ای از اعداد واقعی قالب‌بندی شود.

140
00:09:46,740 --> 00:09:51,234
این می تواند به معنای لیستی از اعداد باشد، می تواند یک آرایه دو بعدی باشد، یا اغلب 

141
00:09:51,234 --> 00:09:56,000
شما با آرایه های بعدی بالاتر سروکار دارید، که در آن اصطلاح عمومی استفاده شده تانسور است.

142
00:09:56,560 --> 00:10:00,635
اغلب تصور می‌کنید که داده‌های ورودی به تدریج به لایه‌های متمایز زیادی تبدیل 

143
00:10:00,635 --> 00:10:04,604
می‌شوند، جایی که دوباره، هر لایه همیشه به عنوان نوعی آرایه از اعداد واقعی 

144
00:10:04,604 --> 00:10:08,680
ساختار می‌یابد، تا زمانی که به لایه نهایی برسید که خروجی را در نظر می‌گیرید.

145
00:10:09,280 --> 00:10:13,108
به عنوان مثال، لایه نهایی در مدل پردازش متن ما لیستی از اعداد 

146
00:10:13,108 --> 00:10:17,060
است که توزیع احتمال را برای همه نشانه های ممکن بعدی نشان می دهد.

147
00:10:17,820 --> 00:10:21,969
در یادگیری عمیق، تقریباً همیشه به این پارامترهای مدل با عنوان وزن گفته می‌شود، 

148
00:10:21,969 --> 00:10:25,908
و این به این دلیل است که یکی از ویژگی‌های کلیدی این مدل‌ها این است که تنها 

149
00:10:25,908 --> 00:10:29,900
راه تعامل این پارامترها با داده‌های در حال پردازش، از طریق جمع‌های وزنی است.

150
00:10:30,340 --> 00:10:32,478
شما همچنین برخی از توابع غیر خطی را در سراسر پاشش 

151
00:10:32,478 --> 00:10:34,360
می‌کنید، اما آنها به پارامترها بستگی ندارند.

152
00:10:35,200 --> 00:10:40,292
با این حال، به طور معمول، به جای اینکه مجموع های وزنی را کاملاً برهنه و به صراحت نوشته 

153
00:10:40,292 --> 00:10:45,327
شده ببینید، در عوض آنها را به عنوان اجزای مختلف در یک محصول بردار ماتریس بسته بندی می 

154
00:10:45,327 --> 00:10:45,620
کنید.

155
00:10:46,740 --> 00:10:50,568
اگر به نحوه عملکرد ضرب بردار ماتریس فکر کنید، هر 

156
00:10:50,568 --> 00:10:54,240
مولفه در خروجی مانند یک جمع وزنی به نظر می رسد.

157
00:10:54,780 --> 00:10:58,268
اغلب از نظر مفهومی برای من و شما فکر کردن به ماتریس هایی که 

158
00:10:58,268 --> 00:11:01,873
با پارامترهای قابل تنظیم پر شده اند و بردارهایی را که از داده 

159
00:11:01,873 --> 00:11:05,420
های در حال پردازش گرفته شده اند را تغییر می دهند، تمیزتر است.

160
00:11:06,340 --> 00:11:14,160
برای مثال، آن 175 میلیارد وزن در GPT-3 در کمتر از 28000 ماتریس مجزا سازماندهی شده است.

161
00:11:14,660 --> 00:11:18,546
آن ماتریس‌ها به نوبه خود در هشت دسته مختلف قرار می‌گیرند، و کاری که من و شما می‌خواهیم 

162
00:11:18,546 --> 00:11:22,387
انجام دهیم این است که از هر یک از آن دسته‌ها عبور کنیم تا بفهمیم آن نوع چه کاری انجام 

163
00:11:22,387 --> 00:11:22,700
می‌دهد.

164
00:11:23,160 --> 00:11:27,260
همانطور که در حال پیشروی هستیم، فکر می‌کنم ارجاع به اعداد خاص از 

165
00:11:27,260 --> 00:11:31,360
GPT-3 برای شمارش دقیقاً از کجا این 175 میلیارد می‌آیند، جالب است.

166
00:11:31,880 --> 00:11:36,310
حتی اگر امروزه مدل های بزرگتر و بهتری وجود داشته باشد، این مدل به عنوان 

167
00:11:36,310 --> 00:11:40,740
مدل زبان بزرگ جذابیت خاصی دارد تا توجه جهان را خارج از جوامع ML جلب کند.

168
00:11:41,440 --> 00:11:44,006
همچنین، از نظر عملی، شرکت‌ها تمایل دارند برای 

169
00:11:44,006 --> 00:11:46,740
شبکه‌های مدرن‌تر، اعداد خاص را محکم‌تر نگه دارند.

170
00:11:47,360 --> 00:11:50,737
من فقط می‌خواهم صحنه‌ای را تنظیم کنم که وقتی به زیر کاپوت نگاه 

171
00:11:50,737 --> 00:11:54,008
می‌کنید تا ببینید در ابزاری مانند ChatGPT چه اتفاقی می‌افتد، 

172
00:11:54,008 --> 00:11:57,440
تقریباً تمام محاسبات واقعی شبیه ضرب برداری ماتریس به نظر می‌رسد.

173
00:11:57,900 --> 00:12:02,458
کمی خطر گم شدن در دریای میلیاردها عدد وجود دارد، اما باید در ذهن خود 

174
00:12:02,458 --> 00:12:07,017
تمایز بسیار واضحی بین وزن مدل، که من همیشه آبی یا قرمز رنگ می‌کنم، و 

175
00:12:07,017 --> 00:12:11,840
داده‌های موجود در نظر بگیرید. پردازش شده، که من همیشه خاکستری رنگ می کنم.

176
00:12:12,180 --> 00:12:14,964
وزنه ها مغز واقعی هستند، چیزهایی هستند که در طول 

177
00:12:14,964 --> 00:12:17,920
تمرین یاد می گیرند و نحوه رفتار آن را تعیین می کنند.

178
00:12:18,280 --> 00:12:22,453
داده‌های در حال پردازش به سادگی هر ورودی خاصی را که برای یک اجرای 

179
00:12:22,453 --> 00:12:26,500
مشخص به مدل وارد می‌شود، رمزگذاری می‌کند، مانند نمونه‌ای از متن.

180
00:12:27,480 --> 00:12:31,898
با همه اینها به عنوان پایه، بیایید به اولین مرحله از این مثال پردازش متن بپردازیم، که 

181
00:12:31,898 --> 00:12:36,420
این است که ورودی را به تکه های کوچک تقسیم می کنیم و آن تکه ها را به بردار تبدیل می کنیم.

182
00:12:37,020 --> 00:12:40,866
من اشاره کردم که چگونه به آن تکه‌ها نشانه می‌گویند، که ممکن است تکه‌های 

183
00:12:40,866 --> 00:12:44,446
کلمه یا نقطه‌گذاری باشند، اما هرازگاهی در این فصل و به خصوص در فصل 

184
00:12:44,446 --> 00:12:48,080
بعدی، می‌خواهم وانمود کنم که به طور واضح‌تری به کلمات تبدیل شده است.

185
00:12:48,600 --> 00:12:51,317
از آنجایی که ما انسان‌ها با کلمات فکر می‌کنیم، این کار ارجاع 

186
00:12:51,317 --> 00:12:54,080
به مثال‌های کوچک و شفاف‌سازی هر مرحله را بسیار آسان‌تر می‌کند.

187
00:12:55,260 --> 00:12:59,318
این مدل دارای یک واژگان از پیش تعریف شده است، فهرستی از تمام کلمات 

188
00:12:59,318 --> 00:13:03,559
ممکن، مثلاً 50000 مورد از آنها، و اولین ماتریسی که با آن مواجه خواهیم 

189
00:13:03,559 --> 00:13:07,800
شد، معروف به ماتریس جاسازی، دارای یک ستون برای هر یک از این کلمات است.

190
00:13:08,940 --> 00:13:13,760
این ستون‌ها هستند که تعیین می‌کنند هر کلمه در مرحله اول به کدام بردار تبدیل می‌شود.

191
00:13:15,100 --> 00:13:18,599
ما آن را برچسب گذاری می کنیم، و مانند همه ماتریس هایی که می بینیم، 

192
00:13:18,599 --> 00:13:22,360
مقادیر آن به صورت تصادفی شروع می شوند، اما بر اساس داده ها یاد می گیرند.

193
00:13:23,620 --> 00:13:27,824
تبدیل کلمات به بردار مدت‌ها قبل از ترانسفورماتورها تمرین رایج در یادگیری ماشینی 

194
00:13:27,824 --> 00:13:31,870
بود، اما اگر قبلاً آن را ندیده‌اید کمی عجیب است و پایه‌ای را برای هر چیزی که 

195
00:13:31,870 --> 00:13:35,760
در ادامه می‌آید ایجاد می‌کند، بنابراین اجازه دهید لحظه‌ای با آن آشنا شویم.

196
00:13:36,040 --> 00:13:39,699
ما اغلب این تعبیه را یک کلمه می نامیم که شما را دعوت می کند تا به این 

197
00:13:39,699 --> 00:13:43,620
بردارها به صورت بسیار هندسی به عنوان نقاطی در فضایی با ابعاد بالا فکر کنید.

198
00:13:44,180 --> 00:13:47,894
تجسم یک لیست از سه عدد به عنوان مختصات برای نقاط در فضای سه بعدی 

199
00:13:47,894 --> 00:13:51,780
مشکلی نخواهد بود، اما جاسازی کلمات معمولا ابعاد بسیار بالاتری دارند.

200
00:13:52,280 --> 00:13:56,165
در GPT-3 آنها 12288 بعد دارند و همانطور که خواهید 

201
00:13:56,165 --> 00:14:00,440
دید، کار در فضایی که جهت های متمایز زیادی دارد مهم است.

202
00:14:01,180 --> 00:14:05,843
همانطور که شما می توانید یک برش دو بعدی را از یک فضای سه بعدی بردارید و 

203
00:14:05,843 --> 00:14:10,765
تمام نقاط را روی آن برش قرار دهید، به خاطر متحرک سازی کلماتی که یک مدل ساده 

204
00:14:10,765 --> 00:14:15,428
به من می دهد، من یک کار مشابه انجام می دهم. با انتخاب یک برش سه بعدی از 

205
00:14:15,428 --> 00:14:20,480
میان این فضای ابعادی بسیار بالا و نمایش بردارهای کلمه بر روی آن و نمایش نتایج.

206
00:14:21,280 --> 00:14:25,754
ایده بزرگ در اینجا این است که وقتی یک مدل وزن‌های خود را تغییر می‌دهد و تنظیم می‌کند 

207
00:14:25,754 --> 00:14:30,070
تا تعیین کند کلمات دقیقاً چگونه به‌عنوان بردار در طول آموزش جاسازی می‌شوند، تمایل 

208
00:14:30,070 --> 00:14:34,440
دارد روی مجموعه‌ای از جاسازی‌ها قرار گیرد که جهت‌ها در فضا نوعی معنای معنایی دارند.

209
00:14:34,980 --> 00:14:38,706
برای مدل ساده کلمه به برداری که من در اینجا اجرا می کنم، اگر همه کلماتی 

210
00:14:38,706 --> 00:14:42,329
را که جاسازی آنها به برج نزدیکتر است را جستجو کنم، متوجه خواهید شد که 

211
00:14:42,329 --> 00:14:45,900
چگونه به نظر می رسد که همه آنها حال و هوای بسیار شبیه به برج می دهند.

212
00:14:46,340 --> 00:14:48,838
و اگر می خواهید پایتون را بالا بکشید و در خانه بازی کنید، 

213
00:14:48,838 --> 00:14:51,380
این مدل خاصی است که من برای ساخت انیمیشن ها استفاده می کنم.

214
00:14:51,620 --> 00:14:54,713
این یک ترانسفورماتور نیست، اما برای نشان دادن این ایده کافی 

215
00:14:54,713 --> 00:14:57,600
است که جهت ها در فضا می توانند معنای معنایی را حمل کنند.

216
00:14:58,300 --> 00:15:03,310
یک مثال بسیار کلاسیک در این مورد این است که اگر تفاوت بین بردارهای زن و مرد 

217
00:15:03,310 --> 00:15:08,321
را در نظر بگیرید، چیزی که به عنوان یک بردار کوچک که نوک یکی را به نوک دیگری 

218
00:15:08,321 --> 00:15:13,200
متصل می کند، تجسم می کنید، بسیار شبیه به تفاوت بین پادشاه و مرد است. ملکه.

219
00:15:15,080 --> 00:15:20,104
بنابراین فرض کنید کلمه پادشاه زن را نمی‌دانستید، می‌توانید با گرفتن پادشاه، 

220
00:15:20,104 --> 00:15:25,460
اضافه کردن این جهت زن-مرد، و جستجوی جاسازی‌های نزدیک به آن نقطه، آن را پیدا کنید.

221
00:15:27,000 --> 00:15:28,200
حداقل یه جورایی

222
00:15:28,480 --> 00:15:32,562
علیرغم اینکه این یک نمونه کلاسیک برای مدلی است که من با آن بازی می کنم، تعبیه 

223
00:15:32,562 --> 00:15:36,540
واقعی ملکه در واقع کمی دورتر از آن چیزی است که نشان می دهد، احتمالاً به این 

224
00:15:36,540 --> 00:15:40,780
دلیل که نحوه استفاده از ملکه در آموزش داده ها صرفاً یک نسخه زنانه از پادشاه نیست.

225
00:15:41,620 --> 00:15:43,382
وقتی در اطراف بازی می کردم، به نظر می رسید که 

226
00:15:43,382 --> 00:15:45,260
روابط خانوادگی این ایده را خیلی بهتر نشان می دهد.

227
00:15:46,340 --> 00:15:50,557
نکته این است که به نظر می‌رسد در طول آموزش، مدل انتخاب جاسازی‌هایی 

228
00:15:50,557 --> 00:15:54,900
را سودمند می‌داند که یک جهت در این فضا اطلاعات جنسیت را رمزگذاری کند.

229
00:15:56,800 --> 00:16:02,337
مثال دیگر این است که اگر تعبیه ایتالیا را بگیرید و جاسازی آلمان را کم کنید و 

230
00:16:02,337 --> 00:16:08,090
آن را به تعبیه هیتلر اضافه کنید، چیزی بسیار نزدیک به تعبیه موسولینی خواهید داشت.

231
00:16:08,570 --> 00:16:12,058
گویی مدل یاد گرفته است که برخی از جهت ها را با ایتالیایی 

232
00:16:12,058 --> 00:16:15,670
بودن و برخی دیگر را با رهبران محور جنگ جهانی دوم مرتبط کند.

233
00:16:16,470 --> 00:16:21,350
شاید مثال مورد علاقه من در این زمینه این باشد که چگونه در برخی مدل ها، اگر تفاوت آلمان 

234
00:16:21,350 --> 00:16:26,230
و ژاپن را در نظر بگیرید و آن را به سوشی اضافه کنید، در نهایت به براتورست نزدیک می شوید.

235
00:16:27,350 --> 00:16:30,512
همچنین در انجام این بازی یافتن نزدیکترین همسایگان، از 

236
00:16:30,512 --> 00:16:33,850
اینکه دیدم کت چقدر به هیولا و هیولا نزدیک است خوشحال شدم.

237
00:16:34,690 --> 00:16:39,295
یک بیت از شهود ریاضی که در نظر گرفتن آن، به ویژه برای فصل بعدی مفید است، این است که چگونه 

238
00:16:39,295 --> 00:16:43,850
حاصل ضرب نقطه ای دو بردار را می توان راهی برای اندازه گیری میزان همسویی آنها در نظر گرفت.

239
00:16:44,870 --> 00:16:49,422
از نظر محاسباتی، محصولات نقطه‌ای شامل ضرب تمام اجزای مربوطه و سپس اضافه کردن 

240
00:16:49,422 --> 00:16:54,330
نتایج است که خوب است، زیرا بسیاری از محاسبات ما باید مانند مجموع وزنی به نظر برسند.

241
00:16:55,190 --> 00:17:00,471
از نظر هندسی، حاصلضرب نقطه زمانی مثبت است که بردارها در جهات مشابه باشند، 

242
00:17:00,471 --> 00:17:05,609
اگر بردارها عمود باشند صفر است و هرگاه در جهت مخالف قرار گیرند منفی است.

243
00:17:06,550 --> 00:17:11,936
برای مثال، فرض کنید با این مدل بازی می‌کردید، و فرض می‌کنید که تعبیه 

244
00:17:11,936 --> 00:17:17,010
گربه‌ها منهای گربه ممکن است نوعی جهت کثرت را در این فضا نشان دهد.

245
00:17:17,430 --> 00:17:21,812
برای آزمایش این، من می‌خواهم این بردار را بگیرم و حاصل ضرب نقطه‌ای آن را در برابر 

246
00:17:21,812 --> 00:17:26,462
تعبیه‌های اسم‌های مفرد خاص محاسبه کنم و آن را با محصولات نقطه‌ای با اسم‌های جمع مربوطه 

247
00:17:26,462 --> 00:17:27,050
مقایسه کنم.

248
00:17:27,270 --> 00:17:30,089
اگر با آن بازی کنید، متوجه خواهید شد که در واقع به نظر می 

249
00:17:30,089 --> 00:17:32,958
رسد که موارد جمع به طور مداوم مقادیر بالاتری نسبت به موارد 

250
00:17:32,958 --> 00:17:36,070
مفرد می دهند، که نشان می دهد آنها بیشتر با این جهت هماهنگ هستند.

251
00:17:37,070 --> 00:17:40,998
همچنین جالب است که اگر این محصول نقطه‌ای را با جاسازی کلمات 1، 2، 3 

252
00:17:40,998 --> 00:17:44,985
و غیره بگیرید، مقادیر فزاینده‌ای می‌دهند، بنابراین گویی می‌توانیم به 

253
00:17:44,985 --> 00:17:49,030
صورت کمی اندازه گیری کنیم که مدل یک کلمه داده شده را چقدر جمع می‌یابد.

254
00:17:50,250 --> 00:17:53,570
باز هم، جزئیات نحوه جاسازی کلمات با استفاده از داده ها آموخته می شود.

255
00:17:54,050 --> 00:17:56,750
این ماتریس جاسازی، که ستون‌های آن به ما می‌گویند برای 

256
00:17:56,750 --> 00:17:59,550
هر کلمه چه اتفاقی می‌افتد، اولین توده وزن در مدل ما است.

257
00:18:00,030 --> 00:18:04,754
با استفاده از اعداد GPT-3، اندازه واژگان به طور خاص 50257 است، و 

258
00:18:04,754 --> 00:18:09,770
باز هم، از نظر فنی این شامل کلمات فی نفسه نیست، بلکه از نشانه‌ها است.

259
00:18:10,630 --> 00:18:17,790
بعد تعبیه شده 12288 است و ضرب آن ها به ما می گوید که این شامل حدود 617 میلیون وزن است.

260
00:18:18,250 --> 00:18:21,030
بیایید جلوتر برویم و این را به آمار جاری اضافه کنیم، به 

261
00:18:21,030 --> 00:18:23,810
یاد داشته باشیم که تا پایان باید تا 175 میلیارد بشماریم.

262
00:18:25,430 --> 00:18:28,826
در مورد ترانسفورماتورها، شما واقعاً می خواهید بردارهای موجود در این فضای 

263
00:18:28,826 --> 00:18:32,130
تعبیه شده را به گونه ای تصور کنید که صرفاً بیانگر کلمات جداگانه نیستند.

264
00:18:32,550 --> 00:18:36,129
برای یک چیز، آنها همچنین اطلاعات مربوط به موقعیت آن کلمه را رمزگذاری 

265
00:18:36,129 --> 00:18:39,397
می کنند، که بعداً در مورد آن صحبت خواهیم کرد، اما مهمتر از آن، 

266
00:18:39,397 --> 00:18:42,770
شما باید آنها را به عنوان ظرفیت غوطه ور شدن در متن در نظر بگیرید.

267
00:18:43,350 --> 00:18:48,772
برای مثال، برداری که زندگی خود را با تعبیه کلمه پادشاه آغاز کرده است، ممکن است به تدریج 

268
00:18:48,772 --> 00:18:53,947
توسط بلوک های مختلف در این شبکه کشیده و کشیده شود، به طوری که در پایان در جهت بسیار 

269
00:18:53,947 --> 00:18:57,582
خاص و ظریف تری قرار گیرد که به نوعی آن را رمزگذاری می کند. 

270
00:18:57,582 --> 00:19:02,881
پادشاهی بود که در اسکاتلند زندگی می کرد و پس از قتل پادشاه قبلی به مقام خود رسیده بود 

271
00:19:02,881 --> 00:19:04,730
و به زبان شکسپیر توصیف می شود.

272
00:19:05,210 --> 00:19:07,790
به درک خود از یک کلمه خاص فکر کنید.

273
00:19:08,250 --> 00:19:13,335
معنای آن کلمه به وضوح توسط محیط اطراف مشخص می شود، و گاهی اوقات این شامل زمینه از فاصله 

274
00:19:13,335 --> 00:19:18,304
دور می شود، بنابراین در کنار هم قرار دادن مدلی که توانایی پیش بینی کلمه بعدی را داشته 

275
00:19:18,304 --> 00:19:23,390
باشد، هدف این است که به نحوی آن را توانمند سازیم تا زمینه را در خود جای دهد. به طور موثر

276
00:19:24,050 --> 00:19:28,146
برای روشن بودن، در اولین قدم، وقتی آرایه بردارها را بر اساس متن ورودی ایجاد 

277
00:19:28,146 --> 00:19:32,242
می‌کنید، هر یک از آن‌ها به سادگی از ماتریس جاسازی خارج می‌شوند، بنابراین در 

278
00:19:32,242 --> 00:19:36,770
ابتدا هر یک فقط می‌تواند معنای یک کلمه را بدون رمزگذاری کند. هر ورودی از محیط اطرافش

279
00:19:37,710 --> 00:19:41,317
اما شما باید هدف اصلی این شبکه را که از طریق آن جریان می‌یابد این 

280
00:19:41,317 --> 00:19:45,034
است که هر یک از آن بردارها را قادر می‌سازد تا معنایی را جذب کنند که 

281
00:19:45,034 --> 00:19:48,970
بسیار غنی‌تر و خاص‌تر از آن چیزی است که کلمات منفرد می‌توانند نشان دهند.

282
00:19:49,510 --> 00:19:51,883
شبکه فقط می تواند تعداد ثابتی از بردارها را در یک زمان 

283
00:19:51,883 --> 00:19:54,170
پردازش کند که به عنوان اندازه زمینه آن شناخته می شود.

284
00:19:54,510 --> 00:19:59,793
برای GPT-3 با اندازه زمینه 2048 آموزش داده شد، بنابراین داده های جریان یافته از 

285
00:19:59,793 --> 00:20:05,010
طریق شبکه همیشه شبیه این آرایه از 2048 ستون است که هر کدام دارای 12000 بعد است.

286
00:20:05,590 --> 00:20:08,910
این اندازه زمینه، مقدار متنی را که ترانسفورماتور می‌تواند 

287
00:20:08,910 --> 00:20:11,830
در هنگام پیش‌بینی کلمه بعدی وارد کند، محدود می‌کند.

288
00:20:12,370 --> 00:20:17,153
به همین دلیل است که مکالمات طولانی با ربات‌های چت خاص، مانند نسخه‌های اولیه ChatGPT، 

289
00:20:17,153 --> 00:20:22,050
اغلب این احساس را به وجود می‌آورد که با ادامه طولانی مدت، رشته مکالمه را از دست می‌دهد.

290
00:20:23,030 --> 00:20:25,788
ما در زمان مناسب به جزئیات توجه خواهیم پرداخت، اما از جلوتر می 

291
00:20:25,788 --> 00:20:28,810
گذرم، می خواهم یک دقیقه در مورد آنچه در پایان اتفاق می افتد صحبت کنم.

292
00:20:29,450 --> 00:20:32,183
به یاد داشته باشید، خروجی مورد نظر یک توزیع احتمال بر روی 

293
00:20:32,183 --> 00:20:34,870
تمام نشانه هایی است که ممکن است در مرحله بعدی قرار گیرند.

294
00:20:35,170 --> 00:20:40,304
به عنوان مثال، اگر آخرین کلمه پروفسور باشد، و متن شامل کلماتی مانند هری پاتر باشد، و 

295
00:20:40,304 --> 00:20:45,439
بلافاصله قبل از آن حداقل معلم مورد علاقه را می بینیم، و همچنین اگر به من اجازه بدهید 

296
00:20:45,439 --> 00:20:50,574
وانمود کنم که نشانه ها به سادگی مانند کلمات کامل هستند، پس یک شبکه کاملاً آموزش دیده 

297
00:20:50,574 --> 00:20:55,830
که دانش هری پاتر را ایجاد کرده بود، احتمالاً عدد بالایی را به کلمه اسنیپ اختصاص می‌داد.

298
00:20:56,510 --> 00:20:57,970
این شامل دو مرحله متفاوت است.

299
00:20:58,310 --> 00:21:02,926
اولین مورد استفاده از ماتریس دیگری است که آخرین بردار را در آن زمینه 

300
00:21:02,926 --> 00:21:07,610
به لیستی از 50000 مقدار، یک عدد برای هر نشانه در واژگان، ترسیم می کند.

301
00:21:08,170 --> 00:21:13,171
سپس تابعی وجود دارد که این را به یک توزیع احتمال نرمال می کند، آن را Softmax می نامند و 

302
00:21:13,171 --> 00:21:18,230
در عرض یک ثانیه بیشتر در مورد آن صحبت خواهیم کرد، اما قبل از آن ممکن است کمی عجیب به نظر 

303
00:21:18,230 --> 00:21:23,345
برسد که فقط از آخرین جاسازی برای پیش بینی استفاده کنیم، زمانی که به هر حال در آخرین مرحله 

304
00:21:23,345 --> 00:21:28,290
هزاران بردار دیگر در لایه وجود دارد که با معانی غنی از زمینه خاص خود در آنجا نشسته اند.

305
00:21:28,930 --> 00:21:32,710
این به این واقعیت مربوط می شود که در فرآیند آموزش اگر از هر یک 

306
00:21:32,710 --> 00:21:36,490
از آن بردارها در لایه نهایی استفاده کنید تا به طور همزمان برای 

307
00:21:36,490 --> 00:21:40,270
آنچه بلافاصله پس از آن می آید پیش بینی کنید بسیار کارآمدتر است.

308
00:21:40,970 --> 00:21:43,010
در مورد تمرینات بعداً چیزهای بیشتری برای گفتن وجود 

309
00:21:43,010 --> 00:21:45,090
دارد، اما من فقط می‌خواهم همین الان به آن اشاره کنم.

310
00:21:45,730 --> 00:21:49,690
این ماتریس ماتریس Unembedding نام دارد و ما به آن برچسب WU می دهیم.

311
00:21:50,210 --> 00:21:52,946
باز هم، مانند همه ماتریس‌های وزنی که می‌بینیم، ورودی‌های آن 

312
00:21:52,946 --> 00:21:55,910
به‌طور تصادفی شروع می‌شوند، اما در طول فرآیند آموزش یاد می‌گیرند.

313
00:21:56,470 --> 00:22:01,144
با حفظ امتیاز در تعداد کل پارامترهای ما، این ماتریس Unembedding دارای یک ردیف برای 

314
00:22:01,144 --> 00:22:05,650
هر کلمه در واژگان است، و هر ردیف دارای همان تعداد عناصر به عنوان بعد جاسازی است.

315
00:22:06,410 --> 00:22:11,385
این بسیار شبیه به ماتریس جاسازی است، فقط با ترتیب تعویض، بنابراین 617 میلیون 

316
00:22:11,385 --> 00:22:16,426
پارامتر دیگر به شبکه اضافه می کند، به این معنی که تعداد ما تا کنون کمی بیش از 

317
00:22:16,426 --> 00:22:21,790
یک میلیارد است، کسری کوچک اما نه کاملاً ناچیز از 175 میلیارد ما. در نهایت با در کل.

318
00:22:22,550 --> 00:22:26,679
به عنوان آخرین درس کوچک برای این فصل، می‌خواهم در مورد این تابع softmax بیشتر صحبت 

319
00:22:26,679 --> 00:22:30,610
کنم، زیرا زمانی که در بلوک‌های توجه فرو رفتیم، ظاهر دیگری برای ما ایجاد می‌کند.

320
00:22:31,430 --> 00:22:35,641
ایده این است که اگر می‌خواهید دنباله‌ای از اعداد به‌عنوان توزیع 

321
00:22:35,641 --> 00:22:39,918
احتمال عمل کند، توزیعی را بر روی همه کلمات ممکن بعدی بگویید، سپس 

322
00:22:39,918 --> 00:22:44,590
هر مقدار باید بین 0 و 1 باشد و همچنین باید همه آنها را به 1 جمع کنید. .

323
00:22:45,250 --> 00:22:48,353
با این حال، اگر در حال بازی یادگیری هستید که در آن هر کاری که 

324
00:22:48,353 --> 00:22:51,456
انجام می‌دهید شبیه ضرب ماتریس-بردار به نظر می‌رسد، خروجی‌هایی 

325
00:22:51,456 --> 00:22:54,810
که به‌طور پیش‌فرض دریافت می‌کنید اصلاً از این موضوع پیروی نمی‌کنند.

326
00:22:55,330 --> 00:22:59,870
مقادیر اغلب منفی یا بسیار بزرگتر از 1 هستند و تقریباً به طور قطع با 1 جمع نمی شوند.

327
00:23:00,510 --> 00:23:05,900
Softmax روشی استاندارد برای تبدیل لیست دلخواه اعداد به یک توزیع معتبر است، 

328
00:23:05,900 --> 00:23:11,290
به گونه ای که بزرگترین مقادیر به 1 نزدیکتر و مقادیر کوچکتر به 0 نزدیک شوند.

329
00:23:11,830 --> 00:23:13,070
این تمام چیزی است که واقعاً باید بدانید.

330
00:23:13,090 --> 00:23:17,096
اما اگر کنجکاو هستید، روش کار به این صورت است که ابتدا e را به توان 

331
00:23:17,096 --> 00:23:21,280
هر یک از اعداد افزایش دهید، به این معنی که اکنون فهرستی از مقادیر مثبت 

332
00:23:21,280 --> 00:23:25,522
دارید، و سپس می توانید مجموع همه آن مقادیر مثبت را بگیرید و تقسیم کنید. 

333
00:23:25,522 --> 00:23:29,470
هر عبارت با آن مجموع، که آن را به لیستی عادی می کند که به 1 می رسد.

334
00:23:30,170 --> 00:23:34,174
متوجه خواهید شد که اگر یکی از اعداد ورودی به طور معنی‌داری بزرگ‌تر از 

335
00:23:34,174 --> 00:23:38,236
بقیه باشد، در خروجی عبارت مربوطه بر توزیع غالب است، بنابراین اگر از آن 

336
00:23:38,236 --> 00:23:42,470
نمونه‌برداری می‌کردید تقریباً مطمئناً فقط ورودی حداکثر را انتخاب می‌کردید.

337
00:23:42,990 --> 00:23:46,894
اما این نرم‌تر از انتخاب حداکثر است، به این معنا که وقتی مقادیر دیگر به 

338
00:23:46,894 --> 00:23:50,745
طور مشابه بزرگ هستند، وزن معنی‌داری در توزیع نیز دریافت می‌کنند، و همه 

339
00:23:50,745 --> 00:23:54,650
چیز به‌طور پیوسته تغییر می‌کند، زیرا شما مدام ورودی‌ها را تغییر می‌دهید.

340
00:23:55,130 --> 00:23:59,654
در برخی موقعیت‌ها، مانند زمانی که ChatGPT از این توزیع برای ایجاد 

341
00:23:59,654 --> 00:24:04,316
کلمه بعدی استفاده می‌کند، با افزودن کمی ادویه اضافی به این تابع، با 

342
00:24:04,316 --> 00:24:08,910
یک t ثابت در مخرج آن نماگرها، فضای کمی برای سرگرمی بیشتر وجود دارد.

343
00:24:09,550 --> 00:24:14,248
ما آن را دما می نامیم، زیرا به طور مبهم به نقش دما در معادلات ترمودینامیک 

344
00:24:14,248 --> 00:24:18,884
خاص شباهت دارد، و اثر این است که وقتی t بزرگتر است، وزن بیشتری به مقادیر 

345
00:24:18,884 --> 00:24:23,582
پایین تر می دهیم، به این معنی که توزیع کمی یکنواخت تر است، و اگر t کوچکتر 

346
00:24:23,582 --> 00:24:28,218
است، سپس مقادیر بزرگتر با شدت بیشتری تسلط خواهند داشت، جایی که در نهایت، 

347
00:24:28,218 --> 00:24:32,790
تنظیم t برابر با صفر به این معنی است که تمام وزن به مقدار حداکثر می رود.

348
00:24:33,470 --> 00:24:38,103
به عنوان مثال، من باید GPT-3 یک داستان با متن seed تولید کند، یک 

349
00:24:38,103 --> 00:24:42,950
زمانی A وجود داشت، اما در هر مورد از دماهای مختلف استفاده خواهم کرد.

350
00:24:43,630 --> 00:24:47,845
دمای صفر به این معنی است که همیشه با قابل پیش بینی ترین کلمه مطابقت 

351
00:24:47,845 --> 00:24:52,370
دارد و چیزی که به دست می آورید در نهایت یک مشتق معمولی از Goldilocks است.

352
00:24:53,010 --> 00:24:57,910
دمای بالاتر به آن فرصتی می دهد تا کلمات کمتر محتمل را انتخاب کند، اما با خطر همراه است.

353
00:24:58,230 --> 00:25:02,057
در این مورد، داستان بیشتر از ابتدا در مورد یک هنرمند وب جوان 

354
00:25:02,057 --> 00:25:06,010
از کره جنوبی شروع می شود، اما به سرعت به یک مزخرف تبدیل می شود.

355
00:25:06,950 --> 00:25:10,830
از نظر فنی، API در واقع به شما اجازه نمی دهد دمایی بالاتر از 2 را انتخاب کنید.

356
00:25:11,170 --> 00:25:15,126
هیچ دلیل ریاضی برای این وجود ندارد، این فقط یک محدودیت دلخواه است که برای 

357
00:25:15,126 --> 00:25:19,350
اینکه ابزار آنها دیده نشود که چیزهای بیش از حد مزخرف تولید می کند اعمال می شود.

358
00:25:19,870 --> 00:25:24,275
بنابراین اگر کنجکاو هستید، نحوه عملکرد این انیمیشن به این صورت است که من از 

359
00:25:24,275 --> 00:25:28,680
20 توکن بعدی که GPT-3 تولید می‌کند استفاده می‌کنم، که به نظر می‌رسد حداکثری 

360
00:25:28,680 --> 00:25:32,970
است که به من می‌دهد، و سپس بر اساس احتمالات را تغییر می‌دهم. روی توان 1 5.

361
00:25:33,130 --> 00:25:37,470
به عنوان یک اصطلاح دیگر، به همان روشی که ممکن است اجزای خروجی این تابع را 

362
00:25:37,470 --> 00:25:41,634
احتمالات نامید، مردم اغلب ورودی ها را به عنوان logit می گویند، یا برخی 

363
00:25:41,634 --> 00:25:46,150
افراد می گویند logit، برخی افراد می گویند logits، من می خواهم بگویم logits. .

364
00:25:46,530 --> 00:25:51,280
بنابراین، برای مثال، هنگامی که در متنی تغذیه می‌کنید، همه این جاسازی‌های کلمه در شبکه 

365
00:25:51,280 --> 00:25:56,142
جریان دارند، و این ضرب نهایی را با ماتریس بدون تعبیه انجام می‌دهید، افراد یادگیری ماشین 

366
00:25:56,142 --> 00:26:00,119
اجزای موجود در آن خروجی خام و غیرعادی را به‌عنوان logits معرفی می‌کنند. 

367
00:26:00,119 --> 00:26:01,390
برای پیش بینی کلمه بعدی

368
00:26:03,330 --> 00:26:10,370
هدف بسیاری از این فصل، پایه‌ریزی برای درک مکانیسم توجه، سبک کاراته کید واکس روی موم بود.

369
00:26:10,850 --> 00:26:16,016
می بینید، اگر شهود قوی برای جاسازی کلمات، برای سافت مکس، برای اینکه چگونه محصولات 

370
00:26:16,016 --> 00:26:21,246
نقطه ای شباهت را اندازه گیری می کنند، و همچنین این فرض اساسی که بیشتر محاسبات باید 

371
00:26:21,246 --> 00:26:26,665
شبیه ضرب ماتریس با ماتریس های پر از پارامترهای قابل تنظیم باشند، داشته باشید، پس توجه 

372
00:26:26,665 --> 00:26:32,210
را درک کنید. مکانیزم، این قطعه سنگ بنای کل رونق مدرن در هوش مصنوعی، باید نسبتا صاف باشد.

373
00:26:32,650 --> 00:26:34,510
برای آن، در فصل بعدی به من بپیوندید.

374
00:26:36,390 --> 00:26:38,848
همانطور که من این را منتشر می کنم، پیش نویس آن فصل 

375
00:26:38,848 --> 00:26:41,210
بعدی برای بررسی توسط حامیان Patreon در دسترس است.

376
00:26:41,770 --> 00:26:44,570
نسخه نهایی باید در عرض یک یا دو هفته در معرض دید عموم قرار 

377
00:26:44,570 --> 00:26:47,370
گیرد، معمولا بستگی به میزان تغییر من بر اساس آن بررسی دارد.

378
00:26:47,810 --> 00:26:50,110
در این میان، اگر می‌خواهید توجه را جلب کنید، و 

379
00:26:50,110 --> 00:26:52,410
اگر می‌خواهید کمی به کانال کمک کنید، منتظر است.

