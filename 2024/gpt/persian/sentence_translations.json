[
 {
  "translatedText": "حروف اول GPT مخفف Generative Pretrained Transformer است.",
  "input": "The initials GPT stand for Generative Pretrained Transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.56
 },
 {
  "translatedText": "بنابراین اولین کلمه به اندازه کافی ساده است، این ربات ها هستند که متن جدیدی تولید می کنند.",
  "input": "So that first word is straightforward enough, these are bots that generate new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.22,
  "end": 9.02
 },
 {
  "translatedText": "Pretrained به نحوه گذراندن مدل از طریق یک فرآیند یادگیری از حجم عظیمی از داده ها اشاره دارد، و پیشوند القا می کند که فضای بیشتری برای تنظیم دقیق آن در وظایف خاص با آموزش اضافی وجود دارد.",
  "input": "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.8,
  "end": 20.04
 },
 {
  "translatedText": "اما آخرین کلمه، این قطعه کلیدی واقعی است.",
  "input": "But the last word, that's the real key piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.72,
  "end": 22.9
 },
 {
  "translatedText": "ترانسفورماتور نوع خاصی از شبکه عصبی، یک مدل یادگیری ماشینی است و اختراع اصلی زیربنای رونق فعلی در هوش مصنوعی است.",
  "input": "A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.38,
  "end": 31.0
 },
 {
  "translatedText": "کاری که من می‌خواهم با این ویدیو و فصل‌های بعدی انجام دهم، توضیحی مبتنی بر بصری برای آنچه که واقعاً در داخل یک ترانسفورماتور اتفاق می‌افتد، است.",
  "input": "What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.74,
  "end": 39.12
 },
 {
  "translatedText": "ما داده‌هایی را که از طریق آن جریان می‌یابد دنبال می‌کنیم و قدم به قدم پیش می‌رویم.",
  "input": "We're going to follow the data that flows through it and go step by step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.7,
  "end": 42.82
 },
 {
  "translatedText": "انواع مختلفی از مدل ها وجود دارد که می توانید با استفاده از ترانسفورماتور بسازید.",
  "input": "There are many different kinds of models that you can build using transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.44,
  "end": 47.38
 },
 {
  "translatedText": "برخی از مدل ها صدا را دریافت می کنند و رونوشت تولید می کنند.",
  "input": "Some models take in audio and produce a transcript.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.8,
  "end": 50.8
 },
 {
  "translatedText": "این جمله از مدلی می آید که برعکس می شود و گفتار ترکیبی را فقط از متن تولید می کند.",
  "input": "This sentence comes from a model going the other way around, producing synthetic speech just from text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 56.22
 },
 {
  "translatedText": "همه آن ابزارهایی که در سال 2022 جهان را طوفان کردند مانند Dolly و Midjourney که توضیحات متنی را می گیرند و یک تصویر تولید می کنند بر اساس ترانسفورماتور هستند.",
  "input": "All those tools that took the world by storm in 2022 like Dolly and Midjourney that take in a text description and produce an image are based on transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.66,
  "end": 65.52
 },
 {
  "translatedText": "حتی اگر نتوانم کاملاً بفهمم که یک موجود پای قرار است چه چیزی باشد، باز هم از این که چنین چیزی حتی از راه دور امکان پذیر است، شگفت زده هستم.",
  "input": "Even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.0,
  "end": 73.1
 },
 {
  "translatedText": "و ترانسفورماتور اصلی که در سال 2017 توسط گوگل معرفی شد برای استفاده خاص از ترجمه متن از یک زبان به زبان دیگر اختراع شد.",
  "input": "And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.9,
  "end": 82.1
 },
 {
  "translatedText": "اما گونه‌ای که من و شما روی آن تمرکز خواهیم کرد، که زیربنای ابزارهایی مانند ChatGPT است، مدلی است که برای گرفتن یک قطعه متن، حتی با برخی از تصاویر اطراف یا صدای همراه آن، آموزش دیده و یک پیش‌بینی ایجاد می‌کند. برای آنچه در قسمت بعدی آمده است.",
  "input": "But the variant that you and I will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.66,
  "end": 98.26
 },
 {
  "translatedText": "این پیش‌بینی به شکل توزیع احتمال بر روی بسیاری از تکه‌های متنی که ممکن است به دنبال آن باشد، می‌شود.",
  "input": "That prediction takes the form of a probability distribution over many different chunks of text that might follow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.6,
  "end": 103.8
 },
 {
  "translatedText": "در نگاه اول، ممکن است فکر کنید که پیش‌بینی کلمه بعدی هدفی بسیار متفاوت از تولید متن جدید است.",
  "input": "At first glance, you might think that predicting the next word feels like a very different goal from generating new text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.04,
  "end": 109.94
 },
 {
  "translatedText": "اما هنگامی که یک مدل پیش‌بینی مانند این دارید، یک چیز ساده که یک قطعه متن طولانی‌تر ایجاد می‌کنید این است که به آن یک قطعه اولیه برای کار بدهید، از توزیعی که به تازگی تولید کرده یک نمونه تصادفی بگیرد، آن نمونه را به متن اضافه کنید. و سپس کل فرآیند را دوباره اجرا کنید تا بر اساس تمام متن جدید، از جمله آنچه که به تازگی اضافه کرده است، پیش بینی جدیدی انجام دهید.",
  "input": "But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.18,
  "end": 129.54
 },
 {
  "translatedText": "من در مورد شما نمی دانم، اما واقعاً به نظر نمی رسد که این واقعاً کار کند.",
  "input": "I don't know about you, but it really doesn't feel like this should actually work.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.1,
  "end": 133.0
 },
 {
  "translatedText": "به عنوان مثال، در این انیمیشن، من GPT-2 را روی لپ‌تاپ خود اجرا می‌کنم و از آن می‌خواهم بارها و بارها قسمت بعدی متن را پیش‌بینی و نمونه‌برداری کند تا داستانی بر اساس متن اولیه تولید کند.",
  "input": "In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 133.42,
  "end": 142.42
 },
 {
  "translatedText": "داستان واقعاً چندان منطقی نیست.",
  "input": "The story just doesn't really make that much sense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.42,
  "end": 146.12
 },
 {
  "translatedText": "اما اگر آن را با فراخوان‌های API به GPT-3 عوض کنم، که همان مدل اصلی است، بسیار بزرگ‌تر، ناگهان تقریباً به طور جادویی داستان معقولی دریافت می‌کنیم، داستانی که حتی به نظر می‌رسد استنباط می‌کند که یک موجود pi در یک سرزمین ریاضیات و محاسبات",
  "input": "But if I swap it out for API calls to GPT-3 instead, which is the same basic model, just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pi creature would live in a land of math and computation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.5,
  "end": 160.88
 },
 {
  "translatedText": "این فرآیند پیش‌بینی و نمونه‌برداری مکرر در اینجا اساساً زمانی اتفاق می‌افتد که با ChatGPT یا هر یک از این مدل‌های زبان بزرگ دیگر تعامل می‌کنید و می‌بینید که آنها در یک زمان یک کلمه را تولید می‌کنند.",
  "input": "This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.58,
  "end": 171.88
 },
 {
  "translatedText": "در واقع، یکی از ویژگی هایی که من بسیار از آن لذت می برم این است که بتوانم توزیع زیربنایی را برای هر کلمه جدیدی که انتخاب می کند، ببینم.",
  "input": "In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.48,
  "end": 179.22
 },
 {
  "translatedText": "بیایید کارها را با یک پیش نمایش سطح بسیار بالا از نحوه جریان داده ها از طریق یک ترانسفورماتور شروع کنیم.",
  "input": "Let's kick things off with a very high level preview of how data flows through a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.82,
  "end": 188.18
 },
 {
  "translatedText": "ما زمان بسیار بیشتری را صرف انگیزه و تفسیر و بسط جزئیات هر مرحله خواهیم کرد، اما به طور کلی، زمانی که یکی از این ربات‌های چت یک کلمه خاص را تولید می‌کند، در اینجا چیزی است که در زیر سرپوش می‌گذرد.",
  "input": "We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.64,
  "end": 198.66
 },
 {
  "translatedText": "ابتدا، ورودی به دسته‌ای از قطعات کوچک تقسیم می‌شود.",
  "input": "First, the input is broken up into a bunch of little pieces.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.08,
  "end": 202.04
 },
 {
  "translatedText": "این قطعات نشانه نامیده می شوند، و در مورد متن، اینها به کلمات یا قطعات کوچک کلمات یا سایر ترکیبات رایج کاراکترها تمایل دارند.",
  "input": "These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.62,
  "end": 209.82
 },
 {
  "translatedText": "اگر تصاویر یا صدا درگیر باشند، نشانه‌ها می‌توانند تکه‌های کوچکی از آن تصویر یا تکه‌های کوچکی از آن صدا باشند.",
  "input": "If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.74,
  "end": 217.08
 },
 {
  "translatedText": "سپس هر یک از این نشانه‌ها با یک بردار، به معنای فهرستی از اعداد، مرتبط می‌شود که به نوعی معنای آن قطعه را رمزگذاری می‌کند.",
  "input": "Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.58,
  "end": 225.36
 },
 {
  "translatedText": "اگر فکر می کنید این بردارها مختصاتی را در فضایی با ابعاد بسیار بالا می دهند، کلمات با معانی مشابه روی بردارهایی قرار می گیرند که در آن فضا به یکدیگر نزدیک هستند.",
  "input": "If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 234.68
 },
 {
  "translatedText": "این دنباله از بردارها سپس از عملیاتی عبور می کند که به عنوان بلوک توجه شناخته می شود، و این به بردارها اجازه می دهد تا با یکدیگر صحبت کنند و اطلاعات را به عقب و جلو انتقال دهند تا مقادیر خود را به روز کنند.",
  "input": "This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.28,
  "end": 244.5
 },
 {
  "translatedText": "برای مثال معنای کلمه model در عبارت a machine learning model با معنای آن در عبارت fashion model متفاوت است.",
  "input": "For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.88,
  "end": 251.8
 },
 {
  "translatedText": "بلوک توجه چیزی است که مسئول تشخیص اینکه کدام کلمات در متن با به روز کردن معانی کدام کلمات دیگر مرتبط هستند و دقیقاً چگونه آن معانی باید به روز شوند.",
  "input": "The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.26,
  "end": 261.96
 },
 {
  "translatedText": "و دوباره، هر زمان که من از کلمه معنی استفاده می کنم، این به نوعی به طور کامل در ورودی های آن بردارها رمزگذاری می شود.",
  "input": "And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.5,
  "end": 268.04
 },
 {
  "translatedText": "پس از آن، این بردارها از نوع دیگری از عملیات عبور می کنند، و بسته به منبعی که در حال خواندن آن هستید، به عنوان یک پرسپترون چند لایه یا شاید یک لایه پیشخور نامیده می شود.",
  "input": "After that, these vectors pass through a different kind of operation, and depending on the source that you're reading this will be referred to as a multi-layer perceptron or maybe a feed-forward layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.18,
  "end": 278.2
 },
 {
  "translatedText": "و در اینجا بردارها با یکدیگر صحبت نمی کنند، همه آنها یک عملیات مشابه را به صورت موازی انجام می دهند.",
  "input": "And here the vectors don't talk to each other, they all go through the same operation in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.58,
  "end": 282.66
 },
 {
  "translatedText": "و در حالی که تفسیر این بلوک کمی سخت‌تر است، بعداً در مورد اینکه چگونه این مرحله کمی شبیه پرسیدن یک لیست طولانی از سؤالات در مورد هر بردار و سپس به‌روزرسانی آنها بر اساس پاسخ به آن سؤالات است، صحبت خواهیم کرد.",
  "input": "And while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.06,
  "end": 294.0
 },
 {
  "translatedText": "همه عملیات در هر دو بلوک شبیه انبوهی از ضرب‌های ماتریس هستند و وظیفه اصلی ما این است که بفهمیم چگونه ماتریس‌های زیرین را بخوانیم.",
  "input": "All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.9,
  "end": 305.32
 },
 {
  "translatedText": "من برخی از جزئیات را در مورد برخی از مراحل عادی سازی که در این بین اتفاق می افتد، پنهان می کنم، اما این یک پیش نمایش سطح بالا است.",
  "input": "I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.98,
  "end": 312.98
 },
 {
  "translatedText": "پس از آن، فرآیند اساساً تکرار می‌شود، بین بلوک‌های توجه و بلوک‌های پرسپترون چندلایه به عقب و جلو می‌روید، تا اینکه در نهایت امید این است که تمام معنای اصلی متن به نحوی در آخرین بردار در آن قرار گرفته باشد. تسلسل و توالی.",
  "input": "After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 328.5
 },
 {
  "translatedText": "سپس عملیات مشخصی را بر روی آخرین بردار انجام می‌دهیم که توزیع احتمال را روی همه نشانه‌های ممکن، همه تکه‌های کوچک ممکن متنی که ممکن است بعداً بیایند، ایجاد می‌کند.",
  "input": "We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.92,
  "end": 338.42
 },
 {
  "translatedText": "و همانطور که گفتم، هنگامی که ابزاری دارید که با توجه به یک قطعه متن، چیزهای بعدی را پیش‌بینی می‌کند، می‌توانید کمی از متن اولیه به آن بدهید و آن را به طور مکرر این بازی پیش‌بینی چیزهای بعدی، نمونه‌برداری از توزیع، ضمیمه کردن را انجام دهید. آن را، و سپس بارها و بارها تکرار کنید.",
  "input": "And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.98,
  "end": 353.08
 },
 {
  "translatedText": "ممکن است برخی از شما که می‌دانید، مدت‌ها قبل از اینکه ChatGPT وارد صحنه شود، به یاد بیاورید، دموی اولیه GPT-3 به این شکل بود، شما می‌خواهید داستان‌ها و مقالات را براساس یک قطعه اولیه تکمیل خودکار کند.",
  "input": "Some of you in the know may remember how long before ChatGPT came into the scene, this is what early demos of GPT-3 looked like, you would have it autocomplete stories and essays based on an initial snippet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.64,
  "end": 364.64
 },
 {
  "translatedText": "برای تبدیل ابزاری مانند این به یک ربات چت، ساده ترین نقطه شروع این است که کمی متن داشته باشید که تنظیمات تعامل کاربر با یک دستیار مفید هوش مصنوعی را تعیین می کند، چیزی که شما آن را اعلان سیستم می نامید، و سپس از سوال یا درخواست اولیه کاربر به عنوان اولین بیت گفتگو، و سپس شما باید شروع به پیش بینی کنید که چنین دستیار هوش مصنوعی مفیدی در پاسخ چه خواهد گفت.",
  "input": "To make a tool like this into a chatbot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.58,
  "end": 386.94
 },
 {
  "translatedText": "چیزهای بیشتری برای گفتن در مورد مرحله ای از آموزش وجود دارد که برای انجام این کار به خوبی لازم است، اما در سطح بالا این ایده است.",
  "input": "There is more to say about an step of training that's required to make this work well, but at a high level this is the idea.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.72,
  "end": 393.94
 },
 {
  "translatedText": "در این فصل، من و شما قصد داریم جزئیات آنچه را که در همان ابتدای شبکه، در انتهای شبکه اتفاق می‌افتد، گسترش دهیم، و همچنین می‌خواهم زمان زیادی را صرف بررسی برخی از بخش‌های مهم دانش پس‌زمینه کنم. ، چیزهایی که تا زمانی که ترانسفورماتورها به وجود آمدند، طبیعت دوم هر مهندس یادگیری ماشینی بود.",
  "input": "In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.72,
  "end": 412.6
 },
 {
  "translatedText": "اگر با این دانش پیشینه راحت هستید و کمی بی حوصله هستید، می توانید راحت به فصل بعدی بروید، که بر روی بلوک های توجه تمرکز می کند، که عموما قلب ترانسفورماتور در نظر گرفته می شود.",
  "input": "If you're comfortable with that background knowledge and a little impatient, you could feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.06,
  "end": 422.78
 },
 {
  "translatedText": "پس از آن، می‌خواهم در مورد این بلوک‌های پرسپترون چند لایه، نحوه عملکرد آموزش و تعدادی از جزئیات دیگر که تا آن مرحله نادیده گرفته شده‌اند، بیشتر صحبت کنم.",
  "input": "After that I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.36,
  "end": 431.68
 },
 {
  "translatedText": "برای زمینه وسیع‌تر، این ویدیوها اضافه‌شده‌ای به یک مینی‌سریال درباره یادگیری عمیق هستند، و اگر ویدیوهای قبلی را تماشا نکرده‌اید، اشکالی ندارد، فکر می‌کنم می‌توانید آن را بدون نظم انجام دهید، اما قبل از فرو رفتن در ترانسفورماتورها، فکر می‌کنم ارزش آن را دارد که مطمئن شوید در مورد فرضیه اصلی و ساختار یادگیری عمیق در یک صفحه هستیم.",
  "input": "For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.18,
  "end": 448.52
 },
 {
  "translatedText": "با خطر بیان چیزهای بدیهی، این یکی از رویکردهای یادگیری ماشینی است که هر مدلی را که در آن از داده ها استفاده می کنید برای تعیین نحوه رفتار یک مدل توصیف می کند.",
  "input": "At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.02,
  "end": 458.3
 },
 {
  "translatedText": "منظور من از آن این است که فرض کنید شما تابعی را می خواهید که یک تصویر را می گیرد و برچسبی برای توصیف آن تولید می کند، یا مثال ما از پیش بینی کلمه بعدی با توجه به قسمتی از متن، یا هر کار دیگری که به نظر می رسد به عنصری نیاز دارد. شهود و تشخیص الگو",
  "input": "What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.14,
  "end": 472.78
 },
 {
  "translatedText": "ما این روزها تقریباً این را بدیهی می دانیم، اما ایده یادگیری ماشینی این است که به جای تلاش برای تعریف صریح رویه ای برای نحوه انجام آن کار در کد، کاری که مردم در اولین روزهای هوش مصنوعی انجام می دادند، به جای شما یک ساختار بسیار انعطاف‌پذیر با پارامترهای قابل تنظیم، مانند دسته‌ای از دستگیره‌ها و شماره‌گیرها، راه‌اندازی کنید، و سپس از نمونه‌های بسیاری استفاده می‌کنید که خروجی برای یک ورودی مشخص چگونه باید باشد تا مقادیر آن پارامترها را تغییر داده و تنظیم کنید تا این رفتار را تقلید کند.",
  "input": "We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.2,
  "end": 499.7
 },
 {
  "translatedText": "به عنوان مثال، شاید ساده‌ترین شکل یادگیری ماشین، رگرسیون خطی باشد، که در آن ورودی‌ها و خروجی‌های شما هر یک اعداد منفرد هستند، چیزی شبیه متراژ مربع یک خانه و قیمت آن، و چیزی که شما می‌خواهید این است که از این طریق بهترین تناسب را پیدا کنید. می دانید، داده ها برای پیش بینی قیمت خانه در آینده.",
  "input": "For example, maybe the simplest form of machine learning is linear regression, where your inputs and outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.7,
  "end": 516.8
 },
 {
  "translatedText": "آن خط با دو پارامتر پیوسته توصیف می شود، مثلاً شیب و قطع y، و هدف رگرسیون خطی تعیین آن پارامترها برای مطابقت نزدیک با داده ها است.",
  "input": "That line is described by two continuous parameters, say the slope and the y-intercept, and the goal of linear regression is to determine those parameters to closely match the data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.44,
  "end": 528.16
 },
 {
  "translatedText": "نیازی به گفتن نیست که مدل های یادگیری عمیق بسیار پیچیده تر می شوند.",
  "input": "Needless to say, deep learning models get much more complicated.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.88,
  "end": 532.1
 },
 {
  "translatedText": "برای مثال GPT-3 نه دو، بلکه 175 میلیارد پارامتر دارد.",
  "input": "GPT-3, for example, has not two, but 175 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 532.62,
  "end": 537.66
 },
 {
  "translatedText": "اما نکته اینجاست، اینکه بتوانید مدلی غول پیکر با تعداد زیادی پارامتر بسازید، بدون اینکه به شدت بر داده های آموزشی تطبیق داده شود یا آموزش کاملاً غیرقابل تحمل باشد.",
  "input": "But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data or being completely intractable to train.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.12,
  "end": 549.56
 },
 {
  "translatedText": "یادگیری عمیق دسته‌ای از مدل‌ها را توصیف می‌کند که در چند دهه اخیر ثابت شده‌اند که مقیاس قابل‌توجهی دارند.",
  "input": "Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.26,
  "end": 556.18
 },
 {
  "translatedText": "چیزی که آنها را متحد می کند همان الگوریتم آموزشی است که به آن پس انتشار می گویند، و زمینه ای که من می خواهم شما داشته باشید این است که برای اینکه این الگوریتم آموزشی به خوبی در مقیاس کار کند، این مدل ها باید از یک قالب خاص پیروی کنند.",
  "input": "What unifies them is the same training algorithm, called backpropagation, and the context I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.48,
  "end": 571.28
 },
 {
  "translatedText": "اگر این قالب را می‌دانید، به توضیح بسیاری از انتخاب‌ها برای نحوه پردازش زبان توسط ترانسفورماتور کمک می‌کند، که در غیر این صورت خطر احساس دلخواه را به دنبال دارد.",
  "input": "If you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling arbitrary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.8,
  "end": 580.4
 },
 {
  "translatedText": "اول، هر مدلی که می‌سازید، ورودی باید به صورت آرایه‌ای از اعداد واقعی قالب‌بندی شود.",
  "input": "First, whatever model you're making, the input has to be formatted as an array of real numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 581.44,
  "end": 586.74
 },
 {
  "translatedText": "این می تواند به معنای لیستی از اعداد باشد، می تواند یک آرایه دو بعدی باشد، یا اغلب شما با آرایه های بعدی بالاتر سروکار دارید، که در آن اصطلاح عمومی استفاده شده تانسور است.",
  "input": "This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.74,
  "end": 596.0
 },
 {
  "translatedText": "اغلب تصور می‌کنید که داده‌های ورودی به تدریج به لایه‌های متمایز زیادی تبدیل می‌شوند، جایی که دوباره، هر لایه همیشه به عنوان نوعی آرایه از اعداد واقعی ساختار می‌یابد، تا زمانی که به لایه نهایی برسید که خروجی را در نظر می‌گیرید.",
  "input": "You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.56,
  "end": 608.68
 },
 {
  "translatedText": "به عنوان مثال، لایه نهایی در مدل پردازش متن ما لیستی از اعداد است که توزیع احتمال را برای همه نشانه های ممکن بعدی نشان می دهد.",
  "input": "For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.28,
  "end": 617.06
 },
 {
  "translatedText": "در یادگیری عمیق، تقریباً همیشه به این پارامترهای مدل با عنوان وزن گفته می‌شود، و این به این دلیل است که یکی از ویژگی‌های کلیدی این مدل‌ها این است که تنها راه تعامل این پارامترها با داده‌های در حال پردازش، از طریق جمع‌های وزنی است.",
  "input": "In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.82,
  "end": 629.9
 },
 {
  "translatedText": "شما همچنین برخی از توابع غیر خطی را در سراسر پاشش می‌کنید، اما آنها به پارامترها بستگی ندارند.",
  "input": "You also sprinkle some non-linear functions throughout, but they won't depend on parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 630.34,
  "end": 634.36
 },
 {
  "translatedText": "با این حال، به طور معمول، به جای اینکه مجموع های وزنی را کاملاً برهنه و به صراحت نوشته شده ببینید، در عوض آنها را به عنوان اجزای مختلف در یک محصول بردار ماتریس بسته بندی می کنید.",
  "input": "Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.2,
  "end": 645.62
 },
 {
  "translatedText": "اگر به نحوه عملکرد ضرب بردار ماتریس فکر کنید، هر مولفه در خروجی مانند یک جمع وزنی به نظر می رسد.",
  "input": "It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.74,
  "end": 654.24
 },
 {
  "translatedText": "اغلب از نظر مفهومی برای من و شما فکر کردن به ماتریس هایی که با پارامترهای قابل تنظیم پر شده اند و بردارهایی را که از داده های در حال پردازش گرفته شده اند را تغییر می دهند، تمیزتر است.",
  "input": "It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.78,
  "end": 665.42
 },
 {
  "translatedText": "برای مثال، آن 175 میلیارد وزن در GPT-3 در کمتر از 28000 ماتریس مجزا سازماندهی شده است.",
  "input": "For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.34,
  "end": 674.16
 },
 {
  "translatedText": "آن ماتریس‌ها به نوبه خود در هشت دسته مختلف قرار می‌گیرند، و کاری که من و شما می‌خواهیم انجام دهیم این است که از هر یک از آن دسته‌ها عبور کنیم تا بفهمیم آن نوع چه کاری انجام می‌دهد.",
  "input": "Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.66,
  "end": 682.7
 },
 {
  "translatedText": "همانطور که در حال پیشروی هستیم، فکر می‌کنم ارجاع به اعداد خاص از GPT-3 برای شمارش دقیقاً از کجا این 175 میلیارد می‌آیند، جالب است.",
  "input": "As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.16,
  "end": 691.36
 },
 {
  "translatedText": "حتی اگر امروزه مدل های بزرگتر و بهتری وجود داشته باشد، این مدل به عنوان مدل زبان بزرگ جذابیت خاصی دارد تا توجه جهان را خارج از جوامع ML جلب کند.",
  "input": "Even if nowadays there are bigger and better models, this one has a certain charm as the large-language model to really capture the world's attention outside of ML communities.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.88,
  "end": 700.74
 },
 {
  "translatedText": "همچنین، از نظر عملی، شرکت‌ها تمایل دارند برای شبکه‌های مدرن‌تر، اعداد خاص را محکم‌تر نگه دارند.",
  "input": "Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 701.44,
  "end": 706.74
 },
 {
  "translatedText": "من فقط می‌خواهم صحنه‌ای را تنظیم کنم که وقتی به زیر کاپوت نگاه می‌کنید تا ببینید در ابزاری مانند ChatGPT چه اتفاقی می‌افتد، تقریباً تمام محاسبات واقعی شبیه ضرب برداری ماتریس به نظر می‌رسد.",
  "input": "I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like ChatGPT, almost all of the actual computation looks like matrix vector multiplication.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.36,
  "end": 717.44
 },
 {
  "translatedText": "کمی خطر گم شدن در دریای میلیاردها عدد وجود دارد، اما باید در ذهن خود تمایز بسیار واضحی بین وزن مدل، که من همیشه آبی یا قرمز رنگ می‌کنم، و داده‌های موجود در نظر بگیرید. پردازش شده، که من همیشه خاکستری رنگ می کنم.",
  "input": "There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.9,
  "end": 731.84
 },
 {
  "translatedText": "وزنه ها مغز واقعی هستند، چیزهایی هستند که در طول تمرین یاد می گیرند و نحوه رفتار آن را تعیین می کنند.",
  "input": "The weights are the actual brains, they are the things learned during training, and they determine how it behaves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.18,
  "end": 737.92
 },
 {
  "translatedText": "داده‌های در حال پردازش به سادگی هر ورودی خاصی را که برای یک اجرای مشخص به مدل وارد می‌شود، رمزگذاری می‌کند، مانند نمونه‌ای از متن.",
  "input": "The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.28,
  "end": 746.5
 },
 {
  "translatedText": "با همه اینها به عنوان پایه، بیایید به اولین مرحله از این مثال پردازش متن بپردازیم، که این است که ورودی را به تکه های کوچک تقسیم می کنیم و آن تکه ها را به بردار تبدیل می کنیم.",
  "input": "With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.48,
  "end": 756.42
 },
 {
  "translatedText": "من اشاره کردم که چگونه به آن تکه‌ها نشانه می‌گویند، که ممکن است تکه‌های کلمه یا نقطه‌گذاری باشند، اما هرازگاهی در این فصل و به خصوص در فصل بعدی، می‌خواهم وانمود کنم که به طور واضح‌تری به کلمات تبدیل شده است.",
  "input": "I mentioned how those chunks are called tokens, which might be pieces of words or punctuation, but every now and then in this chapter and especially in the next one, I'd like to just pretend that it's broken more cleanly into words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.02,
  "end": 768.08
 },
 {
  "translatedText": "از آنجایی که ما انسان‌ها با کلمات فکر می‌کنیم، این کار ارجاع به مثال‌های کوچک و شفاف‌سازی هر مرحله را بسیار آسان‌تر می‌کند.",
  "input": "Because we humans think in words, this will just make it much easier to reference little examples and clarify each step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.6,
  "end": 774.08
 },
 {
  "translatedText": "این مدل دارای یک واژگان از پیش تعریف شده است، فهرستی از تمام کلمات ممکن، مثلاً 50000 مورد از آنها، و اولین ماتریسی که با آن مواجه خواهیم شد، معروف به ماتریس جاسازی، دارای یک ستون برای هر یک از این کلمات است.",
  "input": "The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.26,
  "end": 787.8
 },
 {
  "translatedText": "این ستون‌ها هستند که تعیین می‌کنند هر کلمه در مرحله اول به کدام بردار تبدیل می‌شود.",
  "input": "These columns are what determines what vector each word turns into in that first step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.94,
  "end": 793.76
 },
 {
  "translatedText": "ما آن را برچسب گذاری می کنیم، و مانند همه ماتریس هایی که می بینیم، مقادیر آن به صورت تصادفی شروع می شوند، اما بر اساس داده ها یاد می گیرند.",
  "input": "We label it We, and like all the matrices we see, its values begin random, but they're going to be learned based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.1,
  "end": 802.36
 },
 {
  "translatedText": "تبدیل کلمات به بردار مدت‌ها قبل از ترانسفورماتورها تمرین رایج در یادگیری ماشینی بود، اما اگر قبلاً آن را ندیده‌اید کمی عجیب است و پایه‌ای را برای هر چیزی که در ادامه می‌آید ایجاد می‌کند، بنابراین اجازه دهید لحظه‌ای با آن آشنا شویم.",
  "input": "Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sets the foundation for everything that follows, so let's take a moment to get familiar with it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.62,
  "end": 815.76
 },
 {
  "translatedText": "ما اغلب این تعبیه را یک کلمه می نامیم که شما را دعوت می کند تا به این بردارها به صورت بسیار هندسی به عنوان نقاطی در فضایی با ابعاد بالا فکر کنید.",
  "input": "We often call this embedding a word, which invites you to think of these vectors very geometrically as points in some high dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.04,
  "end": 823.62
 },
 {
  "translatedText": "تجسم یک لیست از سه عدد به عنوان مختصات برای نقاط در فضای سه بعدی مشکلی نخواهد بود، اما جاسازی کلمات معمولا ابعاد بسیار بالاتری دارند.",
  "input": "Visualizing a list of three numbers as coordinates for points in 3D space would be no problem, but word embeddings tend to be much much higher dimensional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.18,
  "end": 831.78
 },
 {
  "translatedText": "در GPT-3 آنها 12288 بعد دارند و همانطور که خواهید دید، کار در فضایی که جهت های متمایز زیادی دارد مهم است.",
  "input": "In GPT-3 they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.28,
  "end": 840.44
 },
 {
  "translatedText": "همانطور که شما می توانید یک برش دو بعدی را از یک فضای سه بعدی بردارید و تمام نقاط را روی آن برش قرار دهید، به خاطر متحرک سازی کلماتی که یک مدل ساده به من می دهد، من یک کار مشابه انجام می دهم. با انتخاب یک برش سه بعدی از میان این فضای ابعادی بسیار بالا و نمایش بردارهای کلمه بر روی آن و نمایش نتایج.",
  "input": "In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.18,
  "end": 860.48
 },
 {
  "translatedText": "ایده بزرگ در اینجا این است که وقتی یک مدل وزن‌های خود را تغییر می‌دهد و تنظیم می‌کند تا تعیین کند کلمات دقیقاً چگونه به‌عنوان بردار در طول آموزش جاسازی می‌شوند، تمایل دارد روی مجموعه‌ای از جاسازی‌ها قرار گیرد که جهت‌ها در فضا نوعی معنای معنایی دارند.",
  "input": "The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.28,
  "end": 874.44
 },
 {
  "translatedText": "برای مدل ساده کلمه به برداری که من در اینجا اجرا می کنم، اگر همه کلماتی را که جاسازی آنها به برج نزدیکتر است را جستجو کنم، متوجه خواهید شد که چگونه به نظر می رسد که همه آنها حال و هوای بسیار شبیه به برج می دهند.",
  "input": "For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.98,
  "end": 885.9
 },
 {
  "translatedText": "و اگر می خواهید پایتون را بالا بکشید و در خانه بازی کنید، این مدل خاصی است که من برای ساخت انیمیشن ها استفاده می کنم.",
  "input": "And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 886.34,
  "end": 891.38
 },
 {
  "translatedText": "این یک ترانسفورماتور نیست، اما برای نشان دادن این ایده کافی است که جهت ها در فضا می توانند معنای معنایی را حمل کنند.",
  "input": "It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 891.62,
  "end": 897.6
 },
 {
  "translatedText": "یک مثال بسیار کلاسیک در این مورد این است که اگر تفاوت بین بردارهای زن و مرد را در نظر بگیرید، چیزی که به عنوان یک بردار کوچک که نوک یکی را به نوک دیگری متصل می کند، تجسم می کنید، بسیار شبیه به تفاوت بین پادشاه و مرد است. ملکه.",
  "input": "A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.3,
  "end": 913.2
 },
 {
  "translatedText": "بنابراین فرض کنید کلمه پادشاه زن را نمی‌دانستید، می‌توانید با گرفتن پادشاه، اضافه کردن این جهت زن-مرد، و جستجوی جاسازی‌های نزدیک به آن نقطه، آن را پیدا کنید.",
  "input": "So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 915.08,
  "end": 925.46
 },
 {
  "translatedText": "حداقل یه جورایی",
  "input": "At least, kind of.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.0,
  "end": 928.2
 },
 {
  "translatedText": "علیرغم اینکه این یک نمونه کلاسیک برای مدلی است که من با آن بازی می کنم، تعبیه واقعی ملکه در واقع کمی دورتر از آن چیزی است که نشان می دهد، احتمالاً به این دلیل که نحوه استفاده از ملکه در آموزش داده ها صرفاً یک نسخه زنانه از پادشاه نیست.",
  "input": "Despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way queen is used in training data is not merely a feminine version of king.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 928.48,
  "end": 940.78
 },
 {
  "translatedText": "وقتی در اطراف بازی می کردم، به نظر می رسید که روابط خانوادگی این ایده را خیلی بهتر نشان می دهد.",
  "input": "When I played around, family relations seemed to illustrate the idea much better.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.62,
  "end": 945.26
 },
 {
  "translatedText": "نکته این است که به نظر می‌رسد در طول آموزش، مدل انتخاب جاسازی‌هایی را سودمند می‌داند که یک جهت در این فضا اطلاعات جنسیت را رمزگذاری کند.",
  "input": "The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.34,
  "end": 954.9
 },
 {
  "translatedText": "مثال دیگر این است که اگر تعبیه ایتالیا را بگیرید و جاسازی آلمان را کم کنید و آن را به تعبیه هیتلر اضافه کنید، چیزی بسیار نزدیک به تعبیه موسولینی خواهید داشت.",
  "input": "Another example is that if you take the embedding of Italy, and you subtract the embedding of Germany, and add that to the embedding of Hitler, you get something very close to the embedding of Mussolini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.8,
  "end": 968.09
 },
 {
  "translatedText": "گویی مدل یاد گرفته است که برخی از جهت ها را با ایتالیایی بودن و برخی دیگر را با رهبران محور جنگ جهانی دوم مرتبط کند.",
  "input": "It's as if the model learned to associate some directions with Italian-ness, and others with WWII axis leaders.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 968.57,
  "end": 975.67
 },
 {
  "translatedText": "شاید مثال مورد علاقه من در این زمینه این باشد که چگونه در برخی مدل ها، اگر تفاوت آلمان و ژاپن را در نظر بگیرید و آن را به سوشی اضافه کنید، در نهایت به براتورست نزدیک می شوید.",
  "input": "Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan, and add it to sushi, you end up very close to bratwurst.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.47,
  "end": 986.23
 },
 {
  "translatedText": "همچنین در انجام این بازی یافتن نزدیکترین همسایگان، از اینکه دیدم کت چقدر به هیولا و هیولا نزدیک است خوشحال شدم.",
  "input": "Also in playing this game of finding nearest neighbors, I was pleased to see how close Kat was to both beast and monster.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.35,
  "end": 993.85
 },
 {
  "translatedText": "یک بیت از شهود ریاضی که در نظر گرفتن آن، به ویژه برای فصل بعدی مفید است، این است که چگونه حاصل ضرب نقطه ای دو بردار را می توان راهی برای اندازه گیری میزان همسویی آنها در نظر گرفت.",
  "input": "One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.69,
  "end": 1003.85
 },
 {
  "translatedText": "از نظر محاسباتی، محصولات نقطه‌ای شامل ضرب تمام اجزای مربوطه و سپس اضافه کردن نتایج است که خوب است، زیرا بسیاری از محاسبات ما باید مانند مجموع وزنی به نظر برسند.",
  "input": "Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good, since so much of our computation has to look like weighted sums.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1004.87,
  "end": 1014.33
 },
 {
  "translatedText": "از نظر هندسی، حاصلضرب نقطه زمانی مثبت است که بردارها در جهات مشابه باشند، اگر بردارها عمود باشند صفر است و هرگاه در جهت مخالف قرار گیرند منفی است.",
  "input": "Geometrically, the dot product is positive when vectors point in similar directions, it's zero if they're perpendicular, and it's negative whenever they point in opposite directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1015.19,
  "end": 1025.61
 },
 {
  "translatedText": "برای مثال، فرض کنید با این مدل بازی می‌کردید، و فرض می‌کنید که تعبیه گربه‌ها منهای گربه ممکن است نوعی جهت کثرت را در این فضا نشان دهد.",
  "input": "For example, let's say you were playing with this model, and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.55,
  "end": 1037.01
 },
 {
  "translatedText": "برای آزمایش این، من می‌خواهم این بردار را بگیرم و حاصل ضرب نقطه‌ای آن را در برابر تعبیه‌های اسم‌های مفرد خاص محاسبه کنم و آن را با محصولات نقطه‌ای با اسم‌های جمع مربوطه مقایسه کنم.",
  "input": "To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns, and compare it to the dot products with the corresponding plural nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1037.43,
  "end": 1047.05
 },
 {
  "translatedText": "اگر با آن بازی کنید، متوجه خواهید شد که در واقع به نظر می رسد که موارد جمع به طور مداوم مقادیر بالاتری نسبت به موارد مفرد می دهند، که نشان می دهد آنها بیشتر با این جهت هماهنگ هستند.",
  "input": "If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.27,
  "end": 1056.07
 },
 {
  "translatedText": "همچنین جالب است که اگر این محصول نقطه‌ای را با جاسازی کلمات 1، 2، 3 و غیره بگیرید، مقادیر فزاینده‌ای می‌دهند، بنابراین گویی می‌توانیم به صورت کمی اندازه گیری کنیم که مدل یک کلمه داده شده را چقدر جمع می‌یابد.",
  "input": "It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3, and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.07,
  "end": 1069.03
 },
 {
  "translatedText": "باز هم، جزئیات نحوه جاسازی کلمات با استفاده از داده ها آموخته می شود.",
  "input": "Again, the specifics for how words get embedded is learned using data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.25,
  "end": 1073.57
 },
 {
  "translatedText": "این ماتریس جاسازی، که ستون‌های آن به ما می‌گویند برای هر کلمه چه اتفاقی می‌افتد، اولین توده وزن در مدل ما است.",
  "input": "This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.05,
  "end": 1079.55
 },
 {
  "translatedText": "با استفاده از اعداد GPT-3، اندازه واژگان به طور خاص 50257 است، و باز هم، از نظر فنی این شامل کلمات فی نفسه نیست، بلکه از نشانه‌ها است.",
  "input": "Using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se, but of tokens.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1080.03,
  "end": 1089.77
 },
 {
  "translatedText": "بعد تعبیه شده 12288 است و ضرب آن ها به ما می گوید که این شامل حدود 617 میلیون وزن است.",
  "input": "The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1090.63,
  "end": 1097.79
 },
 {
  "translatedText": "بیایید جلوتر برویم و این را به آمار جاری اضافه کنیم، به یاد داشته باشیم که تا پایان باید تا 175 میلیارد بشماریم.",
  "input": "Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1098.25,
  "end": 1103.81
 },
 {
  "translatedText": "در مورد ترانسفورماتورها، شما واقعاً می خواهید بردارهای موجود در این فضای تعبیه شده را به گونه ای تصور کنید که صرفاً بیانگر کلمات جداگانه نیستند.",
  "input": "In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.43,
  "end": 1112.13
 },
 {
  "translatedText": "برای یک چیز، آنها همچنین اطلاعات مربوط به موقعیت آن کلمه را رمزگذاری می کنند، که بعداً در مورد آن صحبت خواهیم کرد، اما مهمتر از آن، شما باید آنها را به عنوان ظرفیت غوطه ور شدن در متن در نظر بگیرید.",
  "input": "For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.55,
  "end": 1122.77
 },
 {
  "translatedText": "برای مثال، برداری که زندگی خود را با تعبیه کلمه پادشاه آغاز کرده است، ممکن است به تدریج توسط بلوک های مختلف در این شبکه کشیده و کشیده شود، به طوری که در پایان در جهت بسیار خاص و ظریف تری قرار گیرد که به نوعی آن را رمزگذاری می کند. پادشاهی بود که در اسکاتلند زندگی می کرد و پس از قتل پادشاه قبلی به مقام خود رسیده بود و به زبان شکسپیر توصیف می شود.",
  "input": "A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.35,
  "end": 1144.73
 },
 {
  "translatedText": "به درک خود از یک کلمه خاص فکر کنید.",
  "input": "Think about your own understanding of a given word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1145.21,
  "end": 1147.79
 },
 {
  "translatedText": "معنای آن کلمه به وضوح توسط محیط اطراف مشخص می شود، و گاهی اوقات این شامل زمینه از فاصله دور می شود، بنابراین در کنار هم قرار دادن مدلی که توانایی پیش بینی کلمه بعدی را داشته باشد، هدف این است که به نحوی آن را توانمند سازیم تا زمینه را در خود جای دهد. به طور موثر",
  "input": "The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1148.25,
  "end": 1163.39
 },
 {
  "translatedText": "برای روشن بودن، در اولین قدم، وقتی آرایه بردارها را بر اساس متن ورودی ایجاد می‌کنید، هر یک از آن‌ها به سادگی از ماتریس جاسازی خارج می‌شوند، بنابراین در ابتدا هر یک فقط می‌تواند معنای یک کلمه را بدون رمزگذاری کند. هر ورودی از محیط اطرافش",
  "input": "To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.05,
  "end": 1176.77
 },
 {
  "translatedText": "اما شما باید هدف اصلی این شبکه را که از طریق آن جریان می‌یابد این است که هر یک از آن بردارها را قادر می‌سازد تا معنایی را جذب کنند که بسیار غنی‌تر و خاص‌تر از آن چیزی است که کلمات منفرد می‌توانند نشان دهند.",
  "input": "But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.71,
  "end": 1188.97
 },
 {
  "translatedText": "شبکه فقط می تواند تعداد ثابتی از بردارها را در یک زمان پردازش کند که به عنوان اندازه زمینه آن شناخته می شود.",
  "input": "The network can only process a fixed number of vectors at a time, known as its context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.51,
  "end": 1194.17
 },
 {
  "translatedText": "برای GPT-3 با اندازه زمینه 2048 آموزش داده شد، بنابراین داده های جریان یافته از طریق شبکه همیشه شبیه این آرایه از 2048 ستون است که هر کدام دارای 12000 بعد است.",
  "input": "For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1194.51,
  "end": 1205.01
 },
 {
  "translatedText": "این اندازه زمینه، مقدار متنی را که ترانسفورماتور می‌تواند در هنگام پیش‌بینی کلمه بعدی وارد کند، محدود می‌کند.",
  "input": "This context size limits how much text the transformer can incorporate when it's making a prediction of the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.59,
  "end": 1211.83
 },
 {
  "translatedText": "به همین دلیل است که مکالمات طولانی با ربات‌های چت خاص، مانند نسخه‌های اولیه ChatGPT، اغلب این احساس را به وجود می‌آورد که با ادامه طولانی مدت، رشته مکالمه را از دست می‌دهد.",
  "input": "This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1212.37,
  "end": 1222.05
 },
 {
  "translatedText": "ما در زمان مناسب به جزئیات توجه خواهیم پرداخت، اما از جلوتر می گذرم، می خواهم یک دقیقه در مورد آنچه در پایان اتفاق می افتد صحبت کنم.",
  "input": "We'll go into the details of attention in due time, but skipping ahead I want to talk for a minute about what happens at the very end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1223.03,
  "end": 1228.81
 },
 {
  "translatedText": "به یاد داشته باشید، خروجی مورد نظر یک توزیع احتمال بر روی تمام نشانه هایی است که ممکن است در مرحله بعدی قرار گیرند.",
  "input": "Remember, the desired output is a probability distribution over all tokens that might come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1229.45,
  "end": 1234.87
 },
 {
  "translatedText": "به عنوان مثال، اگر آخرین کلمه پروفسور باشد، و متن شامل کلماتی مانند هری پاتر باشد، و بلافاصله قبل از آن حداقل معلم مورد علاقه را می بینیم، و همچنین اگر به من اجازه بدهید وانمود کنم که نشانه ها به سادگی مانند کلمات کامل هستند، پس یک شبکه کاملاً آموزش دیده که دانش هری پاتر را ایجاد کرده بود، احتمالاً عدد بالایی را به کلمه اسنیپ اختصاص می‌داد.",
  "input": "For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.17,
  "end": 1255.83
 },
 {
  "translatedText": "این شامل دو مرحله متفاوت است.",
  "input": "This involves two different steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.51,
  "end": 1257.97
 },
 {
  "translatedText": "اولین مورد استفاده از ماتریس دیگری است که آخرین بردار را در آن زمینه به لیستی از 50000 مقدار، یک عدد برای هر نشانه در واژگان، ترسیم می کند.",
  "input": "The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.31,
  "end": 1267.61
 },
 {
  "translatedText": "سپس تابعی وجود دارد که این را به یک توزیع احتمال نرمال می کند، آن را Softmax می نامند و در عرض یک ثانیه بیشتر در مورد آن صحبت خواهیم کرد، اما قبل از آن ممکن است کمی عجیب به نظر برسد که فقط از آخرین جاسازی برای پیش بینی استفاده کنیم، زمانی که به هر حال در آخرین مرحله هزاران بردار دیگر در لایه وجود دارد که با معانی غنی از زمینه خاص خود در آنجا نشسته اند.",
  "input": "Then there's a function that normalizes this into a probability distribution, it's called Softmax and we'll talk more about it in just a second, but before that it might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.17,
  "end": 1288.29
 },
 {
  "translatedText": "این به این واقعیت مربوط می شود که در فرآیند آموزش اگر از هر یک از آن بردارها در لایه نهایی استفاده کنید تا به طور همزمان برای آنچه بلافاصله پس از آن می آید پیش بینی کنید بسیار کارآمدتر است.",
  "input": "This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1288.93,
  "end": 1300.27
 },
 {
  "translatedText": "در مورد تمرینات بعداً چیزهای بیشتری برای گفتن وجود دارد، اما من فقط می‌خواهم همین الان به آن اشاره کنم.",
  "input": "There's a lot more to be said about training later on, but I just want to call that out right now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1300.97,
  "end": 1305.09
 },
 {
  "translatedText": "این ماتریس ماتریس Unembedding نام دارد و ما به آن برچسب WU می دهیم.",
  "input": "This matrix is called the Unembedding matrix and we give it the label WU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.73,
  "end": 1309.69
 },
 {
  "translatedText": "باز هم، مانند همه ماتریس‌های وزنی که می‌بینیم، ورودی‌های آن به‌طور تصادفی شروع می‌شوند، اما در طول فرآیند آموزش یاد می‌گیرند.",
  "input": "Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1310.21,
  "end": 1315.91
 },
 {
  "translatedText": "با حفظ امتیاز در تعداد کل پارامترهای ما، این ماتریس Unembedding دارای یک ردیف برای هر کلمه در واژگان است، و هر ردیف دارای همان تعداد عناصر به عنوان بعد جاسازی است.",
  "input": "Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1316.47,
  "end": 1325.65
 },
 {
  "translatedText": "این بسیار شبیه به ماتریس جاسازی است، فقط با ترتیب تعویض، بنابراین 617 میلیون پارامتر دیگر به شبکه اضافه می کند، به این معنی که تعداد ما تا کنون کمی بیش از یک میلیارد است، کسری کوچک اما نه کاملاً ناچیز از 175 میلیارد ما. در نهایت با در کل.",
  "input": "It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1326.41,
  "end": 1341.79
 },
 {
  "translatedText": "به عنوان آخرین درس کوچک برای این فصل، می‌خواهم در مورد این تابع softmax بیشتر صحبت کنم، زیرا زمانی که در بلوک‌های توجه فرو رفتیم، ظاهر دیگری برای ما ایجاد می‌کند.",
  "input": "As the last mini-lesson for this chapter, I want to talk more about this softmax function, since it makes another appearance for us once we dive into the attention blocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.55,
  "end": 1350.61
 },
 {
  "translatedText": "ایده این است که اگر می‌خواهید دنباله‌ای از اعداد به‌عنوان توزیع احتمال عمل کند، توزیعی را بر روی همه کلمات ممکن بعدی بگویید، سپس هر مقدار باید بین 0 و 1 باشد و همچنین باید همه آنها را به 1 جمع کنید. .",
  "input": "The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1351.43,
  "end": 1364.59
 },
 {
  "translatedText": "با این حال، اگر در حال بازی یادگیری هستید که در آن هر کاری که انجام می‌دهید شبیه ضرب ماتریس-بردار به نظر می‌رسد، خروجی‌هایی که به‌طور پیش‌فرض دریافت می‌کنید اصلاً از این موضوع پیروی نمی‌کنند.",
  "input": "However, if you're playing the learning game where everything you do looks like matrix-vector multiplication, the outputs you get by default don't abide by this at all.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.25,
  "end": 1374.81
 },
 {
  "translatedText": "مقادیر اغلب منفی یا بسیار بزرگتر از 1 هستند و تقریباً به طور قطع با 1 جمع نمی شوند.",
  "input": "The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.33,
  "end": 1379.87
 },
 {
  "translatedText": "Softmax روشی استاندارد برای تبدیل لیست دلخواه اعداد به یک توزیع معتبر است، به گونه ای که بزرگترین مقادیر به 1 نزدیکتر و مقادیر کوچکتر به 0 نزدیک شوند.",
  "input": "Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1380.51,
  "end": 1391.29
 },
 {
  "translatedText": "این تمام چیزی است که واقعاً باید بدانید.",
  "input": "That's all you really need to know.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.83,
  "end": 1393.07
 },
 {
  "translatedText": "اما اگر کنجکاو هستید، روش کار به این صورت است که ابتدا e را به توان هر یک از اعداد افزایش دهید، به این معنی که اکنون فهرستی از مقادیر مثبت دارید، و سپس می توانید مجموع همه آن مقادیر مثبت را بگیرید و تقسیم کنید. هر عبارت با آن مجموع، که آن را به لیستی عادی می کند که به 1 می رسد.",
  "input": "But if you're curious, the way it works is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values and divide each term by that sum, which normalizes it into a list that adds up to 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1393.09,
  "end": 1409.47
 },
 {
  "translatedText": "متوجه خواهید شد که اگر یکی از اعداد ورودی به طور معنی‌داری بزرگ‌تر از بقیه باشد، در خروجی عبارت مربوطه بر توزیع غالب است، بنابراین اگر از آن نمونه‌برداری می‌کردید تقریباً مطمئناً فقط ورودی حداکثر را انتخاب می‌کردید.",
  "input": "You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output the corresponding term dominates the distribution, so if you were sampling from it you'd almost certainly just be picking the maximizing input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1410.17,
  "end": 1422.47
 },
 {
  "translatedText": "اما این نرم‌تر از انتخاب حداکثر است، به این معنا که وقتی مقادیر دیگر به طور مشابه بزرگ هستند، وزن معنی‌داری در توزیع نیز دریافت می‌کنند، و همه چیز به‌طور پیوسته تغییر می‌کند، زیرا شما مدام ورودی‌ها را تغییر می‌دهید.",
  "input": "But it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.99,
  "end": 1434.65
 },
 {
  "translatedText": "در برخی موقعیت‌ها، مانند زمانی که ChatGPT از این توزیع برای ایجاد کلمه بعدی استفاده می‌کند، با افزودن کمی ادویه اضافی به این تابع، با یک t ثابت در مخرج آن نماگرها، فضای کمی برای سرگرمی بیشتر وجود دارد.",
  "input": "In some situations, like when ChatGPT is using this distribution to create a next word, there's room for a little bit of extra fun by adding a little extra spice into this function, with a constant t thrown into the denominator of those exponents.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.13,
  "end": 1448.91
 },
 {
  "translatedText": "ما آن را دما می نامیم، زیرا به طور مبهم به نقش دما در معادلات ترمودینامیک خاص شباهت دارد، و اثر این است که وقتی t بزرگتر است، وزن بیشتری به مقادیر پایین تر می دهیم، به این معنی که توزیع کمی یکنواخت تر است، و اگر t کوچکتر است، سپس مقادیر بزرگتر با شدت بیشتری تسلط خواهند داشت، جایی که در نهایت، تنظیم t برابر با صفر به این معنی است که تمام وزن به مقدار حداکثر می رود.",
  "input": "We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when t is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if t is smaller, then the bigger values will dominate more aggressively, where in the extreme, setting t equal to zero means all of the weight goes to maximum value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.55,
  "end": 1472.79
 },
 {
  "translatedText": "به عنوان مثال، من باید GPT-3 یک داستان با متن seed تولید کند، یک زمانی A وجود داشت، اما در هر مورد از دماهای مختلف استفاده خواهم کرد.",
  "input": "For example, I'll have GPT-3 generate a story with the seed text, once upon a time there was A, but I'll use different temperatures in each case.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1473.47,
  "end": 1482.95
 },
 {
  "translatedText": "دمای صفر به این معنی است که همیشه با قابل پیش بینی ترین کلمه مطابقت دارد و چیزی که به دست می آورید در نهایت یک مشتق معمولی از Goldilocks است.",
  "input": "Temperature zero means that it always goes with the most predictable word, and what you get ends up being a trite derivative of Goldilocks.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1483.63,
  "end": 1492.37
 },
 {
  "translatedText": "دمای بالاتر به آن فرصتی می دهد تا کلمات کمتر محتمل را انتخاب کند، اما با خطر همراه است.",
  "input": "A higher temperature gives it a chance to choose less likely words, but it comes with a risk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1493.01,
  "end": 1497.91
 },
 {
  "translatedText": "در این مورد، داستان بیشتر از ابتدا در مورد یک هنرمند وب جوان از کره جنوبی شروع می شود، اما به سرعت به یک مزخرف تبدیل می شود.",
  "input": "In this case, the story starts out more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1498.23,
  "end": 1506.01
 },
 {
  "translatedText": "از نظر فنی، API در واقع به شما اجازه نمی دهد دمایی بالاتر از 2 را انتخاب کنید.",
  "input": "Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1506.95,
  "end": 1510.83
 },
 {
  "translatedText": "هیچ دلیل ریاضی برای این وجود ندارد، این فقط یک محدودیت دلخواه است که برای اینکه ابزار آنها دیده نشود که چیزهای بیش از حد مزخرف تولید می کند اعمال می شود.",
  "input": "There's no mathematical reason for this, it's just an arbitrary constraint imposed to keep their tool from being seen generating things that are too nonsensical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1511.17,
  "end": 1519.35
 },
 {
  "translatedText": "بنابراین اگر کنجکاو هستید، نحوه عملکرد این انیمیشن به این صورت است که من از 20 توکن بعدی که GPT-3 تولید می‌کند استفاده می‌کنم، که به نظر می‌رسد حداکثری است که به من می‌دهد، و سپس بر اساس احتمالات را تغییر می‌دهم. روی توان 1 5.",
  "input": "So if you're curious, the way this animation is actually working is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1 5th.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.87,
  "end": 1532.97
 },
 {
  "translatedText": "به عنوان یک اصطلاح دیگر، به همان روشی که ممکن است اجزای خروجی این تابع را احتمالات نامید، مردم اغلب ورودی ها را به عنوان logit می گویند، یا برخی افراد می گویند logit، برخی افراد می گویند logits، من می خواهم بگویم logits. .",
  "input": "As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm gonna say logits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1533.13,
  "end": 1546.15
 },
 {
  "translatedText": "بنابراین، برای مثال، هنگامی که در متنی تغذیه می‌کنید، همه این جاسازی‌های کلمه در شبکه جریان دارند، و این ضرب نهایی را با ماتریس بدون تعبیه انجام می‌دهید، افراد یادگیری ماشین اجزای موجود در آن خروجی خام و غیرعادی را به‌عنوان logits معرفی می‌کنند. برای پیش بینی کلمه بعدی",
  "input": "So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output as the logits for the next word prediction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.53,
  "end": 1561.39
 },
 {
  "translatedText": "هدف بسیاری از این فصل، پایه‌ریزی برای درک مکانیسم توجه، سبک کاراته کید واکس روی موم بود.",
  "input": "A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid wax-on-wax-off style.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1563.33,
  "end": 1570.37
 },
 {
  "translatedText": "می بینید، اگر شهود قوی برای جاسازی کلمات، برای سافت مکس، برای اینکه چگونه محصولات نقطه ای شباهت را اندازه گیری می کنند، و همچنین این فرض اساسی که بیشتر محاسبات باید شبیه ضرب ماتریس با ماتریس های پر از پارامترهای قابل تنظیم باشند، داشته باشید، پس توجه را درک کنید. مکانیزم، این قطعه سنگ بنای کل رونق مدرن در هوش مصنوعی، باید نسبتا صاف باشد.",
  "input": "You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1570.85,
  "end": 1592.21
 },
 {
  "translatedText": "برای آن، در فصل بعدی به من بپیوندید.",
  "input": "For that, come join me in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.65,
  "end": 1594.51
 },
 {
  "translatedText": "همانطور که من این را منتشر می کنم، پیش نویس آن فصل بعدی برای بررسی توسط حامیان Patreon در دسترس است.",
  "input": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1596.39,
  "end": 1601.21
 },
 {
  "translatedText": "نسخه نهایی باید در عرض یک یا دو هفته در معرض دید عموم قرار گیرد، معمولا بستگی به میزان تغییر من بر اساس آن بررسی دارد.",
  "input": "A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1601.77,
  "end": 1607.37
 },
 {
  "translatedText": "در این میان، اگر می‌خواهید توجه را جلب کنید، و اگر می‌خواهید کمی به کانال کمک کنید، منتظر است.",
  "input": "In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1607.81,
  "end": 1612.41
 }
]